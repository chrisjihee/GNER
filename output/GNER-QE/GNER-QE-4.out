(GNER) chrisjihee@dgx-a100:~/proj/GNER$ ./run-GNER-QE-4.sh
+ DEEPSPEED_CONFIG=configs/deepspeed/ds1_t5.json
++ shuf -i 25000-30000 -n 1
+ DEEPSPEED_PORT=29894
+ CUDA_DEVICES=6,7
+ SOURCE_FILE=run_glue.py
+ TRAIN_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json
+ VALID_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json
+ OUTPUT_NAME=GNER-QE
+ MODEL_NAMES=("google-bert/bert-base-cased" "FacebookAI/roberta-base" "FacebookAI/roberta-large")
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:6,7 --master_port 29894 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:43:26,522] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:32,102] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 13:43:32,103] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29894 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:43:36,506] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:39,575] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2025-03-10 13:43:39,575] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 13:43:39,575] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 13:43:39,575] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 13:43:39,575] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2025-03-10 13:43:39,576] [INFO] [launch.py:256:main] process 4024459 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:39,576] [INFO] [launch.py:256:main] process 4024460 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:44,438] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:44,587] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:45,958] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:43:45,958] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 13:43:45,961] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 13:43:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:43:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/google-bert/bert-base-cased-num=40/runs/Mar10_13-43-44_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/google-bert/bert-base-cased-num=40,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/google-bert/bert-base-cased-num=40,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 13:43:47 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json
03/10/2025 13:43:47 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json
03/10/2025 13:43:47 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-f9fb94bebf0d71be
03/10/2025 13:43:48 - INFO - datasets.builder - Using custom data configuration default-f9fb94bebf0d71be
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 13:43:48 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Generating train split: 52284 examples [00:00, 595355.39 examples/s]
Generating validation split: 2760 examples [00:00, 577802.80 examples/s]
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 13:43:48 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 13:43:48 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 13:43:48,485 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:48,488 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:699] 2025-03-10 13:43:48,695 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:48,696 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file vocab.txt from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file tokenizer.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file tokenizer_config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:48,696 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 13:43:48,696 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:48,697 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:4128] 2025-03-10 13:43:48,762 >> loading weights file model.safetensors from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors
[WARNING|modeling_utils.py:5103] 2025-03-10 13:43:48,790 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|logging.py:344] 2025-03-10 13:43:48,801 >> A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):
* `cls.predictions.transform.LayerNorm.beta` -> `bert.cls.predictions.transform.LayerNorm.bias`
* `cls.predictions.transform.LayerNorm.gamma` -> `bert.cls.predictions.transform.LayerNorm.weight`
If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.
[INFO|modeling_utils.py:5091] 2025-03-10 13:43:48,805 >> Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 13:43:48,805 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7736c4d3fb71442a.arrow
03/10/2025 13:43:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7736c4d3fb71442a.arrow
Running tokenizer on dataset:  11%|██████████████████████████████████▉                                                                                                                                                                                                                                                                             | 6000/52284 [00:02<00:17, 2689.21 examples/s][rank1]:[W310 13:43:51.884791360 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:15<00:00, 3301.19 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2760 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7db0b0ba2c85d806.arrow
03/10/2025 13:44:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7db0b0ba2c85d806.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 3636.17 examples/s]
[rank0]:[W310 13:44:05.341703124 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 13:44:08 - INFO - __main__ - Sample 41905 of the training set: {'sentence1': 'are(O) there(O) any(O) pg(B-genre) movies(I-genre) with(O) car(B-pattern) chases(I-pattern)', 'sentence2': 'are there any pg movies with car chases', 'label': 0.26, 'idx': 41905, 'input_ids': [101, 1132, 113, 152, 114, 1175, 113, 152, 114, 1251, 113, 152, 114, 185, 1403, 113, 139, 118, 6453, 114, 5558, 113, 146, 118, 6453, 114, 1114, 113, 152, 114, 1610, 113, 139, 118, 4844, 114, 9839, 1116, 113, 146, 118, 4844, 114, 102, 1132, 1175, 1251, 185, 1403, 5558, 1114, 1610, 9839, 1116, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:44:08 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'gnuplot(B-algorithm) can(O) be(O) used(O) from(O) various(O) programming(B-language) languages(I-language) to(O) graph(O) data(B-field),(O) including(O) Perl(B-programming language) ((O) via(O) PDL(B-product) and(O) CPAN(B-product) packages(I-product) )(O),(O) Python(B-programming language) ((O) via(O) )(O).(O)', 'sentence2': 'gnuplot can be used from various programming languages to graph data , including Perl ( via PDL and CPAN packages ) , Python ( via ) .', 'label': 2.03, 'idx': 7296, 'input_ids': [101, 176, 14787, 1643, 7841, 113, 139, 118, 9932, 114, 1169, 113, 152, 114, 1129, 113, 152, 114, 1215, 113, 152, 114, 1121, 113, 152, 114, 1672, 113, 152, 114, 4159, 113, 139, 118, 1846, 114, 3483, 113, 146, 118, 1846, 114, 1106, 113, 152, 114, 10873, 113, 152, 114, 2233, 113, 139, 118, 1768, 114, 117, 113, 152, 114, 1259, 113, 152, 114, 14286, 1233, 113, 139, 118, 4159, 1846, 114, 113, 113, 152, 114, 2258, 113, 152, 114, 27802, 2162, 113, 139, 118, 3317, 114, 1105, 113, 152, 114, 20360, 14962, 113, 139, 118, 3317, 114, 15611, 113, 146, 118, 3317, 114, 114, 113, 152, 114, 117, 113, 152, 114, 23334, 113, 139, 118, 4159, 1846, 114, 113, 113, 152, 114, 2258, 113, 152, 114, 114, 113, 152, 114, 119, 113, 152, 114, 102, 176, 14787, 1643, 7841, 1169, 1129, 1215, 1121, 1672, 4159, 3483, 1106, 10873, 2233, 117, 1259, 14286, 1233, 113, 2258, 27802, 2162, 1105, 20360, 14962, 15611, 114, 117, 23334, 113, 2258, 114, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:44:08 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-project) MAC(I-project) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.14, 'idx': 1639, 'input_ids': [101, 1960, 113, 152, 114, 14427, 113, 152, 114, 117, 113, 152, 114, 12193, 113, 139, 118, 1825, 114, 17931, 2142, 113, 146, 118, 1825, 114, 1105, 113, 152, 114, 9532, 113, 139, 118, 1825, 114, 5525, 113, 146, 118, 1825, 114, 15463, 3954, 1399, 113, 146, 118, 1825, 114, 117, 113, 152, 114, 4102, 113, 152, 114, 1106, 113, 152, 114, 3118, 113, 152, 114, 8795, 113, 152, 114, 118, 113, 152, 114, 1147, 113, 152, 114, 1372, 113, 152, 114, 1108, 113, 152, 114, 2752, 113, 152, 114, 1106, 113, 152, 114, 18995, 113, 152, 114, 1112, 113, 152, 114, 4507, 113, 139, 118, 1583, 114, 1105, 113, 152, 114, 4042, 113, 139, 118, 1933, 114, 25424, 113, 146, 118, 1933, 114, 1111, 113, 152, 114, 1103, 113, 152, 114, 1397, 113, 152, 114, 1476, 113, 152, 114, 1201, 113, 152, 114, 119, 113, 152, 114, 102, 1960, 14427, 117, 12193, 17931, 2142, 1105, 9532, 5525, 15463, 3954, 1399, 117, 4102, 1106, 3118, 8795, 118, 1147, 1372, 1108, 2752, 1106, 18995, 1112, 4507, 1105, 4042, 25424, 1111, 1103, 1397, 1476, 1201, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 13:44:08,756 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 13:44:08,945 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 13:44:08,951] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 13:44:08,951] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:15<00:00, 3341.65 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 3509.67 examples/s]
[2025-03-10 13:44:25,724] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 13:44:25,727] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 13:44:25,727] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 13:44:25,731] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 13:44:25,731] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 13:44:25,732] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 13:44:25,732] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 13:44:25,732] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 13:44:25,732] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 13:44:25,732] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 13:44:25,876] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:44:26,132] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 13:44:26,133] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.51 GB         CA 0.51 GB         Max_CA 1 GB
[2025-03-10 13:44:26,133] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.68 GB, percent = 5.2%
[2025-03-10 13:44:26,280] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 13:44:26,280] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.61 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:44:26,281] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.7 GB, percent = 5.2%
[2025-03-10 13:44:26,281] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 13:44:26,426] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 13:44:26,427] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:44:26,427] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.7 GB, percent = 5.2%
[2025-03-10 13:44:26,429] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 13:44:26,429] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:44:26,429] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 13:44:26,429] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fa810d72ba0>
[2025-03-10 13:44:26,429] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 13:44:26,430] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 13:44:26,430] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 13:44:26,430] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 13:44:26,430] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 13:44:26,430] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa8a417f980>
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 13:44:26,431] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 13:44:26,432] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 13:44:26,433] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 13:44:26,434] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 13:44:26,434] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 13:44:26,435 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 13:44:26,435 >>   Num examples = 52,284
[INFO|trainer.py:2416] 2025-03-10 13:44:26,435 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 13:44:26,435 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 13:44:26,435 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 13:44:26,435 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 13:44:26,435 >>   Total optimization steps = 65,360
[INFO|trainer.py:2423] 2025-03-10 13:44:26,436 >>   Number of trainable parameters = 108,311,041
{'loss': 0.9156, 'grad_norm': 11.268321990966797, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [03:13<2:08:58,  8.23it/s][INFO|trainer.py:930] 2025-03-10 13:47:39,723 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:47:39,727 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:47:39,727 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 13:47:39,727 >>   Batch size = 4
{'eval_loss': 1.0693745613098145, 'eval_mse': 1.0691940417540247, 'eval_pearson': 0.6858112050488461, 'eval_spearmanr': 0.6925308877475003, 'eval_runtime': 3.5013, 'eval_samples_per_second': 788.281, 'eval_steps_per_second': 98.535, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [03:16<2:08:58,  8.23it/s]
[INFO|trainer.py:3955] 2025-03-10 13:47:43,375 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634
[INFO|configuration_utils.py:423] 2025-03-10 13:47:43,377 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:47:43,605 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:47:43,606 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:47:43,606 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/special_tokens_map.json
[2025-03-10 13:47:43,631] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1634 is about to be saved!
[2025-03-10 13:47:43,635] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt
[2025-03-10 13:47:43,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt...
[2025-03-10 13:47:43,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt.
[2025-03-10 13:47:43,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:47:44,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:47:44,943] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:47:44,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1634 is ready now!
{'loss': 0.4858, 'grad_norm': 5.686506748199463, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [06:31<2:03:56,  8.35it/s][INFO|trainer.py:930] 2025-03-10 13:50:58,014 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:50:58,017 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:50:58,017 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 13:50:58,017 >>   Batch size = 4
{'eval_loss': 1.1467467546463013, 'eval_mse': 1.1471030717526658, 'eval_pearson': 0.6950029596789657, 'eval_spearmanr': 0.700985042166013, 'eval_runtime': 3.5071, 'eval_samples_per_second': 786.966, 'eval_steps_per_second': 98.371, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [06:35<2:03:56,  8.35it/s]
[INFO|trainer.py:3955] 2025-03-10 13:51:01,670 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268
[INFO|configuration_utils.py:423] 2025-03-10 13:51:01,672 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:51:01,893 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:51:01,894 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:51:01,894 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/special_tokens_map.json
[2025-03-10 13:51:01,917] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3268 is about to be saved!
[2025-03-10 13:51:01,920] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt
[2025-03-10 13:51:01,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt...
[2025-03-10 13:51:02,217] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt.
[2025-03-10 13:51:02,218] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:51:03,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:51:03,187] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:51:03,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3268 is ready now!
{'loss': 0.3098, 'grad_norm': 4.587952613830566, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [09:46<2:00:39,  8.35it/s][INFO|trainer.py:930] 2025-03-10 13:54:13,365 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:54:13,368 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:54:13,368 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 13:54:13,368 >>   Batch size = 4
{'eval_loss': 0.9209471940994263, 'eval_mse': 0.921046673387721, 'eval_pearson': 0.7428278670657132, 'eval_spearmanr': 0.7509553250073234, 'eval_runtime': 3.4989, 'eval_samples_per_second': 788.823, 'eval_steps_per_second': 98.603, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [09:50<2:00:39,  8.35it/s]
[INFO|trainer.py:3955] 2025-03-10 13:54:17,008 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902
[INFO|configuration_utils.py:423] 2025-03-10 13:54:17,010 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:54:17,232 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:54:17,233 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:54:17,233 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/special_tokens_map.json
[2025-03-10 13:54:17,250] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4902 is about to be saved!
[2025-03-10 13:54:17,254] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt
[2025-03-10 13:54:17,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt...
[2025-03-10 13:54:17,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt.
[2025-03-10 13:54:17,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:54:18,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:54:18,530] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:54:18,530] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4902 is ready now!
{'loss': 0.2085, 'grad_norm': 6.663889408111572, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [13:02<1:56:43,  8.40it/s][INFO|trainer.py:930] 2025-03-10 13:57:28,470 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:57:28,473 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:57:28,473 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 13:57:28,473 >>   Batch size = 4
{'eval_loss': 1.0071600675582886, 'eval_mse': 1.0070023989547854, 'eval_pearson': 0.7186966082080268, 'eval_spearmanr': 0.7274764226244971, 'eval_runtime': 3.4982, 'eval_samples_per_second': 788.968, 'eval_steps_per_second': 98.621, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [13:05<1:56:43,  8.40it/s]
[INFO|trainer.py:3955] 2025-03-10 13:57:32,116 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536
[INFO|configuration_utils.py:423] 2025-03-10 13:57:32,119 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:57:32,338 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:57:32,339 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:57:32,339 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/special_tokens_map.json
[2025-03-10 13:57:32,362] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6536 is about to be saved!
[2025-03-10 13:57:32,365] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt
[2025-03-10 13:57:32,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt...
[2025-03-10 13:57:32,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt.
[2025-03-10 13:57:32,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:57:33,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:57:33,636] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:57:33,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6536 is ready now!
{'loss': 0.1389, 'grad_norm': 7.222754001617432, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [16:17<2:09:55,  7.34it/s][INFO|trainer.py:930] 2025-03-10 14:00:44,262 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:00:44,266 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:00:44,266 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:00:44,266 >>   Batch size = 4
{'eval_loss': 0.902610719203949, 'eval_mse': 0.9024492190151975, 'eval_pearson': 0.7515352413330372, 'eval_spearmanr': 0.7644493130825406, 'eval_runtime': 3.5569, 'eval_samples_per_second': 775.952, 'eval_steps_per_second': 96.994, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [16:21<2:09:55,  7.34it/s]
[INFO|trainer.py:3955] 2025-03-10 14:00:47,917 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170
[INFO|configuration_utils.py:423] 2025-03-10 14:00:47,919 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:00:48,134 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:00:48,135 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:00:48,135 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/special_tokens_map.json
[2025-03-10 14:00:48,152] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8170 is about to be saved!
[2025-03-10 14:00:48,155] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt
[2025-03-10 14:00:48,155] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt...
[2025-03-10 14:00:48,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt.
[2025-03-10 14:00:48,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:00:49,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:00:49,399] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:00:49,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8170 is ready now!
{'loss': 0.0988, 'grad_norm': 2.6888041496276855, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [19:34<1:49:07,  8.48it/s][INFO|trainer.py:930] 2025-03-10 14:04:00,568 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:04:00,572 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:04:00,572 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:04:00,572 >>   Batch size = 4
{'eval_loss': 0.9994410276412964, 'eval_mse': 0.9995788735971934, 'eval_pearson': 0.75071032783504, 'eval_spearmanr': 0.7634493558112918, 'eval_runtime': 3.5027, 'eval_samples_per_second': 787.975, 'eval_steps_per_second': 98.497, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [19:37<1:49:07,  8.48it/s]
[INFO|trainer.py:3955] 2025-03-10 14:04:04,218 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804
[INFO|configuration_utils.py:423] 2025-03-10 14:04:04,220 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:04:04,435 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:04:04,436 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:04:04,436 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/special_tokens_map.json
[2025-03-10 14:04:04,457] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9804 is about to be saved!
[2025-03-10 14:04:04,460] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt
[2025-03-10 14:04:04,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt...
[2025-03-10 14:04:04,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt.
[2025-03-10 14:04:04,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:04:05,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:04:05,703] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:04:05,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9804 is ready now!
{'loss': 0.0721, 'grad_norm': 1.8554456233978271, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [22:49<1:45:17,  8.54it/s][INFO|trainer.py:930] 2025-03-10 14:07:15,893 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:07:15,896 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:07:15,896 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:07:15,896 >>   Batch size = 4
{'eval_loss': 0.9643993377685547, 'eval_mse': 0.9641751466669898, 'eval_pearson': 0.7418618751785909, 'eval_spearmanr': 0.7546345774160559, 'eval_runtime': 3.4944, 'eval_samples_per_second': 789.838, 'eval_steps_per_second': 98.73, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [22:52<1:45:17,  8.54it/s]
[INFO|trainer.py:3955] 2025-03-10 14:07:19,529 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438
[INFO|configuration_utils.py:423] 2025-03-10 14:07:19,532 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:07:19,742 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:07:19,743 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:07:19,743 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/special_tokens_map.json
[2025-03-10 14:07:19,765] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11438 is about to be saved!
[2025-03-10 14:07:19,768] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt
[2025-03-10 14:07:19,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt...
[2025-03-10 14:07:20,057] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt.
[2025-03-10 14:07:20,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:07:21,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:07:21,013] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:07:21,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11438 is ready now!
{'loss': 0.0554, 'grad_norm': 1.668804407119751, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [26:03<2:33:40,  5.67it/s][INFO|trainer.py:930] 2025-03-10 14:10:30,121 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:10:30,125 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:10:30,125 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:10:30,125 >>   Batch size = 4
{'eval_loss': 0.9871192574501038, 'eval_mse': 0.9868426463094311, 'eval_pearson': 0.7280640436556111, 'eval_spearmanr': 0.7467455360897529, 'eval_runtime': 3.5613, 'eval_samples_per_second': 774.991, 'eval_steps_per_second': 96.874, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [26:07<2:33:40,  5.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:10:33,778 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072
[INFO|configuration_utils.py:423] 2025-03-10 14:10:33,780 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:10:33,990 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:10:33,990 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:10:33,990 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/special_tokens_map.json
[2025-03-10 14:10:34,009] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13072 is about to be saved!
[2025-03-10 14:10:34,012] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt
[2025-03-10 14:10:34,013] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt...
[2025-03-10 14:10:34,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt.
[2025-03-10 14:10:34,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:10:35,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:10:35,242] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:10:35,243] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13072 is ready now!
{'loss': 0.0443, 'grad_norm': 1.762499213218689, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [29:19<1:42:29,  8.24it/s][INFO|trainer.py:930] 2025-03-10 14:13:46,371 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:13:46,375 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:13:46,375 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:13:46,375 >>   Batch size = 4
{'eval_loss': 0.9548153877258301, 'eval_mse': 0.9548252025592154, 'eval_pearson': 0.7413004526906899, 'eval_spearmanr': 0.7542757363889027, 'eval_runtime': 3.5042, 'eval_samples_per_second': 787.63, 'eval_steps_per_second': 98.454, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [29:23<1:42:29,  8.24it/s]
[INFO|trainer.py:3955] 2025-03-10 14:13:50,019 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706
[INFO|configuration_utils.py:423] 2025-03-10 14:13:50,020 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:13:50,222 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:13:50,223 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:13:50,223 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/special_tokens_map.json
[2025-03-10 14:13:50,244] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14706 is about to be saved!
[2025-03-10 14:13:50,247] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt
[2025-03-10 14:13:50,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt...
[2025-03-10 14:13:50,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt.
[2025-03-10 14:13:50,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:13:51,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:13:51,465] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:13:51,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14706 is ready now!
{'loss': 0.035, 'grad_norm': 4.435069561004639, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 16340/65360 [32:37<2:31:47,  5.38it/s][INFO|trainer.py:930] 2025-03-10 14:17:03,576 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:17:03,580 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:17:03,580 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:17:03,580 >>   Batch size = 4
{'eval_loss': 0.9186813831329346, 'eval_mse': 0.9187012595542963, 'eval_pearson': 0.7607373041106927, 'eval_spearmanr': 0.7674907300992305, 'eval_runtime': 3.5733, 'eval_samples_per_second': 772.389, 'eval_steps_per_second': 96.549, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 16340/65360 [32:40<2:31:47,  5.38it/s]
[INFO|trainer.py:3955] 2025-03-10 14:17:07,246 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340
[INFO|configuration_utils.py:423] 2025-03-10 14:17:07,248 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:17:07,455 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:17:07,456 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:17:07,456 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/special_tokens_map.json
[2025-03-10 14:17:07,478] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16340 is about to be saved!
[2025-03-10 14:17:07,481] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt
[2025-03-10 14:17:07,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt...
[2025-03-10 14:17:07,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt.
[2025-03-10 14:17:07,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:17:08,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:17:08,724] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:17:08,724] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16340 is ready now!
{'loss': 0.0336, 'grad_norm': 1.638566493988037, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 17974/65360 [35:52<1:32:50,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:20:18,615 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:20:18,618 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:20:18,618 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:20:18,618 >>   Batch size = 4
{'eval_loss': 0.9170536398887634, 'eval_mse': 0.9164847129712934, 'eval_pearson': 0.759759827171709, 'eval_spearmanr': 0.7715425951865928, 'eval_runtime': 3.5487, 'eval_samples_per_second': 777.745, 'eval_steps_per_second': 97.218, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 17974/65360 [35:55<1:32:50,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:20:22,256 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974
[INFO|configuration_utils.py:423] 2025-03-10 14:20:22,258 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:20:22,461 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:20:22,462 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:20:22,462 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/special_tokens_map.json
[2025-03-10 14:20:22,483] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17974 is about to be saved!
[2025-03-10 14:20:22,487] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt
[2025-03-10 14:20:22,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt...
[2025-03-10 14:20:22,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt.
[2025-03-10 14:20:22,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:20:23,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:20:23,717] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:20:23,718] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17974 is ready now!
{'loss': 0.0297, 'grad_norm': 1.00338876247406, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 19608/65360 [39:06<1:30:33,  8.42it/s][INFO|trainer.py:930] 2025-03-10 14:23:32,705 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:23:32,709 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:23:32,709 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:23:32,709 >>   Batch size = 4
{'eval_loss': 0.885047972202301, 'eval_mse': 0.8847881322537643, 'eval_pearson': 0.7631959731892145, 'eval_spearmanr': 0.782424193123224, 'eval_runtime': 3.5064, 'eval_samples_per_second': 787.121, 'eval_steps_per_second': 98.39, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 19608/65360 [39:09<1:30:33,  8.42it/s]
[INFO|trainer.py:3955] 2025-03-10 14:23:36,357 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608
[INFO|configuration_utils.py:423] 2025-03-10 14:23:36,359 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:23:36,604 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:23:36,605 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:23:36,605 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/special_tokens_map.json
[2025-03-10 14:23:36,627] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19608 is about to be saved!
[2025-03-10 14:23:36,630] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt
[2025-03-10 14:23:36,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt...
[2025-03-10 14:23:36,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt.
[2025-03-10 14:23:36,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:23:37,927] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:23:37,927] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:23:37,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19608 is ready now!
{'loss': 0.0257, 'grad_norm': 0.5475659370422363, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 21242/65360 [42:21<1:26:25,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:26:47,945 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:26:47,949 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:26:47,949 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:26:47,949 >>   Batch size = 4
{'eval_loss': 0.9342046976089478, 'eval_mse': 0.93409967035919, 'eval_pearson': 0.7575307213132518, 'eval_spearmanr': 0.7679149919696362, 'eval_runtime': 3.497, 'eval_samples_per_second': 789.24, 'eval_steps_per_second': 98.655, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 21242/65360 [42:25<1:26:25,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:26:51,586 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242
[INFO|configuration_utils.py:423] 2025-03-10 14:26:51,588 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:26:51,802 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:26:51,803 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:26:51,803 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/special_tokens_map.json
[2025-03-10 14:26:51,824] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21242 is about to be saved!
[2025-03-10 14:26:51,828] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt
[2025-03-10 14:26:51,828] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt...
[2025-03-10 14:26:52,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt.
[2025-03-10 14:26:52,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:26:53,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:26:53,075] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:26:53,075] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21242 is ready now!
{'loss': 0.0227, 'grad_norm': 2.2537028789520264, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 22876/65360 [45:37<1:23:13,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:30:03,526 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:30:03,529 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:30:03,529 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:30:03,529 >>   Batch size = 4
{'eval_loss': 1.0095912218093872, 'eval_mse': 1.0094269802820854, 'eval_pearson': 0.7534173305158214, 'eval_spearmanr': 0.7608272988808359, 'eval_runtime': 3.4926, 'eval_samples_per_second': 790.238, 'eval_steps_per_second': 98.78, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 22876/65360 [45:40<1:23:13,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:30:07,162 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876
[INFO|configuration_utils.py:423] 2025-03-10 14:30:07,164 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:30:07,382 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:30:07,383 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:30:07,383 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/special_tokens_map.json
[2025-03-10 14:30:07,401] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22876 is about to be saved!
[2025-03-10 14:30:07,405] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt
[2025-03-10 14:30:07,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt...
[2025-03-10 14:30:07,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt.
[2025-03-10 14:30:07,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:30:08,670] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:30:08,671] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:30:08,671] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22876 is ready now!
{'loss': 0.0227, 'grad_norm': 3.827956199645996, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 24510/65360 [48:52<1:20:06,  8.50it/s][INFO|trainer.py:930] 2025-03-10 14:33:18,456 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:33:18,459 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:33:18,459 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:33:18,459 >>   Batch size = 4
{'eval_loss': 0.9548400044441223, 'eval_mse': 0.9549650022301122, 'eval_pearson': 0.748389677833774, 'eval_spearmanr': 0.7691539413835422, 'eval_runtime': 3.5542, 'eval_samples_per_second': 776.536, 'eval_steps_per_second': 97.067, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 24510/65360 [48:55<1:20:06,  8.50it/s]
[INFO|trainer.py:3955] 2025-03-10 14:33:22,106 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510
[INFO|configuration_utils.py:423] 2025-03-10 14:33:22,108 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:33:22,334 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:33:22,335 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:33:22,335 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/special_tokens_map.json
[2025-03-10 14:33:22,357] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24510 is about to be saved!
[2025-03-10 14:33:22,361] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt
[2025-03-10 14:33:22,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt...
[2025-03-10 14:33:22,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt.
[2025-03-10 14:33:22,662] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:33:23,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:33:23,642] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:33:23,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24510 is ready now!
{'loss': 0.0209, 'grad_norm': 0.4040040075778961, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 26144/65360 [52:06<1:16:43,  8.52it/s][INFO|trainer.py:930] 2025-03-10 14:36:33,202 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:36:33,205 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:36:33,205 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:36:33,205 >>   Batch size = 4
{'eval_loss': 0.9004741907119751, 'eval_mse': 0.9007138782005379, 'eval_pearson': 0.7651798453054648, 'eval_spearmanr': 0.7792408575579844, 'eval_runtime': 3.5466, 'eval_samples_per_second': 778.213, 'eval_steps_per_second': 97.277, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 26144/65360 [52:10<1:16:43,  8.52it/s]
[INFO|trainer.py:3955] 2025-03-10 14:36:36,844 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144
[INFO|configuration_utils.py:423] 2025-03-10 14:36:36,846 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:36:37,063 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:36:37,064 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:36:37,064 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/special_tokens_map.json
[2025-03-10 14:36:37,083] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26144 is about to be saved!
[2025-03-10 14:36:37,087] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt
[2025-03-10 14:36:37,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt...
[2025-03-10 14:36:37,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt.
[2025-03-10 14:36:37,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:36:38,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:36:38,352] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:36:38,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26144 is ready now!
{'loss': 0.0205, 'grad_norm': 0.5188689827919006, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 27778/65360 [55:21<1:13:35,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:39:48,250 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:39:48,253 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:39:48,253 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:39:48,253 >>   Batch size = 4
{'eval_loss': 0.9463497400283813, 'eval_mse': 0.946428019715392, 'eval_pearson': 0.7474837847071809, 'eval_spearmanr': 0.756211291639515, 'eval_runtime': 3.503, 'eval_samples_per_second': 787.89, 'eval_steps_per_second': 98.486, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 27778/65360 [55:25<1:13:35,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:39:51,900 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778
[INFO|configuration_utils.py:423] 2025-03-10 14:39:51,902 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:39:52,132 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:39:52,133 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:39:52,133 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/special_tokens_map.json
[2025-03-10 14:39:52,147] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27778 is about to be saved!
[2025-03-10 14:39:52,150] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt
[2025-03-10 14:39:52,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt...
[2025-03-10 14:39:52,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt.
[2025-03-10 14:39:52,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:39:53,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:39:53,451] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:39:53,451] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27778 is ready now!
{'loss': 0.0177, 'grad_norm': 0.35982999205589294, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 29412/65360 [58:36<1:10:19,  8.52it/s][INFO|trainer.py:930] 2025-03-10 14:43:02,727 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:43:02,730 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:43:02,730 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:43:02,731 >>   Batch size = 4
{'eval_loss': 0.832876980304718, 'eval_mse': 0.83263631698446, 'eval_pearson': 0.7797072290519602, 'eval_spearmanr': 0.7942145027236107, 'eval_runtime': 3.4959, 'eval_samples_per_second': 789.504, 'eval_steps_per_second': 98.688, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 29412/65360 [58:39<1:10:19,  8.52it/s]
[INFO|trainer.py:3955] 2025-03-10 14:43:06,370 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412
[INFO|configuration_utils.py:423] 2025-03-10 14:43:06,372 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:43:06,600 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:43:06,601 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:43:06,601 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/special_tokens_map.json
[2025-03-10 14:43:06,623] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29412 is about to be saved!
[2025-03-10 14:43:06,627] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt
[2025-03-10 14:43:06,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt...
[2025-03-10 14:43:06,933] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt.
[2025-03-10 14:43:06,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:43:08,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:43:08,071] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:43:08,071] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29412 is ready now!
{'loss': 0.015, 'grad_norm': 1.0605993270874023, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [1:01:51<1:07:13,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:46:18,010 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:46:18,013 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:46:18,013 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:46:18,013 >>   Batch size = 4
{'eval_loss': 0.962112545967102, 'eval_mse': 0.9618099091061647, 'eval_pearson': 0.7455280610845721, 'eval_spearmanr': 0.7564884812035391, 'eval_runtime': 3.5601, 'eval_samples_per_second': 775.264, 'eval_steps_per_second': 96.908, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [1:01:55<1:07:13,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:46:21,693 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046
[INFO|configuration_utils.py:423] 2025-03-10 14:46:21,695 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:46:21,973 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:46:21,974 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:46:21,974 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/special_tokens_map.json
[2025-03-10 14:46:21,997] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31046 is about to be saved!
[2025-03-10 14:46:22,001] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt
[2025-03-10 14:46:22,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt...
[2025-03-10 14:46:22,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt.
[2025-03-10 14:46:22,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:46:23,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:46:23,510] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:46:23,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31046 is ready now!
{'loss': 0.0144, 'grad_norm': 1.2140079736709595, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [1:05:06<1:04:44,  8.41it/s][INFO|trainer.py:930] 2025-03-10 14:49:33,059 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:49:33,062 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:49:33,062 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:49:33,062 >>   Batch size = 4
{'eval_loss': 0.9054455757141113, 'eval_mse': 0.9053756847977639, 'eval_pearson': 0.7573871678895875, 'eval_spearmanr': 0.7698424399068458, 'eval_runtime': 3.5102, 'eval_samples_per_second': 786.287, 'eval_steps_per_second': 98.286, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [1:05:10<1:04:44,  8.41it/s]
[INFO|trainer.py:3955] 2025-03-10 14:49:36,716 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680
[INFO|configuration_utils.py:423] 2025-03-10 14:49:36,718 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:49:36,943 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:49:36,943 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:49:36,943 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/special_tokens_map.json
[2025-03-10 14:49:36,967] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32680 is about to be saved!
[2025-03-10 14:49:36,970] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt
[2025-03-10 14:49:36,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt...
[2025-03-10 14:49:37,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt.
[2025-03-10 14:49:37,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:49:38,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:49:38,276] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:49:38,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32680 is ready now!
{'loss': 0.0144, 'grad_norm': 2.7456703186035156, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 34314/65360 [1:08:24<1:00:53,  8.50it/s][INFO|trainer.py:930] 2025-03-10 14:52:51,322 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:52:51,325 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:52:51,325 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:52:51,325 >>   Batch size = 4
{'eval_loss': 0.8437605500221252, 'eval_mse': 0.843761393751787, 'eval_pearson': 0.7792127001800245, 'eval_spearmanr': 0.787320256032149, 'eval_runtime': 3.501, 'eval_samples_per_second': 788.348, 'eval_steps_per_second': 98.544, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 34314/65360 [1:08:28<1:00:53,  8.50it/s]
[INFO|trainer.py:3955] 2025-03-10 14:52:54,966 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314
[INFO|configuration_utils.py:423] 2025-03-10 14:52:54,968 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:52:55,203 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:52:55,204 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:52:55,204 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/special_tokens_map.json
[2025-03-10 14:52:55,226] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34314 is about to be saved!
[2025-03-10 14:52:55,229] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt
[2025-03-10 14:52:55,229] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt...
[2025-03-10 14:52:55,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt.
[2025-03-10 14:52:55,527] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:52:56,605] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:52:56,605] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:52:56,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34314 is ready now!
{'loss': 0.0142, 'grad_norm': 0.9987176060676575, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 35948/65360 [1:11:40<59:24,  8.25it/s][INFO|trainer.py:930] 2025-03-10 14:56:06,827 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:56:06,830 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:56:06,830 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:56:06,830 >>   Batch size = 4
{'eval_loss': 0.8549309968948364, 'eval_mse': 0.854947751350161, 'eval_pearson': 0.7743374871150519, 'eval_spearmanr': 0.7844572441898301, 'eval_runtime': 3.5528, 'eval_samples_per_second': 776.843, 'eval_steps_per_second': 97.105, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 35948/65360 [1:11:43<59:24,  8.25it/s]
[INFO|trainer.py:3955] 2025-03-10 14:56:10,491 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948
[INFO|configuration_utils.py:423] 2025-03-10 14:56:10,493 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:56:10,756 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:56:10,757 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:56:10,757 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/special_tokens_map.json
[2025-03-10 14:56:10,775] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35948 is about to be saved!
[2025-03-10 14:56:10,778] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt
[2025-03-10 14:56:10,778] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt...
[2025-03-10 14:56:11,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt.
[2025-03-10 14:56:11,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:56:12,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:56:12,231] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:56:12,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35948 is ready now!
{'loss': 0.0138, 'grad_norm': 0.7905714511871338, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 37582/65360 [1:14:56<54:22,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:59:23,343 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:59:23,347 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:59:23,347 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 14:59:23,347 >>   Batch size = 4
{'eval_loss': 0.9853124618530273, 'eval_mse': 0.9850254508248274, 'eval_pearson': 0.7495918559760482, 'eval_spearmanr': 0.7610569127962709, 'eval_runtime': 3.4911, 'eval_samples_per_second': 790.59, 'eval_steps_per_second': 98.824, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 37582/65360 [1:15:00<54:22,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:59:27,002 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582
[INFO|configuration_utils.py:423] 2025-03-10 14:59:27,004 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:59:27,276 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:59:27,277 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:59:27,277 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/special_tokens_map.json
[2025-03-10 14:59:27,297] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step37582 is about to be saved!
[2025-03-10 14:59:27,300] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt
[2025-03-10 14:59:27,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt...
[2025-03-10 14:59:27,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt.
[2025-03-10 14:59:27,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:59:28,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:59:28,751] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:59:28,751] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37582 is ready now!
{'loss': 0.0139, 'grad_norm': 1.021370768547058, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 39216/65360 [1:18:13<50:58,  8.55it/s][INFO|trainer.py:930] 2025-03-10 15:02:39,963 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:02:39,966 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:02:39,966 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:02:39,966 >>   Batch size = 4
{'eval_loss': 0.9318941235542297, 'eval_mse': 0.9321558901581212, 'eval_pearson': 0.7574773870463865, 'eval_spearmanr': 0.7631919212510155, 'eval_runtime': 3.493, 'eval_samples_per_second': 790.15, 'eval_steps_per_second': 98.769, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 39216/65360 [1:18:17<50:58,  8.55it/s]
[INFO|trainer.py:3955] 2025-03-10 15:02:43,611 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216
[INFO|configuration_utils.py:423] 2025-03-10 15:02:43,613 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:02:43,878 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:02:43,878 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:02:43,879 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/special_tokens_map.json
[2025-03-10 15:02:43,901] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39216 is about to be saved!
[2025-03-10 15:02:43,905] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt
[2025-03-10 15:02:43,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt...
[2025-03-10 15:02:44,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt.
[2025-03-10 15:02:44,228] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:02:45,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:02:45,317] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:02:45,317] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39216 is ready now!
{'loss': 0.0123, 'grad_norm': 0.4864061176776886, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 40850/65360 [1:21:28<48:11,  8.48it/s][INFO|trainer.py:930] 2025-03-10 15:05:55,138 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:05:55,141 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:05:55,141 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:05:55,141 >>   Batch size = 4
{'eval_loss': 1.0223828554153442, 'eval_mse': 1.0223808935802916, 'eval_pearson': 0.7401665909103918, 'eval_spearmanr': 0.7547784781389975, 'eval_runtime': 3.6052, 'eval_samples_per_second': 765.554, 'eval_steps_per_second': 95.694, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 40850/65360 [1:21:32<48:11,  8.48it/s]
[INFO|trainer.py:3955] 2025-03-10 15:05:58,864 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850
[INFO|configuration_utils.py:423] 2025-03-10 15:05:58,866 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:05:59,164 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:05:59,165 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:05:59,165 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/special_tokens_map.json
[2025-03-10 15:05:59,187] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40850 is about to be saved!
[2025-03-10 15:05:59,190] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt
[2025-03-10 15:05:59,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt...
[2025-03-10 15:05:59,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt.
[2025-03-10 15:05:59,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:06:00,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:06:00,732] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:06:00,732] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40850 is ready now!
{'loss': 0.0122, 'grad_norm': 0.23329739272594452, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 42484/65360 [1:24:44<45:09,  8.44it/s][INFO|trainer.py:930] 2025-03-10 15:09:11,132 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:09:11,135 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:09:11,135 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:09:11,135 >>   Batch size = 4
{'eval_loss': 0.987799346446991, 'eval_mse': 0.9876520804089048, 'eval_pearson': 0.7446871767495896, 'eval_spearmanr': 0.7576488041410432, 'eval_runtime': 3.4915, 'eval_samples_per_second': 790.5, 'eval_steps_per_second': 98.812, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 42484/65360 [1:24:48<45:09,  8.44it/s]
[INFO|trainer.py:3955] 2025-03-10 15:09:14,798 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484
[INFO|configuration_utils.py:423] 2025-03-10 15:09:14,800 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:09:15,086 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:09:15,087 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:09:15,087 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/special_tokens_map.json
[2025-03-10 15:09:15,102] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42484 is about to be saved!
[2025-03-10 15:09:15,105] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt
[2025-03-10 15:09:15,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt...
[2025-03-10 15:09:15,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt.
[2025-03-10 15:09:15,482] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:09:16,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:09:16,686] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:09:16,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42484 is ready now!
{'loss': 0.0132, 'grad_norm': 0.24805769324302673, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 44118/65360 [1:27:59<41:37,  8.51it/s][INFO|trainer.py:930] 2025-03-10 15:12:25,813 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:12:25,816 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:12:25,816 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:12:25,816 >>   Batch size = 4
{'eval_loss': 0.990459680557251, 'eval_mse': 0.9904523270933524, 'eval_pearson': 0.7445268059329619, 'eval_spearmanr': 0.7573513345814189, 'eval_runtime': 3.5007, 'eval_samples_per_second': 788.413, 'eval_steps_per_second': 98.552, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 44118/65360 [1:28:02<41:37,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 15:12:29,489 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118
[INFO|configuration_utils.py:423] 2025-03-10 15:12:29,490 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:12:29,775 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:12:29,776 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:12:29,776 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/special_tokens_map.json
[2025-03-10 15:12:29,794] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44118 is about to be saved!
[2025-03-10 15:12:29,798] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt
[2025-03-10 15:12:29,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt...
[2025-03-10 15:12:30,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt.
[2025-03-10 15:12:30,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:12:31,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:12:31,335] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:12:31,335] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44118 is ready now!
{'loss': 0.0105, 'grad_norm': 1.1364494562149048, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 45752/65360 [1:31:13<38:19,  8.53it/s][INFO|trainer.py:930] 2025-03-10 15:15:40,210 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:15:40,213 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:15:40,214 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:15:40,214 >>   Batch size = 4
{'eval_loss': 0.8804342746734619, 'eval_mse': 0.8801757237401562, 'eval_pearson': 0.7707968622905449, 'eval_spearmanr': 0.783670780684045, 'eval_runtime': 3.4986, 'eval_samples_per_second': 788.877, 'eval_steps_per_second': 98.61, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 45752/65360 [1:31:17<38:19,  8.53it/s]
[INFO|trainer.py:3955] 2025-03-10 15:15:43,884 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752
[INFO|configuration_utils.py:423] 2025-03-10 15:15:43,886 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:15:44,168 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:15:44,168 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:15:44,168 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/special_tokens_map.json
[2025-03-10 15:15:44,187] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45752 is about to be saved!
[2025-03-10 15:15:44,190] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt
[2025-03-10 15:15:44,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt...
[2025-03-10 15:15:44,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt.
[2025-03-10 15:15:44,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:15:45,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:15:45,730] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:15:45,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45752 is ready now!
{'loss': 0.012, 'grad_norm': 0.31195369362831116, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 47386/65360 [1:34:28<35:13,  8.50it/s][INFO|trainer.py:930] 2025-03-10 15:18:54,723 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:18:54,727 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:18:54,727 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:18:54,727 >>   Batch size = 4
{'eval_loss': 0.897770345211029, 'eval_mse': 0.8981527518750965, 'eval_pearson': 0.7676434912588593, 'eval_spearmanr': 0.7827107728444443, 'eval_runtime': 3.5042, 'eval_samples_per_second': 787.616, 'eval_steps_per_second': 98.452, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 47386/65360 [1:34:31<35:13,  8.50it/s]
[INFO|trainer.py:3955] 2025-03-10 15:18:58,404 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386
[INFO|configuration_utils.py:423] 2025-03-10 15:18:58,406 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:18:58,695 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:18:58,696 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:18:58,696 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/special_tokens_map.json
[2025-03-10 15:18:58,721] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47386 is about to be saved!
[2025-03-10 15:18:58,724] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt
[2025-03-10 15:18:58,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt...
[2025-03-10 15:18:59,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt.
[2025-03-10 15:18:59,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:19:00,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:19:00,295] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:19:00,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47386 is ready now!
{'loss': 0.0113, 'grad_norm': 0.792464017868042, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 49020/65360 [1:37:43<32:00,  8.51it/s][INFO|trainer.py:930] 2025-03-10 15:22:09,757 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:22:09,761 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:22:09,761 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:22:09,761 >>   Batch size = 4
{'eval_loss': 0.9881061911582947, 'eval_mse': 0.9877816893268323, 'eval_pearson': 0.7400692039568558, 'eval_spearmanr': 0.7537367857655092, 'eval_runtime': 3.5682, 'eval_samples_per_second': 773.5, 'eval_steps_per_second': 96.687, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 49020/65360 [1:37:46<32:00,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 15:22:13,421 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020
[INFO|configuration_utils.py:423] 2025-03-10 15:22:13,424 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:22:13,634 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:22:13,635 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:22:13,635 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/special_tokens_map.json
[2025-03-10 15:22:13,657] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49020 is about to be saved!
[2025-03-10 15:22:13,660] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt
[2025-03-10 15:22:13,660] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt...
[2025-03-10 15:22:13,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt.
[2025-03-10 15:22:13,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:22:14,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:22:14,918] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:22:14,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49020 is ready now!
{'loss': 0.0105, 'grad_norm': 0.36554399132728577, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [1:41:02<29:50,  8.21it/s][INFO|trainer.py:930] 2025-03-10 15:25:29,114 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:25:29,118 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:25:29,118 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:25:29,118 >>   Batch size = 4
{'eval_loss': 0.8826096057891846, 'eval_mse': 0.8824964637557665, 'eval_pearson': 0.7677322735844755, 'eval_spearmanr': 0.7796134320081943, 'eval_runtime': 3.5161, 'eval_samples_per_second': 784.95, 'eval_steps_per_second': 98.119, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [1:41:06<29:50,  8.21it/s]
[INFO|trainer.py:3955] 2025-03-10 15:25:32,804 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654
[INFO|configuration_utils.py:423] 2025-03-10 15:25:32,806 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:25:33,088 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:25:33,089 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:25:33,089 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/special_tokens_map.json
[2025-03-10 15:25:33,108] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step50654 is about to be saved!
[2025-03-10 15:25:33,111] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt
[2025-03-10 15:25:33,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt...
[2025-03-10 15:25:33,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt.
[2025-03-10 15:25:33,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:25:34,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:25:34,622] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:25:34,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50654 is ready now!
{'loss': 0.01, 'grad_norm': 1.918779730796814, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [1:44:18<25:33,  8.52it/s][INFO|trainer.py:930] 2025-03-10 15:28:45,048 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:28:45,051 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:28:45,051 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:28:45,051 >>   Batch size = 4
{'eval_loss': 0.9016126394271851, 'eval_mse': 0.901355176753756, 'eval_pearson': 0.7592796043793154, 'eval_spearmanr': 0.7708479931517475, 'eval_runtime': 3.5469, 'eval_samples_per_second': 778.154, 'eval_steps_per_second': 97.269, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [1:44:22<25:33,  8.52it/s]
[INFO|trainer.py:3955] 2025-03-10 15:28:48,720 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288
[INFO|configuration_utils.py:423] 2025-03-10 15:28:48,722 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:28:49,010 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:28:49,011 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:28:49,011 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/special_tokens_map.json
[2025-03-10 15:28:49,028] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step52288 is about to be saved!
[2025-03-10 15:28:49,031] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt
[2025-03-10 15:28:49,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt...
[2025-03-10 15:28:49,404] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt.
[2025-03-10 15:28:49,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:28:50,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:28:50,583] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:28:50,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step52288 is ready now!
{'loss': 0.01, 'grad_norm': 0.9543392062187195, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [1:47:33<28:43,  6.64it/s][INFO|trainer.py:930] 2025-03-10 15:31:59,687 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:31:59,691 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:31:59,691 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:31:59,691 >>   Batch size = 4
{'eval_loss': 0.8521369099617004, 'eval_mse': 0.8520874682759894, 'eval_pearson': 0.7764781237881743, 'eval_spearmanr': 0.7846800755883788, 'eval_runtime': 3.5505, 'eval_samples_per_second': 777.355, 'eval_steps_per_second': 97.169, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [1:47:36<28:43,  6.64it/s]
[INFO|trainer.py:3955] 2025-03-10 15:32:03,364 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922
[INFO|configuration_utils.py:423] 2025-03-10 15:32:03,366 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:32:03,648 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:32:03,648 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:32:03,648 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/special_tokens_map.json
[2025-03-10 15:32:03,673] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step53922 is about to be saved!
[2025-03-10 15:32:03,676] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt
[2025-03-10 15:32:03,676] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt...
[2025-03-10 15:32:04,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt.
[2025-03-10 15:32:04,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:32:05,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:32:05,210] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:32:05,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step53922 is ready now!
{'loss': 0.0099, 'grad_norm': 2.181129217147827, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [1:50:47<19:05,  8.56it/s][INFO|trainer.py:930] 2025-03-10 15:35:13,909 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:35:13,912 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:35:13,912 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:35:13,912 >>   Batch size = 4
{'eval_loss': 0.9338467121124268, 'eval_mse': 0.9336036371364109, 'eval_pearson': 0.7515807415131761, 'eval_spearmanr': 0.7681902494471868, 'eval_runtime': 3.4957, 'eval_samples_per_second': 789.53, 'eval_steps_per_second': 98.691, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [1:50:50<19:05,  8.56it/s]
[INFO|trainer.py:3955] 2025-03-10 15:35:17,596 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556
[INFO|configuration_utils.py:423] 2025-03-10 15:35:17,598 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:35:17,940 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:35:17,941 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:35:17,941 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/special_tokens_map.json
[2025-03-10 15:35:17,962] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step55556 is about to be saved!
[2025-03-10 15:35:17,965] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt
[2025-03-10 15:35:17,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt...
[2025-03-10 15:35:18,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt.
[2025-03-10 15:35:18,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:35:19,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:35:19,305] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:35:19,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55556 is ready now!
{'loss': 0.0096, 'grad_norm': 0.696556031703949, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [1:54:01<16:33,  8.23it/s][INFO|trainer.py:930] 2025-03-10 15:38:28,013 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:38:28,016 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:38:28,016 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:38:28,016 >>   Batch size = 4
{'eval_loss': 0.8776318430900574, 'eval_mse': 0.8775931612520979, 'eval_pearson': 0.7733742297264123, 'eval_spearmanr': 0.7857173661884452, 'eval_runtime': 3.5011, 'eval_samples_per_second': 788.326, 'eval_steps_per_second': 98.541, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [1:54:05<16:33,  8.23it/s]
[INFO|trainer.py:3955] 2025-03-10 15:38:31,686 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190
[INFO|configuration_utils.py:423] 2025-03-10 15:38:31,688 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:38:31,981 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:38:31,981 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:38:31,981 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/special_tokens_map.json
[2025-03-10 15:38:31,995] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step57190 is about to be saved!
[2025-03-10 15:38:31,998] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt
[2025-03-10 15:38:31,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt...
[2025-03-10 15:38:32,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt.
[2025-03-10 15:38:32,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:38:33,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:38:33,574] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:38:33,574] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step57190 is ready now!
{'loss': 0.0093, 'grad_norm': 1.300794243812561, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [1:57:16<13:06,  8.31it/s][INFO|trainer.py:930] 2025-03-10 15:41:42,755 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:41:42,758 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:41:42,758 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:41:42,759 >>   Batch size = 4
{'eval_loss': 0.866140604019165, 'eval_mse': 0.8660590636773385, 'eval_pearson': 0.7779357352934435, 'eval_spearmanr': 0.795340458124706, 'eval_runtime': 3.5007, 'eval_samples_per_second': 788.405, 'eval_steps_per_second': 98.551, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [1:57:19<13:06,  8.31it/s]
[INFO|trainer.py:3955] 2025-03-10 15:41:46,434 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824
[INFO|configuration_utils.py:423] 2025-03-10 15:41:46,436 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:41:46,732 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:41:46,732 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:41:46,732 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/special_tokens_map.json
[2025-03-10 15:41:46,746] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step58824 is about to be saved!
[2025-03-10 15:41:46,749] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt
[2025-03-10 15:41:46,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt...
[2025-03-10 15:41:47,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt.
[2025-03-10 15:41:47,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:41:48,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:41:48,362] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:41:48,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step58824 is ready now!
{'loss': 0.0096, 'grad_norm': 0.9367735385894775, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [2:00:30<09:44,  8.38it/s][INFO|trainer.py:930] 2025-03-10 15:44:57,441 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:44:57,444 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:44:57,444 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:44:57,444 >>   Batch size = 4
{'eval_loss': 1.1033885478973389, 'eval_mse': 1.1036974791383398, 'eval_pearson': 0.7070798361193824, 'eval_spearmanr': 0.7331969190593233, 'eval_runtime': 3.5507, 'eval_samples_per_second': 777.313, 'eval_steps_per_second': 97.164, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [2:00:34<09:44,  8.38it/s]
[INFO|trainer.py:3955] 2025-03-10 15:45:01,121 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458
[INFO|configuration_utils.py:423] 2025-03-10 15:45:01,123 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:45:01,419 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:45:01,420 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:45:01,420 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/special_tokens_map.json
[2025-03-10 15:45:01,444] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step60458 is about to be saved!
[2025-03-10 15:45:01,447] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt
[2025-03-10 15:45:01,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt...
[2025-03-10 15:45:01,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt.
[2025-03-10 15:45:01,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:45:03,073] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:45:03,073] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:45:03,073] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60458 is ready now!
{'loss': 0.01, 'grad_norm': 0.8253023028373718, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [2:03:47<06:23,  8.52it/s][INFO|trainer.py:930] 2025-03-10 15:48:13,642 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:48:13,645 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:48:13,645 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:48:13,645 >>   Batch size = 4
{'eval_loss': 0.841289222240448, 'eval_mse': 0.8412106377707012, 'eval_pearson': 0.7883285991869056, 'eval_spearmanr': 0.800886361082771, 'eval_runtime': 3.5418, 'eval_samples_per_second': 779.261, 'eval_steps_per_second': 97.408, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [2:03:50<06:23,  8.52it/s]
[INFO|trainer.py:3955] 2025-03-10 15:48:17,316 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092
[INFO|configuration_utils.py:423] 2025-03-10 15:48:17,318 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:48:17,616 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:48:17,617 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:48:17,617 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/special_tokens_map.json
[2025-03-10 15:48:17,641] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step62092 is about to be saved!
[2025-03-10 15:48:17,645] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt
[2025-03-10 15:48:17,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt...
[2025-03-10 15:48:18,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt.
[2025-03-10 15:48:18,027] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:48:19,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:48:19,282] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:48:19,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62092 is ready now!
{'loss': 0.0089, 'grad_norm': 0.2604360282421112, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [2:07:02<03:11,  8.53it/s][INFO|trainer.py:930] 2025-03-10 15:51:28,550 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:51:28,553 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:51:28,553 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:51:28,553 >>   Batch size = 4
{'eval_loss': 0.8689015507698059, 'eval_mse': 0.8688496619247008, 'eval_pearson': 0.7751893862032223, 'eval_spearmanr': 0.7837249650367757, 'eval_runtime': 3.4899, 'eval_samples_per_second': 790.86, 'eval_steps_per_second': 98.858, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [2:07:05<03:11,  8.53it/s]
[INFO|trainer.py:3955] 2025-03-10 15:51:32,217 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726
[INFO|configuration_utils.py:423] 2025-03-10 15:51:32,219 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:51:32,512 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:51:32,512 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:51:32,512 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/special_tokens_map.json
[2025-03-10 15:51:32,531] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step63726 is about to be saved!
[2025-03-10 15:51:32,534] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt
[2025-03-10 15:51:32,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt...
[2025-03-10 15:51:32,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt.
[2025-03-10 15:51:32,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:51:34,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:51:34,131] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:51:34,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step63726 is ready now!
{'loss': 0.0082, 'grad_norm': 1.0106842517852783, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:10:17<00:00,  8.29it/s][INFO|trainer.py:930] 2025-03-10 15:54:43,989 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:43,992 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:43,992 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:54:43,992 >>   Batch size = 4
{'eval_loss': 0.9223024249076843, 'eval_mse': 0.9222825019255928, 'eval_pearson': 0.7669536818908282, 'eval_spearmanr': 0.7867596169514591, 'eval_runtime': 3.5473, 'eval_samples_per_second': 778.064, 'eval_steps_per_second': 97.258, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:10:21<00:00,  8.29it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:47,633 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360
[INFO|configuration_utils.py:423] 2025-03-10 15:54:47,635 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:47,863 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:47,864 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:47,864 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/special_tokens_map.json
[2025-03-10 15:54:47,887] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step65360 is about to be saved!
[2025-03-10 15:54:47,891] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt
[2025-03-10 15:54:47,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt...
[2025-03-10 15:54:48,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt.
[2025-03-10 15:54:48,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:54:49,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:54:49,150] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:54:49,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65360 is ready now!
[INFO|trainer.py:2670] 2025-03-10 15:54:49,153 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 7822.7172, 'train_samples_per_second': 267.344, 'train_steps_per_second': 8.355, 'train_loss': 0.07028141682705359, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:10:22<00:00,  8.36it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:49,255 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=40
[INFO|configuration_utils.py:423] 2025-03-10 15:54:49,257 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=40/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:49,472 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=40/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:49,473 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:49,473 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=40/special_tokens_map.json
***** train metrics *****
  epoch                    =        40.0
  total_flos               = 512504120GF
  train_loss               =      0.0703
  train_runtime            =  2:10:22.71
  train_samples            =       52284
  train_samples_per_second =     267.344
  train_steps_per_second   =       8.355
03/10/2025 15:54:49 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 15:54:49,495 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:49,497 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:49,497 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:54:49,497 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 345/345 [00:03<00:00, 97.70it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =     0.9223
  eval_mse                =     0.9223
  eval_pearson            =      0.767
  eval_runtime            = 0:00:03.54
  eval_samples            =       2760
  eval_samples_per_second =     778.19
  eval_spearmanr          =     0.7868
  eval_steps_per_second   =     97.274
[INFO|modelcard.py:449] 2025-03-10 15:54:53,275 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7867596169514591}]}
[rank0]:[W310 15:54:53.371069953 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 15:54:57,179] [INFO] [launch.py:351:main] Process 4024459 exits successfully.
[2025-03-10 15:54:57,180] [INFO] [launch.py:351:main] Process 4024460 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:6,7 --master_port 29894 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:55:04,439] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:07,580] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 15:55:07,580] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29894 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:55:12,160] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:15,153] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2025-03-10 15:55:15,153] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 15:55:15,153] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 15:55:15,153] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 15:55:15,153] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2025-03-10 15:55:15,154] [INFO] [launch.py:256:main] process 4170557 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:55:15,154] [INFO] [launch.py:256:main] process 4170558 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:55:20,042] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:20,059] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:20,790] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 15:55:20,790] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 15:55:20,801] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 15:55:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 15:55:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-base-num=40/runs/Mar10_15-55-19_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-base-num=40,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-base-num=40,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 15:55:23 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json
03/10/2025 15:55:23 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json
03/10/2025 15:55:23 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-f9fb94bebf0d71be
03/10/2025 15:55:23 - INFO - datasets.builder - Using custom data configuration default-f9fb94bebf0d71be
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 15:55:23 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 15:55:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:55:23 - INFO - datasets.info - Loading Dataset info from .cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 15:55:23 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:55:23 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 15:55:24,019 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:24,022 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 15:55:24,232 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:24,232 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:24,242 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 15:55:24,242 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:24,243 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 15:55:24,385 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 15:55:24,431 >> Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 15:55:24,431 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s][WARNING|modeling_utils.py:5103] 2025-03-10 15:55:24,459 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f6085f36e487cb8f.arrow
03/10/2025 15:55:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f6085f36e487cb8f.arrow
Running tokenizer on dataset:  21%|███████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                               | 11000/52284 [00:02<00:10, 3765.56 examples/s][rank1]:[W310 15:55:27.997579753 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:13<00:00, 3928.05 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2760 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f02a675b13953fc7.arrow
03/10/2025 15:55:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f02a675b13953fc7.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 4464.77 examples/s]
[rank0]:[W310 15:55:38.584204737 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 15:55:41 - INFO - __main__ - Sample 41905 of the training set: {'sentence1': 'are(O) there(O) any(O) pg(B-genre) movies(I-genre) with(O) car(B-pattern) chases(I-pattern)', 'sentence2': 'are there any pg movies with car chases', 'label': 0.26, 'idx': 41905, 'input_ids': [0, 1322, 1640, 673, 43, 89, 1640, 673, 43, 143, 1640, 673, 43, 47194, 1640, 387, 12, 44205, 43, 4133, 1640, 100, 12, 44205, 43, 19, 1640, 673, 43, 512, 1640, 387, 12, 43106, 43, 1855, 9354, 1640, 100, 12, 43106, 43, 2, 2, 1322, 89, 143, 47194, 4133, 19, 512, 1855, 9354, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:55:41 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'gnuplot(B-algorithm) can(O) be(O) used(O) from(O) various(O) programming(B-language) languages(I-language) to(O) graph(O) data(B-field),(O) including(O) Perl(B-programming language) ((O) via(O) PDL(B-product) and(O) CPAN(B-product) packages(I-product) )(O),(O) Python(B-programming language) ((O) via(O) )(O).(O)', 'sentence2': 'gnuplot can be used from various programming languages to graph data , including Perl ( via PDL and CPAN packages ) , Python ( via ) .', 'label': 2.03, 'idx': 7296, 'input_ids': [0, 49022, 40776, 1640, 387, 12, 337, 47549, 43, 64, 1640, 673, 43, 28, 1640, 673, 43, 341, 1640, 673, 43, 31, 1640, 673, 43, 1337, 1640, 673, 43, 8326, 1640, 387, 12, 19527, 43, 11991, 1640, 100, 12, 19527, 43, 7, 1640, 673, 43, 20992, 1640, 673, 43, 414, 1640, 387, 12, 1399, 238, 1640, 673, 43, 217, 1640, 673, 43, 35939, 1640, 387, 12, 28644, 7059, 2777, 43, 41006, 673, 43, 1241, 1640, 673, 43, 11707, 574, 1640, 387, 12, 20565, 43, 8, 1640, 673, 43, 11565, 1889, 1640, 387, 12, 20565, 43, 8368, 1640, 100, 12, 20565, 43, 4839, 1640, 673, 238, 1640, 673, 43, 31886, 1640, 387, 12, 28644, 7059, 2777, 43, 41006, 673, 43, 1241, 1640, 673, 43, 4839, 1640, 673, 322, 1640, 673, 43, 2, 2, 49022, 40776, 64, 28, 341, 31, 1337, 8326, 11991, 7, 20992, 414, 2156, 217, 35939, 36, 1241, 11707, 574, 8, 11565, 1889, 8368, 4839, 2156, 31886, 36, 1241, 4839, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:55:41 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-project) MAC(I-project) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.14, 'idx': 1639, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 28258, 43, 19482, 1640, 100, 12, 28258, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 15:55:41,962 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 15:55:42,155 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 15:55:42,160] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 15:55:42,160] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:13<00:00, 3846.99 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 4476.98 examples/s]
[2025-03-10 15:55:58,893] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 15:55:58,895] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 15:55:58,895] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 15:55:58,900] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 15:55:58,900] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 15:55:58,900] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 15:55:58,900] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 15:55:58,900] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 15:55:58,900] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 15:55:58,900] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 15:55:59,125] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:55:59,308] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 15:55:59,309] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.58 GB         CA 0.58 GB         Max_CA 1 GB
[2025-03-10 15:55:59,309] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.04 GB, percent = 5.5%
[2025-03-10 15:55:59,455] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 15:55:59,456] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.7 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 15:55:59,456] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.05 GB, percent = 5.5%
[2025-03-10 15:55:59,457] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 15:55:59,601] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 15:55:59,602] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 15:55:59,602] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.05 GB, percent = 5.5%
[2025-03-10 15:55:59,604] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 15:55:59,604] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:55:59,604] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 15:55:59,604] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fd31909bd40>
[2025-03-10 15:55:59,604] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 15:55:59,605] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 15:55:59,605] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd360ccf920>
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 15:55:59,606] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 15:55:59,607] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 15:55:59,608] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 15:55:59,609] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 15:55:59,610 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 15:55:59,610 >>   Num examples = 52,284
[INFO|trainer.py:2416] 2025-03-10 15:55:59,610 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 15:55:59,610 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 15:55:59,610 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 15:55:59,610 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 15:55:59,610 >>   Total optimization steps = 65,360
[INFO|trainer.py:2423] 2025-03-10 15:55:59,610 >>   Number of trainable parameters = 124,646,401
{'loss': 1.0049, 'grad_norm': 11.51892375946045, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [03:16<2:08:13,  8.28it/s][INFO|trainer.py:930] 2025-03-10 15:59:16,079 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:59:16,082 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:59:16,083 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 15:59:16,083 >>   Batch size = 4
{'eval_loss': 1.163992166519165, 'eval_mse': 1.1639770562450091, 'eval_pearson': 0.6565378517833071, 'eval_spearmanr': 0.6628682385648145, 'eval_runtime': 3.09, 'eval_samples_per_second': 893.212, 'eval_steps_per_second': 111.651, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [03:19<2:08:13,  8.28it/s]
[INFO|trainer.py:3955] 2025-03-10 15:59:19,321 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634
[INFO|configuration_utils.py:423] 2025-03-10 15:59:19,323 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:59:19,569 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:59:19,570 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:59:19,570 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/special_tokens_map.json
[2025-03-10 15:59:19,617] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1634 is about to be saved!
[2025-03-10 15:59:19,620] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt
[2025-03-10 15:59:19,620] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt...
[2025-03-10 15:59:19,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt.
[2025-03-10 15:59:19,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:59:21,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:59:21,064] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:59:21,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1634 is ready now!
{'loss': 0.6475, 'grad_norm': 12.195517539978027, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [06:33<2:06:43,  8.17it/s][INFO|trainer.py:930] 2025-03-10 16:02:33,327 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:02:33,331 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:02:33,331 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:02:33,331 >>   Batch size = 4
{'eval_loss': 1.4084978103637695, 'eval_mse': 1.4082127945362657, 'eval_pearson': 0.6517560114028189, 'eval_spearmanr': 0.6494119231360714, 'eval_runtime': 3.0627, 'eval_samples_per_second': 901.163, 'eval_steps_per_second': 112.645, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [06:36<2:06:43,  8.17it/s]
[INFO|trainer.py:3955] 2025-03-10 16:02:36,525 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268
[INFO|configuration_utils.py:423] 2025-03-10 16:02:36,527 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:02:36,836 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:02:36,837 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:02:36,837 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/special_tokens_map.json
[2025-03-10 16:02:36,881] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3268 is about to be saved!
[2025-03-10 16:02:36,885] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt
[2025-03-10 16:02:36,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt...
[2025-03-10 16:02:37,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt.
[2025-03-10 16:02:37,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:02:38,353] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:02:38,354] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:02:38,354] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3268 is ready now!
{'loss': 0.5236, 'grad_norm': 7.024844169616699, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [09:51<2:02:01,  8.26it/s][INFO|trainer.py:930] 2025-03-10 16:05:50,680 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:05:50,684 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:05:50,684 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:05:50,684 >>   Batch size = 4
{'eval_loss': 1.249348759651184, 'eval_mse': 1.2499420265572658, 'eval_pearson': 0.6418502741930248, 'eval_spearmanr': 0.6435092057599554, 'eval_runtime': 3.0661, 'eval_samples_per_second': 900.159, 'eval_steps_per_second': 112.52, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [09:54<2:02:01,  8.26it/s]
[INFO|trainer.py:3955] 2025-03-10 16:05:53,873 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902
[INFO|configuration_utils.py:423] 2025-03-10 16:05:53,875 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:05:54,179 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:05:54,180 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:05:54,180 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/special_tokens_map.json
[2025-03-10 16:05:54,221] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4902 is about to be saved!
[2025-03-10 16:05:54,225] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt
[2025-03-10 16:05:54,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt...
[2025-03-10 16:05:54,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt.
[2025-03-10 16:05:54,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:05:55,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:05:55,749] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:05:55,749] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4902 is ready now!
{'loss': 0.4312, 'grad_norm': 7.822263240814209, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [13:08<2:32:40,  6.42it/s][INFO|trainer.py:930] 2025-03-10 16:09:08,211 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:09:08,214 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:09:08,214 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:09:08,214 >>   Batch size = 4
{'eval_loss': 1.2097200155258179, 'eval_mse': 1.2094184615689776, 'eval_pearson': 0.6452702634650782, 'eval_spearmanr': 0.6489960363327174, 'eval_runtime': 3.0603, 'eval_samples_per_second': 901.871, 'eval_steps_per_second': 112.734, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [13:11<2:32:40,  6.42it/s]
[INFO|trainer.py:3955] 2025-03-10 16:09:11,401 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536
[INFO|configuration_utils.py:423] 2025-03-10 16:09:11,403 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:09:11,654 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:09:11,655 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:09:11,655 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/special_tokens_map.json
[2025-03-10 16:09:11,696] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6536 is about to be saved!
[2025-03-10 16:09:11,699] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt
[2025-03-10 16:09:11,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt...
[2025-03-10 16:09:12,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt.
[2025-03-10 16:09:12,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:09:13,152] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:09:13,152] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:09:13,152] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6536 is ready now!
{'loss': 0.3666, 'grad_norm': 15.274331092834473, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [16:25<2:53:30,  5.49it/s][INFO|trainer.py:930] 2025-03-10 16:12:25,134 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:12:25,137 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:12:25,138 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:12:25,138 >>   Batch size = 4
{'eval_loss': 1.3147846460342407, 'eval_mse': 1.314820918764757, 'eval_pearson': 0.6350962534307281, 'eval_spearmanr': 0.6492166662795859, 'eval_runtime': 3.0609, 'eval_samples_per_second': 901.69, 'eval_steps_per_second': 112.711, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [16:28<2:53:30,  5.49it/s]
[INFO|trainer.py:3955] 2025-03-10 16:12:28,325 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170
[INFO|configuration_utils.py:423] 2025-03-10 16:12:28,327 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:12:28,615 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:12:28,616 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:12:28,616 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/special_tokens_map.json
[2025-03-10 16:12:28,657] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8170 is about to be saved!
[2025-03-10 16:12:28,661] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt
[2025-03-10 16:12:28,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt...
[2025-03-10 16:12:28,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt.
[2025-03-10 16:12:28,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:12:30,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:12:30,112] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:12:30,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8170 is ready now!
{'loss': 0.3158, 'grad_norm': 11.464829444885254, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [19:42<1:51:16,  8.32it/s][INFO|trainer.py:930] 2025-03-10 16:15:42,081 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:15:42,084 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:15:42,084 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:15:42,084 >>   Batch size = 4
{'eval_loss': 1.2453933954238892, 'eval_mse': 1.2458643736398738, 'eval_pearson': 0.648939076653533, 'eval_spearmanr': 0.6725242454919975, 'eval_runtime': 3.0614, 'eval_samples_per_second': 901.558, 'eval_steps_per_second': 112.695, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [19:45<1:51:16,  8.32it/s]
[INFO|trainer.py:3955] 2025-03-10 16:15:45,287 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804
[INFO|configuration_utils.py:423] 2025-03-10 16:15:45,289 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:15:45,585 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:15:45,586 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:15:45,586 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/special_tokens_map.json
[2025-03-10 16:15:45,626] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9804 is about to be saved!
[2025-03-10 16:15:45,629] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt
[2025-03-10 16:15:45,629] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt...
[2025-03-10 16:15:46,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt.
[2025-03-10 16:15:46,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:15:47,249] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:15:47,249] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:15:47,249] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9804 is ready now!
{'loss': 0.2695, 'grad_norm': 25.084854125976562, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [23:00<2:59:48,  5.00it/s][INFO|trainer.py:930] 2025-03-10 16:18:59,953 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:18:59,957 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:18:59,957 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:18:59,957 >>   Batch size = 4
{'eval_loss': 1.2065098285675049, 'eval_mse': 1.2063471434125002, 'eval_pearson': 0.6848646115395292, 'eval_spearmanr': 0.6922440190893667, 'eval_runtime': 3.0548, 'eval_samples_per_second': 903.504, 'eval_steps_per_second': 112.938, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [23:03<2:59:48,  5.00it/s]
[INFO|trainer.py:3955] 2025-03-10 16:19:03,139 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438
[INFO|configuration_utils.py:423] 2025-03-10 16:19:03,141 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:19:03,398 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:19:03,399 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:19:03,399 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/special_tokens_map.json
[2025-03-10 16:19:03,440] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11438 is about to be saved!
[2025-03-10 16:19:03,443] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt
[2025-03-10 16:19:03,443] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt...
[2025-03-10 16:19:03,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt.
[2025-03-10 16:19:03,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:19:05,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:19:05,092] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:19:05,092] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11438 is ready now!
{'loss': 0.2306, 'grad_norm': 6.8519487380981445, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [26:15<1:52:12,  7.77it/s][INFO|trainer.py:930] 2025-03-10 16:22:15,615 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:22:15,618 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:22:15,618 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:22:15,618 >>   Batch size = 4
{'eval_loss': 1.0449950695037842, 'eval_mse': 1.0447767847905989, 'eval_pearson': 0.7011839987457527, 'eval_spearmanr': 0.713445851801078, 'eval_runtime': 3.0667, 'eval_samples_per_second': 899.99, 'eval_steps_per_second': 112.499, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [26:19<1:52:12,  7.77it/s]
[INFO|trainer.py:3955] 2025-03-10 16:22:18,816 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072
[INFO|configuration_utils.py:423] 2025-03-10 16:22:18,818 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:22:19,086 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:22:19,087 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:22:19,087 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/special_tokens_map.json
[2025-03-10 16:22:19,128] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13072 is about to be saved!
[2025-03-10 16:22:19,132] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt
[2025-03-10 16:22:19,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt...
[2025-03-10 16:22:19,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt.
[2025-03-10 16:22:19,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:22:20,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:22:20,698] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:22:20,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13072 is ready now!
{'loss': 0.1959, 'grad_norm': 4.221675395965576, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [29:28<1:42:00,  8.28it/s][INFO|trainer.py:930] 2025-03-10 16:25:28,290 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:25:28,294 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:25:28,294 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:25:28,294 >>   Batch size = 4
{'eval_loss': 1.0675188302993774, 'eval_mse': 1.0678992868549582, 'eval_pearson': 0.7193895016704541, 'eval_spearmanr': 0.7269300782801776, 'eval_runtime': 3.2108, 'eval_samples_per_second': 859.605, 'eval_steps_per_second': 107.451, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [29:31<1:42:00,  8.28it/s]
[INFO|trainer.py:3955] 2025-03-10 16:25:31,635 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706
[INFO|configuration_utils.py:423] 2025-03-10 16:25:31,637 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:25:31,964 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:25:31,965 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:25:31,965 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/special_tokens_map.json
[2025-03-10 16:25:32,007] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14706 is about to be saved!
[2025-03-10 16:25:32,010] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt
[2025-03-10 16:25:32,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt...
[2025-03-10 16:25:32,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt.
[2025-03-10 16:25:32,379] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:25:33,437] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:25:33,438] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:25:33,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14706 is ready now!
{'loss': 0.1676, 'grad_norm': 9.594688415527344, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 16340/65360 [32:42<2:06:35,  6.45it/s][INFO|trainer.py:930] 2025-03-10 16:28:42,536 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:28:42,539 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:28:42,539 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:28:42,539 >>   Batch size = 4
{'eval_loss': 0.9779372811317444, 'eval_mse': 0.977783526480198, 'eval_pearson': 0.7338147876414524, 'eval_spearmanr': 0.746025879864946, 'eval_runtime': 3.2038, 'eval_samples_per_second': 861.485, 'eval_steps_per_second': 107.686, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 16340/65360 [32:46<2:06:35,  6.45it/s]
[INFO|trainer.py:3955] 2025-03-10 16:28:45,869 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340
[INFO|configuration_utils.py:423] 2025-03-10 16:28:45,871 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:28:46,119 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:28:46,120 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:28:46,120 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/special_tokens_map.json
[2025-03-10 16:28:46,162] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16340 is about to be saved!
[2025-03-10 16:28:46,165] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt
[2025-03-10 16:28:46,165] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt...
[2025-03-10 16:28:46,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt.
[2025-03-10 16:28:46,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:28:47,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:28:47,696] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:28:47,696] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16340 is ready now!
{'loss': 0.1446, 'grad_norm': 9.306844711303711, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 17974/65360 [35:56<1:30:48,  8.70it/s][INFO|trainer.py:930] 2025-03-10 16:31:55,733 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:31:55,736 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:31:55,736 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:31:55,736 >>   Batch size = 4
{'eval_loss': 0.9538638591766357, 'eval_mse': 0.9537239948260612, 'eval_pearson': 0.7334426746498403, 'eval_spearmanr': 0.7429891341382319, 'eval_runtime': 3.0666, 'eval_samples_per_second': 900.014, 'eval_steps_per_second': 112.502, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 17974/65360 [35:59<1:30:48,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 16:31:58,933 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974
[INFO|configuration_utils.py:423] 2025-03-10 16:31:58,935 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:31:59,222 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:31:59,223 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:31:59,223 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/special_tokens_map.json
[2025-03-10 16:31:59,265] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17974 is about to be saved!
[2025-03-10 16:31:59,268] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt
[2025-03-10 16:31:59,268] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt...
[2025-03-10 16:31:59,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt.
[2025-03-10 16:31:59,640] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:32:00,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:32:00,857] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:32:00,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17974 is ready now!
{'loss': 0.1238, 'grad_norm': 10.907975196838379, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 19608/65360 [39:06<1:27:39,  8.70it/s][INFO|trainer.py:930] 2025-03-10 16:35:05,863 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:35:05,866 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:35:05,867 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:35:05,867 >>   Batch size = 4
{'eval_loss': 0.9626630544662476, 'eval_mse': 0.9623522883955983, 'eval_pearson': 0.7357207596749978, 'eval_spearmanr': 0.7497293063057738, 'eval_runtime': 3.0739, 'eval_samples_per_second': 897.888, 'eval_steps_per_second': 112.236, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 19608/65360 [39:09<1:27:39,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 16:35:09,066 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608
[INFO|configuration_utils.py:423] 2025-03-10 16:35:09,068 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:35:09,309 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:35:09,310 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:35:09,310 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/special_tokens_map.json
[2025-03-10 16:35:09,351] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19608 is about to be saved!
[2025-03-10 16:35:09,354] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt
[2025-03-10 16:35:09,354] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt...
[2025-03-10 16:35:09,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt.
[2025-03-10 16:35:09,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:35:10,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:35:10,759] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:35:10,759] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19608 is ready now!
{'loss': 0.1054, 'grad_norm': 4.750214576721191, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 21242/65360 [42:17<2:17:57,  5.33it/s][INFO|trainer.py:930] 2025-03-10 16:38:17,185 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:38:17,189 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:38:17,189 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:38:17,189 >>   Batch size = 4
{'eval_loss': 1.0523995161056519, 'eval_mse': 1.0524572782974313, 'eval_pearson': 0.7152571773418879, 'eval_spearmanr': 0.7280428424855859, 'eval_runtime': 3.0808, 'eval_samples_per_second': 895.879, 'eval_steps_per_second': 111.985, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 21242/65360 [42:20<2:17:57,  5.33it/s]
[INFO|trainer.py:3955] 2025-03-10 16:38:20,400 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242
[INFO|configuration_utils.py:423] 2025-03-10 16:38:20,402 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:38:20,685 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:38:20,686 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:38:20,686 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/special_tokens_map.json
[2025-03-10 16:38:20,729] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21242 is about to be saved!
[2025-03-10 16:38:20,732] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt
[2025-03-10 16:38:20,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt...
[2025-03-10 16:38:21,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt.
[2025-03-10 16:38:21,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:38:22,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:38:22,213] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:38:22,213] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21242 is ready now!
{'loss': 0.0903, 'grad_norm': 3.446654796600342, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 22876/65360 [45:29<1:23:22,  8.49it/s][INFO|trainer.py:930] 2025-03-10 16:41:28,736 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:41:28,739 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:41:28,739 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:41:28,739 >>   Batch size = 4
{'eval_loss': 1.0049341917037964, 'eval_mse': 1.0047096509216489, 'eval_pearson': 0.7240085101178124, 'eval_spearmanr': 0.741317039256259, 'eval_runtime': 3.083, 'eval_samples_per_second': 895.226, 'eval_steps_per_second': 111.903, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 22876/65360 [45:32<1:23:22,  8.49it/s]
[INFO|trainer.py:3955] 2025-03-10 16:41:31,948 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876
[INFO|configuration_utils.py:423] 2025-03-10 16:41:31,950 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:41:32,219 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:41:32,219 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:41:32,220 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/special_tokens_map.json
[2025-03-10 16:41:32,261] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22876 is about to be saved!
[2025-03-10 16:41:32,264] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt
[2025-03-10 16:41:32,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt...
[2025-03-10 16:41:32,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt.
[2025-03-10 16:41:32,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:41:33,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:41:33,798] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:41:33,798] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22876 is ready now!
{'loss': 0.0777, 'grad_norm': 5.532411575317383, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 24510/65360 [48:41<1:18:18,  8.69it/s][INFO|trainer.py:930] 2025-03-10 16:44:40,863 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:44:40,866 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:44:40,866 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:44:40,867 >>   Batch size = 4
{'eval_loss': 1.026942491531372, 'eval_mse': 1.027006896963154, 'eval_pearson': 0.7275655986750038, 'eval_spearmanr': 0.7390270157185134, 'eval_runtime': 3.0538, 'eval_samples_per_second': 903.807, 'eval_steps_per_second': 112.976, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 24510/65360 [48:44<1:18:18,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 16:44:44,048 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510
[INFO|configuration_utils.py:423] 2025-03-10 16:44:44,050 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:44:44,312 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:44:44,313 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:44:44,313 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/special_tokens_map.json
[2025-03-10 16:44:44,354] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24510 is about to be saved!
[2025-03-10 16:44:44,358] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt
[2025-03-10 16:44:44,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt...
[2025-03-10 16:44:44,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt.
[2025-03-10 16:44:44,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:44:45,902] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:44:45,903] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:44:45,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24510 is ready now!
{'loss': 0.0685, 'grad_norm': 2.9017767906188965, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 26144/65360 [51:51<1:20:21,  8.13it/s][INFO|trainer.py:930] 2025-03-10 16:47:50,780 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:47:50,783 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:47:50,783 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:47:50,783 >>   Batch size = 4
{'eval_loss': 1.112463116645813, 'eval_mse': 1.1125186581758486, 'eval_pearson': 0.7068015506012673, 'eval_spearmanr': 0.7198772113343167, 'eval_runtime': 3.0835, 'eval_samples_per_second': 895.081, 'eval_steps_per_second': 111.885, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 26144/65360 [51:54<1:20:21,  8.13it/s]
[INFO|trainer.py:3955] 2025-03-10 16:47:53,996 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144
[INFO|configuration_utils.py:423] 2025-03-10 16:47:53,998 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:47:54,282 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:47:54,283 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:47:54,283 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/special_tokens_map.json
[2025-03-10 16:47:54,324] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26144 is about to be saved!
[2025-03-10 16:47:54,327] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt
[2025-03-10 16:47:54,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt...
[2025-03-10 16:47:54,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt.
[2025-03-10 16:47:54,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:47:55,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:47:55,932] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:47:55,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26144 is ready now!
{'loss': 0.0584, 'grad_norm': 2.2072770595550537, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 27778/65360 [55:02<1:35:45,  6.54it/s][INFO|trainer.py:930] 2025-03-10 16:51:02,522 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:51:02,525 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:51:02,525 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:51:02,525 >>   Batch size = 4
{'eval_loss': 1.0543584823608398, 'eval_mse': 1.0539932366082634, 'eval_pearson': 0.7096055052280756, 'eval_spearmanr': 0.7235408126225086, 'eval_runtime': 3.0611, 'eval_samples_per_second': 901.64, 'eval_steps_per_second': 112.705, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 27778/65360 [55:05<1:35:45,  6.54it/s]
[INFO|trainer.py:3955] 2025-03-10 16:51:05,715 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778
[INFO|configuration_utils.py:423] 2025-03-10 16:51:05,717 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:51:06,011 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:51:06,011 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:51:06,012 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/special_tokens_map.json
[2025-03-10 16:51:06,052] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27778 is about to be saved!
[2025-03-10 16:51:06,056] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt
[2025-03-10 16:51:06,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt...
[2025-03-10 16:51:06,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt.
[2025-03-10 16:51:06,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:51:07,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:51:07,591] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:51:07,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27778 is ready now!
{'loss': 0.0545, 'grad_norm': 9.425954818725586, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 29412/65360 [58:14<1:09:45,  8.59it/s][INFO|trainer.py:930] 2025-03-10 16:54:14,054 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:54:14,058 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:54:14,058 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:54:14,058 >>   Batch size = 4
{'eval_loss': 1.0295289754867554, 'eval_mse': 1.0291900611658027, 'eval_pearson': 0.7227388914186335, 'eval_spearmanr': 0.7387452966894995, 'eval_runtime': 3.0609, 'eval_samples_per_second': 901.704, 'eval_steps_per_second': 112.713, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 29412/65360 [58:17<1:09:45,  8.59it/s]
[INFO|trainer.py:3955] 2025-03-10 16:54:17,248 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412
[INFO|configuration_utils.py:423] 2025-03-10 16:54:17,250 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:54:17,503 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:54:17,503 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:54:17,503 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/special_tokens_map.json
[2025-03-10 16:54:17,545] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29412 is about to be saved!
[2025-03-10 16:54:17,548] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt
[2025-03-10 16:54:17,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt...
[2025-03-10 16:54:17,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt.
[2025-03-10 16:54:17,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:54:19,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:54:19,157] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:54:19,157] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29412 is ready now!
{'loss': 0.0456, 'grad_norm': 2.27866530418396, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [1:01:26<1:07:37,  8.46it/s][INFO|trainer.py:930] 2025-03-10 16:57:25,876 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:57:25,880 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:57:25,880 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 16:57:25,880 >>   Batch size = 4
{'eval_loss': 0.9735774397850037, 'eval_mse': 0.9737067672869434, 'eval_pearson': 0.7442414589696558, 'eval_spearmanr': 0.7517153054445451, 'eval_runtime': 3.0625, 'eval_samples_per_second': 901.239, 'eval_steps_per_second': 112.655, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [1:01:29<1:07:37,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 16:57:29,076 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046
[INFO|configuration_utils.py:423] 2025-03-10 16:57:29,078 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:57:29,386 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:57:29,386 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:57:29,387 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/special_tokens_map.json
[2025-03-10 16:57:29,435] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31046 is about to be saved!
[2025-03-10 16:57:29,438] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt
[2025-03-10 16:57:29,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt...
[2025-03-10 16:57:29,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt.
[2025-03-10 16:57:29,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:57:31,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:57:31,124] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:57:31,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31046 is ready now!
{'loss': 0.0397, 'grad_norm': 1.2414969205856323, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [1:04:41<1:05:18,  8.34it/s][INFO|trainer.py:930] 2025-03-10 17:00:41,600 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:00:41,603 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:00:41,603 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:00:41,603 >>   Batch size = 4
{'eval_loss': 1.0324143171310425, 'eval_mse': 1.0323462632471236, 'eval_pearson': 0.7184186118944813, 'eval_spearmanr': 0.7313062224727741, 'eval_runtime': 3.0648, 'eval_samples_per_second': 900.542, 'eval_steps_per_second': 112.568, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [1:04:45<1:05:18,  8.34it/s]
[INFO|trainer.py:3955] 2025-03-10 17:00:44,798 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680
[INFO|configuration_utils.py:423] 2025-03-10 17:00:44,800 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:00:45,092 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:00:45,093 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:00:45,093 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/special_tokens_map.json
[2025-03-10 17:00:45,140] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32680 is about to be saved!
[2025-03-10 17:00:45,144] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt
[2025-03-10 17:00:45,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt...
[2025-03-10 17:00:45,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt.
[2025-03-10 17:00:45,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:00:46,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:00:46,694] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:00:46,694] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32680 is ready now!
{'loss': 0.0376, 'grad_norm': 3.07321834564209, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                 | 34314/65360 [1:07:55<59:22,  8.71it/s][INFO|trainer.py:930] 2025-03-10 17:03:55,291 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:03:55,295 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:03:55,295 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:03:55,295 >>   Batch size = 4
{'eval_loss': 0.9504992365837097, 'eval_mse': 0.9502816508861556, 'eval_pearson': 0.7385948410419374, 'eval_spearmanr': 0.7545127745481854, 'eval_runtime': 3.0642, 'eval_samples_per_second': 900.733, 'eval_steps_per_second': 112.592, 'epoch': 21.0}
 52%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                 | 34314/65360 [1:07:58<59:22,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 17:03:58,487 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314
[INFO|configuration_utils.py:423] 2025-03-10 17:03:58,489 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:03:58,749 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:03:58,750 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:03:58,750 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/special_tokens_map.json
[2025-03-10 17:03:58,791] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34314 is about to be saved!
[2025-03-10 17:03:58,794] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt
[2025-03-10 17:03:58,794] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt...
[2025-03-10 17:03:59,135] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt.
[2025-03-10 17:03:59,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:04:00,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:04:00,366] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:04:00,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34314 is ready now!
 53%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                 | 34386/65360 [1:08:08<58:18,  8.85it/s]{'loss': 0.0326, 'grad_norm': 2.117520809173584, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 35948/65360 [1:11:05<56:28,  8.68it/s][INFO|trainer.py:930] 2025-03-10 17:07:04,788 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:07:04,791 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:07:04,791 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:07:04,791 >>   Batch size = 4
{'eval_loss': 1.0003037452697754, 'eval_mse': 1.0004663683582042, 'eval_pearson': 0.7343766523762569, 'eval_spearmanr': 0.7429411826591158, 'eval_runtime': 3.0717, 'eval_samples_per_second': 898.519, 'eval_steps_per_second': 112.315, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 35948/65360 [1:11:08<56:28,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 17:07:07,997 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948
[INFO|configuration_utils.py:423] 2025-03-10 17:07:07,999 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:07:08,303 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:07:08,304 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:07:08,304 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/special_tokens_map.json
[2025-03-10 17:07:08,351] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35948 is about to be saved!
[2025-03-10 17:07:08,355] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt
[2025-03-10 17:07:08,355] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt...
[2025-03-10 17:07:08,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt.
[2025-03-10 17:07:08,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:07:10,037] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:07:10,038] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:07:10,038] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35948 is ready now!
{'loss': 0.0302, 'grad_norm': 1.6061203479766846, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 37582/65360 [1:14:16<53:57,  8.58it/s][INFO|trainer.py:930] 2025-03-10 17:10:15,965 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:10:15,968 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:10:15,968 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:10:15,968 >>   Batch size = 4
{'eval_loss': 1.0182076692581177, 'eval_mse': 1.0182112356242927, 'eval_pearson': 0.7309838040028402, 'eval_spearmanr': 0.7373555505507617, 'eval_runtime': 3.06, 'eval_samples_per_second': 901.973, 'eval_steps_per_second': 112.747, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 37582/65360 [1:14:19<53:57,  8.58it/s]
[INFO|trainer.py:3955] 2025-03-10 17:10:19,156 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582
[INFO|configuration_utils.py:423] 2025-03-10 17:10:19,157 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:10:19,425 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:10:19,426 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:10:19,426 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/special_tokens_map.json
[2025-03-10 17:10:19,468] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step37582 is about to be saved!
[2025-03-10 17:10:19,471] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt
[2025-03-10 17:10:19,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt...
[2025-03-10 17:10:19,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt.
[2025-03-10 17:10:19,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:10:21,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:10:21,005] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:10:21,005] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37582 is ready now!
{'loss': 0.027, 'grad_norm': 1.443976640701294, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 39216/65360 [1:17:28<52:36,  8.28it/s][INFO|trainer.py:930] 2025-03-10 17:13:27,794 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:13:27,797 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:13:27,797 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:13:27,797 >>   Batch size = 4
{'eval_loss': 0.91075599193573, 'eval_mse': 0.9110370438383973, 'eval_pearson': 0.7566981297389992, 'eval_spearmanr': 0.7695779213284716, 'eval_runtime': 3.1143, 'eval_samples_per_second': 886.224, 'eval_steps_per_second': 110.778, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 39216/65360 [1:17:31<52:36,  8.28it/s]
[INFO|trainer.py:3955] 2025-03-10 17:13:31,037 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216
[INFO|configuration_utils.py:423] 2025-03-10 17:13:31,039 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:13:31,281 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:13:31,282 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:13:31,282 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/special_tokens_map.json
[2025-03-10 17:13:31,327] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39216 is about to be saved!
[2025-03-10 17:13:31,331] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt
[2025-03-10 17:13:31,331] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt...
[2025-03-10 17:13:31,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt.
[2025-03-10 17:13:31,664] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:13:32,780] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:13:32,781] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:13:32,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39216 is ready now!
{'loss': 0.0253, 'grad_norm': 1.9490923881530762, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 40850/65360 [1:20:39<47:31,  8.59it/s][INFO|trainer.py:930] 2025-03-10 17:16:39,134 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:16:39,137 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:16:39,137 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:16:39,137 >>   Batch size = 4
{'eval_loss': 0.9254788756370544, 'eval_mse': 0.925652403783971, 'eval_pearson': 0.7586382733965447, 'eval_spearmanr': 0.7703055567275765, 'eval_runtime': 3.0596, 'eval_samples_per_second': 902.081, 'eval_steps_per_second': 112.76, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 40850/65360 [1:20:42<47:31,  8.59it/s]
[INFO|trainer.py:3955] 2025-03-10 17:16:42,325 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850
[INFO|configuration_utils.py:423] 2025-03-10 17:16:42,328 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:16:42,592 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:16:42,593 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:16:42,593 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/special_tokens_map.json
[2025-03-10 17:16:42,642] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40850 is about to be saved!
[2025-03-10 17:16:42,646] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt
[2025-03-10 17:16:42,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt...
[2025-03-10 17:16:42,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt.
[2025-03-10 17:16:42,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:16:44,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:16:44,182] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:16:44,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40850 is ready now!
{'loss': 0.0224, 'grad_norm': 2.4384689331054688, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 42484/65360 [1:23:53<49:09,  7.76it/s][INFO|trainer.py:930] 2025-03-10 17:19:53,381 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:19:53,385 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:19:53,385 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:19:53,385 >>   Batch size = 4
{'eval_loss': 0.9790871143341064, 'eval_mse': 0.979014222799004, 'eval_pearson': 0.7331574921905666, 'eval_spearmanr': 0.7439213335896464, 'eval_runtime': 3.0678, 'eval_samples_per_second': 899.654, 'eval_steps_per_second': 112.457, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 42484/65360 [1:23:56<49:09,  7.76it/s]
[INFO|trainer.py:3955] 2025-03-10 17:19:56,582 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484
[INFO|configuration_utils.py:423] 2025-03-10 17:19:56,584 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:19:56,860 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:19:56,861 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:19:56,861 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/special_tokens_map.json
[2025-03-10 17:19:56,909] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42484 is about to be saved!
[2025-03-10 17:19:56,912] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt
[2025-03-10 17:19:56,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt...
[2025-03-10 17:19:57,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt.
[2025-03-10 17:19:57,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:19:58,490] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:19:58,491] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:19:58,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42484 is ready now!
{'loss': 0.0208, 'grad_norm': 9.830013275146484, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 44118/65360 [1:27:08<45:51,  7.72it/s][INFO|trainer.py:930] 2025-03-10 17:23:08,428 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:23:08,432 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:23:08,432 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:23:08,432 >>   Batch size = 4
{'eval_loss': 0.9910568594932556, 'eval_mse': 0.991533770530984, 'eval_pearson': 0.7347201474577157, 'eval_spearmanr': 0.7442416807751354, 'eval_runtime': 3.1723, 'eval_samples_per_second': 870.023, 'eval_steps_per_second': 108.753, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 44118/65360 [1:27:11<45:51,  7.72it/s]
[INFO|trainer.py:3955] 2025-03-10 17:23:11,708 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118
[INFO|configuration_utils.py:423] 2025-03-10 17:23:11,711 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:23:11,975 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:23:11,976 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:23:11,976 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/special_tokens_map.json
[2025-03-10 17:23:12,031] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44118 is about to be saved!
[2025-03-10 17:23:12,035] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt
[2025-03-10 17:23:12,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt...
[2025-03-10 17:23:12,393] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt.
[2025-03-10 17:23:12,395] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:23:13,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:23:13,536] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:23:13,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44118 is ready now!
{'loss': 0.0202, 'grad_norm': 32.0812873840332, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 45752/65360 [1:30:21<37:26,  8.73it/s][INFO|trainer.py:930] 2025-03-10 17:26:20,988 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:26:20,991 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:26:20,992 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:26:20,992 >>   Batch size = 4
{'eval_loss': 0.9745742678642273, 'eval_mse': 0.9748677698166474, 'eval_pearson': 0.7374414706145676, 'eval_spearmanr': 0.755242751223286, 'eval_runtime': 3.0524, 'eval_samples_per_second': 904.213, 'eval_steps_per_second': 113.027, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 45752/65360 [1:30:24<37:26,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 17:26:24,171 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752
[INFO|configuration_utils.py:423] 2025-03-10 17:26:24,173 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:26:24,417 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:26:24,418 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:26:24,418 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/special_tokens_map.json
[2025-03-10 17:26:24,464] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45752 is about to be saved!
[2025-03-10 17:26:24,468] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt
[2025-03-10 17:26:24,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt...
[2025-03-10 17:26:24,793] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt.
[2025-03-10 17:26:24,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:26:25,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:26:25,882] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:26:25,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45752 is ready now!
{'loss': 0.0201, 'grad_norm': 1.3106873035430908, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 47386/65360 [1:33:31<35:27,  8.45it/s][INFO|trainer.py:930] 2025-03-10 17:29:30,809 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:29:30,812 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:29:30,812 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:29:30,812 >>   Batch size = 4
{'eval_loss': 1.0179427862167358, 'eval_mse': 1.0177305524979812, 'eval_pearson': 0.7336917262244276, 'eval_spearmanr': 0.7388958691064406, 'eval_runtime': 3.1343, 'eval_samples_per_second': 880.581, 'eval_steps_per_second': 110.073, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 47386/65360 [1:33:34<35:27,  8.45it/s]
[INFO|trainer.py:3955] 2025-03-10 17:29:34,035 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386
[INFO|configuration_utils.py:423] 2025-03-10 17:29:34,037 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:29:34,299 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:29:34,300 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:29:34,300 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/special_tokens_map.json
[2025-03-10 17:29:34,348] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47386 is about to be saved!
[2025-03-10 17:29:34,351] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt
[2025-03-10 17:29:34,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt...
[2025-03-10 17:29:34,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt.
[2025-03-10 17:29:34,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:29:35,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:29:35,809] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:29:35,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47386 is ready now!
{'loss': 0.0177, 'grad_norm': 1.145766019821167, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 49020/65360 [1:36:43<32:01,  8.50it/s][INFO|trainer.py:930] 2025-03-10 17:32:43,308 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:32:43,311 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:32:43,311 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:32:43,311 >>   Batch size = 4
{'eval_loss': 1.0194798707962036, 'eval_mse': 1.0193931309425313, 'eval_pearson': 0.7416939978270589, 'eval_spearmanr': 0.7485515669695298, 'eval_runtime': 3.073, 'eval_samples_per_second': 898.133, 'eval_steps_per_second': 112.267, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 49020/65360 [1:36:46<32:01,  8.50it/s]
[INFO|trainer.py:3955] 2025-03-10 17:32:46,511 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020
[INFO|configuration_utils.py:423] 2025-03-10 17:32:46,513 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:32:46,766 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:32:46,767 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:32:46,767 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/special_tokens_map.json
[2025-03-10 17:32:46,814] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49020 is about to be saved!
[2025-03-10 17:32:46,818] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt
[2025-03-10 17:32:46,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt...
[2025-03-10 17:32:47,156] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt.
[2025-03-10 17:32:47,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:32:48,251] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:32:48,251] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:32:48,251] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49020 is ready now!
{'loss': 0.0166, 'grad_norm': 0.9509753584861755, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [1:39:56<28:57,  8.47it/s][INFO|trainer.py:930] 2025-03-10 17:35:56,520 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:35:56,524 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:35:56,524 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:35:56,524 >>   Batch size = 4
{'eval_loss': 0.9789701104164124, 'eval_mse': 0.979005453919155, 'eval_pearson': 0.7428864641792836, 'eval_spearmanr': 0.7613846605267331, 'eval_runtime': 3.1037, 'eval_samples_per_second': 889.252, 'eval_steps_per_second': 111.157, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [1:40:00<28:57,  8.47it/s]
[INFO|trainer.py:3955] 2025-03-10 17:35:59,712 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654
[INFO|configuration_utils.py:423] 2025-03-10 17:35:59,714 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:36:00,041 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:36:00,042 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:36:00,042 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/special_tokens_map.json
[2025-03-10 17:36:00,089] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step50654 is about to be saved!
[2025-03-10 17:36:00,093] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt
[2025-03-10 17:36:00,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt...
[2025-03-10 17:36:00,477] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt.
[2025-03-10 17:36:00,478] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:36:01,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:36:01,710] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:36:01,710] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50654 is ready now!
{'loss': 0.0162, 'grad_norm': 0.8722732663154602, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [1:43:10<25:08,  8.67it/s][INFO|trainer.py:930] 2025-03-10 17:39:09,776 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:39:09,780 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:39:09,780 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:39:09,780 >>   Batch size = 4
{'eval_loss': 0.975914716720581, 'eval_mse': 0.9756918675009755, 'eval_pearson': 0.7535941476495824, 'eval_spearmanr': 0.7616393627396125, 'eval_runtime': 3.0821, 'eval_samples_per_second': 895.485, 'eval_steps_per_second': 111.936, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [1:43:13<25:08,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 17:39:12,991 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288
[INFO|configuration_utils.py:423] 2025-03-10 17:39:12,993 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:39:13,250 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:39:13,251 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:39:13,251 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/special_tokens_map.json
[2025-03-10 17:39:13,301] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step52288 is about to be saved!
[2025-03-10 17:39:13,305] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt
[2025-03-10 17:39:13,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt...
[2025-03-10 17:39:13,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt.
[2025-03-10 17:39:13,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:39:14,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:39:14,788] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:39:14,788] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step52288 is ready now!
{'loss': 0.0143, 'grad_norm': 1.139840006828308, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [1:46:24<22:43,  8.39it/s][INFO|trainer.py:930] 2025-03-10 17:42:24,322 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:42:24,325 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:42:24,325 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:42:24,325 >>   Batch size = 4
{'eval_loss': 0.9453330039978027, 'eval_mse': 0.9453330599095511, 'eval_pearson': 0.7504859832787856, 'eval_spearmanr': 0.7610762740524526, 'eval_runtime': 3.0541, 'eval_samples_per_second': 903.698, 'eval_steps_per_second': 112.962, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [1:46:27<22:43,  8.39it/s]
[INFO|trainer.py:3955] 2025-03-10 17:42:27,509 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922
[INFO|configuration_utils.py:423] 2025-03-10 17:42:27,511 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:42:27,775 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:42:27,776 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:42:27,776 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/special_tokens_map.json
[2025-03-10 17:42:27,823] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step53922 is about to be saved!
[2025-03-10 17:42:27,827] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt
[2025-03-10 17:42:27,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt...
[2025-03-10 17:42:28,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt.
[2025-03-10 17:42:28,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:42:29,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:42:29,320] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:42:29,320] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step53922 is ready now!
{'loss': 0.0162, 'grad_norm': 2.3772084712982178, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [1:49:34<26:03,  6.27it/s][INFO|trainer.py:930] 2025-03-10 17:45:33,724 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:45:33,727 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:45:33,727 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:45:33,727 >>   Batch size = 4
{'eval_loss': 0.9265050292015076, 'eval_mse': 0.9264641079565753, 'eval_pearson': 0.7482201646467013, 'eval_spearmanr': 0.7546379956989107, 'eval_runtime': 3.0713, 'eval_samples_per_second': 898.654, 'eval_steps_per_second': 112.332, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [1:49:37<26:03,  6.27it/s]
[INFO|trainer.py:3955] 2025-03-10 17:45:36,926 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556
[INFO|configuration_utils.py:423] 2025-03-10 17:45:36,928 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:45:37,212 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:45:37,212 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:45:37,213 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/special_tokens_map.json
[2025-03-10 17:45:37,259] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step55556 is about to be saved!
[2025-03-10 17:45:37,262] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt
[2025-03-10 17:45:37,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt...
[2025-03-10 17:45:37,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt.
[2025-03-10 17:45:37,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:45:38,807] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:45:38,808] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:45:38,808] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55556 is ready now!
{'loss': 0.0148, 'grad_norm': 1.0175209045410156, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [1:52:43<23:37,  5.76it/s][INFO|trainer.py:930] 2025-03-10 17:48:43,292 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:48:43,295 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:48:43,295 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:48:43,295 >>   Batch size = 4
{'eval_loss': 0.9409059286117554, 'eval_mse': 0.9404242833671362, 'eval_pearson': 0.7508137860894026, 'eval_spearmanr': 0.7549174250223427, 'eval_runtime': 3.0931, 'eval_samples_per_second': 892.312, 'eval_steps_per_second': 111.539, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [1:52:46<23:37,  5.76it/s]
[INFO|trainer.py:3955] 2025-03-10 17:48:46,515 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190
[INFO|configuration_utils.py:423] 2025-03-10 17:48:46,517 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:48:46,796 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:48:46,796 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:48:46,797 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/special_tokens_map.json
[2025-03-10 17:48:46,841] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step57190 is about to be saved!
[2025-03-10 17:48:46,844] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt
[2025-03-10 17:48:46,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt...
[2025-03-10 17:48:47,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt.
[2025-03-10 17:48:47,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:48:48,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:48:48,323] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:48:48,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step57190 is ready now!
{'loss': 0.013, 'grad_norm': 0.5548412799835205, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [1:55:57<22:33,  4.83it/s][INFO|trainer.py:930] 2025-03-10 17:51:56,997 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:51:57,001 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:51:57,001 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:51:57,001 >>   Batch size = 4
{'eval_loss': 0.8866904377937317, 'eval_mse': 0.8865481668840284, 'eval_pearson': 0.76470627038622, 'eval_spearmanr': 0.7696690063371363, 'eval_runtime': 3.0725, 'eval_samples_per_second': 898.302, 'eval_steps_per_second': 112.288, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [1:56:00<22:33,  4.83it/s]
[INFO|trainer.py:3955] 2025-03-10 17:52:00,202 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824
[INFO|configuration_utils.py:423] 2025-03-10 17:52:00,204 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:52:00,457 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:52:00,458 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:52:00,458 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/special_tokens_map.json
[2025-03-10 17:52:00,504] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step58824 is about to be saved!
[2025-03-10 17:52:00,508] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt
[2025-03-10 17:52:00,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt...
[2025-03-10 17:52:00,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt.
[2025-03-10 17:52:00,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:52:02,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:52:02,113] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:52:02,113] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step58824 is ready now!
{'loss': 0.0138, 'grad_norm': 0.3533506691455841, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [1:59:09<09:22,  8.71it/s][INFO|trainer.py:930] 2025-03-10 17:55:09,420 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:55:09,423 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:55:09,423 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:55:09,423 >>   Batch size = 4
{'eval_loss': 0.9443942308425903, 'eval_mse': 0.9441655268055805, 'eval_pearson': 0.7521702030744973, 'eval_spearmanr': 0.7626396720418377, 'eval_runtime': 3.0855, 'eval_samples_per_second': 894.515, 'eval_steps_per_second': 111.814, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [1:59:12<09:22,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 17:55:12,637 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458
[INFO|configuration_utils.py:423] 2025-03-10 17:55:12,639 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:55:12,895 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:55:12,895 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:55:12,896 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/special_tokens_map.json
[2025-03-10 17:55:12,943] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step60458 is about to be saved!
[2025-03-10 17:55:12,947] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt
[2025-03-10 17:55:12,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt...
[2025-03-10 17:55:13,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt.
[2025-03-10 17:55:13,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:55:14,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:55:14,480] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:55:14,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60458 is ready now!
{'loss': 0.0118, 'grad_norm': 0.3572665750980377, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [2:02:22<06:16,  8.69it/s][INFO|trainer.py:930] 2025-03-10 17:58:22,459 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:58:22,462 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:58:22,462 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 17:58:22,462 >>   Batch size = 4
{'eval_loss': 0.9091135263442993, 'eval_mse': 0.9093082579365676, 'eval_pearson': 0.7596989067012008, 'eval_spearmanr': 0.7639298124060772, 'eval_runtime': 3.0577, 'eval_samples_per_second': 902.642, 'eval_steps_per_second': 112.83, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [2:02:25<06:16,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 17:58:25,649 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092
[INFO|configuration_utils.py:423] 2025-03-10 17:58:25,651 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:58:25,925 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:58:25,926 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:58:25,926 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/special_tokens_map.json
[2025-03-10 17:58:25,974] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step62092 is about to be saved!
[2025-03-10 17:58:25,977] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt
[2025-03-10 17:58:25,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt...
[2025-03-10 17:58:26,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt.
[2025-03-10 17:58:26,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:58:27,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:58:27,673] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:58:27,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62092 is ready now!
{'loss': 0.0132, 'grad_norm': 0.4838241934776306, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [2:05:37<03:13,  8.46it/s][INFO|trainer.py:930] 2025-03-10 18:01:37,234 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:01:37,238 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:01:37,238 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:01:37,238 >>   Batch size = 4
{'eval_loss': 0.912830114364624, 'eval_mse': 0.9128364159793093, 'eval_pearson': 0.7556778474906867, 'eval_spearmanr': 0.7693920763020736, 'eval_runtime': 3.0611, 'eval_samples_per_second': 901.623, 'eval_steps_per_second': 112.703, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [2:05:40<03:13,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 18:01:40,427 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726
[INFO|configuration_utils.py:423] 2025-03-10 18:01:40,429 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:01:40,698 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:01:40,699 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:01:40,699 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/special_tokens_map.json
[2025-03-10 18:01:40,746] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step63726 is about to be saved!
[2025-03-10 18:01:40,750] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt
[2025-03-10 18:01:40,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt...
[2025-03-10 18:01:41,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt.
[2025-03-10 18:01:41,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:01:42,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:01:42,239] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:01:42,239] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step63726 is ready now!
{'loss': 0.0117, 'grad_norm': 2.3815135955810547, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:08:51<00:00,  8.64it/s][INFO|trainer.py:930] 2025-03-10 18:04:51,102 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:04:51,106 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:04:51,106 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:04:51,106 >>   Batch size = 4
{'eval_loss': 0.915836751461029, 'eval_mse': 0.9158164444393005, 'eval_pearson': 0.7550614133175053, 'eval_spearmanr': 0.7639373861828537, 'eval_runtime': 3.2325, 'eval_samples_per_second': 853.821, 'eval_steps_per_second': 106.728, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:08:54<00:00,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 18:04:54,489 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360
[INFO|configuration_utils.py:423] 2025-03-10 18:04:54,491 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:04:54,846 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:04:54,847 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:04:54,847 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/special_tokens_map.json
[2025-03-10 18:04:54,896] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step65360 is about to be saved!
[2025-03-10 18:04:54,900] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt
[2025-03-10 18:04:54,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt...
[2025-03-10 18:04:55,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt.
[2025-03-10 18:04:55,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:04:56,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:04:56,454] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:04:56,454] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65360 is ready now!
[INFO|trainer.py:2670] 2025-03-10 18:04:56,470 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 7736.86, 'train_samples_per_second': 270.311, 'train_steps_per_second': 8.448, 'train_loss': 0.13393795326844568, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [2:08:56<00:00,  8.45it/s]
[INFO|trainer.py:3955] 2025-03-10 18:04:56,532 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=40
[INFO|configuration_utils.py:423] 2025-03-10 18:04:56,534 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=40/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:04:56,844 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=40/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:04:56,845 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:04:56,845 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=40/special_tokens_map.json
***** train metrics *****
  epoch                    =        40.0
  total_flos               = 512504120GF
  train_loss               =      0.1339
  train_runtime            =  2:08:56.85
  train_samples            =       52284
  train_samples_per_second =     270.311
  train_steps_per_second   =       8.448
03/10/2025 18:04:56 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 18:04:56,892 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:04:56,895 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:04:56,895 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:04:56,895 >>   Batch size = 4
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 345/345 [00:03<00:00, 113.06it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =     0.9158
  eval_mse                =     0.9158
  eval_pearson            =     0.7551
  eval_runtime            = 0:00:03.06
  eval_samples            =       2760
  eval_samples_per_second =    900.589
  eval_spearmanr          =     0.7639
  eval_steps_per_second   =    112.574
[INFO|modelcard.py:449] 2025-03-10 18:05:00,196 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7639373861828537}]}
[rank0]:[W310 18:05:00.614076542 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 18:05:02,697] [INFO] [launch.py:351:main] Process 4170558 exits successfully.
[2025-03-10 18:05:02,697] [INFO] [launch.py:351:main] Process 4170557 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:6,7 --master_port 29894 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 18:05:09,745] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 18:05:12,803] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 18:05:12,803] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=29894 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=40 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 18:05:17,386] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 18:05:21,339] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2025-03-10 18:05:21,339] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 18:05:21,339] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 18:05:21,339] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 18:05:21,339] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2025-03-10 18:05:21,340] [INFO] [launch.py:256:main] process 156412 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 18:05:21,340] [INFO] [launch.py:256:main] process 156413 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=40', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 18:05:25,561] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 18:05:26,322] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 18:05:26,322] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 18:05:28,656] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 18:05:29,384] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 18:05:29 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 18:05:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 18:05:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-large-num=40/runs/Mar10_18-05-25_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-large-num=40,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-large-num=40,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 18:05:29 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-train.json
03/10/2025 18:05:29 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=40-val.json
Using custom data configuration default-f9fb94bebf0d71be
03/10/2025 18:05:30 - INFO - datasets.builder - Using custom data configuration default-f9fb94bebf0d71be
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 18:05:30 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 18:05:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 18:05:30 - INFO - datasets.info - Loading Dataset info from .cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 18:05:30 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 18:05:30 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 18:05:30,447 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 18:05:30,449 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 18:05:30,652 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 18:05:30,653 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 18:05:30,663 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 18:05:30,663 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 18:05:30,664 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 18:05:30,804 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
[WARNING|modeling_utils.py:5103] 2025-03-10 18:05:30,862 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:5091] 2025-03-10 18:05:30,921 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 18:05:30,922 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-fbce75bae37612fb.arrow
03/10/2025 18:05:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-fbce75bae37612fb.arrow
Running tokenizer on dataset:  11%|██████████████████████████████████▉                                                                                                                                                                                                                                                                             | 6000/52284 [00:01<00:13, 3415.21 examples/s][rank1]:[W310 18:05:32.417023522 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:12<00:00, 4122.74 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2760 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2f3d2e2d6d93496b.arrow
03/10/2025 18:05:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-f9fb94bebf0d71be/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2f3d2e2d6d93496b.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 4344.65 examples/s]
[rank0]:[W310 18:05:44.197793623 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 18:05:47 - INFO - __main__ - Sample 41905 of the training set: {'sentence1': 'are(O) there(O) any(O) pg(B-genre) movies(I-genre) with(O) car(B-pattern) chases(I-pattern)', 'sentence2': 'are there any pg movies with car chases', 'label': 0.26, 'idx': 41905, 'input_ids': [0, 1322, 1640, 673, 43, 89, 1640, 673, 43, 143, 1640, 673, 43, 47194, 1640, 387, 12, 44205, 43, 4133, 1640, 100, 12, 44205, 43, 19, 1640, 673, 43, 512, 1640, 387, 12, 43106, 43, 1855, 9354, 1640, 100, 12, 43106, 43, 2, 2, 1322, 89, 143, 47194, 4133, 19, 512, 1855, 9354, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 18:05:47 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'gnuplot(B-algorithm) can(O) be(O) used(O) from(O) various(O) programming(B-language) languages(I-language) to(O) graph(O) data(B-field),(O) including(O) Perl(B-programming language) ((O) via(O) PDL(B-product) and(O) CPAN(B-product) packages(I-product) )(O),(O) Python(B-programming language) ((O) via(O) )(O).(O)', 'sentence2': 'gnuplot can be used from various programming languages to graph data , including Perl ( via PDL and CPAN packages ) , Python ( via ) .', 'label': 2.03, 'idx': 7296, 'input_ids': [0, 49022, 40776, 1640, 387, 12, 337, 47549, 43, 64, 1640, 673, 43, 28, 1640, 673, 43, 341, 1640, 673, 43, 31, 1640, 673, 43, 1337, 1640, 673, 43, 8326, 1640, 387, 12, 19527, 43, 11991, 1640, 100, 12, 19527, 43, 7, 1640, 673, 43, 20992, 1640, 673, 43, 414, 1640, 387, 12, 1399, 238, 1640, 673, 43, 217, 1640, 673, 43, 35939, 1640, 387, 12, 28644, 7059, 2777, 43, 41006, 673, 43, 1241, 1640, 673, 43, 11707, 574, 1640, 387, 12, 20565, 43, 8, 1640, 673, 43, 11565, 1889, 1640, 387, 12, 20565, 43, 8368, 1640, 100, 12, 20565, 43, 4839, 1640, 673, 238, 1640, 673, 43, 31886, 1640, 387, 12, 28644, 7059, 2777, 43, 41006, 673, 43, 1241, 1640, 673, 43, 4839, 1640, 673, 322, 1640, 673, 43, 2, 2, 49022, 40776, 64, 28, 341, 31, 1337, 8326, 11991, 7, 20992, 414, 2156, 217, 35939, 36, 1241, 11707, 574, 8, 11565, 1889, 8368, 4839, 2156, 31886, 36, 1241, 4839, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 18:05:47 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-project) MAC(I-project) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.14, 'idx': 1639, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 28258, 43, 19482, 1640, 100, 12, 28258, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/52284 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 18:05:47,789 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 18:05:47,990 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 18:05:47,996] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 18:05:47,996] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52284/52284 [00:12<00:00, 4078.56 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2760/2760 [00:00<00:00, 3544.41 examples/s]
[2025-03-10 18:06:02,510] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 18:06:02,513] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 18:06:02,513] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 18:06:02,528] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 18:06:02,528] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 18:06:02,528] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 18:06:02,528] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 18:06:02,529] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 18:06:02,529] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 18:06:02,529] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 18:06:04,892] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 18:06:04,893] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.66 GB         CA 1.66 GB         Max_CA 2 GB
[2025-03-10 18:06:04,893] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.12 GB, percent = 5.4%
[2025-03-10 18:06:05,473] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 18:06:05,618] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 18:06:05,619] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.99 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 18:06:05,619] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.06 GB, percent = 5.4%
[2025-03-10 18:06:05,619] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 18:06:05,765] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 18:06:05,766] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.32 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 18:06:05,766] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.06 GB, percent = 5.4%
[2025-03-10 18:06:05,769] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 18:06:05,769] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 18:06:05,769] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 18:06:05,769] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f717048dd60>
[2025-03-10 18:06:05,770] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 18:06:05,770] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f71704e08f0>
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 18:06:05,771] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 18:06:05,772] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 18:06:05,773] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 18:06:05,774] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 18:06:05,774] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 18:06:05,776 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 18:06:05,776 >>   Num examples = 52,284
[INFO|trainer.py:2416] 2025-03-10 18:06:05,776 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 18:06:05,776 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 18:06:05,776 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 18:06:05,776 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 18:06:05,776 >>   Total optimization steps = 65,360
[INFO|trainer.py:2423] 2025-03-10 18:06:05,777 >>   Number of trainable parameters = 355,360,769
{'loss': 1.0454, 'grad_norm': 12.41408634185791, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [06:17<4:07:45,  4.29it/s][INFO|trainer.py:930] 2025-03-10 18:12:23,304 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:12:23,308 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:12:23,308 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:12:23,308 >>   Batch size = 4
{'eval_loss': 1.0726101398468018, 'eval_mse': 1.0726433929541836, 'eval_pearson': 0.6744963279997783, 'eval_spearmanr': 0.681105844958068, 'eval_runtime': 5.219, 'eval_samples_per_second': 528.836, 'eval_steps_per_second': 66.105, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1634/65360 [06:22<4:07:45,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 18:12:28,830 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634
[INFO|configuration_utils.py:423] 2025-03-10 18:12:28,832 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:12:29,655 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:12:29,656 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:12:29,656 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/special_tokens_map.json
[2025-03-10 18:12:29,719] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1634 is about to be saved!
[2025-03-10 18:12:29,724] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt
[2025-03-10 18:12:29,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt...
[2025-03-10 18:12:30,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/mp_rank_00_model_states.pt.
[2025-03-10 18:12:30,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:12:34,616] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:12:34,617] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-1634/global_step1634/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:12:34,617] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1634 is ready now!
{'loss': 0.6302, 'grad_norm': 13.34764575958252, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [12:50<5:27:09,  3.16it/s][INFO|trainer.py:930] 2025-03-10 18:18:56,152 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:18:56,155 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:18:56,155 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:18:56,155 >>   Batch size = 4
{'eval_loss': 1.1026318073272705, 'eval_mse': 1.1024465639738068, 'eval_pearson': 0.7093784960537136, 'eval_spearmanr': 0.7159514317440256, 'eval_runtime': 5.2118, 'eval_samples_per_second': 529.564, 'eval_steps_per_second': 66.195, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 3268/65360 [12:55<5:27:09,  3.16it/s]
[INFO|trainer.py:3955] 2025-03-10 18:19:01,706 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268
[INFO|configuration_utils.py:423] 2025-03-10 18:19:01,708 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:19:02,646 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:19:02,647 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:19:02,647 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/special_tokens_map.json
[2025-03-10 18:19:02,727] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3268 is about to be saved!
[2025-03-10 18:19:02,732] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt
[2025-03-10 18:19:02,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt...
[2025-03-10 18:19:03,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/mp_rank_00_model_states.pt.
[2025-03-10 18:19:03,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:19:07,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:19:07,870] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-3268/global_step3268/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:19:07,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3268 is ready now!
{'loss': 0.447, 'grad_norm': 12.520995140075684, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [19:22<3:55:05,  4.29it/s][INFO|trainer.py:930] 2025-03-10 18:25:27,924 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:25:27,928 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:25:27,928 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:25:27,928 >>   Batch size = 4
{'eval_loss': 0.9069564342498779, 'eval_mse': 0.9070827856659889, 'eval_pearson': 0.7393418910100846, 'eval_spearmanr': 0.7481666107605399, 'eval_runtime': 5.2042, 'eval_samples_per_second': 530.341, 'eval_steps_per_second': 66.293, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 4902/65360 [19:27<3:55:05,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 18:25:33,451 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902
[INFO|configuration_utils.py:423] 2025-03-10 18:25:33,453 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:25:34,394 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:25:34,395 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:25:34,395 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/special_tokens_map.json
[2025-03-10 18:25:34,462] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4902 is about to be saved!
[2025-03-10 18:25:34,468] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt
[2025-03-10 18:25:34,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt...
[2025-03-10 18:25:35,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/mp_rank_00_model_states.pt.
[2025-03-10 18:25:35,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:25:39,274] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:25:39,275] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-4902/global_step4902/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:25:39,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4902 is ready now!
{'loss': 0.3038, 'grad_norm': 10.130203247070312, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [25:53<4:26:29,  3.68it/s][INFO|trainer.py:930] 2025-03-10 18:31:58,829 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:31:58,833 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:31:58,833 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:31:58,833 >>   Batch size = 4
{'eval_loss': 0.7733879685401917, 'eval_mse': 0.7733955878494443, 'eval_pearson': 0.7815474355781543, 'eval_spearmanr': 0.7908011858241806, 'eval_runtime': 5.2312, 'eval_samples_per_second': 527.608, 'eval_steps_per_second': 65.951, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 6536/65360 [25:58<4:26:29,  3.68it/s]
[INFO|trainer.py:3955] 2025-03-10 18:32:04,340 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536
[INFO|configuration_utils.py:423] 2025-03-10 18:32:04,343 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:32:05,031 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:32:05,032 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:32:05,033 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/special_tokens_map.json
[2025-03-10 18:32:05,090] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6536 is about to be saved!
[2025-03-10 18:32:05,096] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt
[2025-03-10 18:32:05,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt...
[2025-03-10 18:32:06,041] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/mp_rank_00_model_states.pt.
[2025-03-10 18:32:06,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:32:09,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:32:09,201] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-6536/global_step6536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:32:09,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6536 is ready now!
{'loss': 0.2069, 'grad_norm': 15.54611587524414, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [32:24<3:40:07,  4.33it/s][INFO|trainer.py:930] 2025-03-10 18:38:29,796 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:38:29,799 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:38:29,799 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:38:29,799 >>   Batch size = 4
{'eval_loss': 0.8119310736656189, 'eval_mse': 0.8117983513336251, 'eval_pearson': 0.8009169320535448, 'eval_spearmanr': 0.806975575837993, 'eval_runtime': 5.2105, 'eval_samples_per_second': 529.702, 'eval_steps_per_second': 66.213, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 8170/65360 [32:29<3:40:07,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 18:38:35,301 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170
[INFO|configuration_utils.py:423] 2025-03-10 18:38:35,303 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:38:36,209 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:38:36,210 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:38:36,210 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/special_tokens_map.json
[2025-03-10 18:38:36,268] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8170 is about to be saved!
[2025-03-10 18:38:36,274] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt
[2025-03-10 18:38:36,274] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt...
[2025-03-10 18:38:37,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/mp_rank_00_model_states.pt.
[2025-03-10 18:38:37,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:38:41,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:38:41,026] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-8170/global_step8170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:38:41,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8170 is ready now!
{'loss': 0.1375, 'grad_norm': 3.5264039039611816, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [38:55<3:34:20,  4.32it/s][INFO|trainer.py:930] 2025-03-10 18:45:00,875 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:45:00,881 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:45:00,881 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:45:00,881 >>   Batch size = 4
{'eval_loss': 0.8022018671035767, 'eval_mse': 0.8022262966503267, 'eval_pearson': 0.8085032484148242, 'eval_spearmanr': 0.817853650098072, 'eval_runtime': 5.2885, 'eval_samples_per_second': 521.89, 'eval_steps_per_second': 65.236, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 9804/65360 [39:00<3:34:20,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 18:45:06,398 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804
[INFO|configuration_utils.py:423] 2025-03-10 18:45:06,400 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:45:07,116 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:45:07,117 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:45:07,118 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/special_tokens_map.json
[2025-03-10 18:45:07,173] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9804 is about to be saved!
[2025-03-10 18:45:07,178] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt
[2025-03-10 18:45:07,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt...
[2025-03-10 18:45:08,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/mp_rank_00_model_states.pt.
[2025-03-10 18:45:08,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:45:11,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:45:11,623] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-9804/global_step9804/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:45:11,623] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9804 is ready now!
{'loss': 0.0998, 'grad_norm': 5.9462504386901855, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [45:23<3:48:21,  3.94it/s][INFO|trainer.py:930] 2025-03-10 18:51:29,526 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:51:29,530 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:51:29,530 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:51:29,530 >>   Batch size = 4
{'eval_loss': 0.970025897026062, 'eval_mse': 0.9702044550275457, 'eval_pearson': 0.7783921960668899, 'eval_spearmanr': 0.7857170499669668, 'eval_runtime': 5.2068, 'eval_samples_per_second': 530.077, 'eval_steps_per_second': 66.26, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                        | 11438/65360 [45:28<3:48:21,  3.94it/s]
[INFO|trainer.py:3955] 2025-03-10 18:51:35,018 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438
[INFO|configuration_utils.py:423] 2025-03-10 18:51:35,020 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:51:35,807 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:51:35,808 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:51:35,808 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/special_tokens_map.json
[2025-03-10 18:51:35,863] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11438 is about to be saved!
[2025-03-10 18:51:35,868] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt
[2025-03-10 18:51:35,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt...
[2025-03-10 18:51:36,883] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/mp_rank_00_model_states.pt.
[2025-03-10 18:51:36,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:51:40,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:51:40,095] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-11438/global_step11438/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:51:40,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11438 is ready now!
{'loss': 0.0739, 'grad_norm': 5.0086870193481445, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [51:53<3:59:21,  3.64it/s][INFO|trainer.py:930] 2025-03-10 18:57:58,841 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:57:58,844 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:57:58,844 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 18:57:58,844 >>   Batch size = 4
{'eval_loss': 1.0172042846679688, 'eval_mse': 1.017027547588383, 'eval_pearson': 0.7600405145445691, 'eval_spearmanr': 0.773106746195774, 'eval_runtime': 5.2564, 'eval_samples_per_second': 525.077, 'eval_steps_per_second': 65.635, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                | 13072/65360 [51:58<3:59:21,  3.64it/s]
[INFO|trainer.py:3955] 2025-03-10 18:58:04,341 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072
[INFO|configuration_utils.py:423] 2025-03-10 18:58:04,344 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:58:05,101 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:58:05,102 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:58:05,103 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/special_tokens_map.json
[2025-03-10 18:58:05,164] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13072 is about to be saved!
[2025-03-10 18:58:05,169] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt
[2025-03-10 18:58:05,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt...
[2025-03-10 18:58:06,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/mp_rank_00_model_states.pt.
[2025-03-10 18:58:06,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:58:09,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:58:09,418] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-13072/global_step13072/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:58:09,418] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13072 is ready now!
{'loss': 0.0621, 'grad_norm': 10.883333206176758, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [58:22<4:26:23,  3.17it/s][INFO|trainer.py:930] 2025-03-10 19:04:27,946 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:04:27,949 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:04:27,950 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:04:27,950 >>   Batch size = 4
{'eval_loss': 0.750843346118927, 'eval_mse': 0.7510554517740788, 'eval_pearson': 0.8014917952203856, 'eval_spearmanr': 0.8133171323457739, 'eval_runtime': 5.2075, 'eval_samples_per_second': 530.001, 'eval_steps_per_second': 66.25, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 14706/65360 [58:27<4:26:23,  3.17it/s]
[INFO|trainer.py:3955] 2025-03-10 19:04:33,470 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706
[INFO|configuration_utils.py:423] 2025-03-10 19:04:33,472 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:04:34,344 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:04:34,345 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:04:34,345 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/special_tokens_map.json
[2025-03-10 19:04:34,413] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14706 is about to be saved!
[2025-03-10 19:04:34,418] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt
[2025-03-10 19:04:34,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt...
[2025-03-10 19:04:35,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/mp_rank_00_model_states.pt.
[2025-03-10 19:04:35,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:04:38,687] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:04:38,687] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-14706/global_step14706/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:04:38,687] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14706 is ready now!
{'loss': 0.0522, 'grad_norm': 5.83598518371582, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                             | 16340/65360 [1:04:51<3:12:34,  4.24it/s][INFO|trainer.py:930] 2025-03-10 19:10:57,721 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:10:57,725 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:10:57,725 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:10:57,725 >>   Batch size = 4
{'eval_loss': 0.9481201171875, 'eval_mse': 0.9478140891678092, 'eval_pearson': 0.7711203050030315, 'eval_spearmanr': 0.7816114128092193, 'eval_runtime': 5.2449, 'eval_samples_per_second': 526.225, 'eval_steps_per_second': 65.778, 'epoch': 10.0}
 25%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                             | 16340/65360 [1:04:57<3:12:34,  4.24it/s]
[INFO|trainer.py:3955] 2025-03-10 19:11:03,261 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340
[INFO|configuration_utils.py:423] 2025-03-10 19:11:03,263 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:11:04,004 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:11:04,005 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:11:04,005 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/special_tokens_map.json
[2025-03-10 19:11:04,062] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16340 is about to be saved!
[2025-03-10 19:11:04,067] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt
[2025-03-10 19:11:04,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt...
[2025-03-10 19:11:05,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/mp_rank_00_model_states.pt.
[2025-03-10 19:11:05,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:11:08,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:11:08,300] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-16340/global_step16340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:11:08,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16340 is ready now!
{'loss': 0.0428, 'grad_norm': 2.457312822341919, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                     | 17974/65360 [1:11:20<3:33:04,  3.71it/s][INFO|trainer.py:930] 2025-03-10 19:17:26,377 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:17:26,381 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:17:26,381 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:17:26,381 >>   Batch size = 4
{'eval_loss': 0.8033397793769836, 'eval_mse': 0.8034758335654286, 'eval_pearson': 0.8002999051951145, 'eval_spearmanr': 0.8076394366480325, 'eval_runtime': 5.3494, 'eval_samples_per_second': 515.948, 'eval_steps_per_second': 64.493, 'epoch': 11.0}
 28%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                     | 17974/65360 [1:11:25<3:33:04,  3.71it/s]
[INFO|trainer.py:3955] 2025-03-10 19:17:32,057 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974
[INFO|configuration_utils.py:423] 2025-03-10 19:17:32,060 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:17:32,873 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:17:32,874 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:17:32,874 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/special_tokens_map.json
[2025-03-10 19:17:32,930] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17974 is about to be saved!
[2025-03-10 19:17:32,936] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt
[2025-03-10 19:17:32,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt...
[2025-03-10 19:17:34,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/mp_rank_00_model_states.pt.
[2025-03-10 19:17:34,132] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:17:37,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:17:37,431] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-17974/global_step17974/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:17:37,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17974 is ready now!
{'loss': 0.0359, 'grad_norm': 1.771141767501831, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                            | 19608/65360 [1:17:47<2:56:10,  4.33it/s][INFO|trainer.py:930] 2025-03-10 19:23:52,952 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:23:52,956 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:23:52,956 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:23:52,956 >>   Batch size = 4
{'eval_loss': 0.7929855585098267, 'eval_mse': 0.792857616815878, 'eval_pearson': 0.7989655998670901, 'eval_spearmanr': 0.808119430379021, 'eval_runtime': 5.2125, 'eval_samples_per_second': 529.501, 'eval_steps_per_second': 66.188, 'epoch': 12.0}
 30%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                            | 19608/65360 [1:17:52<2:56:10,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 19:23:58,452 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608
[INFO|configuration_utils.py:423] 2025-03-10 19:23:58,454 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:23:59,199 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:23:59,200 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:23:59,200 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/special_tokens_map.json
[2025-03-10 19:23:59,257] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19608 is about to be saved!
[2025-03-10 19:23:59,262] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt
[2025-03-10 19:23:59,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt...
[2025-03-10 19:24:00,261] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/mp_rank_00_model_states.pt.
[2025-03-10 19:24:00,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:24:03,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:24:03,512] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-19608/global_step19608/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:24:03,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19608 is ready now!
{'loss': 0.0354, 'grad_norm': 1.6256663799285889, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                    | 21242/65360 [1:24:13<3:00:00,  4.08it/s][INFO|trainer.py:930] 2025-03-10 19:30:19,370 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:30:19,373 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:30:19,373 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:30:19,373 >>   Batch size = 4
{'eval_loss': 0.7876620292663574, 'eval_mse': 0.7876513188947802, 'eval_pearson': 0.8104714653761975, 'eval_spearmanr': 0.8186946887645805, 'eval_runtime': 5.2625, 'eval_samples_per_second': 524.465, 'eval_steps_per_second': 65.558, 'epoch': 13.0}
 32%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                    | 21242/65360 [1:24:18<3:00:00,  4.08it/s]
[INFO|trainer.py:3955] 2025-03-10 19:30:24,937 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242
[INFO|configuration_utils.py:423] 2025-03-10 19:30:24,939 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:30:25,749 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:30:25,750 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:30:25,750 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/special_tokens_map.json
[2025-03-10 19:30:25,811] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21242 is about to be saved!
[2025-03-10 19:30:25,817] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt
[2025-03-10 19:30:25,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt...
[2025-03-10 19:30:26,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/mp_rank_00_model_states.pt.
[2025-03-10 19:30:26,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:30:30,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:30:30,171] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-21242/global_step21242/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:30:30,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21242 is ready now!
{'loss': 0.0327, 'grad_norm': 3.9629628658294678, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                           | 22876/65360 [1:30:40<3:21:26,  3.51it/s][INFO|trainer.py:930] 2025-03-10 19:36:46,470 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:36:46,473 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:36:46,473 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:36:46,473 >>   Batch size = 4
{'eval_loss': 0.7981303930282593, 'eval_mse': 0.798216103800181, 'eval_pearson': 0.811265759960152, 'eval_spearmanr': 0.8222374183150831, 'eval_runtime': 5.2055, 'eval_samples_per_second': 530.21, 'eval_steps_per_second': 66.276, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                           | 22876/65360 [1:30:45<3:21:26,  3.51it/s]
[INFO|trainer.py:3955] 2025-03-10 19:36:51,976 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876
[INFO|configuration_utils.py:423] 2025-03-10 19:36:51,978 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:36:52,774 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:36:52,775 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:36:52,775 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/special_tokens_map.json
[2025-03-10 19:36:52,833] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22876 is about to be saved!
[2025-03-10 19:36:52,839] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt
[2025-03-10 19:36:52,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt...
[2025-03-10 19:36:53,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/mp_rank_00_model_states.pt.
[2025-03-10 19:36:53,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:36:57,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:36:57,271] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-22876/global_step22876/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:36:57,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22876 is ready now!
{'loss': 0.0287, 'grad_norm': 6.103127956390381, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                   | 24510/65360 [1:37:08<2:36:50,  4.34it/s][INFO|trainer.py:930] 2025-03-10 19:43:14,308 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:43:14,312 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:43:14,312 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:43:14,312 >>   Batch size = 4
{'eval_loss': 0.8191078901290894, 'eval_mse': 0.818972399290921, 'eval_pearson': 0.7990101338060105, 'eval_spearmanr': 0.8117598320258901, 'eval_runtime': 5.212, 'eval_samples_per_second': 529.551, 'eval_steps_per_second': 66.194, 'epoch': 15.0}
 38%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                   | 24510/65360 [1:37:13<2:36:50,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 19:43:19,791 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510
[INFO|configuration_utils.py:423] 2025-03-10 19:43:19,793 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:43:20,560 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:43:20,561 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:43:20,561 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/special_tokens_map.json
[2025-03-10 19:43:20,614] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24510 is about to be saved!
[2025-03-10 19:43:20,620] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt
[2025-03-10 19:43:20,620] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt...
[2025-03-10 19:43:21,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/mp_rank_00_model_states.pt.
[2025-03-10 19:43:21,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:43:24,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:43:24,970] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-24510/global_step24510/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:43:24,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24510 is ready now!
{'loss': 0.0268, 'grad_norm': 1.4843429327011108, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                          | 26144/65360 [1:43:33<2:31:20,  4.32it/s][INFO|trainer.py:930] 2025-03-10 19:49:39,449 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:49:39,452 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:49:39,452 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:49:39,452 >>   Batch size = 4
{'eval_loss': 0.9833032488822937, 'eval_mse': 0.9834348024881404, 'eval_pearson': 0.7477765328967929, 'eval_spearmanr': 0.7624931315662693, 'eval_runtime': 5.2075, 'eval_samples_per_second': 530.008, 'eval_steps_per_second': 66.251, 'epoch': 16.0}
 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                          | 26144/65360 [1:43:38<2:31:20,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 19:49:45,020 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144
[INFO|configuration_utils.py:423] 2025-03-10 19:49:45,022 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:49:45,816 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:49:45,817 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:49:45,817 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/special_tokens_map.json
[2025-03-10 19:49:45,869] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26144 is about to be saved!
[2025-03-10 19:49:45,874] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt
[2025-03-10 19:49:45,874] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt...
[2025-03-10 19:49:46,891] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/mp_rank_00_model_states.pt.
[2025-03-10 19:49:46,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:49:50,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:49:50,287] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-26144/global_step26144/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:49:50,287] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26144 is ready now!
{'loss': 0.0231, 'grad_norm': 3.0728704929351807, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                  | 27778/65360 [1:50:01<3:15:58,  3.20it/s][INFO|trainer.py:930] 2025-03-10 19:56:07,202 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:56:07,206 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:56:07,206 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 19:56:07,206 >>   Batch size = 4
{'eval_loss': 0.8238512873649597, 'eval_mse': 0.8237094117463499, 'eval_pearson': 0.7957864709273244, 'eval_spearmanr': 0.8084299664085427, 'eval_runtime': 5.2085, 'eval_samples_per_second': 529.898, 'eval_steps_per_second': 66.237, 'epoch': 17.0}
 42%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                  | 27778/65360 [1:50:06<3:15:58,  3.20it/s]
[INFO|trainer.py:3955] 2025-03-10 19:56:12,670 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778
[INFO|configuration_utils.py:423] 2025-03-10 19:56:12,673 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:56:13,467 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:56:13,468 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:56:13,468 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/special_tokens_map.json
[2025-03-10 19:56:13,523] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27778 is about to be saved!
[2025-03-10 19:56:13,528] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt
[2025-03-10 19:56:13,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt...
[2025-03-10 19:56:14,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/mp_rank_00_model_states.pt.
[2025-03-10 19:56:14,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:56:17,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:56:17,820] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-27778/global_step27778/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:56:17,820] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27778 is ready now!
{'loss': 0.0202, 'grad_norm': 2.2730839252471924, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                          | 29412/65360 [1:56:25<2:19:37,  4.29it/s][INFO|trainer.py:930] 2025-03-10 20:02:31,779 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:02:31,782 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:02:31,782 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:02:31,782 >>   Batch size = 4
{'eval_loss': 0.9153714776039124, 'eval_mse': 0.9152234383467315, 'eval_pearson': 0.7734688299791799, 'eval_spearmanr': 0.7796414721717286, 'eval_runtime': 5.2073, 'eval_samples_per_second': 530.03, 'eval_steps_per_second': 66.254, 'epoch': 18.0}
 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                          | 29412/65360 [1:56:31<2:19:37,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 20:02:37,246 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412
[INFO|configuration_utils.py:423] 2025-03-10 20:02:37,248 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:02:37,972 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:02:37,973 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:02:37,973 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/special_tokens_map.json
[2025-03-10 20:02:38,025] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29412 is about to be saved!
[2025-03-10 20:02:38,030] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt
[2025-03-10 20:02:38,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt...
[2025-03-10 20:02:39,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/mp_rank_00_model_states.pt.
[2025-03-10 20:02:39,034] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:02:42,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:02:42,245] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-29412/global_step29412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:02:42,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29412 is ready now!
{'loss': 0.0203, 'grad_norm': 0.6250602006912231, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [2:02:51<2:12:12,  4.33it/s][INFO|trainer.py:930] 2025-03-10 20:08:57,114 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:08:57,118 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:08:57,118 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:08:57,118 >>   Batch size = 4
{'eval_loss': 0.931530773639679, 'eval_mse': 0.9314453354996184, 'eval_pearson': 0.7811043749030948, 'eval_spearmanr': 0.7943276804926942, 'eval_runtime': 5.2064, 'eval_samples_per_second': 530.115, 'eval_steps_per_second': 66.264, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 31046/65360 [2:02:56<2:12:12,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 20:09:02,584 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046
[INFO|configuration_utils.py:423] 2025-03-10 20:09:02,586 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:09:03,359 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:09:03,360 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:09:03,360 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/special_tokens_map.json
[2025-03-10 20:09:03,411] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31046 is about to be saved!
[2025-03-10 20:09:03,417] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt
[2025-03-10 20:09:03,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt...
[2025-03-10 20:09:04,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/mp_rank_00_model_states.pt.
[2025-03-10 20:09:04,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:09:07,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:09:07,544] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-31046/global_step31046/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:09:07,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31046 is ready now!
{'loss': 0.0184, 'grad_norm': 3.71919322013855, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [2:09:17<2:06:49,  4.29it/s][INFO|trainer.py:930] 2025-03-10 20:15:22,828 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:15:22,831 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:15:22,831 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:15:22,832 >>   Batch size = 4
{'eval_loss': 0.871942937374115, 'eval_mse': 0.8718726286205692, 'eval_pearson': 0.7719512714437136, 'eval_spearmanr': 0.7838087843293826, 'eval_runtime': 5.2087, 'eval_samples_per_second': 529.881, 'eval_steps_per_second': 66.235, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 32680/65360 [2:09:22<2:06:49,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 20:15:28,302 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680
[INFO|configuration_utils.py:423] 2025-03-10 20:15:28,304 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:15:29,031 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:15:29,032 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:15:29,032 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/special_tokens_map.json
[2025-03-10 20:15:29,085] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32680 is about to be saved!
[2025-03-10 20:15:29,091] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt
[2025-03-10 20:15:29,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt...
[2025-03-10 20:15:30,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/mp_rank_00_model_states.pt.
[2025-03-10 20:15:30,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:15:33,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:15:33,242] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-32680/global_step32680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:15:33,242] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32680 is ready now!
{'loss': 0.0176, 'grad_norm': 1.7462836503982544, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 34314/65360 [2:15:46<1:59:42,  4.32it/s][INFO|trainer.py:930] 2025-03-10 20:21:52,180 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:21:52,183 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:21:52,183 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:21:52,183 >>   Batch size = 4
{'eval_loss': 0.8848859071731567, 'eval_mse': 0.8846895984333495, 'eval_pearson': 0.7853448633528438, 'eval_spearmanr': 0.7968588471792407, 'eval_runtime': 5.2196, 'eval_samples_per_second': 528.773, 'eval_steps_per_second': 66.097, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 34314/65360 [2:15:51<1:59:42,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 20:21:57,662 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314
[INFO|configuration_utils.py:423] 2025-03-10 20:21:57,664 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:21:58,330 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:21:58,331 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:21:58,331 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/special_tokens_map.json
[2025-03-10 20:21:58,388] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34314 is about to be saved!
[2025-03-10 20:21:58,393] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt
[2025-03-10 20:21:58,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt...
[2025-03-10 20:21:59,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/mp_rank_00_model_states.pt.
[2025-03-10 20:21:59,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:22:02,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:22:02,355] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-34314/global_step34314/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:22:02,355] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34314 is ready now!
{'loss': 0.0155, 'grad_norm': 1.2722935676574707, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                        | 35948/65360 [2:22:16<1:54:28,  4.28it/s][INFO|trainer.py:930] 2025-03-10 20:28:22,181 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:28:22,184 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:28:22,184 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:28:22,185 >>   Batch size = 4
{'eval_loss': 0.8630568385124207, 'eval_mse': 0.8629752902880959, 'eval_pearson': 0.7932310360643646, 'eval_spearmanr': 0.8086636972300505, 'eval_runtime': 5.254, 'eval_samples_per_second': 525.314, 'eval_steps_per_second': 65.664, 'epoch': 22.0}
 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                        | 35948/65360 [2:22:21<1:54:28,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 20:28:27,648 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948
[INFO|configuration_utils.py:423] 2025-03-10 20:28:27,650 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:28:28,331 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:28:28,332 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:28:28,332 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/special_tokens_map.json
[2025-03-10 20:28:28,388] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35948 is about to be saved!
[2025-03-10 20:28:28,393] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt
[2025-03-10 20:28:28,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt...
[2025-03-10 20:28:29,327] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/mp_rank_00_model_states.pt.
[2025-03-10 20:28:29,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:28:32,302] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:28:32,303] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-35948/global_step35948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:28:32,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35948 is ready now!
{'loss': 0.0146, 'grad_norm': 0.9298334121704102, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                               | 37582/65360 [2:28:40<1:46:48,  4.33it/s][INFO|trainer.py:930] 2025-03-10 20:34:46,683 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:34:46,686 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:34:46,686 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:34:46,686 >>   Batch size = 4
{'eval_loss': 0.874401867389679, 'eval_mse': 0.8743166385569434, 'eval_pearson': 0.7811338595579994, 'eval_spearmanr': 0.7942368247508104, 'eval_runtime': 5.2028, 'eval_samples_per_second': 530.484, 'eval_steps_per_second': 66.31, 'epoch': 23.0}
 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                               | 37582/65360 [2:28:46<1:46:48,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 20:34:52,147 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582
[INFO|configuration_utils.py:423] 2025-03-10 20:34:52,149 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:34:52,843 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:34:52,844 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:34:52,844 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/special_tokens_map.json
[2025-03-10 20:34:52,899] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step37582 is about to be saved!
[2025-03-10 20:34:52,905] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt
[2025-03-10 20:34:52,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt...
[2025-03-10 20:34:53,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/mp_rank_00_model_states.pt.
[2025-03-10 20:34:53,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:34:56,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:34:56,852] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-37582/global_step37582/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:34:56,852] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step37582 is ready now!
{'loss': 0.0134, 'grad_norm': 0.6007909178733826, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 39216/65360 [2:35:09<1:42:34,  4.25it/s][INFO|trainer.py:930] 2025-03-10 20:41:14,835 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:41:14,839 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:41:14,839 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:41:14,839 >>   Batch size = 4
{'eval_loss': 0.8716843724250793, 'eval_mse': 0.8720625879324001, 'eval_pearson': 0.7847985715810042, 'eval_spearmanr': 0.8001812902538217, 'eval_runtime': 5.2089, 'eval_samples_per_second': 529.866, 'eval_steps_per_second': 66.233, 'epoch': 24.0}
 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 39216/65360 [2:35:14<1:42:34,  4.25it/s]
[INFO|trainer.py:3955] 2025-03-10 20:41:20,306 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216
[INFO|configuration_utils.py:423] 2025-03-10 20:41:20,308 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:41:21,039 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:41:21,040 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:41:21,040 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/special_tokens_map.json
[2025-03-10 20:41:21,095] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39216 is about to be saved!
[2025-03-10 20:41:21,100] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt
[2025-03-10 20:41:21,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt...
[2025-03-10 20:41:22,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/mp_rank_00_model_states.pt.
[2025-03-10 20:41:22,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:41:25,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:41:25,294] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-39216/global_step39216/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:41:25,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39216 is ready now!
{'loss': 0.0134, 'grad_norm': 1.1593129634857178, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                              | 40850/65360 [2:41:39<2:08:22,  3.18it/s][INFO|trainer.py:930] 2025-03-10 20:47:45,127 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:47:45,130 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:47:45,131 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:47:45,131 >>   Batch size = 4
{'eval_loss': 0.9404685497283936, 'eval_mse': 0.940194514285827, 'eval_pearson': 0.7697609547092801, 'eval_spearmanr': 0.7823238937903185, 'eval_runtime': 5.2029, 'eval_samples_per_second': 530.473, 'eval_steps_per_second': 66.309, 'epoch': 25.0}
 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                              | 40850/65360 [2:41:44<2:08:22,  3.18it/s]
[INFO|trainer.py:3955] 2025-03-10 20:47:50,591 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850
[INFO|configuration_utils.py:423] 2025-03-10 20:47:50,593 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:47:51,347 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:47:51,348 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:47:51,349 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/special_tokens_map.json
[2025-03-10 20:47:51,405] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40850 is about to be saved!
[2025-03-10 20:47:51,410] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt
[2025-03-10 20:47:51,411] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt...
[2025-03-10 20:47:52,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/mp_rank_00_model_states.pt.
[2025-03-10 20:47:52,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:47:55,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:47:55,585] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-40850/global_step40850/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:47:55,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40850 is ready now!
{'loss': 0.0139, 'grad_norm': 2.012263298034668, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 42484/65360 [2:48:03<1:29:42,  4.25it/s][INFO|trainer.py:930] 2025-03-10 20:54:09,452 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:54:09,455 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:54:09,455 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 20:54:09,456 >>   Batch size = 4
{'eval_loss': 1.0384547710418701, 'eval_mse': 1.0387225453620372, 'eval_pearson': 0.7643332747281935, 'eval_spearmanr': 0.7741754615852465, 'eval_runtime': 5.2074, 'eval_samples_per_second': 530.012, 'eval_steps_per_second': 66.252, 'epoch': 26.0}
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 42484/65360 [2:48:08<1:29:42,  4.25it/s]
[INFO|trainer.py:3955] 2025-03-10 20:54:14,925 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484
[INFO|configuration_utils.py:423] 2025-03-10 20:54:14,927 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:54:15,596 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:54:15,597 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:54:15,597 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/special_tokens_map.json
[2025-03-10 20:54:15,654] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42484 is about to be saved!
[2025-03-10 20:54:15,659] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt
[2025-03-10 20:54:15,660] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt...
[2025-03-10 20:54:16,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/mp_rank_00_model_states.pt.
[2025-03-10 20:54:16,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:54:19,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:54:19,864] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-42484/global_step42484/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:54:19,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42484 is ready now!
{'loss': 0.0115, 'grad_norm': 1.2174646854400635, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 44118/65360 [2:54:28<1:38:01,  3.61it/s][INFO|trainer.py:930] 2025-03-10 21:00:33,937 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:00:33,940 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:00:33,940 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:00:33,940 >>   Batch size = 4
{'eval_loss': 0.8942623138427734, 'eval_mse': 0.894371179210535, 'eval_pearson': 0.7835339454209126, 'eval_spearmanr': 0.7979771076852209, 'eval_runtime': 5.2053, 'eval_samples_per_second': 530.234, 'eval_steps_per_second': 66.279, 'epoch': 27.0}
 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 44118/65360 [2:54:33<1:38:01,  3.61it/s]
[INFO|trainer.py:3955] 2025-03-10 21:00:39,407 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118
[INFO|configuration_utils.py:423] 2025-03-10 21:00:39,409 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:00:40,099 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:00:40,100 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:00:40,100 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/special_tokens_map.json
[2025-03-10 21:00:40,157] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44118 is about to be saved!
[2025-03-10 21:00:40,163] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt
[2025-03-10 21:00:40,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt...
[2025-03-10 21:00:41,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/mp_rank_00_model_states.pt.
[2025-03-10 21:00:41,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:00:44,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:00:44,282] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-44118/global_step44118/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:00:44,282] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44118 is ready now!
{'loss': 0.011, 'grad_norm': 0.4388681650161743, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                     | 45752/65360 [3:00:51<1:15:28,  4.33it/s][INFO|trainer.py:930] 2025-03-10 21:06:57,565 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:06:57,568 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:06:57,568 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:06:57,568 >>   Batch size = 4
{'eval_loss': 0.9566095471382141, 'eval_mse': 0.9565550353432047, 'eval_pearson': 0.776311032857979, 'eval_spearmanr': 0.7930020438364122, 'eval_runtime': 5.205, 'eval_samples_per_second': 530.256, 'eval_steps_per_second': 66.282, 'epoch': 28.0}
 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                     | 45752/65360 [3:00:56<1:15:28,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 21:07:03,035 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752
[INFO|configuration_utils.py:423] 2025-03-10 21:07:03,037 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:07:03,735 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:07:03,736 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:07:03,736 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/special_tokens_map.json
[2025-03-10 21:07:03,793] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45752 is about to be saved!
[2025-03-10 21:07:03,798] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt
[2025-03-10 21:07:03,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt...
[2025-03-10 21:07:04,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/mp_rank_00_model_states.pt.
[2025-03-10 21:07:04,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:07:07,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:07:07,994] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-45752/global_step45752/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:07:07,994] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45752 is ready now!
{'loss': 0.0123, 'grad_norm': 0.9581620097160339, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                             | 47386/65360 [3:07:16<1:31:00,  3.29it/s][INFO|trainer.py:930] 2025-03-10 21:13:22,203 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:13:22,207 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:13:22,207 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:13:22,207 >>   Batch size = 4
{'eval_loss': 1.045003056526184, 'eval_mse': 1.044841150291588, 'eval_pearson': 0.7525491802887518, 'eval_spearmanr': 0.767801497878749, 'eval_runtime': 5.2054, 'eval_samples_per_second': 530.218, 'eval_steps_per_second': 66.277, 'epoch': 29.0}
 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                             | 47386/65360 [3:07:21<1:31:00,  3.29it/s]
[INFO|trainer.py:3955] 2025-03-10 21:13:27,672 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386
[INFO|configuration_utils.py:423] 2025-03-10 21:13:27,674 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:13:28,379 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:13:28,380 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:13:28,380 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/special_tokens_map.json
[2025-03-10 21:13:28,436] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47386 is about to be saved!
[2025-03-10 21:13:28,442] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt
[2025-03-10 21:13:28,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt...
[2025-03-10 21:13:29,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/mp_rank_00_model_states.pt.
[2025-03-10 21:13:29,398] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:13:32,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:13:32,733] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-47386/global_step47386/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:13:32,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47386 is ready now!
{'loss': 0.0113, 'grad_norm': 1.4519456624984741, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 49020/65360 [3:13:40<1:03:34,  4.28it/s][INFO|trainer.py:930] 2025-03-10 21:19:46,695 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:19:46,698 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:19:46,698 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:19:46,698 >>   Batch size = 4
{'eval_loss': 0.8189466595649719, 'eval_mse': 0.8186863371643467, 'eval_pearson': 0.7884109151173131, 'eval_spearmanr': 0.8034037696323374, 'eval_runtime': 5.2569, 'eval_samples_per_second': 525.029, 'eval_steps_per_second': 65.629, 'epoch': 30.0}
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 49020/65360 [3:13:46<1:03:34,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 21:19:52,167 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020
[INFO|configuration_utils.py:423] 2025-03-10 21:19:52,169 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:19:52,847 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:19:52,848 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:19:52,848 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/special_tokens_map.json
[2025-03-10 21:19:52,905] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49020 is about to be saved!
[2025-03-10 21:19:52,910] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt
[2025-03-10 21:19:52,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt...
[2025-03-10 21:19:53,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/mp_rank_00_model_states.pt.
[2025-03-10 21:19:53,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:19:57,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:19:57,116] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-49020/global_step49020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:19:57,116] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49020 is ready now!
{'loss': 0.0116, 'grad_norm': 20.797237396240234, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [3:20:04<56:11,  4.36it/s][INFO|trainer.py:930] 2025-03-10 21:26:10,359 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:26:10,363 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:26:10,363 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:26:10,363 >>   Batch size = 4
{'eval_loss': 0.7780343294143677, 'eval_mse': 0.7780240762492885, 'eval_pearson': 0.8107644798926225, 'eval_spearmanr': 0.8235678488070755, 'eval_runtime': 5.2033, 'eval_samples_per_second': 530.432, 'eval_steps_per_second': 66.304, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 50654/65360 [3:20:09<56:11,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 21:26:15,827 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654
[INFO|configuration_utils.py:423] 2025-03-10 21:26:15,829 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:26:16,519 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:26:16,520 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:26:16,520 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/special_tokens_map.json
[2025-03-10 21:26:16,577] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step50654 is about to be saved!
[2025-03-10 21:26:16,583] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt
[2025-03-10 21:26:16,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt...
[2025-03-10 21:26:17,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/mp_rank_00_model_states.pt.
[2025-03-10 21:26:17,542] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:26:20,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:26:20,898] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-50654/global_step50654/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:26:20,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50654 is ready now!
{'loss': 0.0146, 'grad_norm': 28.56694221496582, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [3:26:29<50:19,  4.33it/s][INFO|trainer.py:930] 2025-03-10 21:32:35,102 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:32:35,105 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:32:35,106 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:32:35,106 >>   Batch size = 4
{'eval_loss': 0.8131228089332581, 'eval_mse': 0.813161463085292, 'eval_pearson': 0.7907577455180843, 'eval_spearmanr': 0.8057957607497829, 'eval_runtime': 5.203, 'eval_samples_per_second': 530.462, 'eval_steps_per_second': 66.308, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 52288/65360 [3:26:34<50:19,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 21:32:40,569 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288
[INFO|configuration_utils.py:423] 2025-03-10 21:32:40,572 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:32:41,335 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:32:41,336 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:32:41,336 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/special_tokens_map.json
[2025-03-10 21:32:41,393] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step52288 is about to be saved!
[2025-03-10 21:32:41,399] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt
[2025-03-10 21:32:41,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt...
[2025-03-10 21:32:42,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/mp_rank_00_model_states.pt.
[2025-03-10 21:32:42,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:32:45,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:32:45,514] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-52288/global_step52288/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:32:45,514] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step52288 is ready now!
{'loss': 0.0116, 'grad_norm': 0.2982230484485626, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [3:32:53<43:56,  4.34it/s][INFO|trainer.py:930] 2025-03-10 21:38:59,008 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:38:59,011 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:38:59,011 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:38:59,011 >>   Batch size = 4
{'eval_loss': 0.880082368850708, 'eval_mse': 0.8801747477788856, 'eval_pearson': 0.7829570676981875, 'eval_spearmanr': 0.7917250125168397, 'eval_runtime': 5.2015, 'eval_samples_per_second': 530.62, 'eval_steps_per_second': 66.327, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 53922/65360 [3:32:58<43:56,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 21:39:04,470 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922
[INFO|configuration_utils.py:423] 2025-03-10 21:39:04,473 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:39:05,173 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:39:05,174 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:39:05,174 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/special_tokens_map.json
[2025-03-10 21:39:05,231] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step53922 is about to be saved!
[2025-03-10 21:39:05,236] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt
[2025-03-10 21:39:05,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt...
[2025-03-10 21:39:06,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/mp_rank_00_model_states.pt.
[2025-03-10 21:39:06,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:39:09,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:39:09,475] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-53922/global_step53922/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:39:09,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step53922 is ready now!
{'loss': 0.0084, 'grad_norm': 1.651872992515564, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [3:39:17<46:19,  3.53it/s][INFO|trainer.py:930] 2025-03-10 21:45:22,934 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:45:22,937 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:45:22,937 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:45:22,938 >>   Batch size = 4
{'eval_loss': 0.937228262424469, 'eval_mse': 0.9372239417140035, 'eval_pearson': 0.7684368186750077, 'eval_spearmanr': 0.7788396569433774, 'eval_runtime': 5.2037, 'eval_samples_per_second': 530.396, 'eval_steps_per_second': 66.3, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 55556/65360 [3:39:22<46:19,  3.53it/s]
[INFO|trainer.py:3955] 2025-03-10 21:45:28,398 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556
[INFO|configuration_utils.py:423] 2025-03-10 21:45:28,401 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:45:29,078 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:45:29,079 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:45:29,079 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/special_tokens_map.json
[2025-03-10 21:45:29,134] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step55556 is about to be saved!
[2025-03-10 21:45:29,140] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt
[2025-03-10 21:45:29,140] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt...
[2025-03-10 21:45:30,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/mp_rank_00_model_states.pt.
[2025-03-10 21:45:30,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:45:33,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:45:33,133] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-55556/global_step55556/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:45:33,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55556 is ready now!
{'loss': 0.0132, 'grad_norm': 0.5836912989616394, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [3:45:42<31:22,  4.34it/s][INFO|trainer.py:930] 2025-03-10 21:51:47,950 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:51:47,954 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:51:47,954 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:51:47,954 >>   Batch size = 4
{'eval_loss': 0.8845210671424866, 'eval_mse': 0.8844004478359568, 'eval_pearson': 0.7848668191464765, 'eval_spearmanr': 0.7981893432093654, 'eval_runtime': 5.2014, 'eval_samples_per_second': 530.631, 'eval_steps_per_second': 66.329, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 57190/65360 [3:45:47<31:22,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 21:51:53,413 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190
[INFO|configuration_utils.py:423] 2025-03-10 21:51:53,415 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:51:54,161 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:51:54,162 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:51:54,162 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/special_tokens_map.json
[2025-03-10 21:51:54,216] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step57190 is about to be saved!
[2025-03-10 21:51:54,222] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt
[2025-03-10 21:51:54,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt...
[2025-03-10 21:51:55,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/mp_rank_00_model_states.pt.
[2025-03-10 21:51:55,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:51:58,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:51:58,275] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-57190/global_step57190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:51:58,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step57190 is ready now!
{'loss': 0.0118, 'grad_norm': 1.3441277742385864, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [3:52:06<24:57,  4.36it/s][INFO|trainer.py:930] 2025-03-10 21:58:12,191 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 21:58:12,195 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 21:58:12,195 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 21:58:12,195 >>   Batch size = 4
{'eval_loss': 0.875123143196106, 'eval_mse': 0.8749829803040062, 'eval_pearson': 0.7966336072239185, 'eval_spearmanr': 0.8110716525165611, 'eval_runtime': 5.1998, 'eval_samples_per_second': 530.789, 'eval_steps_per_second': 66.349, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 58824/65360 [3:52:11<24:57,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 21:58:17,653 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824
[INFO|configuration_utils.py:423] 2025-03-10 21:58:17,655 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 21:58:18,315 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 21:58:18,316 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 21:58:18,316 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/special_tokens_map.json
[2025-03-10 21:58:18,370] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step58824 is about to be saved!
[2025-03-10 21:58:18,376] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt
[2025-03-10 21:58:18,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt...
[2025-03-10 21:58:19,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/mp_rank_00_model_states.pt.
[2025-03-10 21:58:19,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 21:58:22,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 21:58:22,453] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-58824/global_step58824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 21:58:22,453] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step58824 is ready now!
{'loss': 0.0091, 'grad_norm': 0.32489562034606934, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [3:58:29<18:52,  4.33it/s][INFO|trainer.py:930] 2025-03-10 22:04:35,336 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 22:04:35,339 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 22:04:35,339 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 22:04:35,339 >>   Batch size = 4
{'eval_loss': 0.8742313385009766, 'eval_mse': 0.8739707802732786, 'eval_pearson': 0.7895425010940715, 'eval_spearmanr': 0.8029269914649115, 'eval_runtime': 5.2012, 'eval_samples_per_second': 530.646, 'eval_steps_per_second': 66.331, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 60458/65360 [3:58:34<18:52,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 22:04:40,796 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458
[INFO|configuration_utils.py:423] 2025-03-10 22:04:40,798 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 22:04:41,536 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 22:04:41,537 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 22:04:41,537 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/special_tokens_map.json
[2025-03-10 22:04:41,590] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step60458 is about to be saved!
[2025-03-10 22:04:41,596] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt
[2025-03-10 22:04:41,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt...
[2025-03-10 22:04:42,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/mp_rank_00_model_states.pt.
[2025-03-10 22:04:42,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 22:04:45,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 22:04:45,801] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-60458/global_step60458/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 22:04:45,801] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60458 is ready now!
{'loss': 0.012, 'grad_norm': 3.1568820476531982, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [4:04:53<12:33,  4.34it/s][INFO|trainer.py:930] 2025-03-10 22:10:59,472 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 22:10:59,475 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 22:10:59,475 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 22:10:59,476 >>   Batch size = 4
{'eval_loss': 0.853126049041748, 'eval_mse': 0.8533673666741537, 'eval_pearson': 0.7773210947005065, 'eval_spearmanr': 0.791685593647657, 'eval_runtime': 5.2064, 'eval_samples_per_second': 530.12, 'eval_steps_per_second': 66.265, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 62092/65360 [4:04:58<12:33,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 22:11:04,939 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092
[INFO|configuration_utils.py:423] 2025-03-10 22:11:04,941 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 22:11:05,614 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 22:11:05,615 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 22:11:05,615 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/special_tokens_map.json
[2025-03-10 22:11:05,670] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step62092 is about to be saved!
[2025-03-10 22:11:05,675] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt
[2025-03-10 22:11:05,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt...
[2025-03-10 22:11:06,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/mp_rank_00_model_states.pt.
[2025-03-10 22:11:06,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 22:11:09,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 22:11:09,740] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-62092/global_step62092/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 22:11:09,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step62092 is ready now!
{'loss': 0.0104, 'grad_norm': 0.614364504814148, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [4:11:17<06:14,  4.36it/s][INFO|trainer.py:930] 2025-03-10 22:17:23,022 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 22:17:23,026 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 22:17:23,026 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 22:17:23,026 >>   Batch size = 4
{'eval_loss': 0.9182944297790527, 'eval_mse': 0.9181712367612382, 'eval_pearson': 0.7756934260930882, 'eval_spearmanr': 0.7910949185735008, 'eval_runtime': 5.2638, 'eval_samples_per_second': 524.341, 'eval_steps_per_second': 65.543, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 63726/65360 [4:11:22<06:14,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 22:17:28,498 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726
[INFO|configuration_utils.py:423] 2025-03-10 22:17:28,500 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 22:17:29,179 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 22:17:29,180 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 22:17:29,180 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/special_tokens_map.json
[2025-03-10 22:17:29,235] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step63726 is about to be saved!
[2025-03-10 22:17:29,240] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt
[2025-03-10 22:17:29,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt...
[2025-03-10 22:17:30,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/mp_rank_00_model_states.pt.
[2025-03-10 22:17:30,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 22:17:33,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 22:17:33,388] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-63726/global_step63726/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 22:17:33,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step63726 is ready now!
{'loss': 0.0093, 'grad_norm': 0.3645329475402832, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [4:17:41<00:00,  4.33it/s][INFO|trainer.py:930] 2025-03-10 22:23:46,813 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 22:23:46,816 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 22:23:46,816 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 22:23:46,816 >>   Batch size = 4
{'eval_loss': 0.8513596057891846, 'eval_mse': 0.8514355423009914, 'eval_pearson': 0.7817871864188219, 'eval_spearmanr': 0.7984857390133996, 'eval_runtime': 5.2075, 'eval_samples_per_second': 530.005, 'eval_steps_per_second': 66.251, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [4:17:46<00:00,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 22:23:52,280 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360
[INFO|configuration_utils.py:423] 2025-03-10 22:23:52,283 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 22:23:52,986 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 22:23:52,987 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 22:23:52,987 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/special_tokens_map.json
[2025-03-10 22:23:53,035] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step65360 is about to be saved!
[2025-03-10 22:23:53,040] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt
[2025-03-10 22:23:53,041] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt...
[2025-03-10 22:23:54,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/mp_rank_00_model_states.pt.
[2025-03-10 22:23:54,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 22:23:57,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 22:23:57,135] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=40/checkpoint-65360/global_step65360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 22:23:57,135] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65360 is ready now!
[INFO|trainer.py:2670] 2025-03-10 22:23:57,211 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 15471.4344, 'train_samples_per_second': 135.176, 'train_steps_per_second': 4.225, 'train_loss': 0.08974286777444977, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65360/65360 [4:17:51<00:00,  4.22it/s]
[INFO|trainer.py:3955] 2025-03-10 22:23:57,341 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=40
[INFO|configuration_utils.py:423] 2025-03-10 22:23:57,343 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=40/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 22:23:58,060 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=40/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 22:23:58,060 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 22:23:58,061 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=40/special_tokens_map.json
***** train metrics *****
  epoch                    =         40.0
  total_flos               = 1815284160GF
  train_loss               =       0.0897
  train_runtime            =   4:17:51.43
  train_samples            =        52284
  train_samples_per_second =      135.176
  train_steps_per_second   =        4.225
03/10/2025 22:23:58 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 22:23:58,113 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 22:23:58,115 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 22:23:58,115 >>   Num examples = 2760
[INFO|trainer.py:4276] 2025-03-10 22:23:58,115 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 345/345 [00:05<00:00, 65.85it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =     0.8514
  eval_mse                =     0.8514
  eval_pearson            =     0.7818
  eval_runtime            = 0:00:05.25
  eval_samples            =       2760
  eval_samples_per_second =    524.866
  eval_spearmanr          =     0.7985
  eval_steps_per_second   =     65.608
[INFO|modelcard.py:449] 2025-03-10 22:24:03,608 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7984857390133996}]}
[rank0]:[W310 22:24:04.835627878 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 22:24:06,383] [INFO] [launch.py:351:main] Process 156412 exits successfully.
[2025-03-10 22:24:06,383] [INFO] [launch.py:351:main] Process 156413 exits successfully.
(GNER) chrisjihee@dgx-a100:~/proj/GNER$ [21~

