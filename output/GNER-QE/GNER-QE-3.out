(GNER) chrisjihee@dgx-a100:~/proj/GNER$ ./run-GNER-QE-3.sh
+ DEEPSPEED_CONFIG=configs/deepspeed/ds1_t5.json
++ shuf -i 25000-30000 -n 1
+ DEEPSPEED_PORT=27134
+ CUDA_DEVICES=4,5
+ SOURCE_FILE=run_glue.py
+ TRAIN_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json
+ VALID_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json
+ OUTPUT_NAME=GNER-QE
+ MODEL_NAMES=("google-bert/bert-base-cased" "FacebookAI/roberta-base" "FacebookAI/roberta-large")
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:4,5 --master_port 27134 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:43:14,543] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:17,539] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 13:43:17,541] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNV19 --master_addr=127.0.0.1 --master_port=27134 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:43:22,052] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:25,005] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [4, 5]}
[2025-03-10 13:43:25,005] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 13:43:25,005] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 13:43:25,005] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 13:43:25,005] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=4,5
[2025-03-10 13:43:25,005] [INFO] [launch.py:256:main] process 4023116 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:25,006] [INFO] [launch.py:256:main] process 4023117 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:29,833] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:29,891] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:30,660] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:43:30,742] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:43:30,742] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/10/2025 13:43:32 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:43:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:43:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/google-bert/bert-base-cased-num=30/runs/Mar10_13-43-29_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/google-bert/bert-base-cased-num=30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/google-bert/bert-base-cased-num=30,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 13:43:32 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json
03/10/2025 13:43:32 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json
Using custom data configuration default-6b90b90b9712ba2d
03/10/2025 13:43:33 - INFO - datasets.builder - Using custom data configuration default-6b90b90b9712ba2d
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 13:43:33 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Generating dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 13:43:33 - INFO - datasets.builder - Generating dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Downloading and preparing dataset json/default to /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...
03/10/2025 13:43:33 - INFO - datasets.builder - Downloading and preparing dataset json/default to /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...
Downloading took 0.0 min
03/10/2025 13:43:33 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
03/10/2025 13:43:33 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
03/10/2025 13:43:33 - INFO - datasets.builder - Generating train split
Generating train split: 39286 examples [00:00, 572603.12 examples/s]
Generating validation split
03/10/2025 13:43:33 - INFO - datasets.builder - Generating validation split
Generating validation split: 2074 examples [00:00, 420159.70 examples/s]
Unable to verify splits sizes.
03/10/2025 13:43:33 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.
03/10/2025 13:43:33 - INFO - datasets.builder - Dataset json downloaded and prepared to /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.
[INFO|configuration_utils.py:699] 2025-03-10 13:43:33,920 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:33,922 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:699] 2025-03-10 13:43:34,149 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:34,150 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file vocab.txt from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file tokenizer.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file tokenizer_config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:34,150 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 13:43:34,150 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:34,151 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:4128] 2025-03-10 13:43:34,212 >> loading weights file model.safetensors from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors
[INFO|logging.py:344] 2025-03-10 13:43:34,250 >> A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):
* `cls.predictions.transform.LayerNorm.beta` -> `bert.cls.predictions.transform.LayerNorm.bias`
* `cls.predictions.transform.LayerNorm.gamma` -> `bert.cls.predictions.transform.LayerNorm.weight`
If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.
[INFO|modeling_utils.py:5091] 2025-03-10 13:43:34,254 >> Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 13:43:34,254 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s][WARNING|modeling_utils.py:5103] 2025-03-10 13:43:34,274 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-83ad37c1f6c9ba6a.arrow
03/10/2025 13:43:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-83ad37c1f6c9ba6a.arrow
Running tokenizer on dataset:  18%|██████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                         | 7000/39286 [00:02<00:10, 3098.80 examples/s][rank1]:[W310 13:43:36.374630500 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:11<00:00, 3385.02 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2074 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b63395d0de0d42cb.arrow
03/10/2025 13:43:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b63395d0de0d42cb.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 3489.85 examples/s]
[rank0]:[W310 13:43:47.460686398 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 13:43:48 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'It(O) was(O) praised(O) by(O) scientists(O) such(O) as(O) Darwin(B-writer) ((O) to(O) whom(O) the(O) book(O) was(O) dedicated(O) )(O),(O) and(O) Charles(B-person) Lyell(I-person),(O) and(O) by(O) non-scientists(O) such(O) as(O) the(O) novelist(O) Joseph(B-person) Conrad(I-person),(O) who(O) called(O) it(O) his(O) favorite(O) bedside(O) companion(O) and(O) used(O) it(O) as(O) source(O) of(O) information(O) for(O) several(O) of(O) his(O) novels(O),(O) especially(O) Lord(B-book) Jim(I-book).(O)', 'sentence2': 'It was praised by scientists such as Darwin ( to whom the book was dedicated ) , and Charles Lyell , and by non-scientists such as the novelist Joseph Conrad , who called it his favorite bedside companion and used it as source of information for several of his novels , especially Lord Jim .', 'label': 1.03, 'idx': 7296, 'input_ids': [101, 1135, 113, 152, 114, 1108, 113, 152, 114, 5185, 113, 152, 114, 1118, 113, 152, 114, 6479, 113, 152, 114, 1216, 113, 152, 114, 1112, 113, 152, 114, 11555, 113, 139, 118, 2432, 114, 113, 113, 152, 114, 1106, 113, 152, 114, 2292, 113, 152, 114, 1103, 113, 152, 114, 1520, 113, 152, 114, 1108, 113, 152, 114, 3256, 113, 152, 114, 114, 113, 152, 114, 117, 113, 152, 114, 1105, 113, 152, 114, 1889, 113, 139, 118, 1825, 114, 149, 4980, 2339, 113, 146, 118, 1825, 114, 117, 113, 152, 114, 1105, 113, 152, 114, 1118, 113, 152, 114, 1664, 118, 6479, 113, 152, 114, 1216, 113, 152, 114, 1112, 113, 152, 114, 1103, 113, 152, 114, 10050, 113, 152, 114, 2419, 113, 139, 118, 1825, 114, 10904, 113, 146, 118, 1825, 114, 117, 113, 152, 114, 1150, 113, 152, 114, 1270, 113, 152, 114, 1122, 113, 152, 114, 1117, 113, 152, 114, 5095, 113, 152, 114, 21685, 113, 152, 114, 8573, 113, 152, 114, 1105, 113, 152, 114, 1215, 113, 152, 114, 1122, 113, 152, 114, 1112, 113, 152, 114, 2674, 113, 152, 114, 1104, 113, 152, 114, 1869, 113, 152, 114, 1111, 113, 152, 114, 1317, 113, 152, 114, 1104, 113, 152, 114, 1117, 113, 152, 114, 5520, 113, 152, 114, 117, 113, 152, 114, 2108, 113, 152, 114, 2188, 113, 139, 118, 1520, 114, 3104, 113, 146, 118, 1520, 114, 119, 113, 152, 114, 102, 1135, 1108, 5185, 1118, 6479, 1216, 1112, 11555, 113, 1106, 2292, 1103, 1520, 1108, 3256, 114, 117, 1105, 1889, 149, 4980, 2339, 117, 1105, 1118, 1664, 118, 6479, 1216, 1112, 1103, 10050, 2419, 10904, 117, 1150, 1270, 1122, 1117, 5095, 21685, 8573, 1105, 1215, 1122, 1112, 2674, 1104, 1869, 1111, 1317, 1104, 1117, 5520, 117, 2108, 2188, 3104, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:43:48 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'The(O) natural(B-metric) gradient(I-metric) of(O) mathE(B-algorithm) f(O) ((O) x(O) )(O) /(O) math(O),(O) complying(O) with(O) the(O) Fisher(B-metric) information(I-metric) metric(I-metric) ((O) an(O) informational(O) distance(O) measure(O) between(O) probability(O) distributions(O) and(O) the(O) curvature(O) of(O) the(O) relative(O) entropy(O) )(O),(O) now(O) reads(O)', 'sentence2': 'The natural gradient of mathE f ( x ) / math , complying with the Fisher information metric ( an informational distance measure between probability distributions and the curvature of the relative entropy ) , now reads', 'label': 1.48, 'idx': 1639, 'input_ids': [101, 1109, 113, 152, 114, 2379, 113, 139, 118, 12676, 114, 19848, 113, 146, 118, 12676, 114, 1104, 113, 152, 114, 12523, 2036, 113, 139, 118, 9932, 114, 175, 113, 152, 114, 113, 113, 152, 114, 193, 113, 152, 114, 114, 113, 152, 114, 120, 113, 152, 114, 12523, 113, 152, 114, 117, 113, 152, 114, 14616, 1158, 113, 152, 114, 1114, 113, 152, 114, 1103, 113, 152, 114, 8476, 113, 139, 118, 12676, 114, 1869, 113, 146, 118, 12676, 114, 12676, 113, 146, 118, 12676, 114, 113, 113, 152, 114, 1126, 113, 152, 114, 1869, 1348, 113, 152, 114, 2462, 113, 152, 114, 4929, 113, 152, 114, 1206, 113, 152, 114, 9750, 113, 152, 114, 23190, 113, 152, 114, 1105, 113, 152, 114, 1103, 113, 152, 114, 16408, 13461, 5332, 113, 152, 114, 1104, 113, 152, 114, 1103, 113, 152, 114, 5236, 113, 152, 114, 4035, 25444, 113, 152, 114, 114, 113, 152, 114, 117, 113, 152, 114, 1208, 113, 152, 114, 9568, 113, 152, 114, 102, 1109, 2379, 19848, 1104, 12523, 2036, 175, 113, 193, 114, 120, 12523, 117, 14616, 1158, 1114, 1103, 8476, 1869, 12676, 113, 1126, 1869, 1348, 2462, 4929, 1206, 9750, 23190, 1105, 1103, 16408, 13461, 5332, 1104, 1103, 5236, 4035, 25444, 114, 117, 1208, 9568, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:43:48 - INFO - __main__ - Sample 18024 of the training set: {'sentence1': 'It(O) was(O) set(O) up(O) by(O) former(O) left-leaning(O) Christian(B-political party) Democrats(I-political party),(O) ((O) former(O) Italian(O) Liberal(B-political party) Party(I-political party) and(O) former(O) Italian(B-political party) Republican(I-political party) Party(I-political party) )(O),(O) as(O) well(O) as(O) other(O) left-wing(O) politicians(O) from(O) the(O) former(O) Italian(O) Socialist(B-political party) Party(I-political party) and(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party).(O)', 'sentence2': 'It was set up by former left-leaning Christian Democrats , ( former Italian Liberal Party and former Italian Republican Party ) , as well as other left-wing politicians from the former Italian Socialist Party and Federation of the Greens .', 'label': 1.64, 'idx': 18024, 'input_ids': [101, 1135, 113, 152, 114, 1108, 113, 152, 114, 1383, 113, 152, 114, 1146, 113, 152, 114, 1118, 113, 152, 114, 1393, 113, 152, 114, 1286, 118, 6784, 113, 152, 114, 2131, 113, 139, 118, 1741, 1710, 114, 7834, 113, 146, 118, 1741, 1710, 114, 117, 113, 152, 114, 113, 113, 152, 114, 1393, 113, 152, 114, 2169, 113, 152, 114, 4561, 113, 139, 118, 1741, 1710, 114, 1786, 113, 146, 118, 1741, 1710, 114, 1105, 113, 152, 114, 1393, 113, 152, 114, 2169, 113, 139, 118, 1741, 1710, 114, 3215, 113, 146, 118, 1741, 1710, 114, 1786, 113, 146, 118, 1741, 1710, 114, 114, 113, 152, 114, 117, 113, 152, 114, 1112, 113, 152, 114, 1218, 113, 152, 114, 1112, 113, 152, 114, 1168, 113, 152, 114, 1286, 118, 3092, 113, 152, 114, 8673, 113, 152, 114, 1121, 113, 152, 114, 1103, 113, 152, 114, 1393, 113, 152, 114, 2169, 113, 152, 114, 7365, 113, 139, 118, 1741, 1710, 114, 1786, 113, 146, 118, 1741, 1710, 114, 1105, 113, 152, 114, 4245, 113, 139, 118, 1741, 1710, 114, 1104, 113, 146, 118, 1741, 1710, 114, 1103, 113, 146, 118, 1741, 1710, 114, 22412, 113, 146, 118, 1741, 1710, 114, 119, 113, 152, 114, 102, 1135, 1108, 1383, 1146, 1118, 1393, 1286, 118, 6784, 2131, 7834, 117, 113, 1393, 2169, 4561, 1786, 1105, 1393, 2169, 3215, 1786, 114, 117, 1112, 1218, 1112, 1168, 1286, 118, 3092, 8673, 1121, 1103, 1393, 2169, 7365, 1786, 1105, 4245, 1104, 1103, 22412, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 13:43:48,931 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 13:43:49,124 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 13:43:49,129] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 13:43:49,129] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:12<00:00, 3240.81 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 3614.89 examples/s]
[2025-03-10 13:44:02,175] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 13:44:02,178] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 13:44:02,178] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 13:44:02,183] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 13:44:02,183] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 13:44:02,183] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 13:44:02,183] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 13:44:02,183] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 13:44:02,184] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 13:44:02,184] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 13:44:02,410] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:44:02,599] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 13:44:02,600] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.51 GB         CA 0.51 GB         Max_CA 1 GB
[2025-03-10 13:44:02,600] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.3 GB, percent = 5.1%
[2025-03-10 13:44:03,276] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 13:44:03,276] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.61 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:44:03,277] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.51 GB, percent = 5.1%
[2025-03-10 13:44:03,277] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 13:44:03,431] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 13:44:03,432] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:44:03,432] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.46 GB, percent = 5.1%
[2025-03-10 13:44:03,434] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 13:44:03,434] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:44:03,434] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 13:44:03,434] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fc3004cecf0>
[2025-03-10 13:44:03,434] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 13:44:03,434] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc32c1f45f0>
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 13:44:03,435] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 13:44:03,436] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 13:44:03,437] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 13:44:03,438] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 13:44:03,438] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 13:44:03,440 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 13:44:03,440 >>   Num examples = 39,286
[INFO|trainer.py:2416] 2025-03-10 13:44:03,440 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 13:44:03,440 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 13:44:03,440 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 13:44:03,440 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 13:44:03,440 >>   Total optimization steps = 49,080
[INFO|trainer.py:2423] 2025-03-10 13:44:03,440 >>   Number of trainable parameters = 108,311,041
{'loss': 1.0101, 'grad_norm': 14.622269630432129, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [02:26<1:32:05,  8.66it/s][INFO|trainer.py:930] 2025-03-10 13:46:29,780 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:46:29,786 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:46:29,786 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:46:29,786 >>   Batch size = 4
{'eval_loss': 1.170129656791687, 'eval_mse': 1.1701422894127458, 'eval_pearson': 0.6495875110795928, 'eval_spearmanr': 0.6667839360204417, 'eval_runtime': 2.71, 'eval_samples_per_second': 765.327, 'eval_steps_per_second': 95.943, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [02:29<1:32:05,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 13:46:32,593 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227
[INFO|configuration_utils.py:423] 2025-03-10 13:46:32,595 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:46:32,829 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:46:32,830 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:46:32,830 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/special_tokens_map.json
[2025-03-10 13:46:32,859] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1227 is about to be saved!
[2025-03-10 13:46:32,863] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt
[2025-03-10 13:46:32,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt...
[2025-03-10 13:46:33,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt.
[2025-03-10 13:46:33,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:46:34,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:46:34,130] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:46:34,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1227 is ready now!
{'loss': 0.6721, 'grad_norm': 4.803602695465088, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [04:55<1:29:46,  8.66it/s][INFO|trainer.py:930] 2025-03-10 13:48:58,522 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:48:58,526 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:48:58,526 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:48:58,526 >>   Batch size = 4
{'eval_loss': 1.0345286130905151, 'eval_mse': 1.034381337657844, 'eval_pearson': 0.6996546875267443, 'eval_spearmanr': 0.7116562928902707, 'eval_runtime': 2.6371, 'eval_samples_per_second': 786.455, 'eval_steps_per_second': 98.591, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [04:57<1:29:46,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 13:49:01,308 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454
[INFO|configuration_utils.py:423] 2025-03-10 13:49:01,310 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:49:01,556 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:49:01,557 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:49:01,557 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/special_tokens_map.json
[2025-03-10 13:49:01,584] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2455 is about to be saved!
[2025-03-10 13:49:01,587] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt
[2025-03-10 13:49:01,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt...
[2025-03-10 13:49:01,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt.
[2025-03-10 13:49:01,906] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:49:02,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:49:02,883] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:49:02,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2455 is ready now!
{'loss': 0.4873, 'grad_norm': 11.464287757873535, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [07:22<1:26:56,  8.70it/s][INFO|trainer.py:930] 2025-03-10 13:51:26,419 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:51:26,422 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:51:26,422 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:51:26,422 >>   Batch size = 4
{'eval_loss': 0.8997892737388611, 'eval_mse': 0.8995490421324595, 'eval_pearson': 0.7472210409704312, 'eval_spearmanr': 0.7569142310096958, 'eval_runtime': 2.6824, 'eval_samples_per_second': 773.199, 'eval_steps_per_second': 96.93, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [07:25<1:26:56,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 13:51:29,201 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681
[INFO|configuration_utils.py:423] 2025-03-10 13:51:29,203 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:51:29,413 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:51:29,413 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:51:29,413 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/special_tokens_map.json
[2025-03-10 13:51:29,437] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3682 is about to be saved!
[2025-03-10 13:51:29,440] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt
[2025-03-10 13:51:29,441] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt...
[2025-03-10 13:51:29,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt.
[2025-03-10 13:51:29,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:51:30,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:51:30,711] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:51:30,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3682 is ready now!
{'loss': 0.3946, 'grad_norm': 3.6485254764556885, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [09:47<1:24:19,  8.73it/s][INFO|trainer.py:930] 2025-03-10 13:53:51,438 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:53:51,441 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:53:51,441 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:53:51,441 >>   Batch size = 4
{'eval_loss': 1.0886578559875488, 'eval_mse': 1.0887958539026568, 'eval_pearson': 0.7306221953834316, 'eval_spearmanr': 0.7432726438297645, 'eval_runtime': 2.6272, 'eval_samples_per_second': 789.434, 'eval_steps_per_second': 98.965, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [09:50<1:24:19,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 13:53:54,213 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908
[INFO|configuration_utils.py:423] 2025-03-10 13:53:54,215 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:53:54,425 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:53:54,426 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:53:54,426 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/special_tokens_map.json
[2025-03-10 13:53:54,440] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4910 is about to be saved!
[2025-03-10 13:53:54,444] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt
[2025-03-10 13:53:54,444] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt...
[2025-03-10 13:53:54,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt.
[2025-03-10 13:53:54,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:53:55,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:53:55,720] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:53:55,720] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4910 is ready now!
{'loss': 0.2969, 'grad_norm': 7.048877716064453, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [12:15<1:22:30,  8.68it/s][INFO|trainer.py:930] 2025-03-10 13:56:18,565 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:56:18,568 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:56:18,568 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:56:18,568 >>   Batch size = 4
{'eval_loss': 0.9258595705032349, 'eval_mse': 0.9254427537853716, 'eval_pearson': 0.7451765063169555, 'eval_spearmanr': 0.7529408733313383, 'eval_runtime': 2.6351, 'eval_samples_per_second': 787.065, 'eval_steps_per_second': 98.668, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [12:17<1:22:30,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 13:56:21,348 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135
[INFO|configuration_utils.py:423] 2025-03-10 13:56:21,350 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:56:21,560 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:56:21,561 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:56:21,561 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/special_tokens_map.json
[2025-03-10 13:56:21,584] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6137 is about to be saved!
[2025-03-10 13:56:21,587] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt
[2025-03-10 13:56:21,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt...
[2025-03-10 13:56:21,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt.
[2025-03-10 13:56:21,905] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:56:22,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:56:22,864] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:56:22,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6137 is ready now!
{'loss': 0.2491, 'grad_norm': 6.1164164543151855, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [14:42<1:20:44,  8.61it/s][INFO|trainer.py:930] 2025-03-10 13:58:45,834 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:58:45,837 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:58:45,837 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 13:58:45,837 >>   Batch size = 4
{'eval_loss': 0.9333488941192627, 'eval_mse': 0.9332899143748711, 'eval_pearson': 0.7421446055022768, 'eval_spearmanr': 0.7486535680994492, 'eval_runtime': 2.6342, 'eval_samples_per_second': 787.334, 'eval_steps_per_second': 98.701, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [14:45<1:20:44,  8.61it/s]
[INFO|trainer.py:3955] 2025-03-10 13:58:48,615 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362
[INFO|configuration_utils.py:423] 2025-03-10 13:58:48,617 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:58:48,825 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:58:48,826 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:58:48,826 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/special_tokens_map.json
[2025-03-10 13:58:48,842] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7365 is about to be saved!
[2025-03-10 13:58:48,846] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt
[2025-03-10 13:58:48,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt...
[2025-03-10 13:58:49,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt.
[2025-03-10 13:58:49,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:58:50,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:58:50,148] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:58:50,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7365 is ready now!
{'loss': 0.1861, 'grad_norm': 10.255165100097656, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [17:08<1:17:49,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:01:12,297 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:01:12,300 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:01:12,300 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:01:12,300 >>   Batch size = 4
{'eval_loss': 0.9254613518714905, 'eval_mse': 0.9252601568816138, 'eval_pearson': 0.7563229827391763, 'eval_spearmanr': 0.76458114177935, 'eval_runtime': 2.6386, 'eval_samples_per_second': 786.018, 'eval_steps_per_second': 98.537, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [17:11<1:17:49,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:01:15,082 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589
[INFO|configuration_utils.py:423] 2025-03-10 14:01:15,084 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:01:15,291 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:01:15,292 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:01:15,292 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/special_tokens_map.json
[2025-03-10 14:01:15,314] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8592 is about to be saved!
[2025-03-10 14:01:15,318] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt
[2025-03-10 14:01:15,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt...
[2025-03-10 14:01:15,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt.
[2025-03-10 14:01:15,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:01:16,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:01:16,561] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:01:16,561] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8592 is ready now!
{'loss': 0.1554, 'grad_norm': 2.7689805030822754, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [19:34<1:14:52,  8.74it/s][INFO|trainer.py:930] 2025-03-10 14:03:37,701 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:03:37,705 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:03:37,705 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:03:37,705 >>   Batch size = 4
{'eval_loss': 0.9130890965461731, 'eval_mse': 0.9130951617929721, 'eval_pearson': 0.7451800503093534, 'eval_spearmanr': 0.7624673473150195, 'eval_runtime': 2.6399, 'eval_samples_per_second': 785.63, 'eval_steps_per_second': 98.488, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [19:36<1:14:52,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 14:03:40,481 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816
[INFO|configuration_utils.py:423] 2025-03-10 14:03:40,483 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:03:40,687 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:03:40,688 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:03:40,688 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/special_tokens_map.json
[2025-03-10 14:03:40,702] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9820 is about to be saved!
[2025-03-10 14:03:40,705] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt
[2025-03-10 14:03:40,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt...
[2025-03-10 14:03:40,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt.
[2025-03-10 14:03:40,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:03:41,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:03:41,894] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:03:41,894] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9820 is ready now!
{'loss': 0.1173, 'grad_norm': 3.224648952484131, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [22:00<1:13:19,  8.64it/s][INFO|trainer.py:930] 2025-03-10 14:06:03,881 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:06:03,885 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:06:03,885 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:06:03,885 >>   Batch size = 4
{'eval_loss': 0.9806440472602844, 'eval_mse': 0.9807675666496402, 'eval_pearson': 0.7503701186107757, 'eval_spearmanr': 0.7637580206105343, 'eval_runtime': 2.6355, 'eval_samples_per_second': 786.96, 'eval_steps_per_second': 98.655, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [22:03<1:13:19,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 14:06:06,654 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043
[INFO|configuration_utils.py:423] 2025-03-10 14:06:06,656 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:06:06,861 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:06:06,862 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:06:06,862 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/special_tokens_map.json
[2025-03-10 14:06:06,881] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11047 is about to be saved!
[2025-03-10 14:06:06,884] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt
[2025-03-10 14:06:06,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt...
[2025-03-10 14:06:07,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt.
[2025-03-10 14:06:07,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:06:08,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:06:08,079] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:06:08,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11047 is ready now!
{'loss': 0.1005, 'grad_norm': 1.5272270441055298, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [24:25<1:10:29,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:08:29,300 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:08:29,304 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:08:29,304 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:08:29,304 >>   Batch size = 4
{'eval_loss': 0.8897168040275574, 'eval_mse': 0.8896259643072792, 'eval_pearson': 0.7554111623549831, 'eval_spearmanr': 0.7739698387291414, 'eval_runtime': 2.6352, 'eval_samples_per_second': 787.05, 'eval_steps_per_second': 98.666, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [24:28<1:10:29,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:08:32,071 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270
[INFO|configuration_utils.py:423] 2025-03-10 14:08:32,073 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:08:32,277 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:08:32,277 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:08:32,278 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/special_tokens_map.json
[2025-03-10 14:08:32,292] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12275 is about to be saved!
[2025-03-10 14:08:32,295] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt
[2025-03-10 14:08:32,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt...
[2025-03-10 14:08:32,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt.
[2025-03-10 14:08:32,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:08:33,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:08:33,477] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:08:33,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12275 is ready now!
{'loss': 0.0766, 'grad_norm': 3.1353673934936523, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [26:52<1:10:03,  8.47it/s][INFO|trainer.py:930] 2025-03-10 14:10:55,808 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:10:55,811 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:10:55,812 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:10:55,812 >>   Batch size = 4
{'eval_loss': 0.9002265930175781, 'eval_mse': 0.9002376451096861, 'eval_pearson': 0.765991595524215, 'eval_spearmanr': 0.7781180706836817, 'eval_runtime': 2.6343, 'eval_samples_per_second': 787.293, 'eval_steps_per_second': 98.696, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [26:55<1:10:03,  8.47it/s]
[INFO|trainer.py:3955] 2025-03-10 14:10:58,580 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497
[INFO|configuration_utils.py:423] 2025-03-10 14:10:58,582 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:10:58,786 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:10:58,787 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:10:58,787 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/special_tokens_map.json
[2025-03-10 14:10:58,804] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13502 is about to be saved!
[2025-03-10 14:10:58,808] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt
[2025-03-10 14:10:58,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt...
[2025-03-10 14:10:59,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt.
[2025-03-10 14:10:59,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:10:59,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:10:59,989] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:10:59,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13502 is ready now!
{'loss': 0.0682, 'grad_norm': 2.707050085067749, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [29:19<1:05:43,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:13:22,799 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:13:22,802 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:13:22,802 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:13:22,802 >>   Batch size = 4
{'eval_loss': 0.8522075414657593, 'eval_mse': 0.8520284827902784, 'eval_pearson': 0.7694630710746808, 'eval_spearmanr': 0.787913623582031, 'eval_runtime': 2.6363, 'eval_samples_per_second': 786.718, 'eval_steps_per_second': 98.624, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [29:21<1:05:43,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:13:25,570 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724
[INFO|configuration_utils.py:423] 2025-03-10 14:13:25,572 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:13:25,775 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:13:25,776 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:13:25,776 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/special_tokens_map.json
[2025-03-10 14:13:25,793] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14730 is about to be saved!
[2025-03-10 14:13:25,797] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt
[2025-03-10 14:13:25,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt...
[2025-03-10 14:13:26,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt.
[2025-03-10 14:13:26,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:13:26,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:13:26,995] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:13:26,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14730 is ready now!
{'loss': 0.0543, 'grad_norm': 1.63461172580719, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 15951/49080 [31:45<1:03:44,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:15:49,280 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:15:49,283 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:15:49,283 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:15:49,283 >>   Batch size = 4
{'eval_loss': 0.9518204927444458, 'eval_mse': 0.9518960648815336, 'eval_pearson': 0.7483790040606924, 'eval_spearmanr': 0.7654394128881177, 'eval_runtime': 2.654, 'eval_samples_per_second': 781.457, 'eval_steps_per_second': 97.965, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 15951/49080 [31:48<1:03:44,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:15:52,071 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951
[INFO|configuration_utils.py:423] 2025-03-10 14:15:52,073 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:15:52,287 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:15:52,288 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:15:52,288 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/special_tokens_map.json
[2025-03-10 14:15:52,306] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15957 is about to be saved!
[2025-03-10 14:15:52,309] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt
[2025-03-10 14:15:52,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt...
[2025-03-10 14:15:52,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt.
[2025-03-10 14:15:52,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:15:53,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:15:53,463] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:15:53,463] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15957 is ready now!
{'loss': 0.0477, 'grad_norm': 0.797327995300293, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 17178/49080 [34:12<1:00:57,  8.72it/s][INFO|trainer.py:930] 2025-03-10 14:18:15,449 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:18:15,452 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:18:15,452 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:18:15,452 >>   Batch size = 4
{'eval_loss': 1.022625207901001, 'eval_mse': 1.0222922104878733, 'eval_pearson': 0.7296911184300439, 'eval_spearmanr': 0.7516870699766611, 'eval_runtime': 2.6344, 'eval_samples_per_second': 787.279, 'eval_steps_per_second': 98.695, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 17178/49080 [34:14<1:00:57,  8.72it/s]
[INFO|trainer.py:3955] 2025-03-10 14:18:18,222 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178
[INFO|configuration_utils.py:423] 2025-03-10 14:18:18,224 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:18:18,430 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:18:18,430 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:18:18,431 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/special_tokens_map.json
[2025-03-10 14:18:18,444] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17185 is about to be saved!
[2025-03-10 14:18:18,447] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt
[2025-03-10 14:18:18,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt...
[2025-03-10 14:18:18,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt.
[2025-03-10 14:18:18,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:18:19,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:18:19,691] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:18:19,691] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17185 is ready now!
{'loss': 0.0424, 'grad_norm': 1.565259337425232, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 18405/49080 [36:38<59:03,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:20:41,879 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:20:41,883 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:20:41,883 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:20:41,883 >>   Batch size = 4
{'eval_loss': 0.8490828275680542, 'eval_mse': 0.84905494018139, 'eval_pearson': 0.7770901146404922, 'eval_spearmanr': 0.7916770981156003, 'eval_runtime': 2.6391, 'eval_samples_per_second': 785.886, 'eval_steps_per_second': 98.52, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 18405/49080 [36:41<59:03,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:20:44,656 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405
[INFO|configuration_utils.py:423] 2025-03-10 14:20:44,658 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:20:44,860 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:20:44,861 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:20:44,861 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/special_tokens_map.json
[2025-03-10 14:20:44,878] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18412 is about to be saved!
[2025-03-10 14:20:44,882] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt
[2025-03-10 14:20:44,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt...
[2025-03-10 14:20:45,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt.
[2025-03-10 14:20:45,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:20:46,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:20:46,039] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:20:46,039] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18412 is ready now!
{'loss': 0.0411, 'grad_norm': 2.0496811866760254, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 19632/49080 [39:03<56:24,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:23:07,248 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:23:07,252 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:23:07,252 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:23:07,252 >>   Batch size = 4
{'eval_loss': 0.9577398896217346, 'eval_mse': 0.9577527333593414, 'eval_pearson': 0.7501993410253307, 'eval_spearmanr': 0.7747942395029164, 'eval_runtime': 2.6374, 'eval_samples_per_second': 786.378, 'eval_steps_per_second': 98.582, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 19632/49080 [39:06<56:24,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:23:10,024 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632
[INFO|configuration_utils.py:423] 2025-03-10 14:23:10,026 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:23:10,229 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:23:10,230 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:23:10,230 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/special_tokens_map.json
[2025-03-10 14:23:10,249] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19640 is about to be saved!
[2025-03-10 14:23:10,253] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt
[2025-03-10 14:23:10,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt...
[2025-03-10 14:23:10,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt.
[2025-03-10 14:23:10,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:23:11,458] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:23:11,458] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:23:11,459] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19640 is ready now!
{'loss': 0.0333, 'grad_norm': 0.5322279334068298, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 20859/49080 [41:30<54:18,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:25:33,808 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:25:33,811 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:25:33,811 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:25:33,811 >>   Batch size = 4
{'eval_loss': 0.8653078675270081, 'eval_mse': 0.865102525042476, 'eval_pearson': 0.7704435257653774, 'eval_spearmanr': 0.7879873755016293, 'eval_runtime': 2.6446, 'eval_samples_per_second': 784.239, 'eval_steps_per_second': 98.313, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 20859/49080 [41:33<54:18,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:25:36,590 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859
[INFO|configuration_utils.py:423] 2025-03-10 14:25:36,592 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:25:36,798 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:25:36,798 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:25:36,799 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/special_tokens_map.json
[2025-03-10 14:25:36,816] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20867 is about to be saved!
[2025-03-10 14:25:36,819] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt
[2025-03-10 14:25:36,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt...
[2025-03-10 14:25:37,096] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt.
[2025-03-10 14:25:37,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:25:38,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:25:38,014] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:25:38,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20867 is ready now!
{'loss': 0.0327, 'grad_norm': 1.9210261106491089, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 22086/49080 [43:55<51:40,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:27:59,222 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:27:59,226 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:27:59,226 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:27:59,226 >>   Batch size = 4
{'eval_loss': 0.914322555065155, 'eval_mse': 0.9141619518154958, 'eval_pearson': 0.7638674748344789, 'eval_spearmanr': 0.779449732149639, 'eval_runtime': 2.649, 'eval_samples_per_second': 782.936, 'eval_steps_per_second': 98.15, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 22086/49080 [43:58<51:40,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:28:02,008 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086
[INFO|configuration_utils.py:423] 2025-03-10 14:28:02,010 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:28:02,209 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:28:02,210 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:28:02,210 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/special_tokens_map.json
[2025-03-10 14:28:02,228] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22095 is about to be saved!
[2025-03-10 14:28:02,232] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt
[2025-03-10 14:28:02,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt...
[2025-03-10 14:28:02,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt.
[2025-03-10 14:28:02,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:28:03,436] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:28:03,437] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:28:03,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22095 is ready now!
{'loss': 0.0294, 'grad_norm': 1.358500599861145, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 23313/49080 [46:21<49:31,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:30:25,407 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:30:25,411 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:30:25,411 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:30:25,411 >>   Batch size = 4
{'eval_loss': 0.9642503261566162, 'eval_mse': 0.9643499653502623, 'eval_pearson': 0.7594310267456993, 'eval_spearmanr': 0.7729014158577593, 'eval_runtime': 2.6513, 'eval_samples_per_second': 782.249, 'eval_steps_per_second': 98.064, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 23313/49080 [46:24<49:31,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:30:28,195 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313
[INFO|configuration_utils.py:423] 2025-03-10 14:30:28,197 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:30:28,397 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:30:28,398 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:30:28,398 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/special_tokens_map.json
[2025-03-10 14:30:28,415] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23322 is about to be saved!
[2025-03-10 14:30:28,419] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt
[2025-03-10 14:30:28,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt...
[2025-03-10 14:30:28,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt.
[2025-03-10 14:30:28,691] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:30:29,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:30:29,588] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:30:29,588] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23322 is ready now!
{'loss': 0.0292, 'grad_norm': 0.22708556056022644, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 24540/49080 [48:47<47:02,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:32:50,947 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:32:50,951 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:32:50,951 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:32:50,951 >>   Batch size = 4
{'eval_loss': 0.8665795922279358, 'eval_mse': 0.8665535505160531, 'eval_pearson': 0.7811451008078101, 'eval_spearmanr': 0.7966559629233567, 'eval_runtime': 2.658, 'eval_samples_per_second': 780.291, 'eval_steps_per_second': 97.819, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 24540/49080 [48:50<47:02,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:32:53,744 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540
[INFO|configuration_utils.py:423] 2025-03-10 14:32:53,746 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:32:53,947 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:32:53,948 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:32:53,948 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/special_tokens_map.json
[2025-03-10 14:32:53,969] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24550 is about to be saved!
[2025-03-10 14:32:53,973] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt
[2025-03-10 14:32:53,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt...
[2025-03-10 14:32:54,246] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt.
[2025-03-10 14:32:54,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:32:55,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:32:55,168] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:32:55,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24550 is ready now!
{'loss': 0.0256, 'grad_norm': 1.4741451740264893, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 25767/49080 [51:14<44:56,  8.64it/s][INFO|trainer.py:930] 2025-03-10 14:35:17,559 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:35:17,562 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:35:17,562 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:35:17,563 >>   Batch size = 4
{'eval_loss': 0.8671779036521912, 'eval_mse': 0.8671184875926033, 'eval_pearson': 0.7799015983968565, 'eval_spearmanr': 0.7922194142337697, 'eval_runtime': 2.6404, 'eval_samples_per_second': 785.493, 'eval_steps_per_second': 98.471, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 25767/49080 [51:16<44:56,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 14:35:20,337 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767
[INFO|configuration_utils.py:423] 2025-03-10 14:35:20,339 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:35:20,541 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:35:20,542 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:35:20,542 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/special_tokens_map.json
[2025-03-10 14:35:20,562] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25777 is about to be saved!
[2025-03-10 14:35:20,565] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt
[2025-03-10 14:35:20,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt...
[2025-03-10 14:35:20,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt.
[2025-03-10 14:35:20,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:35:21,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:35:21,738] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:35:21,738] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25777 is ready now!
{'loss': 0.0277, 'grad_norm': 1.3789150714874268, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 26994/49080 [53:40<42:19,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:37:43,881 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:37:43,884 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:37:43,885 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:37:43,885 >>   Batch size = 4
{'eval_loss': 0.8326756358146667, 'eval_mse': 0.8327477750805787, 'eval_pearson': 0.7859065962277565, 'eval_spearmanr': 0.80009590273422, 'eval_runtime': 2.6344, 'eval_samples_per_second': 787.265, 'eval_steps_per_second': 98.693, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 26994/49080 [53:43<42:19,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:37:46,655 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994
[INFO|configuration_utils.py:423] 2025-03-10 14:37:46,657 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:37:46,882 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:37:46,883 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:37:46,883 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/special_tokens_map.json
[2025-03-10 14:37:46,897] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27005 is about to be saved!
[2025-03-10 14:37:46,901] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt
[2025-03-10 14:37:46,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt...
[2025-03-10 14:37:47,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt.
[2025-03-10 14:37:47,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:37:48,201] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:37:48,201] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:37:48,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27005 is ready now!
{'loss': 0.022, 'grad_norm': 1.6653120517730713, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 28221/49080 [56:08<40:11,  8.65it/s][INFO|trainer.py:930] 2025-03-10 14:40:11,559 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:40:11,562 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:40:11,562 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:40:11,562 >>   Batch size = 4
{'eval_loss': 0.8610093593597412, 'eval_mse': 0.8611186897973085, 'eval_pearson': 0.780907616998654, 'eval_spearmanr': 0.7959738199869933, 'eval_runtime': 2.6362, 'eval_samples_per_second': 786.74, 'eval_steps_per_second': 98.627, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 28221/49080 [56:10<40:11,  8.65it/s]
[INFO|trainer.py:3955] 2025-03-10 14:40:14,335 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221
[INFO|configuration_utils.py:423] 2025-03-10 14:40:14,337 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:40:14,545 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:40:14,545 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:40:14,545 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/special_tokens_map.json
[2025-03-10 14:40:14,565] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28232 is about to be saved!
[2025-03-10 14:40:14,569] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt
[2025-03-10 14:40:14,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt...
[2025-03-10 14:40:14,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt.
[2025-03-10 14:40:14,848] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:40:15,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:40:15,758] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:40:15,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28232 is ready now!
{'loss': 0.0255, 'grad_norm': 0.9933661222457886, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 29448/49080 [58:33<37:29,  8.73it/s][INFO|trainer.py:930] 2025-03-10 14:42:36,716 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:42:36,719 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:42:36,719 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:42:36,719 >>   Batch size = 4
{'eval_loss': 0.8569430112838745, 'eval_mse': 0.8568477720280853, 'eval_pearson': 0.7947360189907958, 'eval_spearmanr': 0.8081007049206601, 'eval_runtime': 2.6423, 'eval_samples_per_second': 784.917, 'eval_steps_per_second': 98.398, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 29448/49080 [58:35<37:29,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 14:42:39,498 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448
[INFO|configuration_utils.py:423] 2025-03-10 14:42:39,500 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:42:39,710 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:42:39,710 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:42:39,710 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/special_tokens_map.json
[2025-03-10 14:42:39,723] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29460 is about to be saved!
[2025-03-10 14:42:39,727] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt
[2025-03-10 14:42:39,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt...
[2025-03-10 14:42:40,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt.
[2025-03-10 14:42:40,013] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:42:40,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:42:40,930] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:42:40,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29460 is ready now!
{'loss': 0.021, 'grad_norm': 1.1003812551498413, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 30675/49080 [1:00:59<35:31,  8.63it/s][INFO|trainer.py:930] 2025-03-10 14:45:03,340 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:45:03,344 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:45:03,344 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:45:03,344 >>   Batch size = 4
{'eval_loss': 0.9877746105194092, 'eval_mse': 0.9880730093042783, 'eval_pearson': 0.747168665124027, 'eval_spearmanr': 0.7667342031314315, 'eval_runtime': 2.6918, 'eval_samples_per_second': 770.49, 'eval_steps_per_second': 96.59, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 30675/49080 [1:01:02<35:31,  8.63it/s]
[INFO|trainer.py:3955] 2025-03-10 14:45:06,156 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675
[INFO|configuration_utils.py:423] 2025-03-10 14:45:06,158 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:45:06,434 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:45:06,435 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:45:06,435 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/special_tokens_map.json
[2025-03-10 14:45:06,457] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30687 is about to be saved!
[2025-03-10 14:45:06,461] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt
[2025-03-10 14:45:06,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt...
[2025-03-10 14:45:06,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt.
[2025-03-10 14:45:06,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:45:07,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:45:08,000] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:45:08,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30687 is ready now!
{'loss': 0.0215, 'grad_norm': 2.052719831466675, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 31902/49080 [1:03:25<32:51,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:47:29,129 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:47:29,132 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:47:29,132 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:47:29,132 >>   Batch size = 4
{'eval_loss': 1.0300580263137817, 'eval_mse': 1.0303933234200988, 'eval_pearson': 0.7299745427992429, 'eval_spearmanr': 0.7507238267558134, 'eval_runtime': 2.6314, 'eval_samples_per_second': 788.159, 'eval_steps_per_second': 98.805, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 31902/49080 [1:03:28<32:51,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:47:31,932 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902
[INFO|configuration_utils.py:423] 2025-03-10 14:47:31,934 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:47:32,212 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:47:32,212 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:47:32,213 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/special_tokens_map.json
[2025-03-10 14:47:32,226] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31915 is about to be saved!
[2025-03-10 14:47:32,230] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt
[2025-03-10 14:47:32,230] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt...
[2025-03-10 14:47:32,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt.
[2025-03-10 14:47:32,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:47:33,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:47:33,761] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:47:33,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31915 is ready now!
{'loss': 0.02, 'grad_norm': 1.6644723415374756, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 33129/49080 [1:05:52<30:42,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:49:55,914 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:49:55,917 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:49:55,917 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:49:55,917 >>   Batch size = 4
{'eval_loss': 0.9055522084236145, 'eval_mse': 0.9055186589405645, 'eval_pearson': 0.7784873367845966, 'eval_spearmanr': 0.7963564843202406, 'eval_runtime': 2.6464, 'eval_samples_per_second': 783.719, 'eval_steps_per_second': 98.248, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 33129/49080 [1:05:55<30:42,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:49:58,732 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129
[INFO|configuration_utils.py:423] 2025-03-10 14:49:58,734 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:49:59,010 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:49:59,011 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:49:59,011 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/special_tokens_map.json
[2025-03-10 14:49:59,032] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step33142 is about to be saved!
[2025-03-10 14:49:59,036] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt
[2025-03-10 14:49:59,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt...
[2025-03-10 14:49:59,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt.
[2025-03-10 14:49:59,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:50:00,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:50:00,592] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:50:00,592] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33142 is ready now!
{'loss': 0.0179, 'grad_norm': 0.9574220776557922, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [1:08:21<28:07,  8.72it/s][INFO|trainer.py:930] 2025-03-10 14:52:25,292 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:52:25,295 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:52:25,295 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:52:25,295 >>   Batch size = 4
{'eval_loss': 1.059441089630127, 'eval_mse': 1.0593126073062937, 'eval_pearson': 0.7339243627385831, 'eval_spearmanr': 0.7558975262326767, 'eval_runtime': 2.6554, 'eval_samples_per_second': 781.062, 'eval_steps_per_second': 97.915, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [1:08:24<28:07,  8.72it/s]
[INFO|trainer.py:3955] 2025-03-10 14:52:28,100 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356
[INFO|configuration_utils.py:423] 2025-03-10 14:52:28,102 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:52:28,342 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:52:28,343 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:52:28,343 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/special_tokens_map.json
[2025-03-10 14:52:28,368] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34370 is about to be saved!
[2025-03-10 14:52:28,371] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt
[2025-03-10 14:52:28,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt...
[2025-03-10 14:52:28,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt.
[2025-03-10 14:52:28,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:52:29,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:52:29,703] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:52:29,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34370 is ready now!
{'loss': 0.0191, 'grad_norm': 0.49756181240081787, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [1:10:48<25:52,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:54:51,472 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:54:51,475 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:54:51,475 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:54:51,475 >>   Batch size = 4
{'eval_loss': 0.9824502468109131, 'eval_mse': 0.9822712613623535, 'eval_pearson': 0.7455992161500389, 'eval_spearmanr': 0.76565175026876, 'eval_runtime': 2.6367, 'eval_samples_per_second': 786.599, 'eval_steps_per_second': 98.609, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [1:10:50<25:52,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:54:54,274 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583
[INFO|configuration_utils.py:423] 2025-03-10 14:54:54,276 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:54:54,535 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:54:54,536 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:54:54,536 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/special_tokens_map.json
[2025-03-10 14:54:54,557] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35597 is about to be saved!
[2025-03-10 14:54:54,560] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt
[2025-03-10 14:54:54,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt...
[2025-03-10 14:54:54,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt.
[2025-03-10 14:54:54,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:54:56,004] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:54:56,004] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:54:56,005] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35597 is ready now!
{'loss': 0.0157, 'grad_norm': 0.9268138408660889, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [1:13:14<23:25,  8.73it/s][INFO|trainer.py:930] 2025-03-10 14:57:17,934 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:57:17,937 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:57:17,937 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:57:17,937 >>   Batch size = 4
{'eval_loss': 1.1164828538894653, 'eval_mse': 1.1167375190738542, 'eval_pearson': 0.7181039993919734, 'eval_spearmanr': 0.7364836409634186, 'eval_runtime': 2.6338, 'eval_samples_per_second': 787.444, 'eval_steps_per_second': 98.715, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [1:13:17<23:25,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 14:57:20,739 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810
[INFO|configuration_utils.py:423] 2025-03-10 14:57:20,741 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:57:21,040 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:57:21,041 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:57:21,041 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/special_tokens_map.json
[2025-03-10 14:57:21,057] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step36825 is about to be saved!
[2025-03-10 14:57:21,060] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt
[2025-03-10 14:57:21,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt...
[2025-03-10 14:57:21,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt.
[2025-03-10 14:57:21,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:57:22,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:57:22,569] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:57:22,569] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36825 is ready now!
{'loss': 0.0171, 'grad_norm': 0.7465828657150269, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [1:15:41<21:37,  8.51it/s][INFO|trainer.py:930] 2025-03-10 14:59:44,834 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:59:44,837 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:59:44,838 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 14:59:44,838 >>   Batch size = 4
{'eval_loss': 0.9784294962882996, 'eval_mse': 0.9779212702435721, 'eval_pearson': 0.7588611419458349, 'eval_spearmanr': 0.7743601631272183, 'eval_runtime': 2.6317, 'eval_samples_per_second': 788.076, 'eval_steps_per_second': 98.794, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [1:15:44<21:37,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 14:59:47,614 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037
[INFO|configuration_utils.py:423] 2025-03-10 14:59:47,616 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:59:47,857 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:59:47,858 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:59:47,858 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/special_tokens_map.json
[2025-03-10 14:59:47,879] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step38052 is about to be saved!
[2025-03-10 14:59:47,883] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt
[2025-03-10 14:59:47,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt...
[2025-03-10 14:59:48,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt.
[2025-03-10 14:59:48,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:59:49,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:59:49,168] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:59:49,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38052 is ready now!
{'loss': 0.017, 'grad_norm': 0.7694674730300903, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [1:18:06<18:43,  8.74it/s][INFO|trainer.py:930] 2025-03-10 15:02:10,422 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:02:10,426 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:02:10,426 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:02:10,426 >>   Batch size = 4
{'eval_loss': 1.0177456140518188, 'eval_mse': 1.0180735135147403, 'eval_pearson': 0.7525729515913187, 'eval_spearmanr': 0.7641903880497316, 'eval_runtime': 2.6365, 'eval_samples_per_second': 786.635, 'eval_steps_per_second': 98.614, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [1:18:09<18:43,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 15:02:13,208 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264
[INFO|configuration_utils.py:423] 2025-03-10 15:02:13,210 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:02:13,444 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:02:13,445 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:02:13,445 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/special_tokens_map.json
[2025-03-10 15:02:13,460] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39280 is about to be saved!
[2025-03-10 15:02:13,463] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt
[2025-03-10 15:02:13,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt...
[2025-03-10 15:02:13,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt.
[2025-03-10 15:02:13,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:02:14,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:02:14,961] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:02:14,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39280 is ready now!
{'loss': 0.0183, 'grad_norm': 0.475523978471756, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [1:20:34<16:31,  8.66it/s][INFO|trainer.py:930] 2025-03-10 15:04:37,745 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:04:37,748 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:04:37,748 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:04:37,748 >>   Batch size = 4
{'eval_loss': 0.9761227369308472, 'eval_mse': 0.9762183047306572, 'eval_pearson': 0.7543056930606618, 'eval_spearmanr': 0.7724938268726745, 'eval_runtime': 2.6484, 'eval_samples_per_second': 783.124, 'eval_steps_per_second': 98.174, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [1:20:36<16:31,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 15:04:40,564 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491
[INFO|configuration_utils.py:423] 2025-03-10 15:04:40,566 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:04:40,844 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:04:40,845 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:04:40,845 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/special_tokens_map.json
[2025-03-10 15:04:40,869] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40507 is about to be saved!
[2025-03-10 15:04:40,872] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt
[2025-03-10 15:04:40,872] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt...
[2025-03-10 15:04:41,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt.
[2025-03-10 15:04:41,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:04:42,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:04:42,377] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:04:42,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40507 is ready now!
{'loss': 0.0149, 'grad_norm': 0.5972025990486145, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [1:22:59<14:01,  8.75it/s][INFO|trainer.py:930] 2025-03-10 15:07:03,267 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:07:03,270 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:07:03,270 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:07:03,270 >>   Batch size = 4
{'eval_loss': 0.8396351337432861, 'eval_mse': 0.8394279144309181, 'eval_pearson': 0.7797968549211686, 'eval_spearmanr': 0.7955319553449293, 'eval_runtime': 2.6436, 'eval_samples_per_second': 784.534, 'eval_steps_per_second': 98.35, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [1:23:02<14:01,  8.75it/s]
[INFO|trainer.py:3955] 2025-03-10 15:07:06,086 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718
[INFO|configuration_utils.py:423] 2025-03-10 15:07:06,088 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:07:06,363 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:07:06,364 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:07:06,364 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/special_tokens_map.json
[2025-03-10 15:07:06,381] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step41735 is about to be saved!
[2025-03-10 15:07:06,384] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt
[2025-03-10 15:07:06,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt...
[2025-03-10 15:07:06,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt.
[2025-03-10 15:07:06,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:07:07,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:07:07,937] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:07:07,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41735 is ready now!
{'loss': 0.0147, 'grad_norm': 0.9758527278900146, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [1:25:27<12:04,  8.46it/s][INFO|trainer.py:930] 2025-03-10 15:09:30,964 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:09:30,967 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:09:30,967 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:09:30,967 >>   Batch size = 4
{'eval_loss': 1.0560755729675293, 'eval_mse': 1.056381231908513, 'eval_pearson': 0.7349591650744483, 'eval_spearmanr': 0.7543148596336224, 'eval_runtime': 2.6718, 'eval_samples_per_second': 776.27, 'eval_steps_per_second': 97.314, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [1:25:30<12:04,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 15:09:33,812 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945
[INFO|configuration_utils.py:423] 2025-03-10 15:09:33,814 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:09:34,094 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:09:34,095 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:09:34,095 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/special_tokens_map.json
[2025-03-10 15:09:34,115] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42962 is about to be saved!
[2025-03-10 15:09:34,119] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt
[2025-03-10 15:09:34,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt...
[2025-03-10 15:09:34,485] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt.
[2025-03-10 15:09:34,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:09:35,663] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:09:35,664] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:09:35,664] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42962 is ready now!
{'loss': 0.0147, 'grad_norm': 0.46897169947624207, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [1:27:53<09:21,  8.74it/s][INFO|trainer.py:930] 2025-03-10 15:11:57,104 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:11:57,107 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:11:57,108 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:11:57,108 >>   Batch size = 4
{'eval_loss': 0.9571995735168457, 'eval_mse': 0.9573000982159474, 'eval_pearson': 0.7726635345551591, 'eval_spearmanr': 0.7877011454852965, 'eval_runtime': 2.6856, 'eval_samples_per_second': 772.257, 'eval_steps_per_second': 96.811, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [1:27:56<09:21,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 15:11:59,916 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172
[INFO|configuration_utils.py:423] 2025-03-10 15:11:59,918 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:12:00,197 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:12:00,198 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:12:00,198 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/special_tokens_map.json
[2025-03-10 15:12:00,215] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44190 is about to be saved!
[2025-03-10 15:12:00,218] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt
[2025-03-10 15:12:00,218] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt...
[2025-03-10 15:12:00,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt.
[2025-03-10 15:12:00,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:12:01,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:12:01,764] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:12:01,764] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44190 is ready now!
{'loss': 0.0152, 'grad_norm': 0.4686225354671478, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [1:30:19<07:05,  8.65it/s][INFO|trainer.py:930] 2025-03-10 15:14:23,429 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:14:23,433 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:14:23,433 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:14:23,433 >>   Batch size = 4
{'eval_loss': 0.8816625475883484, 'eval_mse': 0.8817766582379576, 'eval_pearson': 0.7793231995614716, 'eval_spearmanr': 0.7849207313238323, 'eval_runtime': 2.64, 'eval_samples_per_second': 785.593, 'eval_steps_per_second': 98.483, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [1:30:22<07:05,  8.65it/s]
[INFO|trainer.py:3955] 2025-03-10 15:14:26,240 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399
[INFO|configuration_utils.py:423] 2025-03-10 15:14:26,242 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:14:26,524 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:14:26,525 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:14:26,525 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/special_tokens_map.json
[2025-03-10 15:14:26,549] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45417 is about to be saved!
[2025-03-10 15:14:26,553] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt
[2025-03-10 15:14:26,553] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt...
[2025-03-10 15:14:26,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt.
[2025-03-10 15:14:26,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:14:28,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:14:28,126] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:14:28,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45417 is ready now!
{'loss': 0.0119, 'grad_norm': 0.8692888617515564, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [1:32:47<04:40,  8.74it/s][INFO|trainer.py:930] 2025-03-10 15:16:50,906 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:16:50,909 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:16:50,910 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:16:50,910 >>   Batch size = 4
{'eval_loss': 0.9478998780250549, 'eval_mse': 0.9479684842357066, 'eval_pearson': 0.7697223838975107, 'eval_spearmanr': 0.7812093703248522, 'eval_runtime': 2.6325, 'eval_samples_per_second': 787.841, 'eval_steps_per_second': 98.765, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [1:32:50<04:40,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 15:16:53,715 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626
[INFO|configuration_utils.py:423] 2025-03-10 15:16:53,717 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:16:53,995 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:16:53,996 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:16:53,996 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/special_tokens_map.json
[2025-03-10 15:16:54,021] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step46645 is about to be saved!
[2025-03-10 15:16:54,025] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt
[2025-03-10 15:16:54,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt...
[2025-03-10 15:16:54,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt.
[2025-03-10 15:16:54,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:16:55,659] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:16:55,659] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:16:55,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step46645 is ready now!
{'loss': 0.0136, 'grad_norm': 0.6015645265579224, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [1:35:14<02:21,  8.65it/s][INFO|trainer.py:930] 2025-03-10 15:19:17,589 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:19:17,592 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:19:17,592 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:19:17,592 >>   Batch size = 4
{'eval_loss': 0.9596608877182007, 'eval_mse': 0.9596084412636384, 'eval_pearson': 0.7523394412736915, 'eval_spearmanr': 0.7671151464240332, 'eval_runtime': 2.6313, 'eval_samples_per_second': 788.206, 'eval_steps_per_second': 98.811, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [1:35:16<02:21,  8.65it/s]
[INFO|trainer.py:3955] 2025-03-10 15:19:20,396 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853
[INFO|configuration_utils.py:423] 2025-03-10 15:19:20,398 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:19:20,685 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:19:20,686 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:19:20,686 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/special_tokens_map.json
[2025-03-10 15:19:20,708] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47872 is about to be saved!
[2025-03-10 15:19:20,711] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt
[2025-03-10 15:19:20,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt...
[2025-03-10 15:19:21,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt.
[2025-03-10 15:19:21,079] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:19:22,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:19:22,294] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:19:22,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47872 is ready now!
{'loss': 0.0123, 'grad_norm': 0.19598253071308136, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:37:39<00:00,  8.73it/s][INFO|trainer.py:930] 2025-03-10 15:21:43,101 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:21:43,104 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:21:43,104 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:21:43,104 >>   Batch size = 4
{'eval_loss': 0.9573675990104675, 'eval_mse': 0.9572376273752063, 'eval_pearson': 0.7679257780869932, 'eval_spearmanr': 0.7799697368336306, 'eval_runtime': 2.6324, 'eval_samples_per_second': 787.862, 'eval_steps_per_second': 98.768, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:37:42<00:00,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 15:21:45,911 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080
[INFO|configuration_utils.py:423] 2025-03-10 15:21:45,914 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:21:46,209 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:21:46,210 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:21:46,210 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/special_tokens_map.json
[2025-03-10 15:21:46,224] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49099 is about to be saved!
[2025-03-10 15:21:46,227] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt
[2025-03-10 15:21:46,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt...
[2025-03-10 15:21:46,615] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt.
[2025-03-10 15:21:46,617] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:21:47,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:21:47,839] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:21:47,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49099 is ready now!
[INFO|trainer.py:2670] 2025-03-10 15:21:47,842 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 5864.4017, 'train_samples_per_second': 267.963, 'train_steps_per_second': 8.369, 'train_loss': 0.11225182934320352, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:37:44<00:00,  8.37it/s]
[INFO|trainer.py:3955] 2025-03-10 15:21:47,968 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=30
[INFO|configuration_utils.py:423] 2025-03-10 15:21:47,970 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=30/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:21:48,268 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=30/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:21:48,269 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:21:48,269 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=30/special_tokens_map.json
***** train metrics *****
  epoch                    =     39.9994
  total_flos               = 385001440GF
  train_loss               =      0.1123
  train_runtime            =  1:37:44.40
  train_samples            =       39286
  train_samples_per_second =     267.963
  train_steps_per_second   =       8.369
03/10/2025 15:21:48 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 15:21:48,283 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:21:48,285 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:21:48,286 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:21:48,286 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:02<00:00, 97.38it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     0.9574
  eval_mse                =     0.9572
  eval_pearson            =     0.7679
  eval_runtime            = 0:00:02.68
  eval_samples            =       2074
  eval_samples_per_second =    772.366
  eval_spearmanr          =       0.78
  eval_steps_per_second   =     96.825
[INFO|modelcard.py:449] 2025-03-10 15:21:51,201 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7799697368336306}]}
[rank0]:[W310 15:21:51.362876819 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 15:21:54,335] [INFO] [launch.py:351:main] Process 4023117 exits successfully.
[2025-03-10 15:21:54,336] [INFO] [launch.py:351:main] Process 4023116 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:4,5 --master_port 27134 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:22:00,223] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:22:03,240] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 15:22:03,241] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNV19 --master_addr=127.0.0.1 --master_port=27134 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:22:07,618] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:22:11,689] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [4, 5]}
[2025-03-10 15:22:11,689] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 15:22:11,689] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 15:22:11,689] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 15:22:11,689] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=4,5
[2025-03-10 15:22:11,690] [INFO] [launch.py:256:main] process 4134549 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:22:11,691] [INFO] [launch.py:256:main] process 4134550 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:22:19,007] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:22:19,008] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:22:19,800] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 15:22:19,810] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 15:22:19,810] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/10/2025 15:22:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 15:22:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-base-num=30/runs/Mar10_15-22-18_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-base-num=30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-base-num=30,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 15:22:22 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json
03/10/2025 15:22:22 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json
03/10/2025 15:22:22 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-6b90b90b9712ba2d
03/10/2025 15:22:22 - INFO - datasets.builder - Using custom data configuration default-6b90b90b9712ba2d
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 15:22:22 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 15:22:22 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:22:22 - INFO - datasets.info - Loading Dataset info from .cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 15:22:22 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:22:22 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 15:22:22,855 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:22:22,858 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 15:22:23,092 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:22:23,092 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:22:23,101 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 15:22:23,101 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:22:23,101 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 15:22:23,234 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[WARNING|modeling_utils.py:5103] 2025-03-10 15:22:23,263 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:5091] 2025-03-10 15:22:23,282 >> Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 15:22:23,282 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-01a9149bf071249d.arrow
03/10/2025 15:22:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-01a9149bf071249d.arrow
Running tokenizer on dataset:  23%|█████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                          | 9000/39286 [00:02<00:09, 3159.50 examples/s][rank1]:[W310 15:22:25.608655714 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:09<00:00, 4119.19 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2074 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4659d3f7f6f190b2.arrow
03/10/2025 15:22:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4659d3f7f6f190b2.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 4354.45 examples/s]
[rank0]:[W310 15:22:33.263594142 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 15:22:36 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'It(O) was(O) praised(O) by(O) scientists(O) such(O) as(O) Darwin(B-writer) ((O) to(O) whom(O) the(O) book(O) was(O) dedicated(O) )(O),(O) and(O) Charles(B-person) Lyell(I-person),(O) and(O) by(O) non-scientists(O) such(O) as(O) the(O) novelist(O) Joseph(B-person) Conrad(I-person),(O) who(O) called(O) it(O) his(O) favorite(O) bedside(O) companion(O) and(O) used(O) it(O) as(O) source(O) of(O) information(O) for(O) several(O) of(O) his(O) novels(O),(O) especially(O) Lord(B-book) Jim(I-book).(O)', 'sentence2': 'It was praised by scientists such as Darwin ( to whom the book was dedicated ) , and Charles Lyell , and by non-scientists such as the novelist Joseph Conrad , who called it his favorite bedside companion and used it as source of information for several of his novels , especially Lord Jim .', 'label': 1.03, 'idx': 7296, 'input_ids': [0, 243, 1640, 673, 43, 21, 1640, 673, 43, 6425, 1640, 673, 43, 30, 1640, 673, 43, 4211, 1640, 673, 43, 215, 1640, 673, 43, 25, 1640, 673, 43, 24458, 1640, 387, 12, 9408, 43, 41006, 673, 43, 7, 1640, 673, 43, 2661, 1640, 673, 43, 5, 1640, 673, 43, 1040, 1640, 673, 43, 21, 1640, 673, 43, 3688, 1640, 673, 43, 4839, 1640, 673, 238, 1640, 673, 43, 8, 1640, 673, 43, 3163, 1640, 387, 12, 5970, 43, 10888, 1641, 1640, 100, 12, 5970, 238, 1640, 673, 43, 8, 1640, 673, 43, 30, 1640, 673, 43, 786, 12, 38957, 1952, 1640, 673, 43, 215, 1640, 673, 43, 25, 1640, 673, 43, 5, 1640, 673, 43, 29613, 1640, 673, 43, 3351, 1640, 387, 12, 5970, 43, 24078, 1640, 100, 12, 5970, 238, 1640, 673, 43, 54, 1640, 673, 43, 373, 1640, 673, 43, 24, 1640, 673, 43, 39, 1640, 673, 43, 2674, 1640, 673, 43, 3267, 3730, 1640, 673, 43, 15625, 1640, 673, 43, 8, 1640, 673, 43, 341, 1640, 673, 43, 24, 1640, 673, 43, 25, 1640, 673, 43, 1300, 1640, 673, 43, 9, 1640, 673, 43, 335, 1640, 673, 43, 13, 1640, 673, 43, 484, 1640, 673, 43, 9, 1640, 673, 43, 39, 1640, 673, 43, 19405, 1640, 673, 238, 1640, 673, 43, 941, 1640, 673, 43, 5736, 1640, 387, 12, 6298, 43, 2488, 1640, 100, 12, 6298, 322, 1640, 673, 43, 2, 2, 243, 21, 6425, 30, 4211, 215, 25, 24458, 36, 7, 2661, 5, 1040, 21, 3688, 4839, 2156, 8, 3163, 10888, 1641, 2156, 8, 30, 786, 12, 38957, 1952, 215, 25, 5, 29613, 3351, 24078, 2156, 54, 373, 24, 39, 2674, 3267, 3730, 15625, 8, 341, 24, 25, 1300, 9, 335, 13, 484, 9, 39, 19405, 2156, 941, 5736, 2488, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:22:36 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'The(O) natural(B-metric) gradient(I-metric) of(O) mathE(B-algorithm) f(O) ((O) x(O) )(O) /(O) math(O),(O) complying(O) with(O) the(O) Fisher(B-metric) information(I-metric) metric(I-metric) ((O) an(O) informational(O) distance(O) measure(O) between(O) probability(O) distributions(O) and(O) the(O) curvature(O) of(O) the(O) relative(O) entropy(O) )(O),(O) now(O) reads(O)', 'sentence2': 'The natural gradient of mathE f ( x ) / math , complying with the Fisher information metric ( an informational distance measure between probability distributions and the curvature of the relative entropy ) , now reads', 'label': 1.48, 'idx': 1639, 'input_ids': [0, 133, 1640, 673, 43, 1632, 1640, 387, 12, 5646, 4063, 43, 43141, 1640, 100, 12, 5646, 4063, 43, 9, 1640, 673, 43, 10638, 717, 1640, 387, 12, 337, 47549, 43, 856, 1640, 673, 43, 41006, 673, 43, 3023, 1640, 673, 43, 4839, 1640, 673, 43, 1589, 1640, 673, 43, 10638, 1640, 673, 238, 1640, 673, 43, 27815, 1640, 673, 43, 19, 1640, 673, 43, 5, 1640, 673, 43, 6868, 1640, 387, 12, 5646, 4063, 43, 335, 1640, 100, 12, 5646, 4063, 43, 14823, 1640, 100, 12, 5646, 4063, 43, 41006, 673, 43, 41, 1640, 673, 43, 31603, 1640, 673, 43, 4472, 1640, 673, 43, 2450, 1640, 673, 43, 227, 1640, 673, 43, 18102, 1640, 673, 43, 26070, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 42294, 18830, 1640, 673, 43, 9, 1640, 673, 43, 5, 1640, 673, 43, 5407, 1640, 673, 43, 47382, 1640, 673, 43, 4839, 1640, 673, 238, 1640, 673, 43, 122, 1640, 673, 43, 7005, 1640, 673, 43, 2, 2, 133, 1632, 43141, 9, 10638, 717, 856, 36, 3023, 4839, 1589, 10638, 2156, 27815, 19, 5, 6868, 335, 14823, 36, 41, 31603, 4472, 2450, 227, 18102, 26070, 8, 5, 42294, 18830, 9, 5, 5407, 47382, 4839, 2156, 122, 7005, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:22:36 - INFO - __main__ - Sample 18024 of the training set: {'sentence1': 'It(O) was(O) set(O) up(O) by(O) former(O) left-leaning(O) Christian(B-political party) Democrats(I-political party),(O) ((O) former(O) Italian(O) Liberal(B-political party) Party(I-political party) and(O) former(O) Italian(B-political party) Republican(I-political party) Party(I-political party) )(O),(O) as(O) well(O) as(O) other(O) left-wing(O) politicians(O) from(O) the(O) former(O) Italian(O) Socialist(B-political party) Party(I-political party) and(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party).(O)', 'sentence2': 'It was set up by former left-leaning Christian Democrats , ( former Italian Liberal Party and former Italian Republican Party ) , as well as other left-wing politicians from the former Italian Socialist Party and Federation of the Greens .', 'label': 1.64, 'idx': 18024, 'input_ids': [0, 243, 1640, 673, 43, 21, 1640, 673, 43, 278, 1640, 673, 43, 62, 1640, 673, 43, 30, 1640, 673, 43, 320, 1640, 673, 43, 314, 12, 18813, 1640, 673, 43, 2412, 1640, 387, 12, 17522, 537, 43, 1574, 1640, 100, 12, 17522, 537, 238, 1640, 673, 43, 41006, 673, 43, 320, 1640, 673, 43, 3108, 1640, 673, 43, 7612, 1640, 387, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 8, 1640, 673, 43, 320, 1640, 673, 43, 3108, 1640, 387, 12, 17522, 537, 43, 1172, 1640, 100, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 4839, 1640, 673, 238, 1640, 673, 43, 25, 1640, 673, 43, 157, 1640, 673, 43, 25, 1640, 673, 43, 97, 1640, 673, 43, 314, 12, 5577, 1640, 673, 43, 3770, 1640, 673, 43, 31, 1640, 673, 43, 5, 1640, 673, 43, 320, 1640, 673, 43, 3108, 1640, 673, 43, 19476, 1640, 387, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 8, 1640, 673, 43, 6692, 1640, 387, 12, 17522, 537, 43, 9, 1640, 100, 12, 17522, 537, 43, 5, 1640, 100, 12, 17522, 537, 43, 11876, 1640, 100, 12, 17522, 537, 322, 1640, 673, 43, 2, 2, 243, 21, 278, 62, 30, 320, 314, 12, 18813, 2412, 1574, 2156, 36, 320, 3108, 7612, 1643, 8, 320, 3108, 1172, 1643, 4839, 2156, 25, 157, 25, 97, 314, 12, 5577, 3770, 31, 5, 320, 3108, 19476, 1643, 8, 6692, 9, 5, 11876, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 15:22:36,702 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 15:22:36,896 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 15:22:36,901] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 15:22:36,901] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:09<00:00, 4084.67 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 4327.48 examples/s]
[2025-03-10 15:22:47,355] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 15:22:47,357] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 15:22:47,357] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 15:22:47,363] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 15:22:47,363] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 15:22:47,363] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 15:22:47,363] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 15:22:47,363] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 15:22:47,363] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 15:22:47,363] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 15:22:47,637] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:22:47,797] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 15:22:47,798] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.58 GB         CA 0.58 GB         Max_CA 1 GB
[2025-03-10 15:22:47,798] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.13 GB, percent = 5.4%
[2025-03-10 15:22:47,953] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 15:22:47,953] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.7 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 15:22:47,954] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.16 GB, percent = 5.4%
[2025-03-10 15:22:47,954] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 15:22:48,101] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 15:22:48,102] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 15:22:48,102] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.16 GB, percent = 5.4%
[2025-03-10 15:22:48,103] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 15:22:48,103] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:22:48,103] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 15:22:48,103] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f850f4b59a0>
[2025-03-10 15:22:48,104] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 15:22:48,104] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 15:22:48,104] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 15:22:48,104] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 15:22:48,104] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f85297676e0>
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 15:22:48,105] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 15:22:48,106] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 15:22:48,107] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 15:22:48,108] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 15:22:48,108] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 15:22:48,109 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 15:22:48,109 >>   Num examples = 39,286
[INFO|trainer.py:2416] 2025-03-10 15:22:48,109 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 15:22:48,109 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 15:22:48,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 15:22:48,109 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 15:22:48,109 >>   Total optimization steps = 49,080
[INFO|trainer.py:2423] 2025-03-10 15:22:48,110 >>   Number of trainable parameters = 124,646,401
{'loss': 1.0434, 'grad_norm': 13.738326072692871, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [02:34<1:39:32,  8.01it/s][INFO|trainer.py:930] 2025-03-10 15:25:22,785 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:25:22,788 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:25:22,789 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:25:22,789 >>   Batch size = 4
{'eval_loss': 1.3611586093902588, 'eval_mse': 1.3605272641177348, 'eval_pearson': 0.6209643066974064, 'eval_spearmanr': 0.6308939237954797, 'eval_runtime': 2.3476, 'eval_samples_per_second': 883.439, 'eval_steps_per_second': 110.749, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [02:37<1:39:32,  8.01it/s]
[INFO|trainer.py:3955] 2025-03-10 15:25:25,334 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227
[INFO|configuration_utils.py:423] 2025-03-10 15:25:25,336 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:25:25,664 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:25:25,665 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:25:25,665 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/special_tokens_map.json
[2025-03-10 15:25:25,711] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1227 is about to be saved!
[2025-03-10 15:25:25,715] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt
[2025-03-10 15:25:25,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt...
[2025-03-10 15:25:26,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt.
[2025-03-10 15:25:26,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:25:27,551] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:25:27,551] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:25:27,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1227 is ready now!
{'loss': 0.6976, 'grad_norm': 5.766076564788818, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [05:11<1:36:58,  8.01it/s][INFO|trainer.py:930] 2025-03-10 15:27:59,178 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:27:59,182 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:27:59,182 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:27:59,182 >>   Batch size = 4
{'eval_loss': 1.2988240718841553, 'eval_mse': 1.297800222036349, 'eval_pearson': 0.6233620061790075, 'eval_spearmanr': 0.6341138458495233, 'eval_runtime': 2.3213, 'eval_samples_per_second': 893.478, 'eval_steps_per_second': 112.008, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [05:13<1:36:58,  8.01it/s]
[INFO|trainer.py:3955] 2025-03-10 15:28:01,713 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454
[INFO|configuration_utils.py:423] 2025-03-10 15:28:01,715 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:28:02,054 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:28:02,055 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:28:02,055 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/special_tokens_map.json
[2025-03-10 15:28:02,111] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2455 is about to be saved!
[2025-03-10 15:28:02,115] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt
[2025-03-10 15:28:02,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt...
[2025-03-10 15:28:02,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt.
[2025-03-10 15:28:02,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:28:03,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:28:03,971] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:28:03,971] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2455 is ready now!
{'loss': 0.529, 'grad_norm': 9.412515640258789, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [07:48<1:34:43,  7.99it/s][INFO|trainer.py:930] 2025-03-10 15:30:36,948 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:30:36,952 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:30:36,952 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:30:36,952 >>   Batch size = 4
{'eval_loss': 1.1190812587738037, 'eval_mse': 1.1184010935611413, 'eval_pearson': 0.6826517017233213, 'eval_spearmanr': 0.6860400444898141, 'eval_runtime': 2.4198, 'eval_samples_per_second': 857.105, 'eval_steps_per_second': 107.448, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [07:51<1:34:43,  7.99it/s]
[INFO|trainer.py:3955] 2025-03-10 15:30:39,528 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681
[INFO|configuration_utils.py:423] 2025-03-10 15:30:39,530 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:30:39,860 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:30:39,861 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:30:39,861 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/special_tokens_map.json
[2025-03-10 15:30:39,922] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3682 is about to be saved!
[2025-03-10 15:30:39,926] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt
[2025-03-10 15:30:39,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt...
[2025-03-10 15:30:40,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt.
[2025-03-10 15:30:40,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:30:41,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:30:41,751] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:30:41,751] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3682 is ready now!
{'loss': 0.4588, 'grad_norm': 10.46472454071045, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [10:24<1:30:51,  8.10it/s][INFO|trainer.py:930] 2025-03-10 15:33:12,718 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:33:12,722 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:33:12,722 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:33:12,722 >>   Batch size = 4
{'eval_loss': 1.2999449968338013, 'eval_mse': 1.2994769424228796, 'eval_pearson': 0.6472631358885466, 'eval_spearmanr': 0.6592093510104856, 'eval_runtime': 2.3145, 'eval_samples_per_second': 896.108, 'eval_steps_per_second': 112.338, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [10:26<1:30:51,  8.10it/s]
[INFO|trainer.py:3955] 2025-03-10 15:33:15,239 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908
[INFO|configuration_utils.py:423] 2025-03-10 15:33:15,241 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:33:15,567 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:33:15,568 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:33:15,568 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/special_tokens_map.json
[2025-03-10 15:33:15,624] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4910 is about to be saved!
[2025-03-10 15:33:15,627] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt
[2025-03-10 15:33:15,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt...
[2025-03-10 15:33:16,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt.
[2025-03-10 15:33:16,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:33:17,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:33:17,450] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:33:17,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4910 is ready now!
{'loss': 0.3665, 'grad_norm': 9.52518367767334, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [13:01<1:29:14,  8.02it/s][INFO|trainer.py:930] 2025-03-10 15:35:50,032 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:35:50,037 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:35:50,037 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:35:50,037 >>   Batch size = 4
{'eval_loss': 1.143189549446106, 'eval_mse': 1.1425323341483664, 'eval_pearson': 0.6773964241169437, 'eval_spearmanr': 0.6821005001070539, 'eval_runtime': 2.3903, 'eval_samples_per_second': 867.676, 'eval_steps_per_second': 108.773, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [13:04<1:29:14,  8.02it/s]
[INFO|trainer.py:3955] 2025-03-10 15:35:52,534 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135
[INFO|configuration_utils.py:423] 2025-03-10 15:35:52,536 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:35:52,772 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:35:52,774 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:35:52,774 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/special_tokens_map.json
[2025-03-10 15:35:52,831] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6137 is about to be saved!
[2025-03-10 15:35:52,835] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt
[2025-03-10 15:35:52,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt...
[2025-03-10 15:35:53,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt.
[2025-03-10 15:35:53,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:35:54,502] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:35:54,502] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:35:54,502] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6137 is ready now!
{'loss': 0.3243, 'grad_norm': 2.641069173812866, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [15:36<1:25:13,  8.16it/s][INFO|trainer.py:930] 2025-03-10 15:38:25,002 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:38:25,006 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:38:25,006 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:38:25,006 >>   Batch size = 4
{'eval_loss': 1.105728030204773, 'eval_mse': 1.105804898814732, 'eval_pearson': 0.6891315400394675, 'eval_spearmanr': 0.6905338714981881, 'eval_runtime': 2.3371, 'eval_samples_per_second': 887.437, 'eval_steps_per_second': 111.25, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [15:39<1:25:13,  8.16it/s]
[INFO|trainer.py:3955] 2025-03-10 15:38:27,540 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362
[INFO|configuration_utils.py:423] 2025-03-10 15:38:27,542 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:38:27,893 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:38:27,894 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:38:27,894 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/special_tokens_map.json
[2025-03-10 15:38:27,948] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7365 is about to be saved!
[2025-03-10 15:38:27,952] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt
[2025-03-10 15:38:27,952] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt...
[2025-03-10 15:38:28,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt.
[2025-03-10 15:38:28,371] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:38:29,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:38:29,765] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:38:29,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7365 is ready now!
{'loss': 0.2571, 'grad_norm': 17.146081924438477, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [18:10<1:24:12,  8.01it/s][INFO|trainer.py:930] 2025-03-10 15:40:58,697 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:40:58,701 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:40:58,701 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:40:58,701 >>   Batch size = 4
{'eval_loss': 1.196001648902893, 'eval_mse': 1.1956978030466872, 'eval_pearson': 0.6974258370120157, 'eval_spearmanr': 0.7085009359228183, 'eval_runtime': 2.3328, 'eval_samples_per_second': 889.077, 'eval_steps_per_second': 111.456, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [18:12<1:24:12,  8.01it/s]
[INFO|trainer.py:3955] 2025-03-10 15:41:01,238 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589
[INFO|configuration_utils.py:423] 2025-03-10 15:41:01,240 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:41:01,593 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:41:01,594 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:41:01,594 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/special_tokens_map.json
[2025-03-10 15:41:01,655] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8592 is about to be saved!
[2025-03-10 15:41:01,660] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt
[2025-03-10 15:41:01,660] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt...
[2025-03-10 15:41:02,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt.
[2025-03-10 15:41:02,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:41:03,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:41:03,544] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:41:03,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8592 is ready now!
{'loss': 0.2299, 'grad_norm': 4.498457431793213, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [20:46<1:20:25,  8.14it/s][INFO|trainer.py:930] 2025-03-10 15:43:34,502 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:43:34,507 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:43:34,507 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:43:34,507 >>   Batch size = 4
{'eval_loss': 1.0847197771072388, 'eval_mse': 1.0842929284161058, 'eval_pearson': 0.6939820097298965, 'eval_spearmanr': 0.7051737030162981, 'eval_runtime': 2.3371, 'eval_samples_per_second': 887.409, 'eval_steps_per_second': 111.247, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [20:48<1:20:25,  8.14it/s]
[INFO|trainer.py:3955] 2025-03-10 15:43:37,048 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816
[INFO|configuration_utils.py:423] 2025-03-10 15:43:37,050 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:43:37,398 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:43:37,399 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:43:37,399 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/special_tokens_map.json
[2025-03-10 15:43:37,455] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9820 is about to be saved!
[2025-03-10 15:43:37,459] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt
[2025-03-10 15:43:37,459] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt...
[2025-03-10 15:43:37,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt.
[2025-03-10 15:43:37,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:43:39,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:43:39,333] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:43:39,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9820 is ready now!
{'loss': 0.1897, 'grad_norm': 7.8146772384643555, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [23:23<1:18:17,  8.10it/s][INFO|trainer.py:930] 2025-03-10 15:46:11,949 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:46:11,953 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:46:11,953 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:46:11,953 >>   Batch size = 4
{'eval_loss': 1.140483021736145, 'eval_mse': 1.1397815263374793, 'eval_pearson': 0.6853443130468513, 'eval_spearmanr': 0.7030018946566141, 'eval_runtime': 2.3214, 'eval_samples_per_second': 893.407, 'eval_steps_per_second': 111.999, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [23:26<1:18:17,  8.10it/s]
[INFO|trainer.py:3955] 2025-03-10 15:46:14,476 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043
[INFO|configuration_utils.py:423] 2025-03-10 15:46:14,478 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:46:14,823 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:46:14,824 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:46:14,824 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/special_tokens_map.json
[2025-03-10 15:46:14,883] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11047 is about to be saved!
[2025-03-10 15:46:14,887] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt
[2025-03-10 15:46:14,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt...
[2025-03-10 15:46:15,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt.
[2025-03-10 15:46:15,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:46:16,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:46:16,743] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:46:16,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11047 is ready now!
{'loss': 0.1713, 'grad_norm': 3.166757345199585, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [25:59<1:15:27,  8.13it/s][INFO|trainer.py:930] 2025-03-10 15:48:47,691 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:48:47,694 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:48:47,694 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:48:47,694 >>   Batch size = 4
{'eval_loss': 1.049790382385254, 'eval_mse': 1.049438560480322, 'eval_pearson': 0.7268390926027641, 'eval_spearmanr': 0.7376296704433891, 'eval_runtime': 2.3158, 'eval_samples_per_second': 895.572, 'eval_steps_per_second': 112.27, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [26:01<1:15:27,  8.13it/s]
[INFO|trainer.py:3955] 2025-03-10 15:48:50,212 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270
[INFO|configuration_utils.py:423] 2025-03-10 15:48:50,214 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:48:50,560 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:48:50,561 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:48:50,561 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/special_tokens_map.json
[2025-03-10 15:48:50,615] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12275 is about to be saved!
[2025-03-10 15:48:50,619] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt
[2025-03-10 15:48:50,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt...
[2025-03-10 15:48:51,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt.
[2025-03-10 15:48:51,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:48:52,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:48:52,496] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:48:52,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12275 is ready now!
{'loss': 0.1355, 'grad_norm': 4.444932460784912, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [28:36<1:13:26,  8.07it/s][INFO|trainer.py:930] 2025-03-10 15:51:24,521 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:51:24,525 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:51:24,525 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:51:24,525 >>   Batch size = 4
{'eval_loss': 1.108707308769226, 'eval_mse': 1.1081976382458796, 'eval_pearson': 0.7274117081123652, 'eval_spearmanr': 0.7344888697684777, 'eval_runtime': 2.3377, 'eval_samples_per_second': 887.205, 'eval_steps_per_second': 111.221, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [28:38<1:13:26,  8.07it/s]
[INFO|trainer.py:3955] 2025-03-10 15:51:27,062 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497
[INFO|configuration_utils.py:423] 2025-03-10 15:51:27,064 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:51:27,407 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:51:27,408 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:51:27,408 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/special_tokens_map.json
[2025-03-10 15:51:27,468] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13502 is about to be saved!
[2025-03-10 15:51:27,472] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt
[2025-03-10 15:51:27,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt...
[2025-03-10 15:51:27,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt.
[2025-03-10 15:51:27,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:51:29,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:51:29,325] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:51:29,325] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13502 is ready now!
{'loss': 0.1279, 'grad_norm': 3.2607266902923584, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [31:12<1:10:05,  8.17it/s][INFO|trainer.py:930] 2025-03-10 15:54:00,310 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:00,313 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:00,313 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:54:00,314 >>   Batch size = 4
{'eval_loss': 1.1698687076568604, 'eval_mse': 1.1696384601445875, 'eval_pearson': 0.7161127973819306, 'eval_spearmanr': 0.7242205168828718, 'eval_runtime': 2.3412, 'eval_samples_per_second': 885.88, 'eval_steps_per_second': 111.055, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [31:14<1:10:05,  8.17it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:02,856 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724
[INFO|configuration_utils.py:423] 2025-03-10 15:54:02,858 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:03,219 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:03,220 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:03,220 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/special_tokens_map.json
[2025-03-10 15:54:03,278] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14730 is about to be saved!
[2025-03-10 15:54:03,281] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt
[2025-03-10 15:54:03,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt...
[2025-03-10 15:54:03,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt.
[2025-03-10 15:54:03,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:54:05,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:54:05,216] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:54:05,216] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14730 is ready now!
{'loss': 0.0995, 'grad_norm': 3.7249810695648193, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 15951/49080 [33:47<1:05:11,  8.47it/s][INFO|trainer.py:930] 2025-03-10 15:56:35,947 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:56:35,951 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:56:35,951 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:56:35,951 >>   Batch size = 4
{'eval_loss': 0.9657832384109497, 'eval_mse': 0.9656774124046186, 'eval_pearson': 0.7384282456380736, 'eval_spearmanr': 0.7523266926206658, 'eval_runtime': 2.3192, 'eval_samples_per_second': 894.284, 'eval_steps_per_second': 112.109, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 15951/49080 [33:50<1:05:11,  8.47it/s]
[INFO|trainer.py:3955] 2025-03-10 15:56:38,432 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951
[INFO|configuration_utils.py:423] 2025-03-10 15:56:38,434 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:56:38,685 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:56:38,685 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:56:38,686 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/special_tokens_map.json
[2025-03-10 15:56:38,741] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15957 is about to be saved!
[2025-03-10 15:56:38,744] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt
[2025-03-10 15:56:38,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt...
[2025-03-10 15:56:39,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt.
[2025-03-10 15:56:39,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:56:40,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:56:40,178] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:56:40,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15957 is ready now!
{'loss': 0.0971, 'grad_norm': 3.5163497924804688, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 17178/49080 [36:12<59:48,  8.89it/s][INFO|trainer.py:930] 2025-03-10 15:59:00,948 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:59:00,951 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:59:00,951 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 15:59:00,951 >>   Batch size = 4
{'eval_loss': 1.066803216934204, 'eval_mse': 1.0664748871912721, 'eval_pearson': 0.7329134335089325, 'eval_spearmanr': 0.7393487287466113, 'eval_runtime': 2.3142, 'eval_samples_per_second': 896.221, 'eval_steps_per_second': 112.352, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 17178/49080 [36:15<59:48,  8.89it/s]
[INFO|trainer.py:3955] 2025-03-10 15:59:03,428 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178
[INFO|configuration_utils.py:423] 2025-03-10 15:59:03,430 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:59:03,686 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:59:03,687 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:59:03,687 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/special_tokens_map.json
[2025-03-10 15:59:03,735] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17185 is about to be saved!
[2025-03-10 15:59:03,738] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt
[2025-03-10 15:59:03,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt...
[2025-03-10 15:59:04,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt.
[2025-03-10 15:59:04,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:59:05,179] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:59:05,180] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:59:05,180] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17185 is ready now!
{'loss': 0.0791, 'grad_norm': 8.339714050292969, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 18405/49080 [38:37<57:51,  8.84it/s][INFO|trainer.py:930] 2025-03-10 16:01:25,982 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:01:25,986 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:01:25,986 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:01:25,987 >>   Batch size = 4
{'eval_loss': 1.1177719831466675, 'eval_mse': 1.1174879973051979, 'eval_pearson': 0.7075009667555472, 'eval_spearmanr': 0.7220660788173351, 'eval_runtime': 2.3396, 'eval_samples_per_second': 886.492, 'eval_steps_per_second': 111.132, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 18405/49080 [38:40<57:51,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 16:01:28,496 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405
[INFO|configuration_utils.py:423] 2025-03-10 16:01:28,498 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:01:28,740 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:01:28,741 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:01:28,741 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/special_tokens_map.json
[2025-03-10 16:01:28,800] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18412 is about to be saved!
[2025-03-10 16:01:28,803] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt
[2025-03-10 16:01:28,803] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt...
[2025-03-10 16:01:29,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt.
[2025-03-10 16:01:29,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:01:30,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:01:30,245] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:01:30,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18412 is ready now!
{'loss': 0.0741, 'grad_norm': 1.3231950998306274, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 19632/49080 [41:02<56:40,  8.66it/s][INFO|trainer.py:930] 2025-03-10 16:03:50,634 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:03:50,637 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:03:50,637 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:03:50,637 >>   Batch size = 4
{'eval_loss': 1.090216040611267, 'eval_mse': 1.0901575490846698, 'eval_pearson': 0.7270226207853854, 'eval_spearmanr': 0.7419236978450984, 'eval_runtime': 2.3129, 'eval_samples_per_second': 896.726, 'eval_steps_per_second': 112.415, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 19632/49080 [41:04<56:40,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 16:03:53,116 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632
[INFO|configuration_utils.py:423] 2025-03-10 16:03:53,118 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:03:53,385 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:03:53,386 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:03:53,386 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/special_tokens_map.json
[2025-03-10 16:03:53,439] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19640 is about to be saved!
[2025-03-10 16:03:53,442] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt
[2025-03-10 16:03:53,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt...
[2025-03-10 16:03:53,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt.
[2025-03-10 16:03:53,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:03:54,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:03:54,939] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:03:54,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19640 is ready now!
{'loss': 0.063, 'grad_norm': 3.0331203937530518, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 20859/49080 [43:26<53:04,  8.86it/s][INFO|trainer.py:930] 2025-03-10 16:06:15,048 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:06:15,051 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:06:15,051 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:06:15,051 >>   Batch size = 4
{'eval_loss': 1.083235502243042, 'eval_mse': 1.0828939152085792, 'eval_pearson': 0.7193321043671785, 'eval_spearmanr': 0.7314117968048236, 'eval_runtime': 2.3246, 'eval_samples_per_second': 892.213, 'eval_steps_per_second': 111.849, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 20859/49080 [43:29<53:04,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 16:06:17,533 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859
[INFO|configuration_utils.py:423] 2025-03-10 16:06:17,535 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:06:17,799 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:06:17,800 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:06:17,800 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/special_tokens_map.json
[2025-03-10 16:06:17,851] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20867 is about to be saved!
[2025-03-10 16:06:17,854] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt
[2025-03-10 16:06:17,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt...
[2025-03-10 16:06:18,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt.
[2025-03-10 16:06:18,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:06:19,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:06:19,287] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:06:19,287] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20867 is ready now!
{'loss': 0.06, 'grad_norm': 11.611381530761719, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 22086/49080 [45:50<50:43,  8.87it/s][INFO|trainer.py:930] 2025-03-10 16:08:38,343 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:08:38,346 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:08:38,346 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:08:38,346 >>   Batch size = 4
{'eval_loss': 1.0388976335525513, 'eval_mse': 1.038400171577873, 'eval_pearson': 0.7276037886377689, 'eval_spearmanr': 0.7411463962916175, 'eval_runtime': 2.3245, 'eval_samples_per_second': 892.239, 'eval_steps_per_second': 111.852, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 22086/49080 [45:52<50:43,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 16:08:40,827 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086
[INFO|configuration_utils.py:423] 2025-03-10 16:08:40,829 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:08:41,094 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:08:41,095 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:08:41,095 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/special_tokens_map.json
[2025-03-10 16:08:41,145] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22095 is about to be saved!
[2025-03-10 16:08:41,149] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt
[2025-03-10 16:08:41,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt...
[2025-03-10 16:08:41,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt.
[2025-03-10 16:08:41,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:08:42,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:08:42,606] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:08:42,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22095 is ready now!
{'loss': 0.0518, 'grad_norm': 2.310575008392334, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 23313/49080 [48:14<48:43,  8.81it/s][INFO|trainer.py:930] 2025-03-10 16:11:02,652 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:11:02,655 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:11:02,655 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:11:02,655 >>   Batch size = 4
{'eval_loss': 0.9702886939048767, 'eval_mse': 0.9701840564852855, 'eval_pearson': 0.7534093802404465, 'eval_spearmanr': 0.764492584135522, 'eval_runtime': 2.3306, 'eval_samples_per_second': 889.882, 'eval_steps_per_second': 111.557, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 23313/49080 [48:16<48:43,  8.81it/s]
[INFO|trainer.py:3955] 2025-03-10 16:11:05,142 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313
[INFO|configuration_utils.py:423] 2025-03-10 16:11:05,144 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:11:05,415 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:11:05,416 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:11:05,416 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/special_tokens_map.json
[2025-03-10 16:11:05,466] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23322 is about to be saved!
[2025-03-10 16:11:05,469] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt
[2025-03-10 16:11:05,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt...
[2025-03-10 16:11:05,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt.
[2025-03-10 16:11:05,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:11:06,951] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:11:06,951] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:11:06,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23322 is ready now!
{'loss': 0.0482, 'grad_norm': 3.3522722721099854, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 24540/49080 [50:38<47:10,  8.67it/s][INFO|trainer.py:930] 2025-03-10 16:13:26,307 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:13:26,311 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:13:26,311 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:13:26,311 >>   Batch size = 4
{'eval_loss': 1.054166555404663, 'eval_mse': 1.0537344789780898, 'eval_pearson': 0.7234335663800473, 'eval_spearmanr': 0.7426160235334268, 'eval_runtime': 2.3237, 'eval_samples_per_second': 892.525, 'eval_steps_per_second': 111.888, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 24540/49080 [50:40<47:10,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 16:13:28,790 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540
[INFO|configuration_utils.py:423] 2025-03-10 16:13:28,792 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:13:29,057 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:13:29,058 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:13:29,058 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/special_tokens_map.json
[2025-03-10 16:13:29,105] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24550 is about to be saved!
[2025-03-10 16:13:29,109] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt
[2025-03-10 16:13:29,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt...
[2025-03-10 16:13:29,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt.
[2025-03-10 16:13:29,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:13:30,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:13:30,562] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:13:30,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24550 is ready now!
{'loss': 0.0445, 'grad_norm': 1.7856289148330688, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 25767/49080 [53:04<44:01,  8.83it/s][INFO|trainer.py:930] 2025-03-10 16:15:52,139 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:15:52,142 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:15:52,142 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:15:52,142 >>   Batch size = 4
{'eval_loss': 0.9442387819290161, 'eval_mse': 0.944334368811509, 'eval_pearson': 0.750970304902124, 'eval_spearmanr': 0.7652527560263332, 'eval_runtime': 2.332, 'eval_samples_per_second': 889.357, 'eval_steps_per_second': 111.491, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 25767/49080 [53:06<44:01,  8.83it/s]
[INFO|trainer.py:3955] 2025-03-10 16:15:54,631 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767
[INFO|configuration_utils.py:423] 2025-03-10 16:15:54,633 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:15:54,888 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:15:54,889 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:15:54,889 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/special_tokens_map.json
[2025-03-10 16:15:54,942] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25777 is about to be saved!
[2025-03-10 16:15:54,945] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt
[2025-03-10 16:15:54,945] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt...
[2025-03-10 16:15:55,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt.
[2025-03-10 16:15:55,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:15:56,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:15:56,422] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:15:56,422] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25777 is ready now!
{'loss': 0.0426, 'grad_norm': 1.2330622673034668, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 26994/49080 [55:30<42:37,  8.64it/s][INFO|trainer.py:930] 2025-03-10 16:18:18,268 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:18:18,272 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:18:18,272 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:18:18,272 >>   Batch size = 4
{'eval_loss': 1.176781415939331, 'eval_mse': 1.1763287704499041, 'eval_pearson': 0.7062754452471449, 'eval_spearmanr': 0.7182651299579993, 'eval_runtime': 2.3469, 'eval_samples_per_second': 883.716, 'eval_steps_per_second': 110.784, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 26994/49080 [55:32<42:37,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 16:18:20,769 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994
[INFO|configuration_utils.py:423] 2025-03-10 16:18:20,771 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:18:21,026 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:18:21,026 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:18:21,026 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/special_tokens_map.json
[2025-03-10 16:18:21,073] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27005 is about to be saved!
[2025-03-10 16:18:21,076] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt
[2025-03-10 16:18:21,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt...
[2025-03-10 16:18:21,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt.
[2025-03-10 16:18:21,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:18:22,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:18:22,520] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:18:22,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27005 is ready now!
{'loss': 0.0359, 'grad_norm': 2.6841371059417725, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 28221/49080 [57:57<40:39,  8.55it/s][INFO|trainer.py:930] 2025-03-10 16:20:45,434 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:20:45,437 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:20:45,437 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:20:45,437 >>   Batch size = 4
{'eval_loss': 1.1355293989181519, 'eval_mse': 1.1350797829770536, 'eval_pearson': 0.7118001686651411, 'eval_spearmanr': 0.7301336389606775, 'eval_runtime': 2.3167, 'eval_samples_per_second': 895.239, 'eval_steps_per_second': 112.229, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 28221/49080 [57:59<40:39,  8.55it/s]
[INFO|trainer.py:3955] 2025-03-10 16:20:47,919 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221
[INFO|configuration_utils.py:423] 2025-03-10 16:20:47,921 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:20:48,182 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:20:48,183 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:20:48,183 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/special_tokens_map.json
[2025-03-10 16:20:48,241] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28232 is about to be saved!
[2025-03-10 16:20:48,245] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt
[2025-03-10 16:20:48,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt...
[2025-03-10 16:20:48,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt.
[2025-03-10 16:20:48,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:20:49,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:20:49,796] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:20:49,796] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28232 is ready now!
{'loss': 0.0354, 'grad_norm': 1.9633188247680664, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 29448/49080 [1:00:23<37:40,  8.68it/s][INFO|trainer.py:930] 2025-03-10 16:23:11,381 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:23:11,384 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:23:11,384 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:23:11,384 >>   Batch size = 4
{'eval_loss': 0.9888937473297119, 'eval_mse': 0.9888617423369317, 'eval_pearson': 0.7507492308692005, 'eval_spearmanr': 0.7636619681695627, 'eval_runtime': 2.3323, 'eval_samples_per_second': 889.263, 'eval_steps_per_second': 111.479, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 29448/49080 [1:00:25<37:40,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 16:23:13,886 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448
[INFO|configuration_utils.py:423] 2025-03-10 16:23:13,888 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:23:14,148 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:23:14,149 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:23:14,149 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/special_tokens_map.json
[2025-03-10 16:23:14,199] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29460 is about to be saved!
[2025-03-10 16:23:14,202] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt
[2025-03-10 16:23:14,202] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt...
[2025-03-10 16:23:14,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt.
[2025-03-10 16:23:14,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:23:15,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:23:15,801] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:23:15,801] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29460 is ready now!
{'loss': 0.0318, 'grad_norm': 0.8312442302703857, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 30675/49080 [1:02:52<35:45,  8.58it/s][INFO|trainer.py:930] 2025-03-10 16:25:40,293 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:25:40,297 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:25:40,297 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:25:40,297 >>   Batch size = 4
{'eval_loss': 1.012190580368042, 'eval_mse': 1.01224161320965, 'eval_pearson': 0.740045980096055, 'eval_spearmanr': 0.7505900515419986, 'eval_runtime': 2.4091, 'eval_samples_per_second': 860.893, 'eval_steps_per_second': 107.923, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 30675/49080 [1:02:54<35:45,  8.58it/s]
[INFO|trainer.py:3955] 2025-03-10 16:25:42,816 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675
[INFO|configuration_utils.py:423] 2025-03-10 16:25:42,818 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:25:43,085 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:25:43,086 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:25:43,086 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/special_tokens_map.json
[2025-03-10 16:25:43,142] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30687 is about to be saved!
[2025-03-10 16:25:43,145] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt
[2025-03-10 16:25:43,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt...
[2025-03-10 16:25:43,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt.
[2025-03-10 16:25:43,494] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:25:44,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:25:44,625] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:25:44,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30687 is ready now!
{'loss': 0.0332, 'grad_norm': 1.4791218042373657, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 31902/49080 [1:05:18<32:13,  8.88it/s][INFO|trainer.py:930] 2025-03-10 16:28:06,466 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:28:06,470 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:28:06,470 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:28:06,470 >>   Batch size = 4
{'eval_loss': 1.0261846780776978, 'eval_mse': 1.0260055070902825, 'eval_pearson': 0.7422590091752084, 'eval_spearmanr': 0.7563106424482002, 'eval_runtime': 2.3165, 'eval_samples_per_second': 895.309, 'eval_steps_per_second': 112.237, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 31902/49080 [1:05:20<32:13,  8.88it/s]
[INFO|trainer.py:3955] 2025-03-10 16:28:08,950 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902
[INFO|configuration_utils.py:423] 2025-03-10 16:28:08,953 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:28:09,205 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:28:09,206 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:28:09,206 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/special_tokens_map.json
[2025-03-10 16:28:09,255] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31915 is about to be saved!
[2025-03-10 16:28:09,259] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt
[2025-03-10 16:28:09,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt...
[2025-03-10 16:28:09,615] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt.
[2025-03-10 16:28:09,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:28:10,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:28:10,772] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:28:10,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31915 is ready now!
{'loss': 0.0283, 'grad_norm': 0.6116952300071716, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 33129/49080 [1:07:46<30:51,  8.62it/s][INFO|trainer.py:930] 2025-03-10 16:30:34,519 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:30:34,523 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:30:34,523 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:30:34,523 >>   Batch size = 4
{'eval_loss': 1.1640996932983398, 'eval_mse': 1.1638781100137945, 'eval_pearson': 0.7009003524533446, 'eval_spearmanr': 0.7265170109881015, 'eval_runtime': 2.3169, 'eval_samples_per_second': 895.181, 'eval_steps_per_second': 112.221, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 33129/49080 [1:07:48<30:51,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 16:30:37,006 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129
[INFO|configuration_utils.py:423] 2025-03-10 16:30:37,008 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:30:37,276 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:30:37,277 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:30:37,277 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/special_tokens_map.json
[2025-03-10 16:30:37,333] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step33142 is about to be saved!
[2025-03-10 16:30:37,336] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt
[2025-03-10 16:30:37,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt...
[2025-03-10 16:30:37,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt.
[2025-03-10 16:30:37,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:30:38,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:30:38,876] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:30:38,876] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33142 is ready now!
{'loss': 0.0268, 'grad_norm': 2.7736356258392334, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [1:10:11<28:31,  8.60it/s][INFO|trainer.py:930] 2025-03-10 16:32:59,605 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:32:59,609 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:32:59,609 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:32:59,609 >>   Batch size = 4
{'eval_loss': 1.1100667715072632, 'eval_mse': 1.1097652184468916, 'eval_pearson': 0.7153118684996371, 'eval_spearmanr': 0.7308859290550694, 'eval_runtime': 2.3185, 'eval_samples_per_second': 894.529, 'eval_steps_per_second': 112.14, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [1:10:13<28:31,  8.60it/s]
[INFO|trainer.py:3955] 2025-03-10 16:33:02,082 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356
[INFO|configuration_utils.py:423] 2025-03-10 16:33:02,084 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:33:02,356 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:33:02,357 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:33:02,357 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/special_tokens_map.json
[2025-03-10 16:33:02,405] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34370 is about to be saved!
[2025-03-10 16:33:02,408] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt
[2025-03-10 16:33:02,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt...
[2025-03-10 16:33:02,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt.
[2025-03-10 16:33:02,762] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:33:04,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:33:04,027] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:33:04,027] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34370 is ready now!
{'loss': 0.0264, 'grad_norm': 2.448712110519409, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [1:12:39<26:05,  8.62it/s][INFO|trainer.py:930] 2025-03-10 16:35:27,235 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:35:27,238 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:35:27,238 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:35:27,238 >>   Batch size = 4
{'eval_loss': 1.0159684419631958, 'eval_mse': 1.0157973311101458, 'eval_pearson': 0.7519128270634321, 'eval_spearmanr': 0.7669267393137799, 'eval_runtime': 2.323, 'eval_samples_per_second': 892.824, 'eval_steps_per_second': 111.926, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [1:12:41<26:05,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 16:35:29,720 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583
[INFO|configuration_utils.py:423] 2025-03-10 16:35:29,722 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:35:29,977 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:35:29,978 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:35:29,978 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/special_tokens_map.json
[2025-03-10 16:35:30,026] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35597 is about to be saved!
[2025-03-10 16:35:30,030] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt
[2025-03-10 16:35:30,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt...
[2025-03-10 16:35:30,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt.
[2025-03-10 16:35:30,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:35:31,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:35:31,516] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:35:31,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35597 is ready now!
{'loss': 0.0246, 'grad_norm': 2.2694902420043945, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [1:15:05<23:09,  8.83it/s][INFO|trainer.py:930] 2025-03-10 16:37:53,611 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:37:53,614 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:37:53,614 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:37:53,614 >>   Batch size = 4
{'eval_loss': 1.1176190376281738, 'eval_mse': 1.1178502864065143, 'eval_pearson': 0.7167251560132283, 'eval_spearmanr': 0.723607238060003, 'eval_runtime': 2.3238, 'eval_samples_per_second': 892.488, 'eval_steps_per_second': 111.884, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [1:15:07<23:09,  8.83it/s]
[INFO|trainer.py:3955] 2025-03-10 16:37:56,099 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810
[INFO|configuration_utils.py:423] 2025-03-10 16:37:56,101 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:37:56,356 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:37:56,357 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:37:56,357 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/special_tokens_map.json
[2025-03-10 16:37:56,410] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step36825 is about to be saved!
[2025-03-10 16:37:56,413] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt
[2025-03-10 16:37:56,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt...
[2025-03-10 16:37:56,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt.
[2025-03-10 16:37:56,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:37:57,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:37:58,000] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:37:58,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36825 is ready now!
{'loss': 0.0232, 'grad_norm': 0.6099149584770203, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [1:17:30<20:53,  8.81it/s][INFO|trainer.py:930] 2025-03-10 16:40:18,726 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:40:18,730 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:40:18,730 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:40:18,730 >>   Batch size = 4
{'eval_loss': 1.0074620246887207, 'eval_mse': 1.00734211853638, 'eval_pearson': 0.736237799855379, 'eval_spearmanr': 0.7468509735974349, 'eval_runtime': 2.3228, 'eval_samples_per_second': 892.886, 'eval_steps_per_second': 111.934, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [1:17:32<20:53,  8.81it/s]
[INFO|trainer.py:3955] 2025-03-10 16:40:21,214 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037
[INFO|configuration_utils.py:423] 2025-03-10 16:40:21,216 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:40:21,493 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:40:21,494 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:40:21,494 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/special_tokens_map.json
[2025-03-10 16:40:21,551] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step38052 is about to be saved!
[2025-03-10 16:40:21,554] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt
[2025-03-10 16:40:21,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt...
[2025-03-10 16:40:21,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt.
[2025-03-10 16:40:21,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:40:23,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:40:23,210] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:40:23,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38052 is ready now!
{'loss': 0.0243, 'grad_norm': 1.3829669952392578, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [1:19:57<18:54,  8.65it/s][INFO|trainer.py:930] 2025-03-10 16:42:45,669 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:42:45,672 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:42:45,672 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:42:45,672 >>   Batch size = 4
{'eval_loss': 1.0114566087722778, 'eval_mse': 1.01162582538167, 'eval_pearson': 0.7436866949223973, 'eval_spearmanr': 0.75549083083068, 'eval_runtime': 2.3309, 'eval_samples_per_second': 889.8, 'eval_steps_per_second': 111.547, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [1:19:59<18:54,  8.65it/s]
[INFO|trainer.py:3955] 2025-03-10 16:42:48,160 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264
[INFO|configuration_utils.py:423] 2025-03-10 16:42:48,162 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:42:48,420 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:42:48,421 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:42:48,421 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/special_tokens_map.json
[2025-03-10 16:42:48,471] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39280 is about to be saved!
[2025-03-10 16:42:48,475] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt
[2025-03-10 16:42:48,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt...
[2025-03-10 16:42:48,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt.
[2025-03-10 16:42:48,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:42:49,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:42:49,954] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:42:49,954] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39280 is ready now!
{'loss': 0.0207, 'grad_norm': 0.5425558090209961, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [1:22:22<16:10,  8.85it/s][INFO|trainer.py:930] 2025-03-10 16:45:11,013 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:45:11,016 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:45:11,016 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:45:11,016 >>   Batch size = 4
{'eval_loss': 1.058101773262024, 'eval_mse': 1.0580891575394853, 'eval_pearson': 0.7368823891466549, 'eval_spearmanr': 0.7453838937884828, 'eval_runtime': 2.3296, 'eval_samples_per_second': 890.281, 'eval_steps_per_second': 111.607, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [1:22:25<16:10,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 16:45:13,509 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491
[INFO|configuration_utils.py:423] 2025-03-10 16:45:13,511 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:45:13,804 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:45:13,805 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:45:13,805 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/special_tokens_map.json
[2025-03-10 16:45:13,858] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40507 is about to be saved!
[2025-03-10 16:45:13,862] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt
[2025-03-10 16:45:13,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt...
[2025-03-10 16:45:14,216] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt.
[2025-03-10 16:45:14,217] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:45:15,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:45:15,340] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:45:15,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40507 is ready now!
{'loss': 0.0227, 'grad_norm': 0.4341470003128052, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [1:24:46<13:52,  8.84it/s][INFO|trainer.py:930] 2025-03-10 16:47:34,183 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:47:34,187 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:47:34,187 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:47:34,187 >>   Batch size = 4
{'eval_loss': 1.033028483390808, 'eval_mse': 1.0329218308514085, 'eval_pearson': 0.7391598745524247, 'eval_spearmanr': 0.7488951344935935, 'eval_runtime': 2.318, 'eval_samples_per_second': 894.729, 'eval_steps_per_second': 112.165, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [1:24:48<13:52,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 16:47:36,667 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718
[INFO|configuration_utils.py:423] 2025-03-10 16:47:36,669 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:47:36,909 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:47:36,910 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:47:36,910 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/special_tokens_map.json
[2025-03-10 16:47:36,964] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step41735 is about to be saved!
[2025-03-10 16:47:36,968] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt
[2025-03-10 16:47:36,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt...
[2025-03-10 16:47:37,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt.
[2025-03-10 16:47:37,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:47:38,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:47:38,430] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:47:38,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41735 is ready now!
{'loss': 0.0203, 'grad_norm': 0.41198626160621643, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [1:27:13<11:58,  8.53it/s][INFO|trainer.py:930] 2025-03-10 16:50:01,703 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:50:01,707 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:50:01,707 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:50:01,707 >>   Batch size = 4
{'eval_loss': 1.0821326971054077, 'eval_mse': 1.0817556045554297, 'eval_pearson': 0.7275537951442625, 'eval_spearmanr': 0.7332512727861683, 'eval_runtime': 2.32, 'eval_samples_per_second': 893.984, 'eval_steps_per_second': 112.071, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [1:27:15<11:58,  8.53it/s]
[INFO|trainer.py:3955] 2025-03-10 16:50:04,184 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945
[INFO|configuration_utils.py:423] 2025-03-10 16:50:04,186 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:50:04,444 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:50:04,445 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:50:04,445 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/special_tokens_map.json
[2025-03-10 16:50:04,498] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42962 is about to be saved!
[2025-03-10 16:50:04,502] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt
[2025-03-10 16:50:04,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt...
[2025-03-10 16:50:04,832] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt.
[2025-03-10 16:50:04,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:50:06,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:50:06,016] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:50:06,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42962 is ready now!
{'loss': 0.0191, 'grad_norm': 0.49088048934936523, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [1:29:38<09:29,  8.62it/s][INFO|trainer.py:930] 2025-03-10 16:52:26,804 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:52:26,807 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:52:26,807 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:52:26,807 >>   Batch size = 4
{'eval_loss': 1.0643229484558105, 'eval_mse': 1.0641182375965836, 'eval_pearson': 0.7352991213867925, 'eval_spearmanr': 0.7443397982779981, 'eval_runtime': 2.3337, 'eval_samples_per_second': 888.71, 'eval_steps_per_second': 111.41, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [1:29:41<09:29,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 16:52:29,296 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172
[INFO|configuration_utils.py:423] 2025-03-10 16:52:29,298 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:52:29,546 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:52:29,547 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:52:29,547 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/special_tokens_map.json
[2025-03-10 16:52:29,596] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44190 is about to be saved!
[2025-03-10 16:52:29,599] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt
[2025-03-10 16:52:29,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt...
[2025-03-10 16:52:29,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt.
[2025-03-10 16:52:29,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:52:31,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:52:31,114] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:52:31,114] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44190 is ready now!
{'loss': 0.02, 'grad_norm': 0.7713024020195007, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [1:32:06<07:12,  8.51it/s][INFO|trainer.py:930] 2025-03-10 16:54:54,226 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:54:54,230 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:54:54,230 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:54:54,230 >>   Batch size = 4
{'eval_loss': 0.9971117377281189, 'eval_mse': 0.9968313874251128, 'eval_pearson': 0.7540454062919186, 'eval_spearmanr': 0.7628629836762996, 'eval_runtime': 2.3297, 'eval_samples_per_second': 890.253, 'eval_steps_per_second': 111.604, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [1:32:08<07:12,  8.51it/s]
[INFO|trainer.py:3955] 2025-03-10 16:54:56,715 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399
[INFO|configuration_utils.py:423] 2025-03-10 16:54:56,717 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:54:56,974 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:54:56,974 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:54:56,974 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/special_tokens_map.json
[2025-03-10 16:54:57,026] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45417 is about to be saved!
[2025-03-10 16:54:57,029] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt
[2025-03-10 16:54:57,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt...
[2025-03-10 16:54:57,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt.
[2025-03-10 16:54:57,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:54:58,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:54:58,517] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:54:58,517] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45417 is ready now!
{'loss': 0.0203, 'grad_norm': 0.8005384206771851, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [1:34:32<04:42,  8.69it/s][INFO|trainer.py:930] 2025-03-10 16:57:20,710 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:57:20,713 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:57:20,714 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:57:20,714 >>   Batch size = 4
{'eval_loss': 1.0270030498504639, 'eval_mse': 1.027035536393595, 'eval_pearson': 0.7413546252057843, 'eval_spearmanr': 0.7445967352259796, 'eval_runtime': 2.3291, 'eval_samples_per_second': 890.474, 'eval_steps_per_second': 111.631, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [1:34:34<04:42,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 16:57:23,200 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626
[INFO|configuration_utils.py:423] 2025-03-10 16:57:23,202 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:57:23,461 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:57:23,462 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:57:23,462 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/special_tokens_map.json
[2025-03-10 16:57:23,509] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step46645 is about to be saved!
[2025-03-10 16:57:23,512] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt
[2025-03-10 16:57:23,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt...
[2025-03-10 16:57:23,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt.
[2025-03-10 16:57:23,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:57:25,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:57:25,053] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:57:25,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step46645 is ready now!
{'loss': 0.0171, 'grad_norm': 1.312988519668579, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [1:36:59<02:22,  8.61it/s][INFO|trainer.py:930] 2025-03-10 16:59:47,982 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:59:47,986 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:59:47,986 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 16:59:47,986 >>   Batch size = 4
{'eval_loss': 1.0737078189849854, 'eval_mse': 1.0736755517777505, 'eval_pearson': 0.7324532370641416, 'eval_spearmanr': 0.7465139501961601, 'eval_runtime': 2.3225, 'eval_samples_per_second': 892.988, 'eval_steps_per_second': 111.946, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [1:37:02<02:22,  8.61it/s]
[INFO|trainer.py:3955] 2025-03-10 16:59:50,482 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853
[INFO|configuration_utils.py:423] 2025-03-10 16:59:50,484 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:59:50,773 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:59:50,774 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:59:50,774 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/special_tokens_map.json
[2025-03-10 16:59:50,831] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47872 is about to be saved!
[2025-03-10 16:59:50,834] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt
[2025-03-10 16:59:50,834] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt...
[2025-03-10 16:59:51,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt.
[2025-03-10 16:59:51,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:59:52,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:59:52,429] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:59:52,429] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47872 is ready now!
{'loss': 0.0165, 'grad_norm': 4.057666301727295, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:39:24<00:00,  8.80it/s][INFO|trainer.py:930] 2025-03-10 17:02:12,454 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:02:12,458 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:02:12,458 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:02:12,458 >>   Batch size = 4
{'eval_loss': 0.9767434597015381, 'eval_mse': 0.9766162932781267, 'eval_pearson': 0.755406637920252, 'eval_spearmanr': 0.7668550843714481, 'eval_runtime': 2.3456, 'eval_samples_per_second': 884.192, 'eval_steps_per_second': 110.844, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:39:26<00:00,  8.80it/s]
[INFO|trainer.py:3955] 2025-03-10 17:02:15,006 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080
[INFO|configuration_utils.py:423] 2025-03-10 17:02:15,008 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:02:15,326 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:02:15,327 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:02:15,327 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/special_tokens_map.json
[2025-03-10 17:02:15,382] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49099 is about to be saved!
[2025-03-10 17:02:15,385] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt
[2025-03-10 17:02:15,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt...
[2025-03-10 17:02:15,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt.
[2025-03-10 17:02:15,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:02:16,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:02:16,964] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:02:16,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49099 is ready now!
[INFO|trainer.py:2670] 2025-03-10 17:02:16,967 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 5968.8574, 'train_samples_per_second': 263.273, 'train_steps_per_second': 8.223, 'train_loss': 0.14093107318334105, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [1:39:28<00:00,  8.22it/s]
[INFO|trainer.py:3955] 2025-03-10 17:02:17,069 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=30
[INFO|configuration_utils.py:423] 2025-03-10 17:02:17,071 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=30/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:02:17,353 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=30/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:02:17,354 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:02:17,354 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=30/special_tokens_map.json
***** train metrics *****
  epoch                    =     39.9994
  total_flos               = 385001440GF
  train_loss               =      0.1409
  train_runtime            =  1:39:28.85
  train_samples            =       39286
  train_samples_per_second =     263.273
  train_steps_per_second   =       8.223
03/10/2025 17:02:17 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 17:02:17,401 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:02:17,404 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:02:17,404 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:02:17,404 >>   Batch size = 4
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:02<00:00, 107.74it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     0.9767
  eval_mse                =     0.9766
  eval_pearson            =     0.7554
  eval_runtime            = 0:00:02.42
  eval_samples            =       2074
  eval_samples_per_second =    854.825
  eval_spearmanr          =     0.7669
  eval_steps_per_second   =    107.162
[INFO|modelcard.py:449] 2025-03-10 17:02:20,049 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7668550843714481}]}
[rank0]:[W310 17:02:20.210138254 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 17:02:23,081] [INFO] [launch.py:351:main] Process 4134549 exits successfully.
[2025-03-10 17:02:23,082] [INFO] [launch.py:351:main] Process 4134550 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:4,5 --master_port 27134 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 17:02:29,911] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 17:02:32,991] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 17:02:32,992] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNV19 --master_addr=127.0.0.1 --master_port=27134 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=30 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 17:02:37,558] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 17:02:40,466] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [4, 5]}
[2025-03-10 17:02:40,466] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 17:02:40,466] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 17:02:40,466] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 17:02:40,466] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=4,5
[2025-03-10 17:02:40,467] [INFO] [launch.py:256:main] process 59792 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 17:02:40,467] [INFO] [launch.py:256:main] process 59793 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=30', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 17:02:45,417] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 17:02:45,505] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 17:02:46,172] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 17:02:46,235] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 17:02:46,235] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/10/2025 17:02:48 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 17:02:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 17:02:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-large-num=30/runs/Mar10_17-02-45_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-large-num=30,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-large-num=30,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 17:02:48 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-train.json
03/10/2025 17:02:48 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=30-val.json
Using custom data configuration default-6b90b90b9712ba2d
03/10/2025 17:02:49 - INFO - datasets.builder - Using custom data configuration default-6b90b90b9712ba2d
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 17:02:49 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 17:02:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 17:02:49 - INFO - datasets.info - Loading Dataset info from .cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 17:02:49 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 17:02:49 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 17:02:49,456 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 17:02:49,459 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 17:02:49,655 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 17:02:49,656 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,663 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 17:02:49,664 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 17:02:49,664 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 17:02:49,664 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 17:02:49,806 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 17:02:49,919 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 17:02:49,919 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:5103] 2025-03-10 17:02:49,933 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4be72a9c9506050.arrow
03/10/2025 17:02:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c4be72a9c9506050.arrow
Running tokenizer on dataset:  31%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                  | 12000/39286 [00:02<00:06, 4282.81 examples/s][rank1]:[W310 17:02:52.610570888 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:08<00:00, 4409.77 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/2074 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4486db7bf0eb3ccb.arrow
03/10/2025 17:02:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-6b90b90b9712ba2d/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-4486db7bf0eb3ccb.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 3270.59 examples/s]
[rank0]:[W310 17:03:00.847176846 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 17:03:00 - INFO - __main__ - Sample 7296 of the training set: {'sentence1': 'It(O) was(O) praised(O) by(O) scientists(O) such(O) as(O) Darwin(B-writer) ((O) to(O) whom(O) the(O) book(O) was(O) dedicated(O) )(O),(O) and(O) Charles(B-person) Lyell(I-person),(O) and(O) by(O) non-scientists(O) such(O) as(O) the(O) novelist(O) Joseph(B-person) Conrad(I-person),(O) who(O) called(O) it(O) his(O) favorite(O) bedside(O) companion(O) and(O) used(O) it(O) as(O) source(O) of(O) information(O) for(O) several(O) of(O) his(O) novels(O),(O) especially(O) Lord(B-book) Jim(I-book).(O)', 'sentence2': 'It was praised by scientists such as Darwin ( to whom the book was dedicated ) , and Charles Lyell , and by non-scientists such as the novelist Joseph Conrad , who called it his favorite bedside companion and used it as source of information for several of his novels , especially Lord Jim .', 'label': 1.03, 'idx': 7296, 'input_ids': [0, 243, 1640, 673, 43, 21, 1640, 673, 43, 6425, 1640, 673, 43, 30, 1640, 673, 43, 4211, 1640, 673, 43, 215, 1640, 673, 43, 25, 1640, 673, 43, 24458, 1640, 387, 12, 9408, 43, 41006, 673, 43, 7, 1640, 673, 43, 2661, 1640, 673, 43, 5, 1640, 673, 43, 1040, 1640, 673, 43, 21, 1640, 673, 43, 3688, 1640, 673, 43, 4839, 1640, 673, 238, 1640, 673, 43, 8, 1640, 673, 43, 3163, 1640, 387, 12, 5970, 43, 10888, 1641, 1640, 100, 12, 5970, 238, 1640, 673, 43, 8, 1640, 673, 43, 30, 1640, 673, 43, 786, 12, 38957, 1952, 1640, 673, 43, 215, 1640, 673, 43, 25, 1640, 673, 43, 5, 1640, 673, 43, 29613, 1640, 673, 43, 3351, 1640, 387, 12, 5970, 43, 24078, 1640, 100, 12, 5970, 238, 1640, 673, 43, 54, 1640, 673, 43, 373, 1640, 673, 43, 24, 1640, 673, 43, 39, 1640, 673, 43, 2674, 1640, 673, 43, 3267, 3730, 1640, 673, 43, 15625, 1640, 673, 43, 8, 1640, 673, 43, 341, 1640, 673, 43, 24, 1640, 673, 43, 25, 1640, 673, 43, 1300, 1640, 673, 43, 9, 1640, 673, 43, 335, 1640, 673, 43, 13, 1640, 673, 43, 484, 1640, 673, 43, 9, 1640, 673, 43, 39, 1640, 673, 43, 19405, 1640, 673, 238, 1640, 673, 43, 941, 1640, 673, 43, 5736, 1640, 387, 12, 6298, 43, 2488, 1640, 100, 12, 6298, 322, 1640, 673, 43, 2, 2, 243, 21, 6425, 30, 4211, 215, 25, 24458, 36, 7, 2661, 5, 1040, 21, 3688, 4839, 2156, 8, 3163, 10888, 1641, 2156, 8, 30, 786, 12, 38957, 1952, 215, 25, 5, 29613, 3351, 24078, 2156, 54, 373, 24, 39, 2674, 3267, 3730, 15625, 8, 341, 24, 25, 1300, 9, 335, 13, 484, 9, 39, 19405, 2156, 941, 5736, 2488, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 17:03:00 - INFO - __main__ - Sample 1639 of the training set: {'sentence1': 'The(O) natural(B-metric) gradient(I-metric) of(O) mathE(B-algorithm) f(O) ((O) x(O) )(O) /(O) math(O),(O) complying(O) with(O) the(O) Fisher(B-metric) information(I-metric) metric(I-metric) ((O) an(O) informational(O) distance(O) measure(O) between(O) probability(O) distributions(O) and(O) the(O) curvature(O) of(O) the(O) relative(O) entropy(O) )(O),(O) now(O) reads(O)', 'sentence2': 'The natural gradient of mathE f ( x ) / math , complying with the Fisher information metric ( an informational distance measure between probability distributions and the curvature of the relative entropy ) , now reads', 'label': 1.48, 'idx': 1639, 'input_ids': [0, 133, 1640, 673, 43, 1632, 1640, 387, 12, 5646, 4063, 43, 43141, 1640, 100, 12, 5646, 4063, 43, 9, 1640, 673, 43, 10638, 717, 1640, 387, 12, 337, 47549, 43, 856, 1640, 673, 43, 41006, 673, 43, 3023, 1640, 673, 43, 4839, 1640, 673, 43, 1589, 1640, 673, 43, 10638, 1640, 673, 238, 1640, 673, 43, 27815, 1640, 673, 43, 19, 1640, 673, 43, 5, 1640, 673, 43, 6868, 1640, 387, 12, 5646, 4063, 43, 335, 1640, 100, 12, 5646, 4063, 43, 14823, 1640, 100, 12, 5646, 4063, 43, 41006, 673, 43, 41, 1640, 673, 43, 31603, 1640, 673, 43, 4472, 1640, 673, 43, 2450, 1640, 673, 43, 227, 1640, 673, 43, 18102, 1640, 673, 43, 26070, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 42294, 18830, 1640, 673, 43, 9, 1640, 673, 43, 5, 1640, 673, 43, 5407, 1640, 673, 43, 47382, 1640, 673, 43, 4839, 1640, 673, 238, 1640, 673, 43, 122, 1640, 673, 43, 7005, 1640, 673, 43, 2, 2, 133, 1632, 43141, 9, 10638, 717, 856, 36, 3023, 4839, 1589, 10638, 2156, 27815, 19, 5, 6868, 335, 14823, 36, 41, 31603, 4472, 2450, 227, 18102, 26070, 8, 5, 42294, 18830, 9, 5, 5407, 47382, 4839, 2156, 122, 7005, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 17:03:00 - INFO - __main__ - Sample 18024 of the training set: {'sentence1': 'It(O) was(O) set(O) up(O) by(O) former(O) left-leaning(O) Christian(B-political party) Democrats(I-political party),(O) ((O) former(O) Italian(O) Liberal(B-political party) Party(I-political party) and(O) former(O) Italian(B-political party) Republican(I-political party) Party(I-political party) )(O),(O) as(O) well(O) as(O) other(O) left-wing(O) politicians(O) from(O) the(O) former(O) Italian(O) Socialist(B-political party) Party(I-political party) and(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party).(O)', 'sentence2': 'It was set up by former left-leaning Christian Democrats , ( former Italian Liberal Party and former Italian Republican Party ) , as well as other left-wing politicians from the former Italian Socialist Party and Federation of the Greens .', 'label': 1.64, 'idx': 18024, 'input_ids': [0, 243, 1640, 673, 43, 21, 1640, 673, 43, 278, 1640, 673, 43, 62, 1640, 673, 43, 30, 1640, 673, 43, 320, 1640, 673, 43, 314, 12, 18813, 1640, 673, 43, 2412, 1640, 387, 12, 17522, 537, 43, 1574, 1640, 100, 12, 17522, 537, 238, 1640, 673, 43, 41006, 673, 43, 320, 1640, 673, 43, 3108, 1640, 673, 43, 7612, 1640, 387, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 8, 1640, 673, 43, 320, 1640, 673, 43, 3108, 1640, 387, 12, 17522, 537, 43, 1172, 1640, 100, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 4839, 1640, 673, 238, 1640, 673, 43, 25, 1640, 673, 43, 157, 1640, 673, 43, 25, 1640, 673, 43, 97, 1640, 673, 43, 314, 12, 5577, 1640, 673, 43, 3770, 1640, 673, 43, 31, 1640, 673, 43, 5, 1640, 673, 43, 320, 1640, 673, 43, 3108, 1640, 673, 43, 19476, 1640, 387, 12, 17522, 537, 43, 1643, 1640, 100, 12, 17522, 537, 43, 8, 1640, 673, 43, 6692, 1640, 387, 12, 17522, 537, 43, 9, 1640, 100, 12, 17522, 537, 43, 5, 1640, 100, 12, 17522, 537, 43, 11876, 1640, 100, 12, 17522, 537, 322, 1640, 673, 43, 2, 2, 243, 21, 278, 62, 30, 320, 314, 12, 18813, 2412, 1574, 2156, 36, 320, 3108, 7612, 1643, 8, 320, 3108, 1172, 1643, 4839, 2156, 25, 157, 25, 97, 314, 12, 5577, 3770, 31, 5, 320, 3108, 19476, 1643, 8, 6692, 9, 5, 11876, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/39286 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 17:03:00,763 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 17:03:00,929 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 17:03:00,936] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 17:03:00,936] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39286/39286 [00:09<00:00, 4199.51 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2074/2074 [00:00<00:00, 3203.52 examples/s]
[2025-03-10 17:03:11,597] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 17:03:11,599] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 17:03:11,599] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 17:03:11,615] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 17:03:11,616] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 17:03:11,616] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 17:03:11,616] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 17:03:11,616] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 17:03:11,616] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 17:03:11,616] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 17:03:14,556] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 17:03:14,668] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 17:03:14,669] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.66 GB         CA 1.66 GB         Max_CA 2 GB
[2025-03-10 17:03:14,669] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.47 GB, percent = 5.1%
[2025-03-10 17:03:14,819] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 17:03:14,820] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.99 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 17:03:14,820] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.43 GB, percent = 5.1%
[2025-03-10 17:03:14,820] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 17:03:15,000] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 17:03:15,001] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.32 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 17:03:15,001] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 51.21 GB, percent = 5.1%
[2025-03-10 17:03:15,005] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 17:03:15,005] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 17:03:15,005] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 17:03:15,005] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe961c977d0>
[2025-03-10 17:03:15,005] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 17:03:15,006] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 17:03:15,007] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe8ac088920>
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 17:03:15,008] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 17:03:15,009] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 17:03:15,010] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 17:03:15,011] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 17:03:15,011] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 17:03:15,011] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 17:03:15,011] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 17:03:15,013 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 17:03:15,013 >>   Num examples = 39,286
[INFO|trainer.py:2416] 2025-03-10 17:03:15,013 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 17:03:15,013 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 17:03:15,013 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 17:03:15,013 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 17:03:15,013 >>   Total optimization steps = 49,080
[INFO|trainer.py:2423] 2025-03-10 17:03:15,014 >>   Number of trainable parameters = 355,360,769
{'loss': 1.1606, 'grad_norm': 31.683109283447266, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [04:57<3:11:02,  4.17it/s][INFO|trainer.py:930] 2025-03-10 17:08:12,142 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:08:12,145 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:08:12,145 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:08:12,145 >>   Batch size = 4
{'eval_loss': 1.1432374715805054, 'eval_mse': 1.1440073087337277, 'eval_pearson': 0.6994562351833973, 'eval_spearmanr': 0.7140950707303056, 'eval_runtime': 3.925, 'eval_samples_per_second': 528.41, 'eval_steps_per_second': 66.242, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                            | 1227/49080 [05:01<3:11:02,  4.17it/s]
[INFO|trainer.py:3955] 2025-03-10 17:08:16,403 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227
[INFO|configuration_utils.py:423] 2025-03-10 17:08:16,406 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:08:17,278 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:08:17,279 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:08:17,279 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/special_tokens_map.json
[2025-03-10 17:08:17,341] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1227 is about to be saved!
[2025-03-10 17:08:17,347] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt
[2025-03-10 17:08:17,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt...
[2025-03-10 17:08:18,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/mp_rank_00_model_states.pt.
[2025-03-10 17:08:18,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:08:22,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:08:22,393] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-1227/global_step1227/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:08:22,393] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1227 is ready now!
{'loss': 0.7531, 'grad_norm': 5.884942054748535, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [09:53<3:04:24,  4.21it/s][INFO|trainer.py:930] 2025-03-10 17:13:08,602 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:13:08,608 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:13:08,608 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:13:08,608 >>   Batch size = 4
{'eval_loss': 1.075852632522583, 'eval_mse': 1.075081725244697, 'eval_pearson': 0.6977295240044583, 'eval_spearmanr': 0.7023813875899796, 'eval_runtime': 4.0057, 'eval_samples_per_second': 517.768, 'eval_steps_per_second': 64.908, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 2454/49080 [09:57<3:04:24,  4.21it/s]
[INFO|trainer.py:3955] 2025-03-10 17:13:12,860 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454
[INFO|configuration_utils.py:423] 2025-03-10 17:13:12,862 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:13:13,626 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:13:13,627 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:13:13,627 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/special_tokens_map.json
[2025-03-10 17:13:13,689] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2455 is about to be saved!
[2025-03-10 17:13:13,695] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt
[2025-03-10 17:13:13,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt...
[2025-03-10 17:13:14,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/mp_rank_00_model_states.pt.
[2025-03-10 17:13:14,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:13:18,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:13:18,095] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-2454/global_step2455/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:13:18,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2455 is ready now!
{'loss': 0.5129, 'grad_norm': 19.317127227783203, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [14:49<3:00:32,  4.19it/s][INFO|trainer.py:930] 2025-03-10 17:18:04,045 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:18:04,050 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:18:04,050 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:18:04,050 >>   Batch size = 4
{'eval_loss': 0.8535292744636536, 'eval_mse': 0.8534328394938573, 'eval_pearson': 0.7617103370517073, 'eval_spearmanr': 0.7691011324728403, 'eval_runtime': 3.9955, 'eval_samples_per_second': 519.087, 'eval_steps_per_second': 65.074, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 3681/49080 [14:53<3:00:32,  4.19it/s]
[INFO|trainer.py:3955] 2025-03-10 17:18:08,418 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681
[INFO|configuration_utils.py:423] 2025-03-10 17:18:08,421 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:18:09,082 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:18:09,083 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:18:09,083 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/special_tokens_map.json
[2025-03-10 17:18:09,131] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3682 is about to be saved!
[2025-03-10 17:18:09,137] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt
[2025-03-10 17:18:09,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt...
[2025-03-10 17:18:10,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/mp_rank_00_model_states.pt.
[2025-03-10 17:18:10,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:18:13,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:18:13,280] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-3681/global_step3682/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:18:13,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3682 is ready now!
{'loss': 0.4114, 'grad_norm': 6.997673511505127, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [19:42<2:48:04,  4.38it/s][INFO|trainer.py:930] 2025-03-10 17:22:57,091 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:22:57,094 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:22:57,094 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:22:57,095 >>   Batch size = 4
{'eval_loss': 1.0946924686431885, 'eval_mse': 1.0944224086768832, 'eval_pearson': 0.7557901036542982, 'eval_spearmanr': 0.7604530422149359, 'eval_runtime': 3.9349, 'eval_samples_per_second': 527.074, 'eval_steps_per_second': 66.075, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 4908/49080 [19:46<2:48:04,  4.38it/s]
[INFO|trainer.py:3955] 2025-03-10 17:23:01,323 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908
[INFO|configuration_utils.py:423] 2025-03-10 17:23:01,325 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:23:02,024 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:23:02,025 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:23:02,025 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/special_tokens_map.json
[2025-03-10 17:23:02,085] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4910 is about to be saved!
[2025-03-10 17:23:02,091] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt
[2025-03-10 17:23:02,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt...
[2025-03-10 17:23:03,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/mp_rank_00_model_states.pt.
[2025-03-10 17:23:03,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:23:06,311] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:23:06,311] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-4908/global_step4910/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:23:06,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4910 is ready now!
{'loss': 0.2961, 'grad_norm': 7.191349506378174, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [24:37<2:43:37,  4.37it/s][INFO|trainer.py:930] 2025-03-10 17:27:52,092 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:27:52,095 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:27:52,095 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:27:52,095 >>   Batch size = 4
{'eval_loss': 0.8099374771118164, 'eval_mse': 0.8095934777273621, 'eval_pearson': 0.7772233420833992, 'eval_spearmanr': 0.787583860187723, 'eval_runtime': 3.9444, 'eval_samples_per_second': 525.81, 'eval_steps_per_second': 65.916, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 6135/49080 [24:41<2:43:37,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 17:27:56,334 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135
[INFO|configuration_utils.py:423] 2025-03-10 17:27:56,337 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:27:57,165 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:27:57,166 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:27:57,166 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/special_tokens_map.json
[2025-03-10 17:27:57,211] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6137 is about to be saved!
[2025-03-10 17:27:57,216] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt
[2025-03-10 17:27:57,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt...
[2025-03-10 17:27:58,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/mp_rank_00_model_states.pt.
[2025-03-10 17:27:58,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:28:01,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:28:01,428] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-6135/global_step6137/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:28:01,428] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6137 is ready now!
{'loss': 0.2439, 'grad_norm': 9.122899055480957, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [29:28<2:40:18,  4.34it/s][INFO|trainer.py:930] 2025-03-10 17:32:43,799 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:32:43,803 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:32:43,803 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:32:43,803 >>   Batch size = 4
{'eval_loss': 0.763844907283783, 'eval_mse': 0.7637617275363109, 'eval_pearson': 0.8053143965488407, 'eval_spearmanr': 0.8219594280011315, 'eval_runtime': 3.9352, 'eval_samples_per_second': 527.04, 'eval_steps_per_second': 66.071, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 7362/49080 [29:32<2:40:18,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 17:32:48,027 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362
[INFO|configuration_utils.py:423] 2025-03-10 17:32:48,030 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:32:48,754 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:32:48,755 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:32:48,755 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/special_tokens_map.json
[2025-03-10 17:32:48,825] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7365 is about to be saved!
[2025-03-10 17:32:48,831] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt
[2025-03-10 17:32:48,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt...
[2025-03-10 17:32:49,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/mp_rank_00_model_states.pt.
[2025-03-10 17:32:50,008] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:32:53,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:32:53,303] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-7362/global_step7365/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:32:53,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7365 is ready now!
{'loss': 0.1702, 'grad_norm': 36.99271774291992, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [34:24<2:35:44,  4.33it/s][INFO|trainer.py:930] 2025-03-10 17:37:39,445 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:37:39,448 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:37:39,448 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:37:39,449 >>   Batch size = 4
{'eval_loss': 0.775147557258606, 'eval_mse': 0.775069682209563, 'eval_pearson': 0.8161545581064296, 'eval_spearmanr': 0.8291905619889444, 'eval_runtime': 3.9684, 'eval_samples_per_second': 522.634, 'eval_steps_per_second': 65.518, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 8589/49080 [34:28<2:35:44,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 17:37:43,717 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589
[INFO|configuration_utils.py:423] 2025-03-10 17:37:43,719 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:37:44,466 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:37:44,467 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:37:44,467 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/special_tokens_map.json
[2025-03-10 17:37:44,529] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8592 is about to be saved!
[2025-03-10 17:37:44,534] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt
[2025-03-10 17:37:44,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt...
[2025-03-10 17:37:45,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/mp_rank_00_model_states.pt.
[2025-03-10 17:37:45,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:37:48,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:37:48,729] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-8589/global_step8592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:37:48,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8592 is ready now!
{'loss': 0.1532, 'grad_norm': 5.589733600616455, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [39:17<2:31:31,  4.32it/s][INFO|trainer.py:930] 2025-03-10 17:42:32,223 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:42:32,226 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:42:32,226 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:42:32,226 >>   Batch size = 4
{'eval_loss': 0.7476294636726379, 'eval_mse': 0.747524115940414, 'eval_pearson': 0.811163126625646, 'eval_spearmanr': 0.8281442870102962, 'eval_runtime': 3.925, 'eval_samples_per_second': 528.413, 'eval_steps_per_second': 66.243, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 9816/49080 [39:21<2:31:31,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 17:42:36,442 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816
[INFO|configuration_utils.py:423] 2025-03-10 17:42:36,445 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:42:37,100 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:42:37,101 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:42:37,101 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/special_tokens_map.json
[2025-03-10 17:42:37,162] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9820 is about to be saved!
[2025-03-10 17:42:37,167] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt
[2025-03-10 17:42:37,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt...
[2025-03-10 17:42:38,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/mp_rank_00_model_states.pt.
[2025-03-10 17:42:38,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:42:41,175] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:42:41,175] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-9816/global_step9820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:42:41,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9820 is ready now!
{'loss': 0.109, 'grad_norm': 5.677745342254639, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [44:10<2:28:11,  4.28it/s][INFO|trainer.py:930] 2025-03-10 17:47:25,501 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:47:25,505 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:47:25,505 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:47:25,505 >>   Batch size = 4
{'eval_loss': 0.7313882112503052, 'eval_mse': 0.7313963856508555, 'eval_pearson': 0.8173171360980316, 'eval_spearmanr': 0.8278708271139874, 'eval_runtime': 3.9276, 'eval_samples_per_second': 528.061, 'eval_steps_per_second': 66.199, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                       | 11043/49080 [44:14<2:28:11,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 17:47:29,713 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043
[INFO|configuration_utils.py:423] 2025-03-10 17:47:29,715 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:47:30,376 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:47:30,377 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:47:30,377 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/special_tokens_map.json
[2025-03-10 17:47:30,431] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11047 is about to be saved!
[2025-03-10 17:47:30,437] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt
[2025-03-10 17:47:30,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt...
[2025-03-10 17:47:31,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/mp_rank_00_model_states.pt.
[2025-03-10 17:47:31,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:47:34,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:47:34,406] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-11043/global_step11047/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:47:34,406] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11047 is ready now!
{'loss': 0.104, 'grad_norm': 6.108005046844482, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [48:59<2:21:11,  4.35it/s][INFO|trainer.py:930] 2025-03-10 17:52:14,298 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:52:14,301 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:52:14,301 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:52:14,301 >>   Batch size = 4
{'eval_loss': 0.8817538619041443, 'eval_mse': 0.8817444833978048, 'eval_pearson': 0.7828984503464659, 'eval_spearmanr': 0.8000342302770725, 'eval_runtime': 4.0428, 'eval_samples_per_second': 513.017, 'eval_steps_per_second': 64.313, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                               | 12270/49080 [49:03<2:21:11,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 17:52:18,598 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270
[INFO|configuration_utils.py:423] 2025-03-10 17:52:18,600 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:52:19,287 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:52:19,288 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:52:19,288 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/special_tokens_map.json
[2025-03-10 17:52:19,343] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12275 is about to be saved!
[2025-03-10 17:52:19,348] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt
[2025-03-10 17:52:19,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt...
[2025-03-10 17:52:20,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/mp_rank_00_model_states.pt.
[2025-03-10 17:52:20,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:52:23,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:52:23,420] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-12270/global_step12275/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:52:23,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12275 is ready now!
{'loss': 0.0824, 'grad_norm': 3.4716320037841797, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [53:54<2:16:33,  4.34it/s][INFO|trainer.py:930] 2025-03-10 17:57:09,087 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:57:09,091 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:57:09,091 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 17:57:09,091 >>   Batch size = 4
{'eval_loss': 0.7520257830619812, 'eval_mse': 0.7519476387286945, 'eval_pearson': 0.8223293514744461, 'eval_spearmanr': 0.8352831849911069, 'eval_runtime': 3.9332, 'eval_samples_per_second': 527.312, 'eval_steps_per_second': 66.105, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                      | 13497/49080 [53:58<2:16:33,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 17:57:13,306 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497
[INFO|configuration_utils.py:423] 2025-03-10 17:57:13,309 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:57:14,082 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:57:14,083 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:57:14,083 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/special_tokens_map.json
[2025-03-10 17:57:14,140] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13502 is about to be saved!
[2025-03-10 17:57:14,145] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt
[2025-03-10 17:57:14,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt...
[2025-03-10 17:57:15,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/mp_rank_00_model_states.pt.
[2025-03-10 17:57:15,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:57:18,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:57:18,554] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-13497/global_step13502/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:57:18,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13502 is ready now!
{'loss': 0.0826, 'grad_norm': 9.11188793182373, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [58:49<2:20:48,  4.07it/s][INFO|trainer.py:930] 2025-03-10 18:02:04,082 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:02:04,086 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:02:04,086 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:02:04,086 >>   Batch size = 4
{'eval_loss': 0.9057295918464661, 'eval_mse': 0.9060047137242964, 'eval_pearson': 0.8002350894313803, 'eval_spearmanr': 0.8102814767420257, 'eval_runtime': 3.9319, 'eval_samples_per_second': 527.483, 'eval_steps_per_second': 66.126, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                              | 14724/49080 [58:52<2:20:48,  4.07it/s]
[INFO|trainer.py:3955] 2025-03-10 18:02:08,305 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724
[INFO|configuration_utils.py:423] 2025-03-10 18:02:08,308 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:02:08,999 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:02:09,000 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:02:09,000 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/special_tokens_map.json
[2025-03-10 18:02:09,060] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14730 is about to be saved!
[2025-03-10 18:02:09,065] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt
[2025-03-10 18:02:09,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt...
[2025-03-10 18:02:10,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/mp_rank_00_model_states.pt.
[2025-03-10 18:02:10,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:02:13,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:02:13,414] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-14724/global_step14730/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:02:13,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14730 is ready now!
{'loss': 0.0636, 'grad_norm': 6.318150520324707, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                    | 15951/49080 [1:03:46<2:08:48,  4.29it/s][INFO|trainer.py:930] 2025-03-10 18:07:01,680 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:07:01,684 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:07:01,684 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:07:01,684 >>   Batch size = 4
{'eval_loss': 0.7675938010215759, 'eval_mse': 0.7671811989901036, 'eval_pearson': 0.812160334185297, 'eval_spearmanr': 0.8227823379592424, 'eval_runtime': 4.0269, 'eval_samples_per_second': 515.036, 'eval_steps_per_second': 64.566, 'epoch': 13.0}
 32%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                    | 15951/49080 [1:03:50<2:08:48,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 18:07:05,977 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951
[INFO|configuration_utils.py:423] 2025-03-10 18:07:05,980 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:07:06,681 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:07:06,682 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:07:06,682 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/special_tokens_map.json
[2025-03-10 18:07:06,739] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15957 is about to be saved!
[2025-03-10 18:07:06,744] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt
[2025-03-10 18:07:06,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt...
[2025-03-10 18:07:07,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/mp_rank_00_model_states.pt.
[2025-03-10 18:07:07,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:07:10,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:07:10,943] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-15951/global_step15957/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:07:10,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15957 is ready now!
{'loss': 0.0624, 'grad_norm': 1.4001054763793945, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                           | 17178/49080 [1:08:36<2:00:45,  4.40it/s][INFO|trainer.py:930] 2025-03-10 18:11:51,866 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:11:51,870 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:11:51,870 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:11:51,870 >>   Batch size = 4
{'eval_loss': 0.8920956254005432, 'eval_mse': 0.8923630022381862, 'eval_pearson': 0.7943755433837315, 'eval_spearmanr': 0.8055298870447422, 'eval_runtime': 3.9225, 'eval_samples_per_second': 528.744, 'eval_steps_per_second': 66.284, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                           | 17178/49080 [1:08:40<2:00:45,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 18:11:56,082 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178
[INFO|configuration_utils.py:423] 2025-03-10 18:11:56,084 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:11:56,777 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:11:56,778 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:11:56,778 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/special_tokens_map.json
[2025-03-10 18:11:56,839] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17185 is about to be saved!
[2025-03-10 18:11:56,845] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt
[2025-03-10 18:11:56,845] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt...
[2025-03-10 18:11:57,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/mp_rank_00_model_states.pt.
[2025-03-10 18:11:57,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:12:01,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:12:01,100] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-17178/global_step17185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:12:01,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17185 is ready now!
{'loss': 0.0534, 'grad_norm': 1.9255732297897339, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                   | 18405/49080 [1:13:29<1:57:12,  4.36it/s][INFO|trainer.py:930] 2025-03-10 18:16:44,345 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:16:44,349 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:16:44,349 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:16:44,349 >>   Batch size = 4
{'eval_loss': 0.8336731791496277, 'eval_mse': 0.8334597817037467, 'eval_pearson': 0.7985456431529361, 'eval_spearmanr': 0.8113321523075433, 'eval_runtime': 3.9855, 'eval_samples_per_second': 520.385, 'eval_steps_per_second': 65.236, 'epoch': 15.0}
 38%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                   | 18405/49080 [1:13:33<1:57:12,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 18:16:48,580 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405
[INFO|configuration_utils.py:423] 2025-03-10 18:16:48,582 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:16:49,319 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:16:49,320 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:16:49,320 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/special_tokens_map.json
[2025-03-10 18:16:49,364] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18412 is about to be saved!
[2025-03-10 18:16:49,370] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt
[2025-03-10 18:16:49,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt...
[2025-03-10 18:16:50,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/mp_rank_00_model_states.pt.
[2025-03-10 18:16:50,443] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:16:53,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:16:53,637] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-18405/global_step18412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:16:53,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18412 is ready now!
{'loss': 0.0514, 'grad_norm': 0.4605056047439575, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                          | 19632/49080 [1:18:25<1:51:35,  4.40it/s][INFO|trainer.py:930] 2025-03-10 18:21:40,181 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:21:40,185 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:21:40,185 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:21:40,185 >>   Batch size = 4
{'eval_loss': 0.8387000560760498, 'eval_mse': 0.8385280548893957, 'eval_pearson': 0.7938666843145084, 'eval_spearmanr': 0.8037230676727178, 'eval_runtime': 3.9339, 'eval_samples_per_second': 527.215, 'eval_steps_per_second': 66.092, 'epoch': 16.0}
 40%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                          | 19632/49080 [1:18:29<1:51:35,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 18:21:44,405 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632
[INFO|configuration_utils.py:423] 2025-03-10 18:21:44,408 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:21:45,225 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:21:45,226 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:21:45,226 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/special_tokens_map.json
[2025-03-10 18:21:45,287] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19640 is about to be saved!
[2025-03-10 18:21:45,292] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt
[2025-03-10 18:21:45,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt...
[2025-03-10 18:21:46,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/mp_rank_00_model_states.pt.
[2025-03-10 18:21:46,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:21:49,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:21:49,707] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-19632/global_step19640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:21:49,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19640 is ready now!
{'loss': 0.0461, 'grad_norm': 2.5316529273986816, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                  | 20859/49080 [1:23:21<1:54:56,  4.09it/s][INFO|trainer.py:930] 2025-03-10 18:26:36,399 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:26:36,402 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:26:36,402 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:26:36,402 >>   Batch size = 4
{'eval_loss': 0.8597679734230042, 'eval_mse': 0.8598780298934263, 'eval_pearson': 0.7988205743659247, 'eval_spearmanr': 0.8138745833441821, 'eval_runtime': 4.0384, 'eval_samples_per_second': 513.568, 'eval_steps_per_second': 64.382, 'epoch': 17.0}
 42%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                  | 20859/49080 [1:23:25<1:54:56,  4.09it/s]
[INFO|trainer.py:3955] 2025-03-10 18:26:40,738 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859
[INFO|configuration_utils.py:423] 2025-03-10 18:26:40,740 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:26:41,519 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:26:41,520 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:26:41,521 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/special_tokens_map.json
[2025-03-10 18:26:41,569] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20867 is about to be saved!
[2025-03-10 18:26:41,574] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt
[2025-03-10 18:26:41,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt...
[2025-03-10 18:26:42,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/mp_rank_00_model_states.pt.
[2025-03-10 18:26:42,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:26:46,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:26:46,119] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-20859/global_step20867/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:26:46,119] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20867 is ready now!
{'loss': 0.0488, 'grad_norm': 1.3321130275726318, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                          | 22086/49080 [1:28:13<1:42:09,  4.40it/s][INFO|trainer.py:930] 2025-03-10 18:31:28,288 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:31:28,292 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:31:28,292 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:31:28,292 >>   Batch size = 4
{'eval_loss': 0.8315398097038269, 'eval_mse': 0.8313570760600018, 'eval_pearson': 0.800388746458311, 'eval_spearmanr': 0.8150982462765115, 'eval_runtime': 3.9446, 'eval_samples_per_second': 525.783, 'eval_steps_per_second': 65.913, 'epoch': 18.0}
 45%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                          | 22086/49080 [1:28:17<1:42:09,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 18:31:32,525 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086
[INFO|configuration_utils.py:423] 2025-03-10 18:31:32,527 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:31:33,413 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:31:33,414 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:31:33,414 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/special_tokens_map.json
[2025-03-10 18:31:33,468] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22095 is about to be saved!
[2025-03-10 18:31:33,473] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt
[2025-03-10 18:31:33,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt...
[2025-03-10 18:31:34,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/mp_rank_00_model_states.pt.
[2025-03-10 18:31:34,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:31:37,803] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:31:37,804] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-22086/global_step22095/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:31:37,804] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22095 is ready now!
{'loss': 0.0412, 'grad_norm': 1.1983660459518433, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 23313/49080 [1:33:09<1:38:19,  4.37it/s][INFO|trainer.py:930] 2025-03-10 18:36:24,286 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:36:24,289 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:36:24,289 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:36:24,289 >>   Batch size = 4
{'eval_loss': 0.7821429967880249, 'eval_mse': 0.7824419019537239, 'eval_pearson': 0.8093374793514576, 'eval_spearmanr': 0.8239622990713052, 'eval_runtime': 3.9914, 'eval_samples_per_second': 519.617, 'eval_steps_per_second': 65.14, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 23313/49080 [1:33:13<1:38:19,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 18:36:28,519 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313
[INFO|configuration_utils.py:423] 2025-03-10 18:36:28,521 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:36:29,212 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:36:29,213 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:36:29,213 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/special_tokens_map.json
[2025-03-10 18:36:29,270] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23322 is about to be saved!
[2025-03-10 18:36:29,276] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt
[2025-03-10 18:36:29,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt...
[2025-03-10 18:36:30,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/mp_rank_00_model_states.pt.
[2025-03-10 18:36:30,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:36:33,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:36:33,570] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-23313/global_step23322/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:36:33,570] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23322 is ready now!
{'loss': 0.0426, 'grad_norm': 0.604311466217041, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 24540/49080 [1:38:03<1:33:50,  4.36it/s][INFO|trainer.py:930] 2025-03-10 18:41:18,756 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:41:18,759 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:41:18,759 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:41:18,759 >>   Batch size = 4
{'eval_loss': 0.8226540684700012, 'eval_mse': 0.8226149753526414, 'eval_pearson': 0.8019223010270964, 'eval_spearmanr': 0.8130611731949859, 'eval_runtime': 3.9224, 'eval_samples_per_second': 528.76, 'eval_steps_per_second': 66.286, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 24540/49080 [1:38:07<1:33:50,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 18:41:22,965 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540
[INFO|configuration_utils.py:423] 2025-03-10 18:41:22,968 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:41:23,684 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:41:23,685 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:41:23,685 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/special_tokens_map.json
[2025-03-10 18:41:23,740] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24550 is about to be saved!
[2025-03-10 18:41:23,745] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt
[2025-03-10 18:41:23,746] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt...
[2025-03-10 18:41:24,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/mp_rank_00_model_states.pt.
[2025-03-10 18:41:24,717] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:41:28,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:41:28,063] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-24540/global_step24550/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:41:28,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24550 is ready now!
{'loss': 0.0353, 'grad_norm': 3.0676252841949463, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 25767/49080 [1:42:58<1:30:20,  4.30it/s][INFO|trainer.py:930] 2025-03-10 18:46:13,581 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:46:13,584 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:46:13,584 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:46:13,584 >>   Batch size = 4
{'eval_loss': 0.9079047441482544, 'eval_mse': 0.9076018261012828, 'eval_pearson': 0.7900049263582063, 'eval_spearmanr': 0.8039993612550582, 'eval_runtime': 3.9353, 'eval_samples_per_second': 527.029, 'eval_steps_per_second': 66.069, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 25767/49080 [1:43:02<1:30:20,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 18:46:17,811 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767
[INFO|configuration_utils.py:423] 2025-03-10 18:46:17,813 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:46:18,535 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:46:18,536 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:46:18,536 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/special_tokens_map.json
[2025-03-10 18:46:18,578] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25777 is about to be saved!
[2025-03-10 18:46:18,584] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt
[2025-03-10 18:46:18,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt...
[2025-03-10 18:46:19,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/mp_rank_00_model_states.pt.
[2025-03-10 18:46:19,545] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:46:22,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:46:22,891] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-25767/global_step25777/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:46:22,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25777 is ready now!
{'loss': 0.0341, 'grad_norm': 2.1726648807525635, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                        | 26994/49080 [1:47:51<1:25:35,  4.30it/s][INFO|trainer.py:930] 2025-03-10 18:51:06,531 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:51:06,534 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:51:06,535 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:51:06,535 >>   Batch size = 4
{'eval_loss': 0.8161973357200623, 'eval_mse': 0.8161379775982583, 'eval_pearson': 0.8045673187047562, 'eval_spearmanr': 0.8161095632597986, 'eval_runtime': 4.0058, 'eval_samples_per_second': 517.754, 'eval_steps_per_second': 64.906, 'epoch': 22.0}
 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                        | 26994/49080 [1:47:55<1:25:35,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 18:51:10,825 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994
[INFO|configuration_utils.py:423] 2025-03-10 18:51:10,828 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:51:11,574 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:51:11,575 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:51:11,575 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/special_tokens_map.json
[2025-03-10 18:51:11,633] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27005 is about to be saved!
[2025-03-10 18:51:11,640] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt
[2025-03-10 18:51:11,640] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt...
[2025-03-10 18:51:12,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/mp_rank_00_model_states.pt.
[2025-03-10 18:51:12,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:51:15,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:51:15,828] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-26994/global_step27005/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:51:15,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27005 is ready now!
{'loss': 0.0324, 'grad_norm': 0.907990038394928, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                               | 28221/49080 [1:52:45<1:21:09,  4.28it/s][INFO|trainer.py:930] 2025-03-10 18:56:00,868 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:56:00,872 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:56:00,872 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 18:56:00,872 >>   Batch size = 4
{'eval_loss': 0.8269113898277283, 'eval_mse': 0.826880703465573, 'eval_pearson': 0.8010194260392927, 'eval_spearmanr': 0.8131696082808173, 'eval_runtime': 3.9957, 'eval_samples_per_second': 519.06, 'eval_steps_per_second': 65.07, 'epoch': 23.0}
 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                               | 28221/49080 [1:52:49<1:21:09,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 18:56:05,162 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221
[INFO|configuration_utils.py:423] 2025-03-10 18:56:05,164 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:56:05,976 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:56:05,977 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:56:05,977 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/special_tokens_map.json
[2025-03-10 18:56:06,036] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28232 is about to be saved!
[2025-03-10 18:56:06,041] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt
[2025-03-10 18:56:06,041] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt...
[2025-03-10 18:56:07,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/mp_rank_00_model_states.pt.
[2025-03-10 18:56:07,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:56:10,554] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:56:10,555] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-28221/global_step28232/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:56:10,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28232 is ready now!
{'loss': 0.0289, 'grad_norm': 3.66500186920166, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 29448/49080 [1:57:38<1:14:24,  4.40it/s][INFO|trainer.py:930] 2025-03-10 19:00:53,209 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:00:53,213 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:00:53,213 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:00:53,213 >>   Batch size = 4
{'eval_loss': 0.8789409399032593, 'eval_mse': 0.8788194406917723, 'eval_pearson': 0.7846039060335451, 'eval_spearmanr': 0.8000152389284237, 'eval_runtime': 3.9388, 'eval_samples_per_second': 526.56, 'eval_steps_per_second': 66.01, 'epoch': 24.0}
 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 29448/49080 [1:57:42<1:14:24,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 19:00:57,446 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448
[INFO|configuration_utils.py:423] 2025-03-10 19:00:57,449 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:00:58,268 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:00:58,269 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:00:58,269 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/special_tokens_map.json
[2025-03-10 19:00:58,326] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29460 is about to be saved!
[2025-03-10 19:00:58,332] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt
[2025-03-10 19:00:58,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt...
[2025-03-10 19:00:59,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/mp_rank_00_model_states.pt.
[2025-03-10 19:00:59,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:01:02,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:01:02,572] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-29448/global_step29460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:01:02,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29460 is ready now!
{'loss': 0.0249, 'grad_norm': 1.1030064821243286, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                              | 30675/49080 [2:02:34<1:10:45,  4.33it/s][INFO|trainer.py:930] 2025-03-10 19:05:49,069 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:05:49,072 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:05:49,072 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:05:49,072 >>   Batch size = 4
{'eval_loss': 0.8881201148033142, 'eval_mse': 0.8879951142069229, 'eval_pearson': 0.7872290035192004, 'eval_spearmanr': 0.7977414587706904, 'eval_runtime': 3.927, 'eval_samples_per_second': 528.134, 'eval_steps_per_second': 66.208, 'epoch': 25.0}
 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                              | 30675/49080 [2:02:37<1:10:45,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 19:05:53,280 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675
[INFO|configuration_utils.py:423] 2025-03-10 19:05:53,283 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:05:53,947 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:05:53,948 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:05:53,948 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/special_tokens_map.json
[2025-03-10 19:05:54,004] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30687 is about to be saved!
[2025-03-10 19:05:54,009] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt
[2025-03-10 19:05:54,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt...
[2025-03-10 19:05:55,020] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/mp_rank_00_model_states.pt.
[2025-03-10 19:05:55,022] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:05:58,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:05:58,216] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-30675/global_step30687/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:05:58,216] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30687 is ready now!
{'loss': 0.0285, 'grad_norm': 1.0013185739517212, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 31902/49080 [2:07:27<1:06:12,  4.32it/s][INFO|trainer.py:930] 2025-03-10 19:10:42,522 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:10:42,526 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:10:42,526 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:10:42,526 >>   Batch size = 4
{'eval_loss': 0.890796422958374, 'eval_mse': 0.8911588606976957, 'eval_pearson': 0.7952999405924603, 'eval_spearmanr': 0.8094669718958782, 'eval_runtime': 3.9302, 'eval_samples_per_second': 527.71, 'eval_steps_per_second': 66.155, 'epoch': 26.0}
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 31902/49080 [2:07:31<1:06:12,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 19:10:46,741 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902
[INFO|configuration_utils.py:423] 2025-03-10 19:10:46,744 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:10:47,422 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:10:47,423 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:10:47,423 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/special_tokens_map.json
[2025-03-10 19:10:47,479] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31915 is about to be saved!
[2025-03-10 19:10:47,485] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt
[2025-03-10 19:10:47,485] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt...
[2025-03-10 19:10:48,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/mp_rank_00_model_states.pt.
[2025-03-10 19:10:48,496] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:10:51,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:10:51,906] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-31902/global_step31915/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:10:51,906] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31915 is ready now!
{'loss': 0.0234, 'grad_norm': 0.7970598340034485, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 33129/49080 [2:12:23<1:02:07,  4.28it/s][INFO|trainer.py:930] 2025-03-10 19:15:38,159 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:15:38,163 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:15:38,163 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:15:38,163 >>   Batch size = 4
{'eval_loss': 0.8258040547370911, 'eval_mse': 0.8258490414433217, 'eval_pearson': 0.8024650583072006, 'eval_spearmanr': 0.8174929060443714, 'eval_runtime': 3.9386, 'eval_samples_per_second': 526.582, 'eval_steps_per_second': 66.013, 'epoch': 27.0}
 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 33129/49080 [2:12:27<1:02:07,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 19:15:42,392 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129
[INFO|configuration_utils.py:423] 2025-03-10 19:15:42,394 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:15:43,140 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:15:43,141 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:15:43,141 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/special_tokens_map.json
[2025-03-10 19:15:43,197] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step33142 is about to be saved!
[2025-03-10 19:15:43,203] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt
[2025-03-10 19:15:43,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt...
[2025-03-10 19:15:44,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/mp_rank_00_model_states.pt.
[2025-03-10 19:15:44,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:15:47,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:15:47,521] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-33129/global_step33142/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:15:47,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step33142 is ready now!
{'loss': 0.0222, 'grad_norm': 1.2603102922439575, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [2:17:15<56:19,  4.36it/s][INFO|trainer.py:930] 2025-03-10 19:20:31,017 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:20:31,021 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:20:31,021 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:20:31,021 >>   Batch size = 4
{'eval_loss': 0.985064685344696, 'eval_mse': 0.9846856938953454, 'eval_pearson': 0.7656787342374121, 'eval_spearmanr': 0.7826490757146285, 'eval_runtime': 3.9685, 'eval_samples_per_second': 522.611, 'eval_steps_per_second': 65.515, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 34356/49080 [2:17:19<56:19,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 19:20:35,281 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356
[INFO|configuration_utils.py:423] 2025-03-10 19:20:35,283 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:20:36,085 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:20:36,086 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:20:36,086 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/special_tokens_map.json
[2025-03-10 19:20:36,142] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step34370 is about to be saved!
[2025-03-10 19:20:36,148] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt
[2025-03-10 19:20:36,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt...
[2025-03-10 19:20:37,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/mp_rank_00_model_states.pt.
[2025-03-10 19:20:37,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:20:40,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:20:40,662] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-34356/global_step34370/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:20:40,662] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step34370 is ready now!
{'loss': 0.0215, 'grad_norm': 2.0164759159088135, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [2:22:10<52:00,  4.33it/s][INFO|trainer.py:930] 2025-03-10 19:25:25,250 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:25:25,254 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:25:25,254 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:25:25,254 >>   Batch size = 4
{'eval_loss': 0.8242367506027222, 'eval_mse': 0.8243851585811222, 'eval_pearson': 0.8041380261264347, 'eval_spearmanr': 0.8150023926772791, 'eval_runtime': 3.9935, 'eval_samples_per_second': 519.35, 'eval_steps_per_second': 65.106, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 35583/49080 [2:22:14<52:00,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 19:25:29,530 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583
[INFO|configuration_utils.py:423] 2025-03-10 19:25:29,533 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:25:30,276 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:25:30,277 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:25:30,277 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/special_tokens_map.json
[2025-03-10 19:25:30,319] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step35597 is about to be saved!
[2025-03-10 19:25:30,324] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt
[2025-03-10 19:25:30,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt...
[2025-03-10 19:25:31,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/mp_rank_00_model_states.pt.
[2025-03-10 19:25:31,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:25:34,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:25:34,719] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-35583/global_step35597/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:25:34,720] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35597 is ready now!
{'loss': 0.0231, 'grad_norm': 1.426679015159607, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [2:27:01<46:34,  4.39it/s][INFO|trainer.py:930] 2025-03-10 19:30:16,877 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:30:16,880 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:30:16,880 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:30:16,880 >>   Batch size = 4
{'eval_loss': 1.040687918663025, 'eval_mse': 1.0406715244543356, 'eval_pearson': 0.7677539103281368, 'eval_spearmanr': 0.7814459586069261, 'eval_runtime': 3.9249, 'eval_samples_per_second': 528.421, 'eval_steps_per_second': 66.244, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 36810/49080 [2:27:05<46:34,  4.39it/s]
[INFO|trainer.py:3955] 2025-03-10 19:30:21,096 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810
[INFO|configuration_utils.py:423] 2025-03-10 19:30:21,099 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:30:22,023 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:30:22,024 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:30:22,024 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/special_tokens_map.json
[2025-03-10 19:30:22,080] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step36825 is about to be saved!
[2025-03-10 19:30:22,086] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt
[2025-03-10 19:30:22,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt...
[2025-03-10 19:30:23,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/mp_rank_00_model_states.pt.
[2025-03-10 19:30:23,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:30:26,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:30:26,462] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-36810/global_step36825/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:30:26,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step36825 is ready now!
{'loss': 0.0191, 'grad_norm': 0.7593013048171997, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [2:31:55<42:30,  4.33it/s][INFO|trainer.py:930] 2025-03-10 19:35:10,862 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:35:10,865 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:35:10,865 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:35:10,865 >>   Batch size = 4
{'eval_loss': 0.8829625844955444, 'eval_mse': 0.882846377739681, 'eval_pearson': 0.7964735356970246, 'eval_spearmanr': 0.814459666271016, 'eval_runtime': 3.9262, 'eval_samples_per_second': 528.252, 'eval_steps_per_second': 66.223, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 38037/49080 [2:31:59<42:30,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 19:35:15,105 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037
[INFO|configuration_utils.py:423] 2025-03-10 19:35:15,107 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:35:16,049 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:35:16,050 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:35:16,050 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/special_tokens_map.json
[2025-03-10 19:35:16,110] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step38052 is about to be saved!
[2025-03-10 19:35:16,115] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt
[2025-03-10 19:35:16,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt...
[2025-03-10 19:35:17,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/mp_rank_00_model_states.pt.
[2025-03-10 19:35:17,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:35:20,648] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:35:20,649] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-38037/global_step38052/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:35:20,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step38052 is ready now!
{'loss': 0.0199, 'grad_norm': 0.2389826774597168, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [2:36:49<37:09,  4.40it/s][INFO|trainer.py:930] 2025-03-10 19:40:04,206 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:40:04,209 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:40:04,209 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:40:04,209 >>   Batch size = 4
{'eval_loss': 0.8726930618286133, 'eval_mse': 0.8729129727344274, 'eval_pearson': 0.7811449919783151, 'eval_spearmanr': 0.796688199973439, 'eval_runtime': 3.9216, 'eval_samples_per_second': 528.859, 'eval_steps_per_second': 66.299, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 39264/49080 [2:36:53<37:09,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 19:40:08,420 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264
[INFO|configuration_utils.py:423] 2025-03-10 19:40:08,422 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:40:09,244 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:40:09,245 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:40:09,245 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/special_tokens_map.json
[2025-03-10 19:40:09,300] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step39280 is about to be saved!
[2025-03-10 19:40:09,306] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt
[2025-03-10 19:40:09,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt...
[2025-03-10 19:40:10,375] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/mp_rank_00_model_states.pt.
[2025-03-10 19:40:10,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:40:13,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:40:13,641] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-39264/global_step39280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:40:13,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39280 is ready now!
{'loss': 0.0176, 'grad_norm': 0.7134547829627991, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [2:41:45<33:22,  4.29it/s][INFO|trainer.py:930] 2025-03-10 19:45:00,995 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:45:00,998 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:45:00,999 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:45:00,999 >>   Batch size = 4
{'eval_loss': 0.8454563021659851, 'eval_mse': 0.8449752200879839, 'eval_pearson': 0.7977321808116234, 'eval_spearmanr': 0.8109248942682039, 'eval_runtime': 3.9683, 'eval_samples_per_second': 522.645, 'eval_steps_per_second': 65.52, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 40491/49080 [2:41:49<33:22,  4.29it/s]
[INFO|trainer.py:3955] 2025-03-10 19:45:05,259 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491
[INFO|configuration_utils.py:423] 2025-03-10 19:45:05,261 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:45:06,028 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:45:06,028 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:45:06,029 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/special_tokens_map.json
[2025-03-10 19:45:06,085] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step40507 is about to be saved!
[2025-03-10 19:45:06,090] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt
[2025-03-10 19:45:06,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt...
[2025-03-10 19:45:07,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/mp_rank_00_model_states.pt.
[2025-03-10 19:45:07,136] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:45:10,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:45:10,415] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-40491/global_step40507/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:45:10,416] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40507 is ready now!
{'loss': 0.0225, 'grad_norm': 0.34427475929260254, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [2:46:41<28:09,  4.36it/s][INFO|trainer.py:930] 2025-03-10 19:49:56,037 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:49:56,041 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:49:56,041 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:49:56,041 >>   Batch size = 4
{'eval_loss': 0.7014803886413574, 'eval_mse': 0.7014057850078999, 'eval_pearson': 0.8266091905014004, 'eval_spearmanr': 0.8377082972019675, 'eval_runtime': 3.9312, 'eval_samples_per_second': 527.574, 'eval_steps_per_second': 66.138, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 41718/49080 [2:46:44<28:09,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 19:50:00,263 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718
[INFO|configuration_utils.py:423] 2025-03-10 19:50:00,265 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:50:01,067 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:50:01,068 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:50:01,068 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/special_tokens_map.json
[2025-03-10 19:50:01,123] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step41735 is about to be saved!
[2025-03-10 19:50:01,129] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt
[2025-03-10 19:50:01,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt...
[2025-03-10 19:50:02,153] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/mp_rank_00_model_states.pt.
[2025-03-10 19:50:02,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:50:05,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:50:05,409] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-41718/global_step41735/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:50:05,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41735 is ready now!
{'loss': 0.0159, 'grad_norm': 1.0644465684890747, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [2:51:35<23:24,  4.37it/s][INFO|trainer.py:930] 2025-03-10 19:54:50,733 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:54:50,737 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:54:50,737 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:54:50,737 >>   Batch size = 4
{'eval_loss': 0.8060886859893799, 'eval_mse': 0.8060392139043044, 'eval_pearson': 0.7996673369606548, 'eval_spearmanr': 0.8147010811255708, 'eval_runtime': 3.9319, 'eval_samples_per_second': 527.479, 'eval_steps_per_second': 66.126, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 42945/49080 [2:51:39<23:24,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 19:54:54,952 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945
[INFO|configuration_utils.py:423] 2025-03-10 19:54:54,954 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:54:55,815 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:54:55,816 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:54:55,816 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/special_tokens_map.json
[2025-03-10 19:54:55,859] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step42962 is about to be saved!
[2025-03-10 19:54:55,865] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt
[2025-03-10 19:54:55,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt...
[2025-03-10 19:54:56,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/mp_rank_00_model_states.pt.
[2025-03-10 19:54:56,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:55:00,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:55:00,034] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-42945/global_step42962/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:55:00,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42962 is ready now!
{'loss': 0.014, 'grad_norm': 0.38577428460121155, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [2:56:25<18:32,  4.41it/s][INFO|trainer.py:930] 2025-03-10 19:59:40,249 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 19:59:40,252 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 19:59:40,252 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 19:59:40,252 >>   Batch size = 4
{'eval_loss': 0.8028210997581482, 'eval_mse': 0.8027718000494756, 'eval_pearson': 0.811741062155922, 'eval_spearmanr': 0.8202547138521218, 'eval_runtime': 3.9256, 'eval_samples_per_second': 528.331, 'eval_steps_per_second': 66.232, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 44172/49080 [2:56:29<18:32,  4.41it/s]
[INFO|trainer.py:3955] 2025-03-10 19:59:44,460 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172
[INFO|configuration_utils.py:423] 2025-03-10 19:59:44,462 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 19:59:45,287 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 19:59:45,288 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 19:59:45,288 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/special_tokens_map.json
[2025-03-10 19:59:45,341] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step44190 is about to be saved!
[2025-03-10 19:59:45,347] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt
[2025-03-10 19:59:45,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt...
[2025-03-10 19:59:46,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/mp_rank_00_model_states.pt.
[2025-03-10 19:59:46,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 19:59:49,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 19:59:49,762] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-44172/global_step44190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 19:59:49,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step44190 is ready now!
{'loss': 0.0131, 'grad_norm': 7.2481913566589355, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [3:01:17<14:03,  4.36it/s][INFO|trainer.py:930] 2025-03-10 20:04:32,622 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:04:32,626 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:04:32,626 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 20:04:32,626 >>   Batch size = 4
{'eval_loss': 0.938916802406311, 'eval_mse': 0.9390090385536334, 'eval_pearson': 0.777835980269153, 'eval_spearmanr': 0.7946593820099811, 'eval_runtime': 3.927, 'eval_samples_per_second': 528.137, 'eval_steps_per_second': 66.208, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 45399/49080 [3:01:21<14:03,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 20:04:36,838 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399
[INFO|configuration_utils.py:423] 2025-03-10 20:04:36,840 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:04:37,699 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:04:37,700 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:04:37,700 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/special_tokens_map.json
[2025-03-10 20:04:37,755] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step45417 is about to be saved!
[2025-03-10 20:04:37,760] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt
[2025-03-10 20:04:37,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt...
[2025-03-10 20:04:38,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/mp_rank_00_model_states.pt.
[2025-03-10 20:04:38,816] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:04:42,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:04:42,054] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-45399/global_step45417/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:04:42,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45417 is ready now!
{'loss': 0.0147, 'grad_norm': 0.8359635472297668, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [3:06:06<09:17,  4.40it/s][INFO|trainer.py:930] 2025-03-10 20:09:21,930 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:09:21,933 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:09:21,933 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 20:09:21,933 >>   Batch size = 4
{'eval_loss': 0.8526225090026855, 'eval_mse': 0.8529710431682914, 'eval_pearson': 0.796015641542051, 'eval_spearmanr': 0.8126868862238383, 'eval_runtime': 3.9239, 'eval_samples_per_second': 528.554, 'eval_steps_per_second': 66.26, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 46626/49080 [3:06:10<09:17,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 20:09:26,146 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626
[INFO|configuration_utils.py:423] 2025-03-10 20:09:26,149 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:09:27,034 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:09:27,035 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:09:27,035 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/special_tokens_map.json
[2025-03-10 20:09:27,091] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step46645 is about to be saved!
[2025-03-10 20:09:27,096] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt
[2025-03-10 20:09:27,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt...
[2025-03-10 20:09:28,133] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/mp_rank_00_model_states.pt.
[2025-03-10 20:09:28,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:09:31,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:09:31,534] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-46626/global_step46645/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:09:31,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step46645 is ready now!
{'loss': 0.0134, 'grad_norm': 1.00131094455719, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [3:10:58<04:40,  4.37it/s][INFO|trainer.py:930] 2025-03-10 20:14:13,817 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:14:13,821 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:14:13,821 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 20:14:13,821 >>   Batch size = 4
{'eval_loss': 0.8549599647521973, 'eval_mse': 0.855036669090146, 'eval_pearson': 0.7989996243489318, 'eval_spearmanr': 0.8126713665838188, 'eval_runtime': 3.9304, 'eval_samples_per_second': 527.678, 'eval_steps_per_second': 66.151, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 47853/49080 [3:11:02<04:40,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 20:14:18,037 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853
[INFO|configuration_utils.py:423] 2025-03-10 20:14:18,040 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:14:18,718 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:14:18,719 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:14:18,719 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/special_tokens_map.json
[2025-03-10 20:14:18,761] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step47872 is about to be saved!
[2025-03-10 20:14:18,766] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt
[2025-03-10 20:14:18,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt...
[2025-03-10 20:14:19,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/mp_rank_00_model_states.pt.
[2025-03-10 20:14:19,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:14:22,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:14:22,937] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-47853/global_step47872/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:14:22,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step47872 is ready now!
{'loss': 0.0142, 'grad_norm': 2.754147529602051, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [3:15:48<00:00,  4.40it/s][INFO|trainer.py:930] 2025-03-10 20:19:03,393 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:19:03,397 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:19:03,397 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 20:19:03,397 >>   Batch size = 4
{'eval_loss': 0.9598376750946045, 'eval_mse': 0.9595160255551684, 'eval_pearson': 0.7737590343065828, 'eval_spearmanr': 0.7926168558428041, 'eval_runtime': 3.9243, 'eval_samples_per_second': 528.499, 'eval_steps_per_second': 66.254, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [3:15:52<00:00,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 20:19:07,604 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080
[INFO|configuration_utils.py:423] 2025-03-10 20:19:07,606 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:19:08,405 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:19:08,406 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:19:08,406 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/special_tokens_map.json
[2025-03-10 20:19:08,461] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step49099 is about to be saved!
[2025-03-10 20:19:08,466] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt
[2025-03-10 20:19:08,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt...
[2025-03-10 20:19:09,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/mp_rank_00_model_states.pt.
[2025-03-10 20:19:09,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 20:19:12,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 20:19:12,735] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=30/checkpoint-49080/global_step49099/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 20:19:12,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step49099 is ready now!
[INFO|trainer.py:2670] 2025-03-10 20:19:12,738 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 11757.7241, 'train_samples_per_second': 133.652, 'train_steps_per_second': 4.174, 'train_loss': 0.12319067284258381, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49080/49080 [3:15:57<00:00,  4.17it/s]
[INFO|trainer.py:3955] 2025-03-10 20:19:12,931 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=30
[INFO|configuration_utils.py:423] 2025-03-10 20:19:12,933 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=30/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 20:19:13,609 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=30/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 20:19:13,610 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 20:19:13,610 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=30/special_tokens_map.json
***** train metrics *****
  epoch                    =      39.9994
  total_flos               = 1363671072GF
  train_loss               =       0.1232
  train_runtime            =   3:15:57.72
  train_samples            =        39286
  train_samples_per_second =      133.652
  train_steps_per_second   =        4.174
03/10/2025 20:19:13 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 20:19:13,667 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 20:19:13,670 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 20:19:13,670 >>   Num examples = 2074
[INFO|trainer.py:4276] 2025-03-10 20:19:13,670 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 260/260 [00:04<00:00, 61.53it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     0.9598
  eval_mse                =     0.9595
  eval_pearson            =     0.7738
  eval_runtime            = 0:00:04.24
  eval_samples            =       2074
  eval_samples_per_second =    488.141
  eval_spearmanr          =     0.7926
  eval_steps_per_second   =     61.194
[INFO|modelcard.py:449] 2025-03-10 20:19:18,143 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7926168558428041}]}
[rank0]:[W310 20:19:18.623520193 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 20:19:21,313] [INFO] [launch.py:351:main] Process 59792 exits successfully.
[2025-03-10 20:19:21,313] [INFO] [launch.py:351:main] Process 59793 exits successfully.
(GNER) chrisjihee@dgx-a100:~/proj/GNER$

