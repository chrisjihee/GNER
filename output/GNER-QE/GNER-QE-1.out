(GNER) chrisjihee@dgx-a100:~/proj/GNER$ ./run-GNER-QE-1.sh
+ DEEPSPEED_CONFIG=configs/deepspeed/ds1_t5.json
++ shuf -i 25000-30000 -n 1
+ DEEPSPEED_PORT=27481
+ CUDA_DEVICES=0,1
+ SOURCE_FILE=run_glue.py
+ TRAIN_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json
+ VALID_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json
+ OUTPUT_NAME=GNER-QE
+ MODEL_NAMES=("google-bert/bert-base-cased" "FacebookAI/roberta-base" "FacebookAI/roberta-large")
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:0,1 --master_port 27481 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:42:31,418] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:42:34,428] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 13:42:34,428] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=27481 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:42:39,150] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:42:43,172] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-03-10 13:42:43,173] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 13:42:43,173] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 13:42:43,173] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 13:42:43,173] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-03-10 13:42:43,173] [INFO] [launch.py:256:main] process 4019473 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:42:43,174] [INFO] [launch.py:256:main] process 4019474 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:42:47,606] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:42:47,762] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:42:48,339] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:42:48,497] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:42:48,497] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/10/2025 13:42:50 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:42:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:42:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/google-bert/bert-base-cased-num=10/runs/Mar10_13-42-47_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/google-bert/bert-base-cased-num=10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/google-bert/bert-base-cased-num=10,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 13:42:50 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json
03/10/2025 13:42:50 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json
Using custom data configuration default-0ba8dc2d49302199
03/10/2025 13:42:51 - INFO - datasets.builder - Using custom data configuration default-0ba8dc2d49302199
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 13:42:51 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 13:42:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 13:42:51 - INFO - datasets.info - Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 13:42:51 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 13:42:51 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 13:42:51,701 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:42:51,704 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:699] 2025-03-10 13:42:51,922 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:42:51,923 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,923 >> loading file vocab.txt from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,923 >> loading file tokenizer.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,923 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,923 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,923 >> loading file tokenizer_config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:42:51,924 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 13:42:51,924 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:42:51,924 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:4128] 2025-03-10 13:42:51,989 >> loading weights file model.safetensors from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors
[INFO|logging.py:344] 2025-03-10 13:42:52,028 >> A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):
* `cls.predictions.transform.LayerNorm.beta` -> `bert.cls.predictions.transform.LayerNorm.bias`
* `cls.predictions.transform.LayerNorm.gamma` -> `bert.cls.predictions.transform.LayerNorm.weight`
If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.
[INFO|modeling_utils.py:5091] 2025-03-10 13:42:52,032 >> Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 13:42:52,032 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s][WARNING|modeling_utils.py:5103] 2025-03-10 13:42:52,052 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-056364d3775643cb.arrow
03/10/2025 13:42:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-056364d3775643cb.arrow
Running tokenizer on dataset:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                | 9000/13177 [00:02<00:01, 3108.28 examples/s][rank1]:[W310 13:42:55.777460112 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:04<00:00, 3234.54 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                               | 0/691 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-01476aec0113f6ed.arrow
03/10/2025 13:42:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-01476aec0113f6ed.arrow
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 3478.62 examples/s]
[rank0]:[W310 13:42:59.715743276 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 13:42:59 - INFO - __main__ - Sample 10476 of the training set: {'sentence1': 'what(O) movie(O) stars(O) christopher(O) walken(O) an(O) sean(B-actor) penn(I-actor)', 'sentence2': 'what movie stars christopher walken an sean penn', 'label': 2.61, 'idx': 10476, 'input_ids': [101, 1184, 113, 152, 114, 2523, 113, 152, 114, 2940, 113, 152, 114, 22572, 26691, 15940, 113, 152, 114, 2647, 1424, 113, 152, 114, 1126, 113, 152, 114, 2343, 1179, 113, 139, 118, 2811, 114, 8228, 1179, 113, 146, 118, 2811, 114, 102, 1184, 2523, 2940, 22572, 26691, 15940, 2647, 1424, 1126, 2343, 1179, 8228, 1179, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:42:59 - INFO - __main__ - Sample 1824 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-conference) County(I-conference) Fair(I-conference) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 2.64, 'idx': 1824, 'input_ids': [101, 1109, 113, 152, 114, 2124, 113, 139, 118, 1846, 114, 2596, 113, 152, 114, 1104, 113, 152, 114, 8838, 113, 139, 118, 3511, 114, 1786, 113, 146, 118, 3511, 114, 1144, 113, 152, 114, 1151, 113, 152, 114, 1316, 113, 152, 114, 1120, 113, 152, 114, 1103, 113, 152, 114, 4922, 10712, 113, 139, 118, 2755, 114, 7975, 113, 146, 118, 2755, 114, 22844, 113, 146, 118, 2755, 114, 117, 113, 152, 114, 24664, 5886, 3361, 113, 139, 118, 2450, 114, 117, 113, 152, 114, 1105, 113, 152, 114, 1103, 113, 152, 114, 7360, 113, 139, 118, 2450, 114, 7331, 113, 146, 118, 2450, 114, 6492, 113, 146, 118, 2450, 114, 1104, 113, 146, 118, 2450, 114, 3096, 1348, 1306, 5589, 2883, 1161, 113, 146, 118, 2450, 114, 1107, 113, 152, 114, 150, 5589, 18974, 1161, 113, 139, 118, 2450, 114, 117, 113, 152, 114, 2722, 113, 139, 118, 1583, 114, 132, 113, 152, 114, 1105, 113, 152, 114, 1120, 113, 152, 114, 1241, 113, 152, 114, 1103, 113, 152, 114, 13697, 113, 139, 118, 3511, 114, 1391, 113, 146, 118, 3511, 114, 6632, 113, 146, 118, 3511, 114, 1105, 113, 152, 114, 1103, 113, 152, 114, 1392, 113, 139, 118, 2450, 114, 1104, 113, 146, 118, 2450, 114, 2334, 113, 146, 118, 2450, 114, 1105, 113, 146, 118, 2450, 114, 4052, 113, 146, 118, 2450, 114, 1107, 113, 152, 114, 13697, 113, 139, 118, 2450, 114, 1166, 113, 152, 114, 1103, 113, 152, 114, 1763, 113, 152, 114, 1405, 113, 152, 114, 1201, 113, 152, 114, 119, 113, 152, 114, 102, 1109, 2124, 2596, 1104, 8838, 1786, 1144, 1151, 1316, 1120, 1103, 4922, 10712, 7975, 22844, 117, 24664, 5886, 3361, 117, 1105, 1103, 7360, 7331, 6492, 1104, 3096, 1348, 1306, 5589, 2883, 1161, 1107, 150, 5589, 18974, 1161, 117, 2722, 132, 1105, 1120, 1241, 1103, 13697, 1391, 6632, 1105, 1103, 1392, 1104, 2334, 1105, 4052, 1107, 13697, 1166, 1103, 1763, 1405, 1201, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:42:59 - INFO - __main__ - Sample 409 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-conference) MAC(I-conference) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 0.96, 'idx': 409, 'input_ids': [101, 1960, 113, 152, 114, 14427, 113, 152, 114, 117, 113, 152, 114, 12193, 113, 139, 118, 1825, 114, 17931, 2142, 113, 146, 118, 1825, 114, 1105, 113, 152, 114, 9532, 113, 139, 118, 1825, 114, 5525, 113, 146, 118, 1825, 114, 15463, 3954, 1399, 113, 146, 118, 1825, 114, 117, 113, 152, 114, 4102, 113, 152, 114, 1106, 113, 152, 114, 3118, 113, 152, 114, 8795, 113, 152, 114, 118, 113, 152, 114, 1147, 113, 152, 114, 1372, 113, 152, 114, 1108, 113, 152, 114, 2752, 113, 152, 114, 1106, 113, 152, 114, 18995, 113, 152, 114, 1112, 113, 152, 114, 4507, 113, 139, 118, 1583, 114, 1105, 113, 152, 114, 4042, 113, 139, 118, 3511, 114, 25424, 113, 146, 118, 3511, 114, 1111, 113, 152, 114, 1103, 113, 152, 114, 1397, 113, 152, 114, 1476, 113, 152, 114, 1201, 113, 152, 114, 119, 113, 152, 114, 102, 1960, 14427, 117, 12193, 17931, 2142, 1105, 9532, 5525, 15463, 3954, 1399, 117, 4102, 1106, 3118, 8795, 118, 1147, 1372, 1108, 2752, 1106, 18995, 1112, 4507, 1105, 4042, 25424, 1111, 1103, 1397, 1476, 1201, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 13:42:59,763 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 13:42:59,940 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 13:42:59,945] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
Running tokenizer on dataset:   8%|███████████████████████                                                                                                                                                                                                                                                                                         | 1000/13177 [00:00<00:04, 2611.97 examples/s][2025-03-10 13:43:00,921] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:05<00:00, 2614.03 examples/s]
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 3389.28 examples/s]
[2025-03-10 13:43:06,929] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 13:43:06,931] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 13:43:06,931] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 13:43:06,935] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 13:43:06,935] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 13:43:06,936] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 13:43:06,936] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 13:43:06,936] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 13:43:06,936] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 13:43:06,936] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 13:43:07,212] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:43:07,344] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 13:43:07,344] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.51 GB         CA 0.51 GB         Max_CA 1 GB
[2025-03-10 13:43:07,345] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.33 GB, percent = 4.4%
[2025-03-10 13:43:07,496] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 13:43:07,497] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.61 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:43:07,497] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.39 GB, percent = 4.4%
[2025-03-10 13:43:07,497] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 13:43:07,648] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 13:43:07,649] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:43:07,649] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.38 GB, percent = 4.4%
[2025-03-10 13:43:07,651] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 13:43:07,651] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:43:07,652] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 13:43:07,652] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb283d30a40>
[2025-03-10 13:43:07,652] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 13:43:07,652] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb28df2b800>
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 13:43:07,653] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 13:43:07,654] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 13:43:07,655] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 13:43:07,656] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 13:43:07,656] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 13:43:07,658 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 13:43:07,658 >>   Num examples = 13,177
[INFO|trainer.py:2416] 2025-03-10 13:43:07,658 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 13:43:07,658 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 13:43:07,658 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 13:43:07,658 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 13:43:07,658 >>   Total optimization steps = 16,480
[INFO|trainer.py:2423] 2025-03-10 13:43:07,658 >>   Number of trainable parameters = 108,311,041
{'loss': 1.5176, 'grad_norm': 24.988439559936523, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                               | 412/16480 [00:53<32:24,  8.26it/s][INFO|trainer.py:930] 2025-03-10 13:44:01,082 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:44:01,086 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:44:01,086 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:44:01,086 >>   Batch size = 4
{'eval_loss': 1.430349588394165, 'eval_mse': 1.4309643105039032, 'eval_pearson': 0.5930484497349713, 'eval_spearmanr': 0.6021406607796341, 'eval_runtime': 0.9391, 'eval_samples_per_second': 735.809, 'eval_steps_per_second': 92.642, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                               | 412/16480 [00:54<32:24,  8.26it/s]
[INFO|trainer.py:3955] 2025-03-10 13:44:02,157 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412
[INFO|configuration_utils.py:423] 2025-03-10 13:44:02,159 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:44:02,406 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:44:02,407 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:44:02,407 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/special_tokens_map.json
[2025-03-10 13:44:02,430] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step412 is about to be saved!
[2025-03-10 13:44:02,434] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt
[2025-03-10 13:44:02,434] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt...
[2025-03-10 13:44:02,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt.
[2025-03-10 13:44:02,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:44:03,659] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:44:03,660] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:44:03,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step412 is ready now!
{'loss': 0.8894, 'grad_norm': 16.665082931518555, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                      | 824/16480 [01:45<30:48,  8.47it/s][INFO|trainer.py:930] 2025-03-10 13:44:53,584 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:44:53,588 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:44:53,588 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:44:53,588 >>   Batch size = 4
{'eval_loss': 1.363456130027771, 'eval_mse': 1.363422112941052, 'eval_pearson': 0.6338887967785272, 'eval_spearmanr': 0.6424245659021817, 'eval_runtime': 0.8834, 'eval_samples_per_second': 782.21, 'eval_steps_per_second': 98.484, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                      | 824/16480 [01:46<30:48,  8.47it/s]
[INFO|trainer.py:3955] 2025-03-10 13:44:54,604 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824
[INFO|configuration_utils.py:423] 2025-03-10 13:44:54,606 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:44:54,817 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:44:54,817 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:44:54,818 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/special_tokens_map.json
[2025-03-10 13:44:54,836] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step824 is about to be saved!
[2025-03-10 13:44:54,839] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt
[2025-03-10 13:44:54,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt...
[2025-03-10 13:44:55,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt.
[2025-03-10 13:44:55,121] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:44:56,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:44:56,031] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:44:56,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step824 is ready now!
{'loss': 0.7179, 'grad_norm': 11.087432861328125, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 1236/16480 [02:36<29:13,  8.69it/s][INFO|trainer.py:930] 2025-03-10 13:45:44,498 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:45:44,502 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:45:44,502 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:45:44,502 >>   Batch size = 4
{'eval_loss': 1.437075138092041, 'eval_mse': 1.4374973287458186, 'eval_pearson': 0.6417821294630623, 'eval_spearmanr': 0.6645826356755763, 'eval_runtime': 0.8832, 'eval_samples_per_second': 782.407, 'eval_steps_per_second': 98.509, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 1236/16480 [02:37<29:13,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 13:45:45,519 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236
[INFO|configuration_utils.py:423] 2025-03-10 13:45:45,521 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:45:45,731 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:45:45,731 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:45:45,731 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/special_tokens_map.json
[2025-03-10 13:45:45,751] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1236 is about to be saved!
[2025-03-10 13:45:45,754] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt
[2025-03-10 13:45:45,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt...
[2025-03-10 13:45:46,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt.
[2025-03-10 13:45:46,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:45:46,944] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:45:46,944] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:45:46,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1236 is ready now!
{'loss': 0.6174, 'grad_norm': 16.187118530273438, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 1648/16480 [03:27<28:34,  8.65it/s][INFO|trainer.py:930] 2025-03-10 13:46:34,880 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:46:34,884 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:46:34,884 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:46:34,884 >>   Batch size = 4
{'eval_loss': 1.2819795608520508, 'eval_mse': 1.2816030423651557, 'eval_pearson': 0.6505772602425974, 'eval_spearmanr': 0.6446625327383, 'eval_runtime': 0.95, 'eval_samples_per_second': 727.357, 'eval_steps_per_second': 91.578, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 1648/16480 [03:28<28:34,  8.65it/s]
[INFO|trainer.py:3955] 2025-03-10 13:46:35,919 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648
[INFO|configuration_utils.py:423] 2025-03-10 13:46:35,921 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:46:36,133 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:46:36,134 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:46:36,134 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/special_tokens_map.json
[2025-03-10 13:46:36,155] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1648 is about to be saved!
[2025-03-10 13:46:36,158] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt
[2025-03-10 13:46:36,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt...
[2025-03-10 13:46:36,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt.
[2025-03-10 13:46:36,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:46:37,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:46:37,401] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:46:37,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1648 is ready now!
{'loss': 0.532, 'grad_norm': 10.540040016174316, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [04:17<27:40,  8.69it/s][INFO|trainer.py:930] 2025-03-10 13:47:25,228 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:47:25,232 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:47:25,232 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:47:25,232 >>   Batch size = 4
{'eval_loss': 1.2699729204177856, 'eval_mse': 1.2692602719308326, 'eval_pearson': 0.651228890623708, 'eval_spearmanr': 0.6427595352793397, 'eval_runtime': 0.8837, 'eval_samples_per_second': 781.913, 'eval_steps_per_second': 98.446, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [04:18<27:40,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 13:47:26,251 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060
[INFO|configuration_utils.py:423] 2025-03-10 13:47:26,253 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:47:26,462 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:47:26,462 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:47:26,462 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/special_tokens_map.json
[2025-03-10 13:47:26,479] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2060 is about to be saved!
[2025-03-10 13:47:26,482] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt
[2025-03-10 13:47:26,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt...
[2025-03-10 13:47:26,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt.
[2025-03-10 13:47:26,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:47:27,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:47:27,677] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:47:27,677] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2060 is ready now!
{'loss': 0.4651, 'grad_norm': 12.022747993469238, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [05:07<26:52,  8.69it/s][INFO|trainer.py:930] 2025-03-10 13:48:15,563 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:48:15,566 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:48:15,566 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:48:15,566 >>   Batch size = 4
{'eval_loss': 1.3316158056259155, 'eval_mse': 1.3315648611651143, 'eval_pearson': 0.6416140746323676, 'eval_spearmanr': 0.6325000092149633, 'eval_runtime': 0.8851, 'eval_samples_per_second': 780.745, 'eval_steps_per_second': 98.299, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [05:08<26:52,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 13:48:16,584 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472
[INFO|configuration_utils.py:423] 2025-03-10 13:48:16,586 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:48:16,793 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:48:16,794 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:48:16,794 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/special_tokens_map.json
[2025-03-10 13:48:16,809] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2472 is about to be saved!
[2025-03-10 13:48:16,812] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt
[2025-03-10 13:48:16,812] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt...
[2025-03-10 13:48:17,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt.
[2025-03-10 13:48:17,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:48:17,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:48:17,991] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:48:17,991] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2472 is ready now!
{'loss': 0.4189, 'grad_norm': 8.399312019348145, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [05:58<26:06,  8.68it/s][INFO|trainer.py:930] 2025-03-10 13:49:06,631 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:49:06,634 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:49:06,634 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:49:06,634 >>   Batch size = 4
{'eval_loss': 1.2134830951690674, 'eval_mse': 1.2115611305457639, 'eval_pearson': 0.6840143981676658, 'eval_spearmanr': 0.6789908677281529, 'eval_runtime': 0.935, 'eval_samples_per_second': 739.028, 'eval_steps_per_second': 93.047, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [05:59<26:06,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 13:49:07,653 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884
[INFO|configuration_utils.py:423] 2025-03-10 13:49:07,655 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:49:07,861 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:49:07,862 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:49:07,862 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/special_tokens_map.json
[2025-03-10 13:49:07,881] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2884 is about to be saved!
[2025-03-10 13:49:07,884] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt
[2025-03-10 13:49:07,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt...
[2025-03-10 13:49:08,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt.
[2025-03-10 13:49:08,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:49:09,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:49:09,044] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:49:09,044] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2884 is ready now!
{'loss': 0.3586, 'grad_norm': 10.324353218078613, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [06:49<25:44,  8.54it/s][INFO|trainer.py:930] 2025-03-10 13:49:56,984 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:49:56,988 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:49:56,988 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:49:56,988 >>   Batch size = 4
{'eval_loss': 1.267174482345581, 'eval_mse': 1.2664536970914186, 'eval_pearson': 0.6795172406521044, 'eval_spearmanr': 0.6817625600653557, 'eval_runtime': 0.8862, 'eval_samples_per_second': 779.773, 'eval_steps_per_second': 98.177, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [06:50<25:44,  8.54it/s]
[INFO|trainer.py:3955] 2025-03-10 13:49:58,007 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296
[INFO|configuration_utils.py:423] 2025-03-10 13:49:58,009 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:49:58,216 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:49:58,217 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:49:58,217 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/special_tokens_map.json
[2025-03-10 13:49:58,235] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3296 is about to be saved!
[2025-03-10 13:49:58,239] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt
[2025-03-10 13:49:58,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt...
[2025-03-10 13:49:58,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt.
[2025-03-10 13:49:58,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:49:59,414] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:49:59,415] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:49:59,415] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3296 is ready now!
{'loss': 0.3282, 'grad_norm': 7.732054233551025, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [07:40<25:12,  8.44it/s][INFO|trainer.py:930] 2025-03-10 13:50:48,048 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:50:48,052 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:50:48,052 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:50:48,052 >>   Batch size = 4
{'eval_loss': 1.504055142402649, 'eval_mse': 1.5047199142997068, 'eval_pearson': 0.631897113524152, 'eval_spearmanr': 0.631761394292342, 'eval_runtime': 0.9396, 'eval_samples_per_second': 735.454, 'eval_steps_per_second': 92.597, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [07:41<25:12,  8.44it/s]
[INFO|trainer.py:3955] 2025-03-10 13:50:49,075 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708
[INFO|configuration_utils.py:423] 2025-03-10 13:50:49,077 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:50:49,281 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:50:49,282 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:50:49,282 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/special_tokens_map.json
[2025-03-10 13:50:49,303] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3708 is about to be saved!
[2025-03-10 13:50:49,306] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt
[2025-03-10 13:50:49,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt...
[2025-03-10 13:50:49,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt.
[2025-03-10 13:50:49,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:50:50,498] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:50:50,498] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:50:50,498] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3708 is ready now!
{'loss': 0.2819, 'grad_norm': 9.386104583740234, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [08:31<23:39,  8.71it/s][INFO|trainer.py:930] 2025-03-10 13:51:38,875 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:51:38,878 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:51:38,878 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:51:38,878 >>   Batch size = 4
{'eval_loss': 1.28667414188385, 'eval_mse': 1.2870142704843612, 'eval_pearson': 0.6515722479342749, 'eval_spearmanr': 0.6484394017901388, 'eval_runtime': 0.8838, 'eval_samples_per_second': 781.881, 'eval_steps_per_second': 98.442, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [08:32<23:39,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 13:51:39,899 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120
[INFO|configuration_utils.py:423] 2025-03-10 13:51:39,901 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:51:40,104 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:51:40,105 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:51:40,105 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/special_tokens_map.json
[2025-03-10 13:51:40,119] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4120 is about to be saved!
[2025-03-10 13:51:40,123] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt
[2025-03-10 13:51:40,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt...
[2025-03-10 13:51:40,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt.
[2025-03-10 13:51:40,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:51:41,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:51:41,321] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:51:41,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4120 is ready now!
{'loss': 0.2503, 'grad_norm': 8.326375007629395, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [09:21<22:58,  8.67it/s][INFO|trainer.py:930] 2025-03-10 13:52:29,130 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:52:29,133 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:52:29,133 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:52:29,133 >>   Batch size = 4
{'eval_loss': 1.4824328422546387, 'eval_mse': 1.4823501568972288, 'eval_pearson': 0.6754660163765158, 'eval_spearmanr': 0.6759613830419412, 'eval_runtime': 0.8845, 'eval_samples_per_second': 781.264, 'eval_steps_per_second': 98.365, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [09:22<22:58,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 13:52:30,155 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532
[INFO|configuration_utils.py:423] 2025-03-10 13:52:30,157 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:52:30,364 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:52:30,365 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:52:30,365 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/special_tokens_map.json
[2025-03-10 13:52:30,387] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4532 is about to be saved!
[2025-03-10 13:52:30,390] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt
[2025-03-10 13:52:30,390] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt...
[2025-03-10 13:52:30,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt.
[2025-03-10 13:52:30,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:52:31,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:52:31,670] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:52:31,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4532 is ready now!
{'loss': 0.22, 'grad_norm': 7.514915943145752, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [10:12<22:05,  8.71it/s][INFO|trainer.py:930] 2025-03-10 13:53:19,702 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:53:19,705 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:53:19,705 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:53:19,705 >>   Batch size = 4
{'eval_loss': 1.2806504964828491, 'eval_mse': 1.2818429421757478, 'eval_pearson': 0.6777052849488108, 'eval_spearmanr': 0.6845489307685965, 'eval_runtime': 0.8825, 'eval_samples_per_second': 783.025, 'eval_steps_per_second': 98.586, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [10:12<22:05,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 13:53:20,725 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944
[INFO|configuration_utils.py:423] 2025-03-10 13:53:20,727 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:53:20,928 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:53:20,929 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:53:20,929 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/special_tokens_map.json
[2025-03-10 13:53:20,951] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4944 is about to be saved!
[2025-03-10 13:53:20,955] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt
[2025-03-10 13:53:20,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt...
[2025-03-10 13:53:21,240] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt.
[2025-03-10 13:53:21,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:53:22,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:53:22,171] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:53:22,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4944 is ready now!
{'loss': 0.1948, 'grad_norm': 5.437107086181641, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [11:02<21:20,  8.69it/s][INFO|trainer.py:930] 2025-03-10 13:54:09,987 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:54:09,990 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:54:09,990 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:54:09,990 >>   Batch size = 4
{'eval_loss': 1.1653587818145752, 'eval_mse': 1.1641293558819077, 'eval_pearson': 0.707796885772374, 'eval_spearmanr': 0.7128938653736565, 'eval_runtime': 0.8882, 'eval_samples_per_second': 777.951, 'eval_steps_per_second': 97.947, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [11:03<21:20,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 13:54:11,018 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356
[INFO|configuration_utils.py:423] 2025-03-10 13:54:11,020 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:54:11,221 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:54:11,222 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:54:11,222 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/special_tokens_map.json
[2025-03-10 13:54:11,245] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5356 is about to be saved!
[2025-03-10 13:54:11,248] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt
[2025-03-10 13:54:11,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt...
[2025-03-10 13:54:11,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt.
[2025-03-10 13:54:11,536] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:54:12,469] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:54:12,470] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:54:12,470] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5356 is ready now!
{'loss': 0.1753, 'grad_norm': 7.291269779205322, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [11:52<20:33,  8.68it/s][INFO|trainer.py:930] 2025-03-10 13:55:00,480 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:55:00,483 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:55:00,483 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:55:00,483 >>   Batch size = 4
{'eval_loss': 1.226580023765564, 'eval_mse': 1.2239084361086017, 'eval_pearson': 0.7014724643795531, 'eval_spearmanr': 0.7054359034099522, 'eval_runtime': 0.8804, 'eval_samples_per_second': 784.839, 'eval_steps_per_second': 98.815, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [11:53<20:33,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 13:55:01,503 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768
[INFO|configuration_utils.py:423] 2025-03-10 13:55:01,505 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:55:01,711 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:55:01,712 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:55:01,712 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/special_tokens_map.json
[2025-03-10 13:55:01,728] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5768 is about to be saved!
[2025-03-10 13:55:01,732] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt
[2025-03-10 13:55:01,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt...
[2025-03-10 13:55:02,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt.
[2025-03-10 13:55:02,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:55:02,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:55:02,959] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:55:02,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5768 is ready now!
{'loss': 0.1559, 'grad_norm': 6.357134819030762, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [12:43<19:42,  8.71it/s][INFO|trainer.py:930] 2025-03-10 13:55:50,880 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:55:50,883 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:55:50,883 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:55:50,883 >>   Batch size = 4
{'eval_loss': 1.2199938297271729, 'eval_mse': 1.2191984194577516, 'eval_pearson': 0.7031943791886089, 'eval_spearmanr': 0.704524021733827, 'eval_runtime': 0.9293, 'eval_samples_per_second': 743.538, 'eval_steps_per_second': 93.615, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [12:44<19:42,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 13:55:51,904 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180
[INFO|configuration_utils.py:423] 2025-03-10 13:55:51,906 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:55:52,106 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:55:52,107 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:55:52,107 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/special_tokens_map.json
[2025-03-10 13:55:52,130] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6180 is about to be saved!
[2025-03-10 13:55:52,134] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt
[2025-03-10 13:55:52,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt...
[2025-03-10 13:55:52,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt.
[2025-03-10 13:55:52,423] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:55:53,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:55:53,366] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:55:53,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6180 is ready now!
{'loss': 0.1415, 'grad_norm': 4.068461894989014, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [13:33<18:57,  8.69it/s][INFO|trainer.py:930] 2025-03-10 13:56:41,102 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:56:41,105 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:56:41,105 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:56:41,105 >>   Batch size = 4
{'eval_loss': 1.2307432889938354, 'eval_mse': 1.2311797280042458, 'eval_pearson': 0.695433099106902, 'eval_spearmanr': 0.6964603238905324, 'eval_runtime': 0.8794, 'eval_samples_per_second': 785.776, 'eval_steps_per_second': 98.933, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [13:34<18:57,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 13:56:42,125 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592
[INFO|configuration_utils.py:423] 2025-03-10 13:56:42,127 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:56:42,330 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:56:42,331 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:56:42,331 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/special_tokens_map.json
[2025-03-10 13:56:42,352] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6592 is about to be saved!
[2025-03-10 13:56:42,355] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt
[2025-03-10 13:56:42,356] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt...
[2025-03-10 13:56:42,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt.
[2025-03-10 13:56:42,640] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:56:43,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:56:43,573] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:56:43,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6592 is ready now!
{'loss': 0.1263, 'grad_norm': 12.32699966430664, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [14:24<18:39,  8.47it/s][INFO|trainer.py:930] 2025-03-10 13:57:32,299 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:57:32,303 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:57:32,303 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:57:32,303 >>   Batch size = 4
{'eval_loss': 1.2883615493774414, 'eval_mse': 1.2884893783094569, 'eval_pearson': 0.6919303773652538, 'eval_spearmanr': 0.6950278861999558, 'eval_runtime': 0.8854, 'eval_samples_per_second': 780.479, 'eval_steps_per_second': 98.266, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [14:25<18:39,  8.47it/s]
[INFO|trainer.py:3955] 2025-03-10 13:57:33,329 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004
[INFO|configuration_utils.py:423] 2025-03-10 13:57:33,332 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:57:33,553 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:57:33,553 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:57:33,554 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/special_tokens_map.json
[2025-03-10 13:57:33,578] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7004 is about to be saved!
[2025-03-10 13:57:33,582] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt
[2025-03-10 13:57:33,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt...
[2025-03-10 13:57:33,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt.
[2025-03-10 13:57:33,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:57:34,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:57:34,819] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:57:34,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7004 is ready now!
{'loss': 0.1135, 'grad_norm': 3.0493669509887695, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [15:16<17:45,  8.50it/s][INFO|trainer.py:930] 2025-03-10 13:58:23,906 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:58:23,909 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:58:23,909 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:58:23,909 >>   Batch size = 4
{'eval_loss': 1.2209341526031494, 'eval_mse': 1.2212447229929149, 'eval_pearson': 0.7107555062477102, 'eval_spearmanr': 0.7160916904154006, 'eval_runtime': 0.8862, 'eval_samples_per_second': 779.751, 'eval_steps_per_second': 98.174, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [15:17<17:45,  8.50it/s]
[INFO|trainer.py:3955] 2025-03-10 13:58:24,937 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416
[INFO|configuration_utils.py:423] 2025-03-10 13:58:24,939 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:58:25,142 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:58:25,143 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:58:25,143 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/special_tokens_map.json
[2025-03-10 13:58:25,167] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7416 is about to be saved!
[2025-03-10 13:58:25,171] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt
[2025-03-10 13:58:25,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt...
[2025-03-10 13:58:25,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt.
[2025-03-10 13:58:25,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:58:26,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:58:26,411] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:58:26,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7416 is ready now!
{'loss': 0.1031, 'grad_norm': 4.176753520965576, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [16:06<16:34,  8.70it/s][INFO|trainer.py:930] 2025-03-10 13:59:14,230 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:59:14,233 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:59:14,234 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 13:59:14,234 >>   Batch size = 4
{'eval_loss': 1.1576740741729736, 'eval_mse': 1.155886178768837, 'eval_pearson': 0.6963555943217518, 'eval_spearmanr': 0.7001971747473426, 'eval_runtime': 0.8851, 'eval_samples_per_second': 780.66, 'eval_steps_per_second': 98.289, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [16:07<16:34,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 13:59:15,260 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828
[INFO|configuration_utils.py:423] 2025-03-10 13:59:15,262 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:59:15,465 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:59:15,465 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:59:15,466 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/special_tokens_map.json
[2025-03-10 13:59:15,486] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7828 is about to be saved!
[2025-03-10 13:59:15,489] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt
[2025-03-10 13:59:15,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt...
[2025-03-10 13:59:15,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt.
[2025-03-10 13:59:15,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:59:16,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:59:16,706] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:59:16,706] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7828 is ready now!
{'loss': 0.0927, 'grad_norm': 3.111942768096924, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [16:56<15:46,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:00:04,450 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:00:04,454 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:00:04,454 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:00:04,454 >>   Batch size = 4
{'eval_loss': 1.1605792045593262, 'eval_mse': 1.1602657962638976, 'eval_pearson': 0.7160862411814023, 'eval_spearmanr': 0.7265195975895827, 'eval_runtime': 0.8807, 'eval_samples_per_second': 784.588, 'eval_steps_per_second': 98.783, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [16:57<15:46,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:00:05,477 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240
[INFO|configuration_utils.py:423] 2025-03-10 14:00:05,479 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:00:05,682 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:00:05,682 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:00:05,682 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/special_tokens_map.json
[2025-03-10 14:00:05,700] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8240 is about to be saved!
[2025-03-10 14:00:05,704] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt
[2025-03-10 14:00:05,704] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt...
[2025-03-10 14:00:05,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt.
[2025-03-10 14:00:05,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:00:06,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:00:06,897] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:00:06,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8240 is ready now!
{'loss': 0.0869, 'grad_norm': 6.330880641937256, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [17:47<15:00,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:00:54,712 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:00:54,716 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:00:54,716 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:00:54,716 >>   Batch size = 4
{'eval_loss': 1.2405633926391602, 'eval_mse': 1.2412817460928225, 'eval_pearson': 0.7059564951463208, 'eval_spearmanr': 0.7163906628405943, 'eval_runtime': 0.8852, 'eval_samples_per_second': 780.599, 'eval_steps_per_second': 98.281, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [17:47<15:00,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:00:55,744 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652
[INFO|configuration_utils.py:423] 2025-03-10 14:00:55,746 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:00:55,949 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:00:55,950 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:00:55,950 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/special_tokens_map.json
[2025-03-10 14:00:55,974] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8652 is about to be saved!
[2025-03-10 14:00:55,978] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt
[2025-03-10 14:00:55,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt...
[2025-03-10 14:00:56,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt.
[2025-03-10 14:00:56,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:00:57,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:00:57,228] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:00:57,228] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8652 is ready now!
{'loss': 0.0816, 'grad_norm': 3.162853717803955, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [18:37<14:11,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:01:44,997 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:01:45,000 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:01:45,000 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:01:45,000 >>   Batch size = 4
{'eval_loss': 1.2901878356933594, 'eval_mse': 1.2899013586913797, 'eval_pearson': 0.6885904153754085, 'eval_spearmanr': 0.6928778023389197, 'eval_runtime': 0.883, 'eval_samples_per_second': 782.519, 'eval_steps_per_second': 98.523, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [18:38<14:11,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:01:46,027 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064
[INFO|configuration_utils.py:423] 2025-03-10 14:01:46,029 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:01:46,229 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:01:46,230 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:01:46,230 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/special_tokens_map.json
[2025-03-10 14:01:46,254] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9064 is about to be saved!
[2025-03-10 14:01:46,258] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt
[2025-03-10 14:01:46,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt...
[2025-03-10 14:01:46,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt.
[2025-03-10 14:01:46,550] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:01:47,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:01:47,496] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:01:47,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9064 is ready now!
{'loss': 0.0737, 'grad_norm': 3.8906733989715576, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [19:27<13:27,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:02:35,287 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:02:35,290 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:02:35,290 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:02:35,290 >>   Batch size = 4
{'eval_loss': 1.214923620223999, 'eval_mse': 1.215446394190257, 'eval_pearson': 0.7138821110154339, 'eval_spearmanr': 0.7162909807501715, 'eval_runtime': 0.8849, 'eval_samples_per_second': 780.915, 'eval_steps_per_second': 98.321, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [19:28<13:27,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:02:36,320 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476
[INFO|configuration_utils.py:423] 2025-03-10 14:02:36,322 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:02:36,532 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:02:36,533 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:02:36,533 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/special_tokens_map.json
[2025-03-10 14:02:36,555] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9476 is about to be saved!
[2025-03-10 14:02:36,558] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt
[2025-03-10 14:02:36,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt...
[2025-03-10 14:02:36,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt.
[2025-03-10 14:02:36,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:02:37,822] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:02:37,823] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:02:37,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9476 is ready now!
{'loss': 0.068, 'grad_norm': 8.017523765563965, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [20:17<12:39,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:03:25,557 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:03:25,561 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:03:25,561 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:03:25,561 >>   Batch size = 4
{'eval_loss': 1.274253010749817, 'eval_mse': 1.2738660920718645, 'eval_pearson': 0.7014310053884818, 'eval_spearmanr': 0.7070651171227618, 'eval_runtime': 0.8843, 'eval_samples_per_second': 781.392, 'eval_steps_per_second': 98.381, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [20:18<12:39,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:03:26,589 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888
[INFO|configuration_utils.py:423] 2025-03-10 14:03:26,591 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:03:26,794 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:03:26,794 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:03:26,795 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/special_tokens_map.json
[2025-03-10 14:03:26,813] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9888 is about to be saved!
[2025-03-10 14:03:26,817] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt
[2025-03-10 14:03:26,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt...
[2025-03-10 14:03:27,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt.
[2025-03-10 14:03:27,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:03:28,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:03:28,050] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:03:28,050] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9888 is ready now!
{'loss': 0.0624, 'grad_norm': 2.192340850830078, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [21:08<11:49,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:04:15,860 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:04:15,863 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:04:15,863 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:04:15,863 >>   Batch size = 4
{'eval_loss': 1.3351253271102905, 'eval_mse': 1.3343321497291973, 'eval_pearson': 0.6733388251587968, 'eval_spearmanr': 0.6879299198151518, 'eval_runtime': 0.8839, 'eval_samples_per_second': 781.743, 'eval_steps_per_second': 98.425, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [21:09<11:49,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:04:16,892 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300
[INFO|configuration_utils.py:423] 2025-03-10 14:04:16,894 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:04:17,101 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:04:17,102 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:04:17,102 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/special_tokens_map.json
[2025-03-10 14:04:17,116] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10300 is about to be saved!
[2025-03-10 14:04:17,119] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt
[2025-03-10 14:04:17,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt...
[2025-03-10 14:04:17,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt.
[2025-03-10 14:04:17,402] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:04:18,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:04:18,340] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:04:18,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10300 is ready now!
{'loss': 0.059, 'grad_norm': 3.325704574584961, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [21:58<11:02,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:05:06,099 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:05:06,102 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:05:06,102 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:05:06,103 >>   Batch size = 4
{'eval_loss': 1.2585735321044922, 'eval_mse': 1.2576214646119974, 'eval_pearson': 0.6816008279620058, 'eval_spearmanr': 0.6876870928022744, 'eval_runtime': 0.8845, 'eval_samples_per_second': 781.259, 'eval_steps_per_second': 98.364, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [21:59<11:02,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:05:07,132 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712
[INFO|configuration_utils.py:423] 2025-03-10 14:05:07,134 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:05:07,341 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:05:07,341 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:05:07,341 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/special_tokens_map.json
[2025-03-10 14:05:07,366] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10712 is about to be saved!
[2025-03-10 14:05:07,370] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt
[2025-03-10 14:05:07,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt...
[2025-03-10 14:05:07,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt.
[2025-03-10 14:05:07,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:05:08,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:05:08,641] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:05:08,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10712 is ready now!
{'loss': 0.0549, 'grad_norm': 2.8775198459625244, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [22:48<10:15,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:05:56,393 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:05:56,397 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:05:56,397 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:05:56,397 >>   Batch size = 4
{'eval_loss': 1.250802755355835, 'eval_mse': 1.250519622079543, 'eval_pearson': 0.7023313689889112, 'eval_spearmanr': 0.707386885138651, 'eval_runtime': 0.8803, 'eval_samples_per_second': 784.918, 'eval_steps_per_second': 98.825, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [22:49<10:15,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:05:57,420 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124
[INFO|configuration_utils.py:423] 2025-03-10 14:05:57,422 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:05:57,625 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:05:57,626 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:05:57,626 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/special_tokens_map.json
[2025-03-10 14:05:57,651] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11124 is about to be saved!
[2025-03-10 14:05:57,654] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt
[2025-03-10 14:05:57,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt...
[2025-03-10 14:05:57,947] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt.
[2025-03-10 14:05:57,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:05:58,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:05:58,907] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:05:58,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11124 is ready now!
{'loss': 0.0549, 'grad_norm': 2.7658350467681885, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [23:39<09:28,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:06:46,708 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:06:46,711 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:06:46,711 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:06:46,711 >>   Batch size = 4
{'eval_loss': 1.1782317161560059, 'eval_mse': 1.178970133688964, 'eval_pearson': 0.6971651361653713, 'eval_spearmanr': 0.7061904860627768, 'eval_runtime': 0.8866, 'eval_samples_per_second': 779.417, 'eval_steps_per_second': 98.132, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [23:39<09:28,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:06:47,745 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536
[INFO|configuration_utils.py:423] 2025-03-10 14:06:47,747 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:06:47,986 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:06:47,987 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:06:47,987 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/special_tokens_map.json
[2025-03-10 14:06:48,012] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11536 is about to be saved!
[2025-03-10 14:06:48,016] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt
[2025-03-10 14:06:48,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt...
[2025-03-10 14:06:48,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt.
[2025-03-10 14:06:48,356] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:06:49,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:06:49,306] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:06:49,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11536 is ready now!
{'loss': 0.0497, 'grad_norm': 2.4980733394622803, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [24:29<08:41,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:07:37,122 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:07:37,125 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:07:37,125 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:07:37,125 >>   Batch size = 4
{'eval_loss': 1.1880607604980469, 'eval_mse': 1.187600808964797, 'eval_pearson': 0.70890913839151, 'eval_spearmanr': 0.7169269626721844, 'eval_runtime': 0.8845, 'eval_samples_per_second': 781.189, 'eval_steps_per_second': 98.355, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [24:30<08:41,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:07:38,155 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948
[INFO|configuration_utils.py:423] 2025-03-10 14:07:38,157 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:07:38,356 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:07:38,357 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:07:38,357 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/special_tokens_map.json
[2025-03-10 14:07:38,382] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11948 is about to be saved!
[2025-03-10 14:07:38,385] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt
[2025-03-10 14:07:38,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt...
[2025-03-10 14:07:38,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt.
[2025-03-10 14:07:38,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:07:39,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:07:39,643] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:07:39,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11948 is ready now!
{'loss': 0.0461, 'grad_norm': 1.3888325691223145, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [25:19<07:54,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:08:27,429 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:08:27,432 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:08:27,432 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:08:27,432 >>   Batch size = 4
{'eval_loss': 1.0631511211395264, 'eval_mse': 1.0637710608552748, 'eval_pearson': 0.7494334574363846, 'eval_spearmanr': 0.7590757102175889, 'eval_runtime': 0.8847, 'eval_samples_per_second': 781.067, 'eval_steps_per_second': 98.34, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [25:20<07:54,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:08:28,460 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360
[INFO|configuration_utils.py:423] 2025-03-10 14:08:28,462 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:08:28,660 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:08:28,660 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:08:28,660 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/special_tokens_map.json
[2025-03-10 14:08:28,677] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12360 is about to be saved!
[2025-03-10 14:08:28,680] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt
[2025-03-10 14:08:28,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt...
[2025-03-10 14:08:28,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt.
[2025-03-10 14:08:28,959] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:08:29,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:08:29,885] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:08:29,885] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12360 is ready now!
{'loss': 0.0435, 'grad_norm': 4.727286338806152, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [26:10<07:07,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:09:18,663 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:09:18,666 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:09:18,666 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:09:18,666 >>   Batch size = 4
{'eval_loss': 1.2069718837738037, 'eval_mse': 1.2075515442060147, 'eval_pearson': 0.697256549840565, 'eval_spearmanr': 0.7037425798172073, 'eval_runtime': 0.8841, 'eval_samples_per_second': 781.553, 'eval_steps_per_second': 98.401, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [26:11<07:07,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:09:19,695 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772
[INFO|configuration_utils.py:423] 2025-03-10 14:09:19,697 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:09:19,897 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:09:19,897 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:09:19,897 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/special_tokens_map.json
[2025-03-10 14:09:19,918] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12772 is about to be saved!
[2025-03-10 14:09:19,922] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt
[2025-03-10 14:09:19,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt...
[2025-03-10 14:09:20,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt.
[2025-03-10 14:09:20,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:09:21,173] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:09:21,173] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:09:21,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12772 is ready now!
{'loss': 0.0424, 'grad_norm': 1.0870301723480225, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [27:01<06:19,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:10:09,007 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:10:09,009 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:10:09,009 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:10:09,010 >>   Batch size = 4
{'eval_loss': 1.1306307315826416, 'eval_mse': 1.131587045755124, 'eval_pearson': 0.7170777125505421, 'eval_spearmanr': 0.7202999947786008, 'eval_runtime': 0.882, 'eval_samples_per_second': 783.472, 'eval_steps_per_second': 98.643, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [27:02<06:19,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:10:10,037 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184
[INFO|configuration_utils.py:423] 2025-03-10 14:10:10,039 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:10:10,238 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:10:10,239 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:10:10,239 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/special_tokens_map.json
[2025-03-10 14:10:10,263] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13184 is about to be saved!
[2025-03-10 14:10:10,267] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt
[2025-03-10 14:10:10,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt...
[2025-03-10 14:10:10,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt.
[2025-03-10 14:10:10,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:10:11,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:10:11,518] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:10:11,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13184 is ready now!
{'loss': 0.0396, 'grad_norm': 2.297365188598633, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [27:51<05:31,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:10:59,422 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:10:59,426 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:10:59,426 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:10:59,426 >>   Batch size = 4
{'eval_loss': 1.26023530960083, 'eval_mse': 1.261547032727519, 'eval_pearson': 0.6987905201952787, 'eval_spearmanr': 0.7052178628386433, 'eval_runtime': 0.883, 'eval_samples_per_second': 782.6, 'eval_steps_per_second': 98.533, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [27:52<05:31,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:11:00,453 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596
[INFO|configuration_utils.py:423] 2025-03-10 14:11:00,455 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:11:00,669 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:11:00,670 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:11:00,670 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/special_tokens_map.json
[2025-03-10 14:11:00,689] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13596 is about to be saved!
[2025-03-10 14:11:00,692] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt
[2025-03-10 14:11:00,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt...
[2025-03-10 14:11:00,986] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt.
[2025-03-10 14:11:00,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:11:01,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:11:01,917] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:11:01,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13596 is ready now!
{'loss': 0.0384, 'grad_norm': 1.7080368995666504, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [28:42<04:44,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:11:49,706 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:11:49,710 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:11:49,710 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:11:49,710 >>   Batch size = 4
{'eval_loss': 1.120019793510437, 'eval_mse': 1.1202620362752427, 'eval_pearson': 0.7301989786740969, 'eval_spearmanr': 0.735849257001637, 'eval_runtime': 0.8867, 'eval_samples_per_second': 779.319, 'eval_steps_per_second': 98.12, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [28:42<04:44,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:11:50,741 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008
[INFO|configuration_utils.py:423] 2025-03-10 14:11:50,743 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:11:50,949 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:11:50,950 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:11:50,950 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/special_tokens_map.json
[2025-03-10 14:11:50,964] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14008 is about to be saved!
[2025-03-10 14:11:50,967] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt
[2025-03-10 14:11:50,967] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt...
[2025-03-10 14:11:51,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt.
[2025-03-10 14:11:51,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:11:52,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:11:52,192] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:11:52,192] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14008 is ready now!
{'loss': 0.0369, 'grad_norm': 2.51883864402771, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 14420/16480 [29:32<03:56,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:12:40,210 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:12:40,213 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:12:40,213 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:12:40,213 >>   Batch size = 4
{'eval_loss': 1.1542749404907227, 'eval_mse': 1.154427879626085, 'eval_pearson': 0.7117120581194831, 'eval_spearmanr': 0.7195965460372826, 'eval_runtime': 0.8822, 'eval_samples_per_second': 783.306, 'eval_steps_per_second': 98.622, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 14420/16480 [29:33<03:56,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:12:41,241 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420
[INFO|configuration_utils.py:423] 2025-03-10 14:12:41,243 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:12:41,455 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:12:41,456 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:12:41,456 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/special_tokens_map.json
[2025-03-10 14:12:41,480] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14420 is about to be saved!
[2025-03-10 14:12:41,483] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt
[2025-03-10 14:12:41,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt...
[2025-03-10 14:12:41,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt.
[2025-03-10 14:12:41,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:12:42,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:12:42,740] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:12:42,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14420 is ready now!
{'loss': 0.0336, 'grad_norm': 4.001335144042969, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 14832/16480 [30:22<03:09,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:13:30,496 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:13:30,499 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:13:30,499 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:13:30,499 >>   Batch size = 4
{'eval_loss': 1.1697518825531006, 'eval_mse': 1.1693618235470762, 'eval_pearson': 0.7118535432233899, 'eval_spearmanr': 0.7227062739351704, 'eval_runtime': 0.8824, 'eval_samples_per_second': 783.05, 'eval_steps_per_second': 98.589, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 14832/16480 [30:23<03:09,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:13:31,526 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832
[INFO|configuration_utils.py:423] 2025-03-10 14:13:31,528 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:13:31,729 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:13:31,730 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:13:31,730 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/special_tokens_map.json
[2025-03-10 14:13:31,749] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14832 is about to be saved!
[2025-03-10 14:13:31,753] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt
[2025-03-10 14:13:31,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt...
[2025-03-10 14:13:32,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt.
[2025-03-10 14:13:32,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:13:32,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:13:32,982] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:13:32,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14832 is ready now!
{'loss': 0.0345, 'grad_norm': 2.2024543285369873, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 15244/16480 [31:13<02:22,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:14:20,819 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:14:20,822 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:14:20,822 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:14:20,822 >>   Batch size = 4
{'eval_loss': 1.158158302307129, 'eval_mse': 1.1586970168498767, 'eval_pearson': 0.7242938333932694, 'eval_spearmanr': 0.7288204374030899, 'eval_runtime': 0.8839, 'eval_samples_per_second': 781.743, 'eval_steps_per_second': 98.425, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 15244/16480 [31:14<02:22,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:14:21,849 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244
[INFO|configuration_utils.py:423] 2025-03-10 14:14:21,851 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:14:22,050 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:14:22,051 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:14:22,051 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/special_tokens_map.json
[2025-03-10 14:14:22,064] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15244 is about to be saved!
[2025-03-10 14:14:22,068] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt
[2025-03-10 14:14:22,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt...
[2025-03-10 14:14:22,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt.
[2025-03-10 14:14:22,344] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:14:23,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:14:23,251] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:14:23,251] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15244 is ready now!
{'loss': 0.0311, 'grad_norm': 2.2530226707458496, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 15656/16480 [32:03<01:34,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:15:11,089 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:15:11,092 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:15:11,092 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:15:11,092 >>   Batch size = 4
{'eval_loss': 1.1538372039794922, 'eval_mse': 1.1535821406092555, 'eval_pearson': 0.7194318335794554, 'eval_spearmanr': 0.7264202042941212, 'eval_runtime': 0.8836, 'eval_samples_per_second': 781.996, 'eval_steps_per_second': 98.457, 'epoch': 38.0}
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 15656/16480 [32:04<01:34,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:15:12,118 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656
[INFO|configuration_utils.py:423] 2025-03-10 14:15:12,120 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:15:12,320 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:15:12,321 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:15:12,321 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/special_tokens_map.json
[2025-03-10 14:15:12,344] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15656 is about to be saved!
[2025-03-10 14:15:12,347] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt
[2025-03-10 14:15:12,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt...
[2025-03-10 14:15:12,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt.
[2025-03-10 14:15:12,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:15:13,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:15:13,593] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:15:13,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15656 is ready now!
{'loss': 0.0286, 'grad_norm': 1.200512170791626, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 16068/16480 [32:53<00:47,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:16:01,373 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:16:01,376 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:16:01,376 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:16:01,376 >>   Batch size = 4
{'eval_loss': 1.1513724327087402, 'eval_mse': 1.1514496185672611, 'eval_pearson': 0.7189309948592566, 'eval_spearmanr': 0.7294661327570585, 'eval_runtime': 0.883, 'eval_samples_per_second': 782.579, 'eval_steps_per_second': 98.53, 'epoch': 39.0}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 16068/16480 [32:54<00:47,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:16:02,402 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068
[INFO|configuration_utils.py:423] 2025-03-10 14:16:02,404 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:16:02,607 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:16:02,607 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:16:02,608 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/special_tokens_map.json
[2025-03-10 14:16:02,632] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16068 is about to be saved!
[2025-03-10 14:16:02,636] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt
[2025-03-10 14:16:02,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt...
[2025-03-10 14:16:02,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt.
[2025-03-10 14:16:02,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:16:03,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:16:03,917] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:16:03,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16068 is ready now!
{'loss': 0.029, 'grad_norm': 1.298017144203186, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:44<00:00,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:16:51,707 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:16:51,710 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:16:51,710 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:16:51,710 >>   Batch size = 4
{'eval_loss': 1.1623096466064453, 'eval_mse': 1.1625634792399648, 'eval_pearson': 0.715771898608038, 'eval_spearmanr': 0.7254838558519948, 'eval_runtime': 0.8813, 'eval_samples_per_second': 784.074, 'eval_steps_per_second': 98.718, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:44<00:00,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:16:52,734 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480
[INFO|configuration_utils.py:423] 2025-03-10 14:16:52,736 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:16:52,935 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:16:52,936 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:16:52,936 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/special_tokens_map.json
[2025-03-10 14:16:52,960] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16480 is about to be saved!
[2025-03-10 14:16:52,963] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt
[2025-03-10 14:16:52,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt...
[2025-03-10 14:16:53,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt.
[2025-03-10 14:16:53,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:16:54,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:16:54,205] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:16:54,205] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16480 is ready now!
[INFO|trainer.py:2670] 2025-03-10 14:16:54,208 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2026.5497, 'train_samples_per_second': 260.087, 'train_steps_per_second': 8.132, 'train_loss': 0.21738065434891043, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:46<00:00,  8.13it/s]
[INFO|trainer.py:3955] 2025-03-10 14:16:54,303 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=10
[INFO|configuration_utils.py:423] 2025-03-10 14:16:54,305 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=10/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:16:54,505 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=10/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:16:54,506 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:16:54,506 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=10/special_tokens_map.json
***** train metrics *****
  epoch                    =        40.0
  total_flos               = 129223800GF
  train_loss               =      0.2174
  train_runtime            =  0:33:46.54
  train_samples            =       13177
  train_samples_per_second =     260.087
  train_steps_per_second   =       8.132
03/10/2025 14:16:54 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 14:16:54,529 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:16:54,532 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:16:54,532 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:16:54,532 >>   Batch size = 4
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:00<00:00, 100.13it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =     1.1623
  eval_mse                =     1.1626
  eval_pearson            =     0.7158
  eval_runtime            = 0:00:00.88
  eval_samples            =        691
  eval_samples_per_second =    781.563
  eval_spearmanr          =     0.7255
  eval_steps_per_second   =     98.402
[INFO|modelcard.py:449] 2025-03-10 14:16:56,009 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7254838558519948}]}
[rank0]:[W310 14:16:56.157117925 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 14:16:58,338] [INFO] [launch.py:351:main] Process 4019474 exits successfully.
[2025-03-10 14:16:58,338] [INFO] [launch.py:351:main] Process 4019473 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:0,1 --master_port 27481 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:17:04,900] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:17:07,773] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 14:17:07,773] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=27481 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:17:12,148] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:17:16,346] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-03-10 14:17:16,346] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 14:17:16,346] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 14:17:16,346] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 14:17:16,346] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-03-10 14:17:16,347] [INFO] [launch.py:256:main] process 4062191 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:17:16,348] [INFO] [launch.py:256:main] process 4062192 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:17:23,323] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:17:23,346] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:17:24,058] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 14:17:24,058] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 14:17:24,152] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 14:17:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 14:17:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-base-num=10/runs/Mar10_14-17-23_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-base-num=10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-base-num=10,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 14:17:26 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json
03/10/2025 14:17:26 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json
03/10/2025 14:17:26 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-0ba8dc2d49302199
03/10/2025 14:17:26 - INFO - datasets.builder - Using custom data configuration default-0ba8dc2d49302199
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 14:17:26 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 14:17:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:17:26 - INFO - datasets.info - Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 14:17:26 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:17:26 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 14:17:27,174 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:17:27,177 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 14:17:27,377 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:17:27,377 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:17:27,386 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 14:17:27,386 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:17:27,386 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 14:17:27,530 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 14:17:27,576 >> Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 14:17:27,576 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:5103] 2025-03-10 14:17:27,583 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-80b8a5306e53607a.arrow
03/10/2025 14:17:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-80b8a5306e53607a.arrow
Running tokenizer on dataset:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                | 9000/13177 [00:02<00:01, 3229.33 examples/s][rank1]:[W310 14:17:30.852173099 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:03<00:00, 3987.46 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                               | 0/691 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-de44ebb4b4bb3f42.arrow
03/10/2025 14:17:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-de44ebb4b4bb3f42.arrow
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 4554.58 examples/s]
[rank0]:[W310 14:17:33.380295836 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 14:17:34 - INFO - __main__ - Sample 10476 of the training set: {'sentence1': 'what(O) movie(O) stars(O) christopher(O) walken(O) an(O) sean(B-actor) penn(I-actor)', 'sentence2': 'what movie stars christopher walken an sean penn', 'label': 2.61, 'idx': 10476, 'input_ids': [0, 12196, 1640, 673, 43, 1569, 1640, 673, 43, 2690, 1640, 673, 43, 29224, 35885, 1640, 673, 43, 1656, 225, 1640, 673, 43, 41, 1640, 673, 43, 842, 260, 1640, 387, 12, 24625, 43, 30081, 1640, 100, 12, 24625, 43, 2, 2, 12196, 1569, 2690, 29224, 35885, 1656, 225, 41, 842, 260, 30081, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:17:34 - INFO - __main__ - Sample 1824 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-conference) County(I-conference) Fair(I-conference) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 2.64, 'idx': 1824, 'input_ids': [0, 133, 1640, 673, 43, 3453, 1640, 387, 12, 19527, 43, 5403, 1640, 673, 43, 9, 1640, 673, 43, 14416, 1640, 387, 12, 16459, 43, 1643, 1640, 100, 12, 16459, 43, 34, 1640, 673, 43, 57, 1640, 673, 43, 547, 1640, 673, 43, 23, 1640, 673, 43, 5, 1640, 673, 43, 944, 4308, 1020, 1640, 387, 12, 879, 31104, 43, 12450, 1640, 100, 12, 879, 31104, 43, 289, 3281, 1526, 1187, 5841, 1640, 100, 12, 879, 31104, 238, 1640, 673, 43, 15858, 922, 1176, 1640, 387, 12, 41829, 238, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 11660, 1640, 387, 12, 41829, 43, 5413, 1640, 100, 12, 41829, 43, 5036, 1640, 100, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 1664, 30293, 1526, 3898, 102, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 256, 1526, 462, 6080, 1640, 387, 12, 41829, 238, 1640, 673, 43, 2809, 1640, 387, 12, 12659, 43, 25606, 1640, 673, 43, 8, 1640, 673, 43, 23, 1640, 673, 43, 258, 1640, 673, 43, 5, 1640, 673, 43, 14567, 1640, 387, 12, 16459, 43, 413, 1640, 100, 12, 16459, 43, 3896, 1640, 100, 12, 16459, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 412, 1640, 387, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 4455, 1640, 100, 12, 41829, 43, 8, 1640, 100, 12, 41829, 43, 8841, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 81, 1640, 673, 43, 5, 1640, 673, 43, 375, 1640, 673, 43, 379, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 133, 3453, 5403, 9, 14416, 1643, 34, 57, 547, 23, 5, 944, 4308, 1020, 12450, 289, 3281, 1526, 1187, 5841, 2156, 15858, 922, 1176, 2156, 8, 5, 11660, 5413, 5036, 9, 1664, 30293, 1526, 3898, 102, 11, 256, 1526, 462, 6080, 2156, 2809, 25606, 8, 23, 258, 5, 14567, 413, 3896, 8, 5, 412, 9, 4455, 8, 8841, 11, 14567, 81, 5, 375, 379, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:17:34 - INFO - __main__ - Sample 409 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-conference) MAC(I-conference) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 0.96, 'idx': 409, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 16459, 43, 19482, 1640, 100, 12, 16459, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 14:17:34,390 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 14:17:34,566 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 14:17:34,571] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 14:17:34,571] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:03<00:00, 3854.75 examples/s]
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 4331.07 examples/s]
[2025-03-10 14:17:40,625] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 14:17:40,627] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 14:17:40,627] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 14:17:40,632] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 14:17:40,632] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 14:17:40,632] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 14:17:40,632] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 14:17:40,632] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 14:17:40,632] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 14:17:40,632] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 14:17:40,855] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 14:17:40,982] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 14:17:40,983] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.58 GB         CA 0.58 GB         Max_CA 1 GB
[2025-03-10 14:17:40,983] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.44 GB, percent = 5.2%
[2025-03-10 14:17:41,138] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 14:17:41,138] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.7 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 14:17:41,139] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.53 GB, percent = 5.2%
[2025-03-10 14:17:41,139] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 14:17:41,294] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 14:17:41,295] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 14:17:41,295] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.54 GB, percent = 5.2%
[2025-03-10 14:17:41,296] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 14:17:41,297] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 14:17:41,297] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 14:17:41,297] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f12a11aaf60>
[2025-03-10 14:17:41,297] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 14:17:41,297] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f104c10dca0>
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 14:17:41,298] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 14:17:41,299] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 14:17:41,300] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 14:17:41,301] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 14:17:41,301] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 14:17:41,305 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 14:17:41,305 >>   Num examples = 13,177
[INFO|trainer.py:2416] 2025-03-10 14:17:41,305 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 14:17:41,305 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 14:17:41,305 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 14:17:41,305 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 14:17:41,305 >>   Total optimization steps = 16,480
[INFO|trainer.py:2423] 2025-03-10 14:17:41,305 >>   Number of trainable parameters = 124,646,401
{'loss': 1.6132, 'grad_norm': 33.08810806274414, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                               | 412/16480 [00:50<30:11,  8.87it/s][INFO|trainer.py:930] 2025-03-10 14:18:31,864 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:18:31,867 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:18:31,867 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:18:31,868 >>   Batch size = 4
{'eval_loss': 1.3422505855560303, 'eval_mse': 1.3418912452830247, 'eval_pearson': 0.6239412643834379, 'eval_spearmanr': 0.6402194244222034, 'eval_runtime': 0.8144, 'eval_samples_per_second': 848.454, 'eval_steps_per_second': 106.824, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                               | 412/16480 [00:51<30:11,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 14:18:32,833 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412
[INFO|configuration_utils.py:423] 2025-03-10 14:18:32,835 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:18:33,083 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:18:33,084 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:18:33,084 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/special_tokens_map.json
[2025-03-10 14:18:33,129] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step412 is about to be saved!
[2025-03-10 14:18:33,133] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt
[2025-03-10 14:18:33,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt...
[2025-03-10 14:18:33,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt.
[2025-03-10 14:18:33,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:18:34,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:18:34,542] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:18:34,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step412 is ready now!
{'loss': 0.9431, 'grad_norm': 23.11815643310547, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                      | 824/16480 [01:40<29:37,  8.81it/s][INFO|trainer.py:930] 2025-03-10 14:19:21,540 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:19:21,544 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:19:21,544 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:19:21,544 >>   Batch size = 4
{'eval_loss': 1.4090853929519653, 'eval_mse': 1.4092472821862239, 'eval_pearson': 0.6276469050370626, 'eval_spearmanr': 0.6292623656314849, 'eval_runtime': 0.8525, 'eval_samples_per_second': 810.548, 'eval_steps_per_second': 102.052, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                      | 824/16480 [01:41<29:37,  8.81it/s]
[INFO|trainer.py:3955] 2025-03-10 14:19:22,476 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824
[INFO|configuration_utils.py:423] 2025-03-10 14:19:22,478 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:19:22,732 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:19:22,733 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:19:22,733 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/special_tokens_map.json
[2025-03-10 14:19:22,776] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step824 is about to be saved!
[2025-03-10 14:19:22,780] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt
[2025-03-10 14:19:22,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt...
[2025-03-10 14:19:23,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt.
[2025-03-10 14:19:23,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:19:24,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:19:24,172] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:19:24,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step824 is ready now!
{'loss': 0.762, 'grad_norm': 18.285364151000977, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 1236/16480 [02:31<30:05,  8.45it/s][INFO|trainer.py:930] 2025-03-10 14:20:12,903 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:20:12,906 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:20:12,906 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:20:12,906 >>   Batch size = 4
{'eval_loss': 1.5082160234451294, 'eval_mse': 1.507657072672623, 'eval_pearson': 0.6253327059160811, 'eval_spearmanr': 0.6433201852811969, 'eval_runtime': 0.8045, 'eval_samples_per_second': 858.876, 'eval_steps_per_second': 108.136, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 1236/16480 [02:32<30:05,  8.45it/s]
[INFO|trainer.py:3955] 2025-03-10 14:20:13,836 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236
[INFO|configuration_utils.py:423] 2025-03-10 14:20:13,838 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:20:14,067 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:20:14,068 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:20:14,068 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/special_tokens_map.json
[2025-03-10 14:20:14,111] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1236 is about to be saved!
[2025-03-10 14:20:14,115] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt
[2025-03-10 14:20:14,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt...
[2025-03-10 14:20:14,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt.
[2025-03-10 14:20:14,430] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:20:15,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:20:15,451] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:20:15,451] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1236 is ready now!
{'loss': 0.6394, 'grad_norm': 37.59432601928711, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 1648/16480 [03:22<29:12,  8.46it/s][INFO|trainer.py:930] 2025-03-10 14:21:04,086 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:21:04,092 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:21:04,092 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:21:04,092 >>   Batch size = 4
{'eval_loss': 1.544865369796753, 'eval_mse': 1.5447214398473803, 'eval_pearson': 0.6183795316674165, 'eval_spearmanr': 0.6211130389304693, 'eval_runtime': 0.858, 'eval_samples_per_second': 805.356, 'eval_steps_per_second': 101.398, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 1648/16480 [03:23<29:12,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 14:21:05,028 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648
[INFO|configuration_utils.py:423] 2025-03-10 14:21:05,030 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:21:05,284 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:21:05,285 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:21:05,285 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/special_tokens_map.json
[2025-03-10 14:21:05,328] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1648 is about to be saved!
[2025-03-10 14:21:05,331] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt
[2025-03-10 14:21:05,331] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt...
[2025-03-10 14:21:05,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt.
[2025-03-10 14:21:05,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:21:06,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:21:06,715] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:21:06,715] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1648 is ready now!
{'loss': 0.5614, 'grad_norm': 23.38955307006836, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [04:13<27:36,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:21:55,102 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:21:55,105 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:21:55,105 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:21:55,105 >>   Batch size = 4
{'eval_loss': 1.4091026782989502, 'eval_mse': 1.4068059079381388, 'eval_pearson': 0.6340318454553653, 'eval_spearmanr': 0.632327511554332, 'eval_runtime': 0.8493, 'eval_samples_per_second': 813.601, 'eval_steps_per_second': 102.436, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [04:14<27:36,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:21:56,032 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060
[INFO|configuration_utils.py:423] 2025-03-10 14:21:56,034 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:21:56,272 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:21:56,272 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:21:56,272 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/special_tokens_map.json
[2025-03-10 14:21:56,318] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2060 is about to be saved!
[2025-03-10 14:21:56,322] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt
[2025-03-10 14:21:56,322] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt...
[2025-03-10 14:21:56,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt.
[2025-03-10 14:21:56,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:21:57,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:21:57,679] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:21:57,679] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2060 is ready now!
{'loss': 0.4977, 'grad_norm': 12.727973937988281, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [05:03<26:29,  8.81it/s][INFO|trainer.py:930] 2025-03-10 14:22:44,781 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:22:44,786 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:22:44,786 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:22:44,786 >>   Batch size = 4
{'eval_loss': 1.3568224906921387, 'eval_mse': 1.3576388607494392, 'eval_pearson': 0.63813016370716, 'eval_spearmanr': 0.6404771415752094, 'eval_runtime': 0.8523, 'eval_samples_per_second': 810.787, 'eval_steps_per_second': 102.082, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [05:04<26:29,  8.81it/s]
[INFO|trainer.py:3955] 2025-03-10 14:22:45,773 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472
[INFO|configuration_utils.py:423] 2025-03-10 14:22:45,775 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:22:46,020 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:22:46,021 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:22:46,021 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/special_tokens_map.json
[2025-03-10 14:22:46,067] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2472 is about to be saved!
[2025-03-10 14:22:46,071] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt
[2025-03-10 14:22:46,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt...
[2025-03-10 14:22:46,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt.
[2025-03-10 14:22:46,402] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:22:47,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:22:47,467] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:22:47,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2472 is ready now!
{'loss': 0.442, 'grad_norm': 22.82411766052246, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [05:53<25:37,  8.84it/s][INFO|trainer.py:930] 2025-03-10 14:23:34,719 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:23:34,722 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:23:34,722 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:23:34,722 >>   Batch size = 4
{'eval_loss': 1.3435840606689453, 'eval_mse': 1.3431048296639958, 'eval_pearson': 0.6395811133072038, 'eval_spearmanr': 0.6470952483009773, 'eval_runtime': 0.8468, 'eval_samples_per_second': 816.06, 'eval_steps_per_second': 102.746, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [05:54<25:37,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 14:23:35,646 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884
[INFO|configuration_utils.py:423] 2025-03-10 14:23:35,648 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:23:35,892 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:23:35,892 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:23:35,893 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/special_tokens_map.json
[2025-03-10 14:23:35,938] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2884 is about to be saved!
[2025-03-10 14:23:35,941] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt
[2025-03-10 14:23:35,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt...
[2025-03-10 14:23:36,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt.
[2025-03-10 14:23:36,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:23:37,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:23:37,378] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:23:37,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2884 is ready now!
{'loss': 0.3848, 'grad_norm': 25.383249282836914, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [06:42<24:49,  8.85it/s][INFO|trainer.py:930] 2025-03-10 14:24:24,276 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:24:24,280 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:24:24,280 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:24:24,280 >>   Batch size = 4
{'eval_loss': 1.5578562021255493, 'eval_mse': 1.5577873590196094, 'eval_pearson': 0.6121779683990606, 'eval_spearmanr': 0.6231403857148313, 'eval_runtime': 0.8484, 'eval_samples_per_second': 814.512, 'eval_steps_per_second': 102.551, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [06:43<24:49,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 14:24:25,204 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296
[INFO|configuration_utils.py:423] 2025-03-10 14:24:25,206 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:24:25,447 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:24:25,448 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:24:25,448 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/special_tokens_map.json
[2025-03-10 14:24:25,492] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3296 is about to be saved!
[2025-03-10 14:24:25,495] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt
[2025-03-10 14:24:25,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt...
[2025-03-10 14:24:25,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt.
[2025-03-10 14:24:25,826] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:24:26,913] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:24:26,913] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:24:26,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3296 is ready now!
{'loss': 0.3467, 'grad_norm': 23.55152130126953, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [07:32<24:04,  8.84it/s][INFO|trainer.py:930] 2025-03-10 14:25:14,193 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:25:14,196 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:25:14,196 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:25:14,196 >>   Batch size = 4
{'eval_loss': 1.589644432067871, 'eval_mse': 1.5890080090369572, 'eval_pearson': 0.6360483640811894, 'eval_spearmanr': 0.6329732664571034, 'eval_runtime': 0.8504, 'eval_samples_per_second': 812.557, 'eval_steps_per_second': 102.305, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [07:33<24:04,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 14:25:15,123 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708
[INFO|configuration_utils.py:423] 2025-03-10 14:25:15,125 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:25:15,365 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:25:15,366 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:25:15,366 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/special_tokens_map.json
[2025-03-10 14:25:15,408] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3708 is about to be saved!
[2025-03-10 14:25:15,412] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt
[2025-03-10 14:25:15,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt...
[2025-03-10 14:25:15,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt.
[2025-03-10 14:25:15,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:25:16,792] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:25:16,792] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:25:16,792] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3708 is ready now!
{'loss': 0.3145, 'grad_norm': 10.66242504119873, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [08:22<23:21,  8.82it/s][INFO|trainer.py:930] 2025-03-10 14:26:03,705 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:26:03,708 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:26:03,708 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:26:03,708 >>   Batch size = 4
{'eval_loss': 1.5565584897994995, 'eval_mse': 1.5570207580643693, 'eval_pearson': 0.6102394678782244, 'eval_spearmanr': 0.6240784993820426, 'eval_runtime': 0.7967, 'eval_samples_per_second': 867.342, 'eval_steps_per_second': 109.202, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [08:23<23:21,  8.82it/s]
[INFO|trainer.py:3955] 2025-03-10 14:26:04,613 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120
[INFO|configuration_utils.py:423] 2025-03-10 14:26:04,614 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:26:04,856 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:26:04,857 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:26:04,857 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/special_tokens_map.json
[2025-03-10 14:26:04,901] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4120 is about to be saved!
[2025-03-10 14:26:04,904] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt
[2025-03-10 14:26:04,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt...
[2025-03-10 14:26:05,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt.
[2025-03-10 14:26:05,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:26:06,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:26:06,308] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:26:06,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4120 is ready now!
{'loss': 0.2805, 'grad_norm': 7.604772090911865, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [09:12<23:29,  8.48it/s][INFO|trainer.py:930] 2025-03-10 14:26:53,819 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:26:53,822 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:26:53,822 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:26:53,822 >>   Batch size = 4
{'eval_loss': 1.5362635850906372, 'eval_mse': 1.536368483917413, 'eval_pearson': 0.6426494741019085, 'eval_spearmanr': 0.6525297158040687, 'eval_runtime': 0.8491, 'eval_samples_per_second': 813.788, 'eval_steps_per_second': 102.46, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [09:13<23:29,  8.48it/s]
[INFO|trainer.py:3955] 2025-03-10 14:26:54,748 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532
[INFO|configuration_utils.py:423] 2025-03-10 14:26:54,750 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:26:54,987 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:26:54,988 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:26:54,988 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/special_tokens_map.json
[2025-03-10 14:26:55,031] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4532 is about to be saved!
[2025-03-10 14:26:55,034] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt
[2025-03-10 14:26:55,034] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt...
[2025-03-10 14:26:55,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt.
[2025-03-10 14:26:55,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:26:56,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:26:56,388] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:26:56,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4532 is ready now!
{'loss': 0.2581, 'grad_norm': 14.001432418823242, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [10:03<22:27,  8.56it/s][INFO|trainer.py:930] 2025-03-10 14:27:44,730 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:27:44,732 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:27:44,732 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:27:44,732 >>   Batch size = 4
{'eval_loss': 1.4092130661010742, 'eval_mse': 1.406667362246258, 'eval_pearson': 0.6482583945500311, 'eval_spearmanr': 0.660141099540537, 'eval_runtime': 0.7944, 'eval_samples_per_second': 869.859, 'eval_steps_per_second': 109.519, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [10:04<22:27,  8.56it/s]
[INFO|trainer.py:3955] 2025-03-10 14:27:45,654 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944
[INFO|configuration_utils.py:423] 2025-03-10 14:27:45,656 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:27:45,894 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:27:45,895 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:27:45,895 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/special_tokens_map.json
[2025-03-10 14:27:45,938] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4944 is about to be saved!
[2025-03-10 14:27:45,941] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt
[2025-03-10 14:27:45,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt...
[2025-03-10 14:27:46,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt.
[2025-03-10 14:27:46,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:27:47,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:27:47,344] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:27:47,344] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4944 is ready now!
{'loss': 0.2268, 'grad_norm': 14.372610092163086, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [10:54<21:32,  8.60it/s][INFO|trainer.py:930] 2025-03-10 14:28:35,323 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:28:35,326 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:28:35,326 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:28:35,326 >>   Batch size = 4
{'eval_loss': 1.5584558248519897, 'eval_mse': 1.5585039907529972, 'eval_pearson': 0.6590858026070547, 'eval_spearmanr': 0.6625606390357592, 'eval_runtime': 0.7983, 'eval_samples_per_second': 865.545, 'eval_steps_per_second': 108.976, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [10:54<21:32,  8.60it/s]
[INFO|trainer.py:3955] 2025-03-10 14:28:36,235 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356
[INFO|configuration_utils.py:423] 2025-03-10 14:28:36,237 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:28:36,479 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:28:36,480 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:28:36,480 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/special_tokens_map.json
[2025-03-10 14:28:36,524] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5356 is about to be saved!
[2025-03-10 14:28:36,528] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt
[2025-03-10 14:28:36,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt...
[2025-03-10 14:28:36,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt.
[2025-03-10 14:28:36,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:28:37,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:28:37,925] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:28:37,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5356 is ready now!
{'loss': 0.2102, 'grad_norm': 10.397109985351562, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [11:44<20:49,  8.58it/s][INFO|trainer.py:930] 2025-03-10 14:29:26,157 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:29:26,160 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:29:26,160 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:29:26,160 >>   Batch size = 4
{'eval_loss': 1.4311686754226685, 'eval_mse': 1.4307809014051247, 'eval_pearson': 0.6747558119802517, 'eval_spearmanr': 0.6881899608567811, 'eval_runtime': 0.8484, 'eval_samples_per_second': 814.484, 'eval_steps_per_second': 102.547, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [11:45<20:49,  8.58it/s]
[INFO|trainer.py:3955] 2025-03-10 14:29:27,087 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768
[INFO|configuration_utils.py:423] 2025-03-10 14:29:27,089 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:29:27,327 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:29:27,327 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:29:27,328 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/special_tokens_map.json
[2025-03-10 14:29:27,372] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5768 is about to be saved!
[2025-03-10 14:29:27,375] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt
[2025-03-10 14:29:27,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt...
[2025-03-10 14:29:27,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt.
[2025-03-10 14:29:27,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:29:28,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:29:28,783] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:29:28,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5768 is ready now!
{'loss': 0.1914, 'grad_norm': 10.718634605407715, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [12:35<19:21,  8.87it/s][INFO|trainer.py:930] 2025-03-10 14:30:16,832 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:30:16,834 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:30:16,834 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:30:16,834 >>   Batch size = 4
{'eval_loss': 1.4481570720672607, 'eval_mse': 1.448009306718575, 'eval_pearson': 0.6487263603197462, 'eval_spearmanr': 0.6585811940460077, 'eval_runtime': 0.795, 'eval_samples_per_second': 869.19, 'eval_steps_per_second': 109.435, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [12:36<19:21,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 14:30:17,744 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180
[INFO|configuration_utils.py:423] 2025-03-10 14:30:17,746 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:30:17,981 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:30:17,982 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:30:17,982 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/special_tokens_map.json
[2025-03-10 14:30:18,025] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6180 is about to be saved!
[2025-03-10 14:30:18,029] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt
[2025-03-10 14:30:18,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt...
[2025-03-10 14:30:18,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt.
[2025-03-10 14:30:18,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:30:19,437] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:30:19,438] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:30:19,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6180 is ready now!
{'loss': 0.1824, 'grad_norm': 21.965112686157227, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [13:25<18:36,  8.85it/s][INFO|trainer.py:930] 2025-03-10 14:31:07,006 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:31:07,009 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:31:07,009 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:31:07,010 >>   Batch size = 4
{'eval_loss': 1.4621440172195435, 'eval_mse': 1.4628015779034274, 'eval_pearson': 0.6560346732534108, 'eval_spearmanr': 0.6712857865314139, 'eval_runtime': 0.7961, 'eval_samples_per_second': 868.01, 'eval_steps_per_second': 109.286, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [13:26<18:36,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 14:31:07,931 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592
[INFO|configuration_utils.py:423] 2025-03-10 14:31:07,933 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:31:08,178 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:31:08,179 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:31:08,179 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/special_tokens_map.json
[2025-03-10 14:31:08,221] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6592 is about to be saved!
[2025-03-10 14:31:08,224] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt
[2025-03-10 14:31:08,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt...
[2025-03-10 14:31:08,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt.
[2025-03-10 14:31:08,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:31:09,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:31:09,633] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:31:09,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6592 is ready now!
{'loss': 0.1628, 'grad_norm': 5.545854091644287, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [14:15<17:53,  8.82it/s][INFO|trainer.py:930] 2025-03-10 14:31:56,520 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:31:56,522 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:31:56,522 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:31:56,522 >>   Batch size = 4
{'eval_loss': 1.3253118991851807, 'eval_mse': 1.3252504040984445, 'eval_pearson': 0.6743521314756395, 'eval_spearmanr': 0.6821579048186021, 'eval_runtime': 0.7946, 'eval_samples_per_second': 869.657, 'eval_steps_per_second': 109.494, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [14:16<17:53,  8.82it/s]
[INFO|trainer.py:3955] 2025-03-10 14:31:57,448 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004
[INFO|configuration_utils.py:423] 2025-03-10 14:31:57,450 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:31:57,706 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:31:57,707 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:31:57,707 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/special_tokens_map.json
[2025-03-10 14:31:57,750] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7004 is about to be saved!
[2025-03-10 14:31:57,753] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt
[2025-03-10 14:31:57,753] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt...
[2025-03-10 14:31:58,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt.
[2025-03-10 14:31:58,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:31:59,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:31:59,212] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:31:59,212] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7004 is ready now!
{'loss': 0.1581, 'grad_norm': 8.745185852050781, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [15:04<17:03,  8.86it/s][INFO|trainer.py:930] 2025-03-10 14:32:46,049 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:32:46,052 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:32:46,052 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:32:46,052 >>   Batch size = 4
{'eval_loss': 1.43397057056427, 'eval_mse': 1.4337659586010725, 'eval_pearson': 0.6766848567475303, 'eval_spearmanr': 0.6885528046036797, 'eval_runtime': 0.7969, 'eval_samples_per_second': 867.155, 'eval_steps_per_second': 109.179, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [15:05<17:03,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 14:32:46,965 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416
[INFO|configuration_utils.py:423] 2025-03-10 14:32:46,967 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:32:47,219 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:32:47,220 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:32:47,220 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/special_tokens_map.json
[2025-03-10 14:32:47,263] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7416 is about to be saved!
[2025-03-10 14:32:47,267] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt
[2025-03-10 14:32:47,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt...
[2025-03-10 14:32:47,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt.
[2025-03-10 14:32:47,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:32:48,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:32:48,727] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:32:48,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7416 is ready now!
{'loss': 0.1427, 'grad_norm': 18.133228302001953, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [15:54<16:41,  8.64it/s][INFO|trainer.py:930] 2025-03-10 14:33:36,143 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:33:36,146 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:33:36,146 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:33:36,146 >>   Batch size = 4
{'eval_loss': 1.3397248983383179, 'eval_mse': 1.3400432190570748, 'eval_pearson': 0.6661109889706491, 'eval_spearmanr': 0.6786729425081227, 'eval_runtime': 0.796, 'eval_samples_per_second': 868.039, 'eval_steps_per_second': 109.29, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [15:55<16:41,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 14:33:37,068 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828
[INFO|configuration_utils.py:423] 2025-03-10 14:33:37,070 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:33:37,325 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:33:37,325 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:33:37,325 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/special_tokens_map.json
[2025-03-10 14:33:37,370] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7828 is about to be saved!
[2025-03-10 14:33:37,374] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt
[2025-03-10 14:33:37,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt...
[2025-03-10 14:33:37,713] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt.
[2025-03-10 14:33:37,714] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:33:38,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:33:38,839] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:33:38,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7828 is ready now!
{'loss': 0.1323, 'grad_norm': 2.876150369644165, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [16:44<15:29,  8.86it/s][INFO|trainer.py:930] 2025-03-10 14:34:26,298 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:34:26,301 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:34:26,301 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:34:26,301 >>   Batch size = 4
{'eval_loss': 1.3204829692840576, 'eval_mse': 1.3208405450525573, 'eval_pearson': 0.6844531050068554, 'eval_spearmanr': 0.699335973411937, 'eval_runtime': 0.794, 'eval_samples_per_second': 870.26, 'eval_steps_per_second': 109.57, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [16:45<15:29,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 14:34:27,205 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240
[INFO|configuration_utils.py:423] 2025-03-10 14:34:27,207 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:34:27,459 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:34:27,460 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:34:27,460 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/special_tokens_map.json
[2025-03-10 14:34:27,508] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8240 is about to be saved!
[2025-03-10 14:34:27,511] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt
[2025-03-10 14:34:27,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt...
[2025-03-10 14:34:27,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt.
[2025-03-10 14:34:27,845] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:34:28,930] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:34:28,930] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:34:28,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8240 is ready now!
{'loss': 0.1241, 'grad_norm': 9.475241661071777, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [17:34<14:44,  8.85it/s][INFO|trainer.py:930] 2025-03-10 14:35:15,815 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:35:15,818 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:35:15,818 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:35:15,818 >>   Batch size = 4
{'eval_loss': 1.3566120862960815, 'eval_mse': 1.3573063113409944, 'eval_pearson': 0.6736932531055135, 'eval_spearmanr': 0.6869283701803882, 'eval_runtime': 0.7967, 'eval_samples_per_second': 867.349, 'eval_steps_per_second': 109.203, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [17:35<14:44,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 14:35:16,744 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652
[INFO|configuration_utils.py:423] 2025-03-10 14:35:16,746 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:35:16,999 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:35:16,999 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:35:17,000 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/special_tokens_map.json
[2025-03-10 14:35:17,042] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8652 is about to be saved!
[2025-03-10 14:35:17,045] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt
[2025-03-10 14:35:17,045] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt...
[2025-03-10 14:35:17,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt.
[2025-03-10 14:35:17,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:35:18,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:35:18,493] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:35:18,493] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8652 is ready now!
{'loss': 0.113, 'grad_norm': 9.024115562438965, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [18:24<13:57,  8.85it/s][INFO|trainer.py:930] 2025-03-10 14:36:05,382 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:36:05,384 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:36:05,384 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:36:05,384 >>   Batch size = 4
{'eval_loss': 1.237085223197937, 'eval_mse': 1.237366453438523, 'eval_pearson': 0.686676099610463, 'eval_spearmanr': 0.6988900928400507, 'eval_runtime': 0.8457, 'eval_samples_per_second': 817.096, 'eval_steps_per_second': 102.876, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [18:24<13:57,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 14:36:06,296 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064
[INFO|configuration_utils.py:423] 2025-03-10 14:36:06,297 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:36:06,547 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:36:06,548 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:36:06,548 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/special_tokens_map.json
[2025-03-10 14:36:06,591] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9064 is about to be saved!
[2025-03-10 14:36:06,595] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt
[2025-03-10 14:36:06,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt...
[2025-03-10 14:36:06,941] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt.
[2025-03-10 14:36:06,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:36:08,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:36:08,055] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:36:08,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9064 is ready now!
{'loss': 0.1105, 'grad_norm': 10.616628646850586, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [19:14<13:31,  8.63it/s][INFO|trainer.py:930] 2025-03-10 14:36:55,783 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:36:55,786 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:36:55,786 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:36:55,786 >>   Batch size = 4
{'eval_loss': 1.3253103494644165, 'eval_mse': 1.3258917383450675, 'eval_pearson': 0.6589164237508006, 'eval_spearmanr': 0.6768912280430551, 'eval_runtime': 0.7963, 'eval_samples_per_second': 867.78, 'eval_steps_per_second': 109.257, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [19:15<13:31,  8.63it/s]
[INFO|trainer.py:3955] 2025-03-10 14:36:56,709 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476
[INFO|configuration_utils.py:423] 2025-03-10 14:36:56,711 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:36:56,963 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:36:56,964 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:36:56,964 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/special_tokens_map.json
[2025-03-10 14:36:57,007] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9476 is about to be saved!
[2025-03-10 14:36:57,010] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt
[2025-03-10 14:36:57,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt...
[2025-03-10 14:36:57,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt.
[2025-03-10 14:36:57,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:36:58,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:36:58,482] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:36:58,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9476 is ready now!
{'loss': 0.1, 'grad_norm': 5.254486083984375, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [20:05<12:28,  8.81it/s][INFO|trainer.py:930] 2025-03-10 14:37:46,435 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:37:46,438 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:37:46,438 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:37:46,438 >>   Batch size = 4
{'eval_loss': 1.4162542819976807, 'eval_mse': 1.4171252016048184, 'eval_pearson': 0.6693408053676719, 'eval_spearmanr': 0.6822703292631056, 'eval_runtime': 0.8479, 'eval_samples_per_second': 814.909, 'eval_steps_per_second': 102.601, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [20:05<12:28,  8.81it/s]
[INFO|trainer.py:3955] 2025-03-10 14:37:47,354 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888
[INFO|configuration_utils.py:423] 2025-03-10 14:37:47,356 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:37:47,661 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:37:47,662 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:37:47,662 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/special_tokens_map.json
[2025-03-10 14:37:47,708] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9888 is about to be saved!
[2025-03-10 14:37:47,712] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt
[2025-03-10 14:37:47,712] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt...
[2025-03-10 14:37:48,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt.
[2025-03-10 14:37:48,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:37:49,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:37:49,316] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:37:49,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9888 is ready now!
{'loss': 0.0952, 'grad_norm': 4.8652663230896, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [20:55<11:39,  8.84it/s][INFO|trainer.py:930] 2025-03-10 14:38:36,747 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:38:36,750 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:38:36,750 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:38:36,750 >>   Batch size = 4
{'eval_loss': 1.394438624382019, 'eval_mse': 1.394477440548357, 'eval_pearson': 0.6636751057131024, 'eval_spearmanr': 0.6799711868086648, 'eval_runtime': 0.7992, 'eval_samples_per_second': 864.638, 'eval_steps_per_second': 108.862, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [20:56<11:39,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 14:38:37,675 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300
[INFO|configuration_utils.py:423] 2025-03-10 14:38:37,677 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:38:37,937 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:38:37,938 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:38:37,938 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/special_tokens_map.json
[2025-03-10 14:38:37,982] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10300 is about to be saved!
[2025-03-10 14:38:37,985] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt
[2025-03-10 14:38:37,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt...
[2025-03-10 14:38:38,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt.
[2025-03-10 14:38:38,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:38:39,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:38:39,460] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:38:39,460] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10300 is ready now!
{'loss': 0.0866, 'grad_norm': 4.860921382904053, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [21:45<10:54,  8.82it/s][INFO|trainer.py:930] 2025-03-10 14:39:26,480 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:39:26,482 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:39:26,482 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:39:26,482 >>   Batch size = 4
{'eval_loss': 1.3690811395645142, 'eval_mse': 1.3698568668448288, 'eval_pearson': 0.6790300102994746, 'eval_spearmanr': 0.6863729689243055, 'eval_runtime': 0.7982, 'eval_samples_per_second': 865.652, 'eval_steps_per_second': 108.99, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [21:45<10:54,  8.82it/s]
[INFO|trainer.py:3955] 2025-03-10 14:39:27,409 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712
[INFO|configuration_utils.py:423] 2025-03-10 14:39:27,411 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:39:27,673 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:39:27,674 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:39:27,674 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/special_tokens_map.json
[2025-03-10 14:39:27,717] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10712 is about to be saved!
[2025-03-10 14:39:27,720] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt
[2025-03-10 14:39:27,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt...
[2025-03-10 14:39:28,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt.
[2025-03-10 14:39:28,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:39:29,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:39:29,200] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:39:29,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10712 is ready now!
{'loss': 0.0811, 'grad_norm': 11.17265510559082, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [22:36<10:24,  8.57it/s][INFO|trainer.py:930] 2025-03-10 14:40:17,637 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:40:17,640 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:40:17,640 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:40:17,640 >>   Batch size = 4
{'eval_loss': 1.3177376985549927, 'eval_mse': 1.3177894221028439, 'eval_pearson': 0.6780746187321236, 'eval_spearmanr': 0.6868456142319991, 'eval_runtime': 0.7956, 'eval_samples_per_second': 868.479, 'eval_steps_per_second': 109.345, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [22:37<10:24,  8.57it/s]
[INFO|trainer.py:3955] 2025-03-10 14:40:18,547 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124
[INFO|configuration_utils.py:423] 2025-03-10 14:40:18,549 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:40:18,810 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:40:18,811 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:40:18,811 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/special_tokens_map.json
[2025-03-10 14:40:18,852] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11124 is about to be saved!
[2025-03-10 14:40:18,855] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt
[2025-03-10 14:40:18,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt...
[2025-03-10 14:40:19,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt.
[2025-03-10 14:40:19,204] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:40:20,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:40:20,332] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:40:20,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11124 is ready now!
{'loss': 0.0776, 'grad_norm': 3.111814260482788, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [23:26<09:34,  8.61it/s][INFO|trainer.py:930] 2025-03-10 14:41:07,829 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:41:07,832 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:41:07,832 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:41:07,832 >>   Batch size = 4
{'eval_loss': 1.262137770652771, 'eval_mse': 1.2628246982259792, 'eval_pearson': 0.687016782800285, 'eval_spearmanr': 0.6983208583585415, 'eval_runtime': 0.7984, 'eval_samples_per_second': 865.53, 'eval_steps_per_second': 108.974, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [23:27<09:34,  8.61it/s]
[INFO|trainer.py:3955] 2025-03-10 14:41:08,759 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536
[INFO|configuration_utils.py:423] 2025-03-10 14:41:08,761 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:41:09,021 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:41:09,022 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:41:09,022 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/special_tokens_map.json
[2025-03-10 14:41:09,067] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11536 is about to be saved!
[2025-03-10 14:41:09,070] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt
[2025-03-10 14:41:09,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt...
[2025-03-10 14:41:09,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt.
[2025-03-10 14:41:09,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:41:10,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:41:10,541] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:41:10,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11536 is ready now!
{'loss': 0.0711, 'grad_norm': 2.3407182693481445, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [24:16<08:30,  8.87it/s][INFO|trainer.py:930] 2025-03-10 14:41:57,374 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:41:57,376 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:41:57,376 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:41:57,376 >>   Batch size = 4
{'eval_loss': 1.2969707250595093, 'eval_mse': 1.2965893690216903, 'eval_pearson': 0.6853886663719864, 'eval_spearmanr': 0.6984802272066493, 'eval_runtime': 0.7952, 'eval_samples_per_second': 869.005, 'eval_steps_per_second': 109.412, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [24:16<08:30,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 14:41:58,283 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948
[INFO|configuration_utils.py:423] 2025-03-10 14:41:58,285 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:41:58,545 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:41:58,546 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:41:58,546 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/special_tokens_map.json
[2025-03-10 14:41:58,589] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11948 is about to be saved!
[2025-03-10 14:41:58,593] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt
[2025-03-10 14:41:58,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt...
[2025-03-10 14:41:58,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt.
[2025-03-10 14:41:58,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:42:00,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:42:00,068] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:42:00,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11948 is ready now!
{'loss': 0.0676, 'grad_norm': 2.8208446502685547, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [25:05<07:46,  8.84it/s][INFO|trainer.py:930] 2025-03-10 14:42:46,931 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:42:46,934 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:42:46,934 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:42:46,934 >>   Batch size = 4
{'eval_loss': 1.3124816417694092, 'eval_mse': 1.3126112024276888, 'eval_pearson': 0.688149075572429, 'eval_spearmanr': 0.7025982108646227, 'eval_runtime': 0.796, 'eval_samples_per_second': 868.145, 'eval_steps_per_second': 109.303, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [25:06<07:46,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 14:42:47,859 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360
[INFO|configuration_utils.py:423] 2025-03-10 14:42:47,861 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:42:48,122 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:42:48,123 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:42:48,123 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/special_tokens_map.json
[2025-03-10 14:42:48,167] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12360 is about to be saved!
[2025-03-10 14:42:48,170] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt
[2025-03-10 14:42:48,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt...
[2025-03-10 14:42:48,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt.
[2025-03-10 14:42:48,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:42:49,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:42:49,645] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:42:49,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12360 is ready now!
{'loss': 0.0678, 'grad_norm': 3.9021263122558594, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [25:56<07:11,  8.59it/s][INFO|trainer.py:930] 2025-03-10 14:43:37,594 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:43:37,596 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:43:37,596 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:43:37,596 >>   Batch size = 4
{'eval_loss': 1.3223505020141602, 'eval_mse': 1.3224068591977685, 'eval_pearson': 0.6779854893745219, 'eval_spearmanr': 0.691427798886615, 'eval_runtime': 0.7961, 'eval_samples_per_second': 868.027, 'eval_steps_per_second': 109.288, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [25:57<07:11,  8.59it/s]
[INFO|trainer.py:3955] 2025-03-10 14:43:38,516 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772
[INFO|configuration_utils.py:423] 2025-03-10 14:43:38,518 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:43:38,851 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:43:38,851 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:43:38,852 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/special_tokens_map.json
[2025-03-10 14:43:38,897] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12772 is about to be saved!
[2025-03-10 14:43:38,900] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt
[2025-03-10 14:43:38,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt...
[2025-03-10 14:43:39,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt.
[2025-03-10 14:43:39,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:43:40,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:43:40,655] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:43:40,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12772 is ready now!
{'loss': 0.062, 'grad_norm': 11.969847679138184, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [26:47<06:24,  8.58it/s][INFO|trainer.py:930] 2025-03-10 14:44:28,955 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:44:28,958 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:44:28,958 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:44:28,958 >>   Batch size = 4
{'eval_loss': 1.3931695222854614, 'eval_mse': 1.3932755307419773, 'eval_pearson': 0.6925344342747226, 'eval_spearmanr': 0.7062534460662908, 'eval_runtime': 0.8486, 'eval_samples_per_second': 814.322, 'eval_steps_per_second': 102.527, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [26:48<06:24,  8.58it/s]
[INFO|trainer.py:3955] 2025-03-10 14:44:29,898 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184
[INFO|configuration_utils.py:423] 2025-03-10 14:44:29,900 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:44:30,249 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:44:30,250 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:44:30,250 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/special_tokens_map.json
[2025-03-10 14:44:30,301] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13184 is about to be saved!
[2025-03-10 14:44:30,304] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt
[2025-03-10 14:44:30,304] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt...
[2025-03-10 14:44:30,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt.
[2025-03-10 14:44:30,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:44:32,121] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:44:32,122] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:44:32,122] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13184 is ready now!
{'loss': 0.0561, 'grad_norm': 9.483297348022461, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [27:38<05:25,  8.87it/s][INFO|trainer.py:930] 2025-03-10 14:45:19,866 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:45:19,869 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:45:19,869 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:45:19,869 >>   Batch size = 4
{'eval_loss': 1.3001446723937988, 'eval_mse': 1.3012119526455959, 'eval_pearson': 0.7077639181971637, 'eval_spearmanr': 0.7156045705113664, 'eval_runtime': 0.7959, 'eval_samples_per_second': 868.243, 'eval_steps_per_second': 109.316, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [27:39<05:25,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 14:45:20,808 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596
[INFO|configuration_utils.py:423] 2025-03-10 14:45:20,810 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:45:21,138 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:45:21,139 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:45:21,139 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/special_tokens_map.json
[2025-03-10 14:45:21,183] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13596 is about to be saved!
[2025-03-10 14:45:21,187] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt
[2025-03-10 14:45:21,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt...
[2025-03-10 14:45:21,604] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt.
[2025-03-10 14:45:21,605] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:45:22,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:45:22,964] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:45:22,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13596 is ready now!
{'loss': 0.0521, 'grad_norm': 4.60540771484375, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [28:28<04:38,  8.86it/s][INFO|trainer.py:930] 2025-03-10 14:46:09,749 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:46:09,752 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:46:09,752 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:46:09,752 >>   Batch size = 4
{'eval_loss': 1.2373911142349243, 'eval_mse': 1.2381917695405686, 'eval_pearson': 0.6980283544681931, 'eval_spearmanr': 0.7127415516006939, 'eval_runtime': 0.7965, 'eval_samples_per_second': 867.581, 'eval_steps_per_second': 109.232, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [28:29<04:38,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 14:46:10,678 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008
[INFO|configuration_utils.py:423] 2025-03-10 14:46:10,680 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:46:11,007 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:46:11,007 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:46:11,008 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/special_tokens_map.json
[2025-03-10 14:46:11,055] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14008 is about to be saved!
[2025-03-10 14:46:11,058] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt
[2025-03-10 14:46:11,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt...
[2025-03-10 14:46:11,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt.
[2025-03-10 14:46:11,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:46:12,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:46:12,922] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:46:12,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14008 is ready now!
{'loss': 0.0513, 'grad_norm': 79.78909301757812, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 14420/16480 [29:18<03:52,  8.86it/s][INFO|trainer.py:930] 2025-03-10 14:46:59,883 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:46:59,886 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:46:59,886 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:46:59,886 >>   Batch size = 4
{'eval_loss': 1.2457131147384644, 'eval_mse': 1.2461182881367707, 'eval_pearson': 0.6907134031022311, 'eval_spearmanr': 0.7125681635130654, 'eval_runtime': 0.8483, 'eval_samples_per_second': 814.58, 'eval_steps_per_second': 102.559, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 14420/16480 [29:19<03:52,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 14:47:00,826 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420
[INFO|configuration_utils.py:423] 2025-03-10 14:47:00,828 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:47:01,149 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:47:01,150 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:47:01,150 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/special_tokens_map.json
[2025-03-10 14:47:01,195] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14420 is about to be saved!
[2025-03-10 14:47:01,198] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt
[2025-03-10 14:47:01,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt...
[2025-03-10 14:47:01,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt.
[2025-03-10 14:47:01,614] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:47:02,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:47:02,999] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:47:02,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14420 is ready now!
{'loss': 0.0475, 'grad_norm': 3.709118366241455, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 14832/16480 [30:08<03:05,  8.86it/s][INFO|trainer.py:930] 2025-03-10 14:47:49,827 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:47:49,830 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:47:49,830 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:47:49,830 >>   Batch size = 4
{'eval_loss': 1.2203341722488403, 'eval_mse': 1.218345264622513, 'eval_pearson': 0.7171070655660676, 'eval_spearmanr': 0.7292671126085951, 'eval_runtime': 0.7993, 'eval_samples_per_second': 864.472, 'eval_steps_per_second': 108.841, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 14832/16480 [30:09<03:05,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 14:47:50,770 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832
[INFO|configuration_utils.py:423] 2025-03-10 14:47:50,772 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:47:51,133 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:47:51,134 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:47:51,134 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/special_tokens_map.json
[2025-03-10 14:47:51,180] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14832 is about to be saved!
[2025-03-10 14:47:51,184] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt
[2025-03-10 14:47:51,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt...
[2025-03-10 14:47:51,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt.
[2025-03-10 14:47:51,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:47:53,001] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:47:53,002] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:47:53,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14832 is ready now!
{'loss': 0.0468, 'grad_norm': 3.3673529624938965, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 15244/16480 [30:58<02:19,  8.87it/s][INFO|trainer.py:930] 2025-03-10 14:48:39,828 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:48:39,831 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:48:39,831 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:48:39,831 >>   Batch size = 4
{'eval_loss': 1.1596287488937378, 'eval_mse': 1.1584680887793666, 'eval_pearson': 0.7089667603764518, 'eval_spearmanr': 0.7238376244420394, 'eval_runtime': 0.7941, 'eval_samples_per_second': 870.124, 'eval_steps_per_second': 109.552, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 15244/16480 [30:59<02:19,  8.87it/s]
[INFO|trainer.py:3955] 2025-03-10 14:48:40,753 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244
[INFO|configuration_utils.py:423] 2025-03-10 14:48:40,755 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:48:41,076 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:48:41,077 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:48:41,077 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/special_tokens_map.json
[2025-03-10 14:48:41,124] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15244 is about to be saved!
[2025-03-10 14:48:41,127] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt
[2025-03-10 14:48:41,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt...
[2025-03-10 14:48:41,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt.
[2025-03-10 14:48:41,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:48:42,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:48:42,865] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:48:42,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15244 is ready now!
{'loss': 0.0423, 'grad_norm': 2.0651450157165527, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 15656/16480 [31:48<01:32,  8.89it/s][INFO|trainer.py:930] 2025-03-10 14:49:29,648 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:49:29,651 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:49:29,651 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:49:29,651 >>   Batch size = 4
{'eval_loss': 1.1950335502624512, 'eval_mse': 1.1953038665561704, 'eval_pearson': 0.715563731772439, 'eval_spearmanr': 0.731703690885438, 'eval_runtime': 0.8491, 'eval_samples_per_second': 813.76, 'eval_steps_per_second': 102.456, 'epoch': 38.0}
 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 15656/16480 [31:49<01:32,  8.89it/s]
[INFO|trainer.py:3955] 2025-03-10 14:49:30,581 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656
[INFO|configuration_utils.py:423] 2025-03-10 14:49:30,583 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:49:30,902 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:49:30,903 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:49:30,903 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/special_tokens_map.json
[2025-03-10 14:49:30,949] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15656 is about to be saved!
[2025-03-10 14:49:30,952] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt
[2025-03-10 14:49:30,952] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt...
[2025-03-10 14:49:31,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt.
[2025-03-10 14:49:31,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:49:32,719] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:49:32,720] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:49:32,720] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15656 is ready now!
{'loss': 0.0413, 'grad_norm': 2.4057905673980713, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 16068/16480 [32:40<00:46,  8.83it/s][INFO|trainer.py:930] 2025-03-10 14:50:21,441 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:50:21,443 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:50:21,443 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:50:21,443 >>   Batch size = 4
{'eval_loss': 1.1832129955291748, 'eval_mse': 1.1835377502717435, 'eval_pearson': 0.712119709473277, 'eval_spearmanr': 0.7259847396720602, 'eval_runtime': 0.7953, 'eval_samples_per_second': 868.863, 'eval_steps_per_second': 109.394, 'epoch': 39.0}
 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 16068/16480 [32:40<00:46,  8.83it/s]
[INFO|trainer.py:3955] 2025-03-10 14:50:22,381 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068
[INFO|configuration_utils.py:423] 2025-03-10 14:50:22,383 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:50:22,693 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:50:22,694 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:50:22,694 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/special_tokens_map.json
[2025-03-10 14:50:22,738] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16068 is about to be saved!
[2025-03-10 14:50:22,742] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt
[2025-03-10 14:50:22,742] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt...
[2025-03-10 14:50:23,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt.
[2025-03-10 14:50:23,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:50:24,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:50:24,496] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:50:24,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16068 is ready now!
{'loss': 0.0384, 'grad_norm': 3.794826030731201, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:30<00:00,  8.84it/s][INFO|trainer.py:930] 2025-03-10 14:51:11,356 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:51:11,359 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:51:11,359 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:51:11,359 >>   Batch size = 4
{'eval_loss': 1.2525333166122437, 'eval_mse': 1.2534655168674098, 'eval_pearson': 0.7030279697200819, 'eval_spearmanr': 0.714901583926229, 'eval_runtime': 0.7926, 'eval_samples_per_second': 871.841, 'eval_steps_per_second': 109.769, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:30<00:00,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 14:51:12,278 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480
[INFO|configuration_utils.py:423] 2025-03-10 14:51:12,279 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:51:12,596 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:51:12,596 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:51:12,596 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/special_tokens_map.json
[2025-03-10 14:51:12,640] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16480 is about to be saved!
[2025-03-10 14:51:12,643] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt
[2025-03-10 14:51:12,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt...
[2025-03-10 14:51:13,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt.
[2025-03-10 14:51:13,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:51:14,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:51:14,411] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:51:14,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16480 is ready now!
[INFO|trainer.py:2670] 2025-03-10 14:51:14,414 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2013.1083, 'train_samples_per_second': 261.824, 'train_steps_per_second': 8.186, 'train_loss': 0.2470622408158571, 'epoch': 40.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [33:33<00:00,  8.19it/s]
[INFO|trainer.py:3955] 2025-03-10 14:51:14,471 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=10
[INFO|configuration_utils.py:423] 2025-03-10 14:51:14,473 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=10/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:51:14,793 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=10/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:51:14,794 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:51:14,794 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=10/special_tokens_map.json
***** train metrics *****
  epoch                    =        40.0
  total_flos               = 129223800GF
  train_loss               =      0.2471
  train_runtime            =  0:33:33.10
  train_samples            =       13177
  train_samples_per_second =     261.824
  train_steps_per_second   =       8.186
03/10/2025 14:51:14 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 14:51:14,836 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:51:14,843 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:51:14,843 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:51:14,843 >>   Batch size = 4
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:00<00:00, 104.50it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =     1.2525
  eval_mse                =     1.2535
  eval_pearson            =      0.703
  eval_runtime            = 0:00:00.85
  eval_samples            =        691
  eval_samples_per_second =    812.617
  eval_spearmanr          =     0.7149
  eval_steps_per_second   =    102.312
[INFO|modelcard.py:449] 2025-03-10 14:51:15,908 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.714901583926229}]}
[rank0]:[W310 14:51:16.046248636 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 14:51:19,484] [INFO] [launch.py:351:main] Process 4062191 exits successfully.
[2025-03-10 14:51:19,485] [INFO] [launch.py:351:main] Process 4062192 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:0,1 --master_port 27481 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:51:25,454] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:51:28,319] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 14:51:28,320] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=27481 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=10 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:51:32,706] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:51:36,608] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-03-10 14:51:36,608] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 14:51:36,608] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 14:51:36,609] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 14:51:36,609] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-03-10 14:51:36,609] [INFO] [launch.py:256:main] process 4101180 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:51:36,610] [INFO] [launch.py:256:main] process 4101181 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=10', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:51:43,816] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:51:43,833] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:51:44,565] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 14:51:44,565] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 14:51:44,569] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 14:51:46 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 14:51:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 14:51:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-large-num=10/runs/Mar10_14-51-43_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-large-num=10,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-large-num=10,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 14:51:46 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-train.json
03/10/2025 14:51:46 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=10-val.json
Using custom data configuration default-0ba8dc2d49302199
03/10/2025 14:51:47 - INFO - datasets.builder - Using custom data configuration default-0ba8dc2d49302199
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 14:51:47 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 14:51:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:51:47 - INFO - datasets.info - Loading Dataset info from .cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 14:51:47 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:51:47 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 14:51:47,637 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:51:47,641 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 14:51:47,840 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:51:47,840 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:51:47,853 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 14:51:47,853 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:51:47,854 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 14:51:48,002 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 14:51:48,118 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 14:51:48,118 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:5103] 2025-03-10 14:51:48,125 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c20a34d738b84ff4.arrow
03/10/2025 14:51:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-c20a34d738b84ff4.arrow
Running tokenizer on dataset:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 10000/13177 [00:02<00:00, 4066.03 examples/s][rank1]:[W310 14:51:50.444599115 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:03<00:00, 4070.19 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                               | 0/691 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5f871bcca836db41.arrow
03/10/2025 14:51:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-0ba8dc2d49302199/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5f871bcca836db41.arrow
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 4581.26 examples/s]
[rank0]:[W310 14:51:54.947716972 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 14:51:54 - INFO - __main__ - Sample 10476 of the training set: {'sentence1': 'what(O) movie(O) stars(O) christopher(O) walken(O) an(O) sean(B-actor) penn(I-actor)', 'sentence2': 'what movie stars christopher walken an sean penn', 'label': 2.61, 'idx': 10476, 'input_ids': [0, 12196, 1640, 673, 43, 1569, 1640, 673, 43, 2690, 1640, 673, 43, 29224, 35885, 1640, 673, 43, 1656, 225, 1640, 673, 43, 41, 1640, 673, 43, 842, 260, 1640, 387, 12, 24625, 43, 30081, 1640, 100, 12, 24625, 43, 2, 2, 12196, 1569, 2690, 29224, 35885, 1656, 225, 41, 842, 260, 30081, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:51:54 - INFO - __main__ - Sample 1824 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-conference) County(I-conference) Fair(I-conference) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 2.64, 'idx': 1824, 'input_ids': [0, 133, 1640, 673, 43, 3453, 1640, 387, 12, 19527, 43, 5403, 1640, 673, 43, 9, 1640, 673, 43, 14416, 1640, 387, 12, 16459, 43, 1643, 1640, 100, 12, 16459, 43, 34, 1640, 673, 43, 57, 1640, 673, 43, 547, 1640, 673, 43, 23, 1640, 673, 43, 5, 1640, 673, 43, 944, 4308, 1020, 1640, 387, 12, 879, 31104, 43, 12450, 1640, 100, 12, 879, 31104, 43, 289, 3281, 1526, 1187, 5841, 1640, 100, 12, 879, 31104, 238, 1640, 673, 43, 15858, 922, 1176, 1640, 387, 12, 41829, 238, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 11660, 1640, 387, 12, 41829, 43, 5413, 1640, 100, 12, 41829, 43, 5036, 1640, 100, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 1664, 30293, 1526, 3898, 102, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 256, 1526, 462, 6080, 1640, 387, 12, 41829, 238, 1640, 673, 43, 2809, 1640, 387, 12, 12659, 43, 25606, 1640, 673, 43, 8, 1640, 673, 43, 23, 1640, 673, 43, 258, 1640, 673, 43, 5, 1640, 673, 43, 14567, 1640, 387, 12, 16459, 43, 413, 1640, 100, 12, 16459, 43, 3896, 1640, 100, 12, 16459, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 412, 1640, 387, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 4455, 1640, 100, 12, 41829, 43, 8, 1640, 100, 12, 41829, 43, 8841, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 81, 1640, 673, 43, 5, 1640, 673, 43, 375, 1640, 673, 43, 379, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 133, 3453, 5403, 9, 14416, 1643, 34, 57, 547, 23, 5, 944, 4308, 1020, 12450, 289, 3281, 1526, 1187, 5841, 2156, 15858, 922, 1176, 2156, 8, 5, 11660, 5413, 5036, 9, 1664, 30293, 1526, 3898, 102, 11, 256, 1526, 462, 6080, 2156, 2809, 25606, 8, 23, 258, 5, 14567, 413, 3896, 8, 5, 412, 9, 4455, 8, 8841, 11, 14567, 81, 5, 375, 379, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:51:54 - INFO - __main__ - Sample 409 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-conference) MAC(I-conference) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 0.96, 'idx': 409, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 16459, 43, 19482, 1640, 100, 12, 16459, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/13177 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 14:51:55,027 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 14:51:55,200 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 14:51:55,207] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 14:51:55,207] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13177/13177 [00:03<00:00, 3903.90 examples/s]
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 691/691 [00:00<00:00, 4580.84 examples/s]
[2025-03-10 14:52:01,309] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 14:52:01,312] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 14:52:01,312] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 14:52:01,332] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 14:52:01,332] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 14:52:01,332] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 14:52:01,332] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 14:52:01,332] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 14:52:01,332] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 14:52:01,332] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 14:52:01,964] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 14:52:01,965] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.66 GB         CA 1.66 GB         Max_CA 2 GB
[2025-03-10 14:52:01,965] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 53.91 GB, percent = 5.4%
[2025-03-10 14:52:03,365] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 14:52:03,366] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.99 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 14:52:03,366] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 53.9 GB, percent = 5.3%
[2025-03-10 14:52:03,366] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 14:52:03,520] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 14:52:03,520] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.32 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 14:52:03,521] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 53.9 GB, percent = 5.3%
[2025-03-10 14:52:03,524] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 14:52:03,524] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 14:52:03,524] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 14:52:03,524] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f86e40a13a0>
[2025-03-10 14:52:03,524] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 14:52:03,525] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 14:52:03,525] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 14:52:03,525] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 14:52:03,525] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 14:52:03,525] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 14:52:03,525] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f879908e1b0>
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 14:52:03,526] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 14:52:03,527] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 14:52:03,528] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 14:52:03,529] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 14:52:03,529] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 14:52:03,529] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 14:52:03,529] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 14:52:03,529] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 14:52:03,530 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 14:52:03,530 >>   Num examples = 13,177
[INFO|trainer.py:2416] 2025-03-10 14:52:03,530 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 14:52:03,530 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 14:52:03,530 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 14:52:03,530 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 14:52:03,530 >>   Total optimization steps = 16,480
[INFO|trainer.py:2423] 2025-03-10 14:52:03,531 >>   Number of trainable parameters = 355,360,769
  0%|                                                                                                                                                                                                                                                                                                                                                                  | 0/16480 [00:00<?, ?it/s][2025-03-10 14:52:04,048] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
{'loss': 1.4874, 'grad_norm': 26.070436477661133, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 412/16480 [01:37<1:01:25,  4.36it/s][INFO|trainer.py:930] 2025-03-10 14:53:40,549 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:53:40,552 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:53:40,552 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:53:40,552 >>   Batch size = 4
{'eval_loss': 1.6187816858291626, 'eval_mse': 1.6201261770190447, 'eval_pearson': 0.6405432187876006, 'eval_spearmanr': 0.6485939534128269, 'eval_runtime': 1.3723, 'eval_samples_per_second': 503.518, 'eval_steps_per_second': 63.395, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 412/16480 [01:38<1:01:25,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 14:53:42,173 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412
[INFO|configuration_utils.py:423] 2025-03-10 14:53:42,175 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:53:42,946 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:53:42,947 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:53:42,947 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/special_tokens_map.json
[2025-03-10 14:53:43,014] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step412 is about to be saved!
[2025-03-10 14:53:43,021] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt
[2025-03-10 14:53:43,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt...
[2025-03-10 14:53:44,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/mp_rank_00_model_states.pt.
[2025-03-10 14:53:44,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:53:47,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:53:47,288] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-412/global_step412/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:53:47,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step412 is ready now!
{'loss': 0.9012, 'grad_norm': 18.34438133239746, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                     | 824/16480 [03:22<1:00:07,  4.34it/s][INFO|trainer.py:930] 2025-03-10 14:55:25,619 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:55:25,623 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:55:25,623 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:55:25,623 >>   Batch size = 4
{'eval_loss': 1.200667381286621, 'eval_mse': 1.2001360915331696, 'eval_pearson': 0.6896462129686738, 'eval_spearmanr': 0.6958083840345355, 'eval_runtime': 1.3232, 'eval_samples_per_second': 522.207, 'eval_steps_per_second': 65.748, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                     | 824/16480 [03:23<1:00:07,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 14:55:27,274 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824
[INFO|configuration_utils.py:423] 2025-03-10 14:55:27,276 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:55:28,126 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:55:28,127 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:55:28,127 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/special_tokens_map.json
[2025-03-10 14:55:28,193] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step824 is about to be saved!
[2025-03-10 14:55:28,200] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt
[2025-03-10 14:55:28,200] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt...
[2025-03-10 14:55:29,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/mp_rank_00_model_states.pt.
[2025-03-10 14:55:29,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:55:32,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:55:32,828] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-824/global_step824/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:55:32,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step824 is ready now!
{'loss': 0.7001, 'grad_norm': 17.162673950195312, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 1236/16480 [05:10<1:01:53,  4.10it/s][INFO|trainer.py:930] 2025-03-10 14:57:13,740 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:57:13,743 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:57:13,743 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:57:13,743 >>   Batch size = 4
{'eval_loss': 1.1542410850524902, 'eval_mse': 1.1527910491319537, 'eval_pearson': 0.7128351173967004, 'eval_spearmanr': 0.7214020884293832, 'eval_runtime': 1.3212, 'eval_samples_per_second': 522.993, 'eval_steps_per_second': 65.847, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 1236/16480 [05:11<1:01:53,  4.10it/s]
[INFO|trainer.py:3955] 2025-03-10 14:57:15,389 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236
[INFO|configuration_utils.py:423] 2025-03-10 14:57:15,392 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:57:16,242 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:57:16,243 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:57:16,244 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/special_tokens_map.json
[2025-03-10 14:57:16,312] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1236 is about to be saved!
[2025-03-10 14:57:16,318] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt
[2025-03-10 14:57:16,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt...
[2025-03-10 14:57:17,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/mp_rank_00_model_states.pt.
[2025-03-10 14:57:17,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:57:21,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:57:21,115] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1236/global_step1236/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:57:21,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1236 is ready now!
{'loss': 0.6086, 'grad_norm': 28.19532585144043, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 1648/16480 [06:58<1:00:24,  4.09it/s][INFO|trainer.py:930] 2025-03-10 14:59:02,254 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:59:02,258 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:59:02,258 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 14:59:02,258 >>   Batch size = 4
{'eval_loss': 1.1658650636672974, 'eval_mse': 1.1654476733007584, 'eval_pearson': 0.7148297461033264, 'eval_spearmanr': 0.7238017715038614, 'eval_runtime': 1.3263, 'eval_samples_per_second': 520.987, 'eval_steps_per_second': 65.595, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 1648/16480 [07:00<1:00:24,  4.09it/s]
[INFO|trainer.py:3955] 2025-03-10 14:59:03,911 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648
[INFO|configuration_utils.py:423] 2025-03-10 14:59:03,913 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:59:04,766 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:59:04,767 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:59:04,767 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/special_tokens_map.json
[2025-03-10 14:59:04,834] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1648 is about to be saved!
[2025-03-10 14:59:04,840] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt
[2025-03-10 14:59:04,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt...
[2025-03-10 14:59:05,948] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/mp_rank_00_model_states.pt.
[2025-03-10 14:59:05,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:59:09,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:59:09,500] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-1648/global_step1648/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:59:09,500] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1648 is ready now!
{'loss': 0.5223, 'grad_norm': 14.602018356323242, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [08:43<56:19,  4.27it/s][INFO|trainer.py:930] 2025-03-10 15:00:46,571 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:00:46,575 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:00:46,575 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:00:46,575 >>   Batch size = 4
{'eval_loss': 1.1604615449905396, 'eval_mse': 1.1586483563424537, 'eval_pearson': 0.699737621472716, 'eval_spearmanr': 0.6950312571534586, 'eval_runtime': 1.3723, 'eval_samples_per_second': 503.522, 'eval_steps_per_second': 63.396, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 2060/16480 [08:44<56:19,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 15:00:48,184 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060
[INFO|configuration_utils.py:423] 2025-03-10 15:00:48,187 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:00:48,973 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:00:48,974 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:00:48,974 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/special_tokens_map.json
[2025-03-10 15:00:49,038] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2060 is about to be saved!
[2025-03-10 15:00:49,044] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt
[2025-03-10 15:00:49,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt...
[2025-03-10 15:00:50,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/mp_rank_00_model_states.pt.
[2025-03-10 15:00:50,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:00:53,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:00:53,430] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2060/global_step2060/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:00:53,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2060 is ready now!
{'loss': 0.445, 'grad_norm': 8.21583080291748, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [10:26<54:09,  4.31it/s][INFO|trainer.py:930] 2025-03-10 15:02:30,076 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:02:30,079 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:02:30,079 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:02:30,080 >>   Batch size = 4
{'eval_loss': 1.1864615678787231, 'eval_mse': 1.1849923796315613, 'eval_pearson': 0.697639766423612, 'eval_spearmanr': 0.6941695833624764, 'eval_runtime': 1.3748, 'eval_samples_per_second': 502.633, 'eval_steps_per_second': 63.284, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 2472/16480 [10:27<54:09,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 15:02:31,694 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472
[INFO|configuration_utils.py:423] 2025-03-10 15:02:31,696 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:02:32,694 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:02:32,695 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:02:32,696 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/special_tokens_map.json
[2025-03-10 15:02:32,763] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2472 is about to be saved!
[2025-03-10 15:02:32,769] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt
[2025-03-10 15:02:32,769] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt...
[2025-03-10 15:02:33,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/mp_rank_00_model_states.pt.
[2025-03-10 15:02:34,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:02:37,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:02:37,658] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2472/global_step2472/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:02:37,658] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2472 is ready now!
{'loss': 0.3884, 'grad_norm': 13.63314437866211, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [12:09<52:03,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:04:12,894 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:04:12,897 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:04:12,897 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:04:12,897 >>   Batch size = 4
{'eval_loss': 1.3716952800750732, 'eval_mse': 1.3707977058918535, 'eval_pearson': 0.7104280650791865, 'eval_spearmanr': 0.710820291855569, 'eval_runtime': 1.3757, 'eval_samples_per_second': 502.276, 'eval_steps_per_second': 63.239, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 2884/16480 [12:10<52:03,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:04:14,546 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884
[INFO|configuration_utils.py:423] 2025-03-10 15:04:14,548 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:04:15,450 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:04:15,451 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:04:15,451 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/special_tokens_map.json
[2025-03-10 15:04:15,516] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2884 is about to be saved!
[2025-03-10 15:04:15,522] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt
[2025-03-10 15:04:15,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt...
[2025-03-10 15:04:16,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/mp_rank_00_model_states.pt.
[2025-03-10 15:04:16,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:04:20,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:04:20,332] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-2884/global_step2884/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:04:20,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2884 is ready now!
{'loss': 0.3264, 'grad_norm': 24.23978614807129, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [13:51<50:47,  4.33it/s][INFO|trainer.py:930] 2025-03-10 15:05:55,399 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:05:55,402 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:05:55,402 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:05:55,402 >>   Batch size = 4
{'eval_loss': 1.1919806003570557, 'eval_mse': 1.1916988134039121, 'eval_pearson': 0.7107588583868374, 'eval_spearmanr': 0.726018139224839, 'eval_runtime': 1.3745, 'eval_samples_per_second': 502.724, 'eval_steps_per_second': 63.295, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 3296/16480 [13:53<50:47,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 15:05:57,049 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296
[INFO|configuration_utils.py:423] 2025-03-10 15:05:57,052 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:05:57,941 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:05:57,942 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:05:57,942 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/special_tokens_map.json
[2025-03-10 15:05:58,007] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3296 is about to be saved!
[2025-03-10 15:05:58,012] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt
[2025-03-10 15:05:58,012] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt...
[2025-03-10 15:05:59,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/mp_rank_00_model_states.pt.
[2025-03-10 15:05:59,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:06:02,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:06:02,968] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3296/global_step3296/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:06:02,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3296 is ready now!
{'loss': 0.3024, 'grad_norm': 13.942840576171875, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [15:35<49:31,  4.30it/s][INFO|trainer.py:930] 2025-03-10 15:07:38,748 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:07:38,752 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:07:38,752 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:07:38,752 >>   Batch size = 4
{'eval_loss': 1.2327605485916138, 'eval_mse': 1.232509275593737, 'eval_pearson': 0.7137682629329519, 'eval_spearmanr': 0.7213592301243518, 'eval_runtime': 1.3764, 'eval_samples_per_second': 502.037, 'eval_steps_per_second': 63.209, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 3708/16480 [15:36<49:31,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 15:07:40,422 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708
[INFO|configuration_utils.py:423] 2025-03-10 15:07:40,425 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:07:41,322 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:07:41,323 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:07:41,323 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/special_tokens_map.json
[2025-03-10 15:07:41,390] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3708 is about to be saved!
[2025-03-10 15:07:41,396] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt
[2025-03-10 15:07:41,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt...
[2025-03-10 15:07:42,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/mp_rank_00_model_states.pt.
[2025-03-10 15:07:42,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:07:46,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:07:46,381] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-3708/global_step3708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:07:46,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3708 is ready now!
{'loss': 0.2564, 'grad_norm': 7.378459930419922, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [17:18<47:51,  4.30it/s][INFO|trainer.py:930] 2025-03-10 15:09:22,295 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:09:22,298 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:09:22,298 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:09:22,298 >>   Batch size = 4
{'eval_loss': 1.1159892082214355, 'eval_mse': 1.1152989688381962, 'eval_pearson': 0.721557909088385, 'eval_spearmanr': 0.7299407765801247, 'eval_runtime': 1.3747, 'eval_samples_per_second': 502.65, 'eval_steps_per_second': 63.286, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 4120/16480 [17:20<47:51,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 15:09:23,967 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120
[INFO|configuration_utils.py:423] 2025-03-10 15:09:23,969 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:09:24,870 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:09:24,871 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:09:24,871 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/special_tokens_map.json
[2025-03-10 15:09:24,941] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4120 is about to be saved!
[2025-03-10 15:09:24,946] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt
[2025-03-10 15:09:24,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt...
[2025-03-10 15:09:26,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/mp_rank_00_model_states.pt.
[2025-03-10 15:09:26,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:09:29,855] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:09:29,856] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4120/global_step4120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:09:29,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4120 is ready now!
{'loss': 0.2266, 'grad_norm': 6.313324928283691, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [19:01<45:49,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:11:04,923 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:11:04,926 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:11:04,926 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:11:04,926 >>   Batch size = 4
{'eval_loss': 1.4336355924606323, 'eval_mse': 1.433539940548357, 'eval_pearson': 0.7108916426807161, 'eval_spearmanr': 0.7256520717366941, 'eval_runtime': 1.3741, 'eval_samples_per_second': 502.864, 'eval_steps_per_second': 63.313, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 4532/16480 [19:02<45:49,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:11:06,592 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532
[INFO|configuration_utils.py:423] 2025-03-10 15:11:06,594 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:11:07,481 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:11:07,482 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:11:07,482 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/special_tokens_map.json
[2025-03-10 15:11:07,551] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4532 is about to be saved!
[2025-03-10 15:11:07,557] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt
[2025-03-10 15:11:07,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt...
[2025-03-10 15:11:08,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/mp_rank_00_model_states.pt.
[2025-03-10 15:11:08,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:11:12,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:11:12,452] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4532/global_step4532/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:11:12,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4532 is ready now!
{'loss': 0.1988, 'grad_norm': 10.555998802185059, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [20:43<44:12,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:12:47,465 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:12:47,469 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:12:47,469 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:12:47,469 >>   Batch size = 4
{'eval_loss': 1.328161358833313, 'eval_mse': 1.3260096848097276, 'eval_pearson': 0.6888660020906157, 'eval_spearmanr': 0.7016069127638785, 'eval_runtime': 1.3258, 'eval_samples_per_second': 521.202, 'eval_steps_per_second': 65.622, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 4944/16480 [20:45<44:12,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:12:49,137 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944
[INFO|configuration_utils.py:423] 2025-03-10 15:12:49,139 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:12:50,017 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:12:50,018 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:12:50,018 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/special_tokens_map.json
[2025-03-10 15:12:50,076] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4944 is about to be saved!
[2025-03-10 15:12:50,081] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt
[2025-03-10 15:12:50,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt...
[2025-03-10 15:12:51,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/mp_rank_00_model_states.pt.
[2025-03-10 15:12:51,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:12:54,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:12:54,989] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-4944/global_step4944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:12:54,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4944 is ready now!
{'loss': 0.1806, 'grad_norm': 10.904794692993164, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [22:26<44:18,  4.18it/s][INFO|trainer.py:930] 2025-03-10 15:14:30,063 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:14:30,067 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:14:30,067 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:14:30,067 >>   Batch size = 4
{'eval_loss': 1.107643961906433, 'eval_mse': 1.1077239282224354, 'eval_pearson': 0.7430379163205965, 'eval_spearmanr': 0.7492283980791451, 'eval_runtime': 1.3272, 'eval_samples_per_second': 520.655, 'eval_steps_per_second': 65.553, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                       | 5356/16480 [22:27<44:18,  4.18it/s]
[INFO|trainer.py:3955] 2025-03-10 15:14:31,736 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356
[INFO|configuration_utils.py:423] 2025-03-10 15:14:31,738 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:14:32,626 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:14:32,627 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:14:32,627 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/special_tokens_map.json
[2025-03-10 15:14:32,693] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5356 is about to be saved!
[2025-03-10 15:14:32,698] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt
[2025-03-10 15:14:32,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt...
[2025-03-10 15:14:33,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/mp_rank_00_model_states.pt.
[2025-03-10 15:14:33,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:14:37,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:14:37,593] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5356/global_step5356/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:14:37,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5356 is ready now!
{'loss': 0.1593, 'grad_norm': 15.714284896850586, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [24:09<41:07,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:16:12,651 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:16:12,655 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:16:12,655 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:16:12,655 >>   Batch size = 4
{'eval_loss': 1.4464232921600342, 'eval_mse': 1.4456030953292736, 'eval_pearson': 0.7150366760436089, 'eval_spearmanr': 0.7324283560940298, 'eval_runtime': 1.3738, 'eval_samples_per_second': 502.98, 'eval_steps_per_second': 63.327, 'epoch': 14.0}
 35%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                               | 5768/16480 [24:10<41:07,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:16:14,320 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768
[INFO|configuration_utils.py:423] 2025-03-10 15:16:14,323 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:16:15,217 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:16:15,218 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:16:15,218 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/special_tokens_map.json
[2025-03-10 15:16:15,286] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5768 is about to be saved!
[2025-03-10 15:16:15,292] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt
[2025-03-10 15:16:15,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt...
[2025-03-10 15:16:16,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/mp_rank_00_model_states.pt.
[2025-03-10 15:16:16,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:16:20,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:16:20,379] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-5768/global_step5768/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:16:20,379] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5768 is ready now!
{'loss': 0.1416, 'grad_norm': 9.988048553466797, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [25:51<39:26,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:17:55,348 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:17:55,351 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:17:55,351 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:17:55,351 >>   Batch size = 4
{'eval_loss': 1.2442351579666138, 'eval_mse': 1.2435294957305865, 'eval_pearson': 0.708408063857582, 'eval_spearmanr': 0.7206533389974963, 'eval_runtime': 1.3736, 'eval_samples_per_second': 503.059, 'eval_steps_per_second': 63.337, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                      | 6180/16480 [25:53<39:26,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:17:57,026 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180
[INFO|configuration_utils.py:423] 2025-03-10 15:17:57,028 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:17:58,021 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:17:58,022 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:17:58,022 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/special_tokens_map.json
[2025-03-10 15:17:58,096] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6180 is about to be saved!
[2025-03-10 15:17:58,238] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt
[2025-03-10 15:17:58,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt...
[2025-03-10 15:17:59,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/mp_rank_00_model_states.pt.
[2025-03-10 15:17:59,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:18:03,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:18:03,398] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6180/global_step6180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:18:03,398] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6180 is ready now!
{'loss': 0.121, 'grad_norm': 5.72393274307251, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [27:34<37:55,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:19:38,381 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:19:38,385 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:19:38,385 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:19:38,385 >>   Batch size = 4
{'eval_loss': 1.2552446126937866, 'eval_mse': 1.2550166795291708, 'eval_pearson': 0.7126677827206327, 'eval_spearmanr': 0.7299761797427596, 'eval_runtime': 1.3759, 'eval_samples_per_second': 502.231, 'eval_steps_per_second': 63.233, 'epoch': 16.0}
 40%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                             | 6592/16480 [27:36<37:55,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:19:40,066 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592
[INFO|configuration_utils.py:423] 2025-03-10 15:19:40,068 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:19:41,008 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:19:41,009 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:19:41,009 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/special_tokens_map.json
[2025-03-10 15:19:41,084] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6592 is about to be saved!
[2025-03-10 15:19:41,297] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt
[2025-03-10 15:19:41,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt...
[2025-03-10 15:19:42,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/mp_rank_00_model_states.pt.
[2025-03-10 15:19:42,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:19:46,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:19:46,518] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-6592/global_step6592/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:19:46,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6592 is ready now!
{'loss': 0.1101, 'grad_norm': 2.936323642730713, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [29:17<36:16,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:21:21,463 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:21:21,466 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:21:21,466 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:21:21,466 >>   Batch size = 4
{'eval_loss': 1.2796592712402344, 'eval_mse': 1.279650957988071, 'eval_pearson': 0.7150433802950524, 'eval_spearmanr': 0.7257058318198568, 'eval_runtime': 1.3746, 'eval_samples_per_second': 502.679, 'eval_steps_per_second': 63.29, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                     | 7004/16480 [29:19<36:16,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:21:23,136 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004
[INFO|configuration_utils.py:423] 2025-03-10 15:21:23,138 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:21:24,076 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:21:24,077 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:21:24,077 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/special_tokens_map.json
[2025-03-10 15:21:24,145] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7004 is about to be saved!
[2025-03-10 15:21:24,325] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt
[2025-03-10 15:21:24,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt...
[2025-03-10 15:21:25,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/mp_rank_00_model_states.pt.
[2025-03-10 15:21:25,532] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:21:29,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:21:29,568] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7004/global_step7004/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:21:29,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7004 is ready now!
{'loss': 0.0985, 'grad_norm': 7.286525249481201, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [31:02<34:45,  4.35it/s][INFO|trainer.py:930] 2025-03-10 15:23:06,257 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:23:06,261 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:23:06,261 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:23:06,261 >>   Batch size = 4
{'eval_loss': 1.2890937328338623, 'eval_mse': 1.288572098170279, 'eval_pearson': 0.7258902100569992, 'eval_spearmanr': 0.7463349755897483, 'eval_runtime': 1.3727, 'eval_samples_per_second': 503.382, 'eval_steps_per_second': 63.378, 'epoch': 18.0}
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                            | 7416/16480 [31:04<34:45,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 15:23:07,925 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416
[INFO|configuration_utils.py:423] 2025-03-10 15:23:07,928 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:23:08,860 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:23:08,861 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:23:08,861 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/special_tokens_map.json
[2025-03-10 15:23:08,917] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7416 is about to be saved!
[2025-03-10 15:23:09,057] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt
[2025-03-10 15:23:09,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt...
[2025-03-10 15:23:10,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/mp_rank_00_model_states.pt.
[2025-03-10 15:23:10,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:23:14,081] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:23:14,082] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7416/global_step7416/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:23:14,082] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7416 is ready now!
{'loss': 0.0844, 'grad_norm': 3.084064483642578, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [32:46<34:01,  4.24it/s][INFO|trainer.py:930] 2025-03-10 15:24:50,483 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:24:50,486 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:24:50,487 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:24:50,487 >>   Batch size = 4
{'eval_loss': 1.3944125175476074, 'eval_mse': 1.3937949161281116, 'eval_pearson': 0.6980707840217533, 'eval_spearmanr': 0.7171778128939827, 'eval_runtime': 1.3878, 'eval_samples_per_second': 497.903, 'eval_steps_per_second': 62.688, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                    | 7828/16480 [32:48<34:01,  4.24it/s]
[INFO|trainer.py:3955] 2025-03-10 15:24:52,161 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828
[INFO|configuration_utils.py:423] 2025-03-10 15:24:52,164 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:24:53,049 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:24:53,050 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:24:53,050 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/special_tokens_map.json
[2025-03-10 15:24:53,115] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7828 is about to be saved!
[2025-03-10 15:24:53,120] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt
[2025-03-10 15:24:53,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt...
[2025-03-10 15:24:54,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/mp_rank_00_model_states.pt.
[2025-03-10 15:24:54,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:24:57,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:24:57,988] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-7828/global_step7828/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:24:57,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7828 is ready now!
{'loss': 0.0865, 'grad_norm': 4.169039249420166, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [34:31<31:53,  4.31it/s][INFO|trainer.py:930] 2025-03-10 15:26:34,925 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:26:34,928 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:26:34,929 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:26:34,929 >>   Batch size = 4
{'eval_loss': 1.4003146886825562, 'eval_mse': 1.400604099682202, 'eval_pearson': 0.706665458818791, 'eval_spearmanr': 0.7230251743937949, 'eval_runtime': 1.3723, 'eval_samples_per_second': 503.516, 'eval_steps_per_second': 63.395, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                           | 8240/16480 [34:32<31:53,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 15:26:36,591 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240
[INFO|configuration_utils.py:423] 2025-03-10 15:26:36,594 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:26:37,524 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:26:37,525 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:26:37,525 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/special_tokens_map.json
[2025-03-10 15:26:37,593] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8240 is about to be saved!
[2025-03-10 15:26:37,599] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt
[2025-03-10 15:26:37,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt...
[2025-03-10 15:26:38,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/mp_rank_00_model_states.pt.
[2025-03-10 15:26:38,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:26:42,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:26:42,610] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8240/global_step8240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:26:42,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8240 is ready now!
{'loss': 0.0733, 'grad_norm': 8.499688148498535, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [36:15<30:21,  4.30it/s][INFO|trainer.py:930] 2025-03-10 15:28:18,643 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:28:18,647 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:28:18,647 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:28:18,647 >>   Batch size = 4
{'eval_loss': 1.4553165435791016, 'eval_mse': 1.4552584212009192, 'eval_pearson': 0.7007990366676714, 'eval_spearmanr': 0.7167287866084016, 'eval_runtime': 1.3772, 'eval_samples_per_second': 501.739, 'eval_steps_per_second': 63.171, 'epoch': 21.0}
 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                   | 8652/16480 [36:16<30:21,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 15:28:20,313 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652
[INFO|configuration_utils.py:423] 2025-03-10 15:28:20,316 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:28:21,216 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:28:21,217 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:28:21,218 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/special_tokens_map.json
[2025-03-10 15:28:21,284] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8652 is about to be saved!
[2025-03-10 15:28:21,289] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt
[2025-03-10 15:28:21,290] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt...
[2025-03-10 15:28:22,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/mp_rank_00_model_states.pt.
[2025-03-10 15:28:22,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:28:26,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:28:26,233] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-8652/global_step8652/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:28:26,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8652 is ready now!
{'loss': 0.0676, 'grad_norm': 2.8554322719573975, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [37:58<28:31,  4.33it/s][INFO|trainer.py:930] 2025-03-10 15:30:02,418 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:30:02,422 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:30:02,422 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:30:02,422 >>   Batch size = 4
{'eval_loss': 1.2714589834213257, 'eval_mse': 1.2723026406747733, 'eval_pearson': 0.7257967720350299, 'eval_spearmanr': 0.7432232499320259, 'eval_runtime': 1.3305, 'eval_samples_per_second': 519.371, 'eval_steps_per_second': 65.391, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                          | 9064/16480 [38:00<28:31,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 15:30:04,090 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064
[INFO|configuration_utils.py:423] 2025-03-10 15:30:04,092 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:30:04,990 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:30:04,991 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:30:04,991 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/special_tokens_map.json
[2025-03-10 15:30:05,055] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9064 is about to be saved!
[2025-03-10 15:30:05,060] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt
[2025-03-10 15:30:05,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt...
[2025-03-10 15:30:06,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/mp_rank_00_model_states.pt.
[2025-03-10 15:30:06,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:30:10,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:30:10,026] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9064/global_step9064/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:30:10,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9064 is ready now!
{'loss': 0.0591, 'grad_norm': 3.214386224746704, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [39:41<26:56,  4.33it/s][INFO|trainer.py:930] 2025-03-10 15:31:45,440 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:31:45,443 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:31:45,443 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:31:45,443 >>   Batch size = 4
{'eval_loss': 1.2415350675582886, 'eval_mse': 1.2417125225757206, 'eval_pearson': 0.7343395211977337, 'eval_spearmanr': 0.7505899881625065, 'eval_runtime': 1.3764, 'eval_samples_per_second': 502.026, 'eval_steps_per_second': 63.207, 'epoch': 23.0}
 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                 | 9476/16480 [39:43<26:56,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 15:31:47,110 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476
[INFO|configuration_utils.py:423] 2025-03-10 15:31:47,112 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:31:48,022 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:31:48,023 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:31:48,023 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/special_tokens_map.json
[2025-03-10 15:31:48,090] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9476 is about to be saved!
[2025-03-10 15:31:48,096] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt
[2025-03-10 15:31:48,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt...
[2025-03-10 15:31:49,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/mp_rank_00_model_states.pt.
[2025-03-10 15:31:49,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:31:53,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:31:53,211] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9476/global_step9476/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:31:53,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9476 is ready now!
{'loss': 0.0602, 'grad_norm': 3.2094266414642334, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [41:25<25:17,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:33:28,596 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:33:28,600 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:33:28,600 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:33:28,600 >>   Batch size = 4
{'eval_loss': 1.5636415481567383, 'eval_mse': 1.5626336958922458, 'eval_pearson': 0.6965216201985138, 'eval_spearmanr': 0.7162522487356839, 'eval_runtime': 1.3745, 'eval_samples_per_second': 502.737, 'eval_steps_per_second': 63.297, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                         | 9888/16480 [41:26<25:17,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:33:30,264 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888
[INFO|configuration_utils.py:423] 2025-03-10 15:33:30,266 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:33:31,148 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:33:31,149 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:33:31,149 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/special_tokens_map.json
[2025-03-10 15:33:31,210] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9888 is about to be saved!
[2025-03-10 15:33:31,215] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt
[2025-03-10 15:33:31,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt...
[2025-03-10 15:33:32,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/mp_rank_00_model_states.pt.
[2025-03-10 15:33:32,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:33:36,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:33:36,163] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-9888/global_step9888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:33:36,163] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9888 is ready now!
{'loss': 0.0589, 'grad_norm': 5.048562049865723, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [43:08<24:04,  4.28it/s][INFO|trainer.py:930] 2025-03-10 15:35:11,950 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:35:11,954 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:35:11,954 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:35:11,954 >>   Batch size = 4
{'eval_loss': 1.459040641784668, 'eval_mse': 1.4593253535920736, 'eval_pearson': 0.7206203900915338, 'eval_spearmanr': 0.7386085753500046, 'eval_runtime': 1.3267, 'eval_samples_per_second': 520.849, 'eval_steps_per_second': 65.577, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 10300/16480 [43:09<24:04,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 15:35:13,623 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300
[INFO|configuration_utils.py:423] 2025-03-10 15:35:13,626 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:35:14,543 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:35:14,544 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:35:14,544 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/special_tokens_map.json
[2025-03-10 15:35:14,613] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10300 is about to be saved!
[2025-03-10 15:35:14,619] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt
[2025-03-10 15:35:14,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt...
[2025-03-10 15:35:15,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/mp_rank_00_model_states.pt.
[2025-03-10 15:35:15,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:35:19,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:35:19,419] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10300/global_step10300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:35:19,419] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10300 is ready now!
{'loss': 0.059, 'grad_norm': 10.20224666595459, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [44:52<22:19,  4.31it/s][INFO|trainer.py:930] 2025-03-10 15:36:55,720 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:36:55,727 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:36:55,727 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:36:55,727 >>   Batch size = 4
{'eval_loss': 1.4578721523284912, 'eval_mse': 1.4574286444314828, 'eval_pearson': 0.7147128752862963, 'eval_spearmanr': 0.7304911852777447, 'eval_runtime': 1.3804, 'eval_samples_per_second': 500.569, 'eval_steps_per_second': 63.024, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 10712/16480 [44:53<22:19,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 15:36:57,387 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712
[INFO|configuration_utils.py:423] 2025-03-10 15:36:57,389 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:36:58,366 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:36:58,367 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:36:58,367 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/special_tokens_map.json
[2025-03-10 15:36:58,437] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10712 is about to be saved!
[2025-03-10 15:36:58,443] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt
[2025-03-10 15:36:58,443] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt...
[2025-03-10 15:36:59,653] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/mp_rank_00_model_states.pt.
[2025-03-10 15:36:59,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:37:03,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:37:03,692] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-10712/global_step10712/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:37:03,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10712 is ready now!
{'loss': 0.0501, 'grad_norm': 10.615394592285156, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [46:35<20:37,  4.33it/s][INFO|trainer.py:930] 2025-03-10 15:38:38,953 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:38:38,956 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:38:38,956 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:38:38,956 >>   Batch size = 4
{'eval_loss': 1.4941449165344238, 'eval_mse': 1.4934450879628338, 'eval_pearson': 0.7034895238778414, 'eval_spearmanr': 0.718518080996172, 'eval_runtime': 1.373, 'eval_samples_per_second': 503.266, 'eval_steps_per_second': 63.363, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 11124/16480 [46:36<20:37,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 15:38:40,618 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124
[INFO|configuration_utils.py:423] 2025-03-10 15:38:40,621 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:38:41,595 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:38:41,596 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:38:41,596 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/special_tokens_map.json
[2025-03-10 15:38:41,663] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11124 is about to be saved!
[2025-03-10 15:38:41,669] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt
[2025-03-10 15:38:41,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt...
[2025-03-10 15:38:42,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/mp_rank_00_model_states.pt.
[2025-03-10 15:38:42,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:38:46,826] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:38:46,827] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11124/global_step11124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:38:46,827] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11124 is ready now!
{'loss': 0.0453, 'grad_norm': 6.3693742752075195, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [48:18<19:08,  4.30it/s][INFO|trainer.py:930] 2025-03-10 15:40:22,300 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:40:22,303 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:40:22,303 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:40:22,303 >>   Batch size = 4
{'eval_loss': 1.2863659858703613, 'eval_mse': 1.285605981967556, 'eval_pearson': 0.7075433106837827, 'eval_spearmanr': 0.7248786427134404, 'eval_runtime': 1.3734, 'eval_samples_per_second': 503.121, 'eval_steps_per_second': 63.345, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 11536/16480 [48:20<19:08,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 15:40:23,973 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536
[INFO|configuration_utils.py:423] 2025-03-10 15:40:23,975 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:40:24,957 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:40:24,958 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:40:24,959 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/special_tokens_map.json
[2025-03-10 15:40:25,023] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11536 is about to be saved!
[2025-03-10 15:40:25,181] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt
[2025-03-10 15:40:25,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt...
[2025-03-10 15:40:26,437] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/mp_rank_00_model_states.pt.
[2025-03-10 15:40:26,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:40:30,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:40:30,447] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11536/global_step11536/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:40:30,447] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11536 is ready now!
{'loss': 0.0444, 'grad_norm': 1.7444676160812378, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [50:02<17:23,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:42:05,916 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:42:05,919 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:42:05,919 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:42:05,919 >>   Batch size = 4
{'eval_loss': 1.527662754058838, 'eval_mse': 1.5278721580284549, 'eval_pearson': 0.7001504887782732, 'eval_spearmanr': 0.7205225273564074, 'eval_runtime': 1.3745, 'eval_samples_per_second': 502.743, 'eval_steps_per_second': 63.298, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 11948/16480 [50:03<17:23,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:42:07,591 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948
[INFO|configuration_utils.py:423] 2025-03-10 15:42:07,593 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:42:08,609 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:42:08,610 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:42:08,610 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/special_tokens_map.json
[2025-03-10 15:42:08,671] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11948 is about to be saved!
[2025-03-10 15:42:08,677] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt
[2025-03-10 15:42:08,677] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt...
[2025-03-10 15:42:10,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/mp_rank_00_model_states.pt.
[2025-03-10 15:42:10,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:42:14,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:42:14,301] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-11948/global_step11948/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:42:14,301] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11948 is ready now!
{'loss': 0.0425, 'grad_norm': 1.5442278385162354, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [51:49<16:21,  4.20it/s][INFO|trainer.py:930] 2025-03-10 15:43:53,450 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:43:53,455 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:43:53,455 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:43:53,455 >>   Batch size = 4
{'eval_loss': 1.3560738563537598, 'eval_mse': 1.355953077849017, 'eval_pearson': 0.7162121234097646, 'eval_spearmanr': 0.7325841838752969, 'eval_runtime': 1.3762, 'eval_samples_per_second': 502.09, 'eval_steps_per_second': 63.215, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 12360/16480 [51:51<16:21,  4.20it/s]
[INFO|trainer.py:3955] 2025-03-10 15:43:55,107 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360
[INFO|configuration_utils.py:423] 2025-03-10 15:43:55,109 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:43:55,885 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:43:55,887 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:43:55,887 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/special_tokens_map.json
[2025-03-10 15:43:55,942] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12360 is about to be saved!
[2025-03-10 15:43:55,949] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt
[2025-03-10 15:43:55,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt...
[2025-03-10 15:43:56,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/mp_rank_00_model_states.pt.
[2025-03-10 15:43:57,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:44:00,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:44:00,283] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12360/global_step12360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:44:00,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12360 is ready now!
{'loss': 0.0404, 'grad_norm': 3.180122137069702, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [53:33<14:27,  4.27it/s][INFO|trainer.py:930] 2025-03-10 15:45:36,727 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:45:36,731 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:45:36,731 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:45:36,731 >>   Batch size = 4
{'eval_loss': 1.449833631515503, 'eval_mse': 1.4511880453692159, 'eval_pearson': 0.6879530964454599, 'eval_spearmanr': 0.7088576993629588, 'eval_runtime': 1.3746, 'eval_samples_per_second': 502.675, 'eval_steps_per_second': 63.289, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 12772/16480 [53:34<14:27,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 15:45:38,402 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772
[INFO|configuration_utils.py:423] 2025-03-10 15:45:38,404 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:45:39,359 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:45:39,360 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:45:39,360 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/special_tokens_map.json
[2025-03-10 15:45:39,423] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12772 is about to be saved!
[2025-03-10 15:45:39,510] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt
[2025-03-10 15:45:39,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt...
[2025-03-10 15:45:40,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/mp_rank_00_model_states.pt.
[2025-03-10 15:45:40,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:45:44,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:45:44,689] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-12772/global_step12772/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:45:44,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12772 is ready now!
{'loss': 0.0405, 'grad_norm': 4.447875022888184, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [55:18<13:13,  4.16it/s][INFO|trainer.py:930] 2025-03-10 15:47:22,107 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:47:22,111 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:47:22,111 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:47:22,111 >>   Batch size = 4
{'eval_loss': 1.3546031713485718, 'eval_mse': 1.3545049047677113, 'eval_pearson': 0.7023522262694641, 'eval_spearmanr': 0.7244424398340417, 'eval_runtime': 1.3811, 'eval_samples_per_second': 500.339, 'eval_steps_per_second': 62.995, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 13184/16480 [55:19<13:13,  4.16it/s]
[INFO|trainer.py:3955] 2025-03-10 15:47:23,777 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184
[INFO|configuration_utils.py:423] 2025-03-10 15:47:23,780 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:47:24,601 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:47:24,603 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:47:24,603 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/special_tokens_map.json
[2025-03-10 15:47:24,663] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13184 is about to be saved!
[2025-03-10 15:47:24,670] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt
[2025-03-10 15:47:24,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt...
[2025-03-10 15:47:25,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/mp_rank_00_model_states.pt.
[2025-03-10 15:47:25,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:47:29,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:47:29,352] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13184/global_step13184/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:47:29,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13184 is ready now!
{'loss': 0.0377, 'grad_norm': 1.916538953781128, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [57:01<11:16,  4.26it/s][INFO|trainer.py:930] 2025-03-10 15:49:05,334 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:49:05,338 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:49:05,338 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:49:05,338 >>   Batch size = 4
{'eval_loss': 1.3891900777816772, 'eval_mse': 1.388913082144885, 'eval_pearson': 0.7144289647901354, 'eval_spearmanr': 0.7365097495568652, 'eval_runtime': 1.3746, 'eval_samples_per_second': 502.686, 'eval_steps_per_second': 63.29, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 13596/16480 [57:03<11:16,  4.26it/s]
[INFO|trainer.py:3955] 2025-03-10 15:49:07,011 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596
[INFO|configuration_utils.py:423] 2025-03-10 15:49:07,013 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:49:07,946 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:49:07,948 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:49:07,948 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/special_tokens_map.json
[2025-03-10 15:49:08,010] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13596 is about to be saved!
[2025-03-10 15:49:08,017] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt
[2025-03-10 15:49:08,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt...
[2025-03-10 15:49:09,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/mp_rank_00_model_states.pt.
[2025-03-10 15:49:09,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:49:13,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:49:13,167] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-13596/global_step13596/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:49:13,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13596 is ready now!
{'loss': 0.0368, 'grad_norm': 3.2433409690856934, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [58:45<09:29,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:50:49,320 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:50:49,324 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:50:49,324 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:50:49,324 >>   Batch size = 4
{'eval_loss': 1.3607226610183716, 'eval_mse': 1.361478993240555, 'eval_pearson': 0.723038265587629, 'eval_spearmanr': 0.741173541720654, 'eval_runtime': 1.3762, 'eval_samples_per_second': 502.095, 'eval_steps_per_second': 63.216, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 14008/16480 [58:47<09:29,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:50:50,996 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008
[INFO|configuration_utils.py:423] 2025-03-10 15:50:50,999 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:50:51,933 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:50:51,934 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:50:51,934 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/special_tokens_map.json
[2025-03-10 15:50:51,992] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14008 is about to be saved!
[2025-03-10 15:50:52,014] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt
[2025-03-10 15:50:52,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt...
[2025-03-10 15:50:53,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/mp_rank_00_model_states.pt.
[2025-03-10 15:50:53,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:50:57,204] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:50:57,205] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14008/global_step14008/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:50:57,205] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14008 is ready now!
{'loss': 0.0341, 'grad_norm': 3.1457109451293945, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 14420/16480 [1:00:29<08:02,  4.27it/s][INFO|trainer.py:930] 2025-03-10 15:52:32,969 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:52:32,973 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:52:32,973 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:52:32,973 >>   Batch size = 4
{'eval_loss': 1.4789968729019165, 'eval_mse': 1.4789258422796356, 'eval_pearson': 0.7180695429284549, 'eval_spearmanr': 0.7349281470635536, 'eval_runtime': 1.3747, 'eval_samples_per_second': 502.669, 'eval_steps_per_second': 63.288, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 14420/16480 [1:00:30<08:02,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 15:52:34,645 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420
[INFO|configuration_utils.py:423] 2025-03-10 15:52:34,647 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:52:35,584 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:52:35,585 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:52:35,585 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/special_tokens_map.json
[2025-03-10 15:52:35,640] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14420 is about to be saved!
[2025-03-10 15:52:35,646] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt
[2025-03-10 15:52:35,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt...
[2025-03-10 15:52:36,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/mp_rank_00_model_states.pt.
[2025-03-10 15:52:36,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:52:40,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:52:40,965] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14420/global_step14420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:52:40,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14420 is ready now!
{'loss': 0.033, 'grad_norm': 3.002786874771118, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 14832/16480 [1:02:13<06:23,  4.30it/s][INFO|trainer.py:930] 2025-03-10 15:54:17,505 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:17,508 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:17,508 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:54:17,508 >>   Batch size = 4
{'eval_loss': 1.1714990139007568, 'eval_mse': 1.1725896809104555, 'eval_pearson': 0.7399870273887004, 'eval_spearmanr': 0.7598891256577406, 'eval_runtime': 1.3748, 'eval_samples_per_second': 502.606, 'eval_steps_per_second': 63.28, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 14832/16480 [1:02:15<06:23,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:19,163 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832
[INFO|configuration_utils.py:423] 2025-03-10 15:54:19,166 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:19,876 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:19,877 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:19,877 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/special_tokens_map.json
[2025-03-10 15:54:19,928] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14832 is about to be saved!
[2025-03-10 15:54:19,934] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt
[2025-03-10 15:54:19,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt...
[2025-03-10 15:54:20,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/mp_rank_00_model_states.pt.
[2025-03-10 15:54:20,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:54:24,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:54:24,078] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-14832/global_step14832/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:54:24,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14832 is ready now!
{'loss': 0.033, 'grad_norm': 3.3534231185913086, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 15244/16480 [1:03:58<04:45,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:56:02,361 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:56:02,365 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:56:02,365 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:56:02,365 >>   Batch size = 4
{'eval_loss': 1.1935211420059204, 'eval_mse': 1.1938449118485843, 'eval_pearson': 0.7508026552166216, 'eval_spearmanr': 0.7711173789082457, 'eval_runtime': 1.3784, 'eval_samples_per_second': 501.32, 'eval_steps_per_second': 63.118, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 15244/16480 [1:04:00<04:45,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:56:03,980 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244
[INFO|configuration_utils.py:423] 2025-03-10 15:56:03,982 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:56:04,722 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:56:04,724 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:56:04,724 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/special_tokens_map.json
[2025-03-10 15:56:04,770] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15244 is about to be saved!
[2025-03-10 15:56:04,775] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt
[2025-03-10 15:56:04,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt...
[2025-03-10 15:56:05,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/mp_rank_00_model_states.pt.
[2025-03-10 15:56:05,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:56:09,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:56:09,103] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15244/global_step15244/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:56:09,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15244 is ready now!
{'loss': 0.0302, 'grad_norm': 2.0591025352478027, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 15656/16480 [1:05:40<03:09,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:57:44,427 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:57:44,431 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:57:44,431 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:57:44,431 >>   Batch size = 4
{'eval_loss': 1.3063232898712158, 'eval_mse': 1.3062156264585283, 'eval_pearson': 0.7385850278267185, 'eval_spearmanr': 0.7569183716816039, 'eval_runtime': 1.3313, 'eval_samples_per_second': 519.058, 'eval_steps_per_second': 65.352, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 15656/16480 [1:05:42<03:09,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:57:46,048 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656
[INFO|configuration_utils.py:423] 2025-03-10 15:57:46,050 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:57:46,803 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:57:46,804 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:57:46,804 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/special_tokens_map.json
[2025-03-10 15:57:46,849] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15656 is about to be saved!
[2025-03-10 15:57:46,855] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt
[2025-03-10 15:57:46,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt...
[2025-03-10 15:57:47,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/mp_rank_00_model_states.pt.
[2025-03-10 15:57:47,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:57:51,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:57:51,210] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-15656/global_step15656/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:57:51,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15656 is ready now!
{'loss': 0.0271, 'grad_norm': 1.613272786140442, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 16068/16480 [1:07:22<01:34,  4.34it/s][INFO|trainer.py:930] 2025-03-10 15:59:26,428 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:59:26,431 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:59:26,431 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 15:59:26,432 >>   Batch size = 4
{'eval_loss': 1.4162817001342773, 'eval_mse': 1.4166567639573093, 'eval_pearson': 0.7248655085844469, 'eval_spearmanr': 0.7428257189106142, 'eval_runtime': 1.374, 'eval_samples_per_second': 502.915, 'eval_steps_per_second': 63.319, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 16068/16480 [1:07:24<01:34,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 15:59:28,043 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068
[INFO|configuration_utils.py:423] 2025-03-10 15:59:28,045 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:59:28,878 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:59:28,879 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:59:28,879 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/special_tokens_map.json
[2025-03-10 15:59:28,938] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16068 is about to be saved!
[2025-03-10 15:59:28,944] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt
[2025-03-10 15:59:28,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt...
[2025-03-10 15:59:29,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/mp_rank_00_model_states.pt.
[2025-03-10 15:59:29,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:59:32,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:59:32,877] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16068/global_step16068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:59:32,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16068 is ready now!
{'loss': 0.0295, 'grad_norm': 1.4534832239151, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [1:09:06<00:00,  4.27it/s][INFO|trainer.py:930] 2025-03-10 16:01:10,422 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:01:10,426 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:01:10,426 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 16:01:10,426 >>   Batch size = 4
{'eval_loss': 1.3650310039520264, 'eval_mse': 1.3648089223936222, 'eval_pearson': 0.7192778096427097, 'eval_spearmanr': 0.7376498254581773, 'eval_runtime': 1.3758, 'eval_samples_per_second': 502.25, 'eval_steps_per_second': 63.236, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [1:09:08<00:00,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 16:01:12,128 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480
[INFO|configuration_utils.py:423] 2025-03-10 16:01:12,130 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:01:12,793 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:01:12,794 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:01:12,794 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/special_tokens_map.json
[2025-03-10 16:01:12,842] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16480 is about to be saved!
[2025-03-10 16:01:12,847] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt
[2025-03-10 16:01:12,847] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt...
[2025-03-10 16:01:13,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/mp_rank_00_model_states.pt.
[2025-03-10 16:01:13,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:01:16,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:01:16,690] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=10/checkpoint-16480/global_step16480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:01:16,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16480 is ready now!
[INFO|trainer.py:2670] 2025-03-10 16:01:16,962 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 4153.4306, 'train_samples_per_second': 126.902, 'train_steps_per_second': 3.968, 'train_loss': 0.2062072234825023, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16480/16480 [1:09:13<00:00,  3.97it/s]
[INFO|trainer.py:3955] 2025-03-10 16:01:17,139 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=10
[INFO|configuration_utils.py:423] 2025-03-10 16:01:17,141 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=10/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:01:17,805 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=10/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:01:17,806 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:01:17,806 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=10/special_tokens_map.json
***** train metrics *****
  epoch                    =        40.0
  total_flos               = 457709360GF
  train_loss               =      0.2062
  train_runtime            =  1:09:13.43
  train_samples            =       13177
  train_samples_per_second =     126.902
  train_steps_per_second   =       3.968
03/10/2025 16:01:17 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 16:01:17,858 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:01:17,860 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:01:17,860 >>   Num examples = 691
[INFO|trainer.py:4276] 2025-03-10 16:01:17,860 >>   Batch size = 4
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [00:01<00:00, 66.19it/s]
***** eval metrics *****
  epoch                   =       40.0
  eval_loss               =      1.365
  eval_mse                =     1.3648
  eval_pearson            =     0.7193
  eval_runtime            = 0:00:01.33
  eval_samples            =        691
  eval_samples_per_second =    518.107
  eval_spearmanr          =     0.7376
  eval_steps_per_second   =     65.232
[INFO|modelcard.py:449] 2025-03-10 16:01:19,425 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7376498254581773}]}
[rank0]:[W310 16:01:19.618923935 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 16:01:21,821] [INFO] [launch.py:351:main] Process 4101181 exits successfully.
[2025-03-10 16:01:21,821] [INFO] [launch.py:351:main] Process 4101180 exits successfully.
(GNER) chrisjihee@dgx-a100:~/proj/GNER$

