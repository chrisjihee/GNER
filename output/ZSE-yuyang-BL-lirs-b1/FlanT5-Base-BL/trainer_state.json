{
  "best_metric": 0.5828563108636823,
  "best_model_checkpoint": "output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434",
  "epoch": 11.986069049061175,
  "eval_steps": 500,
  "global_step": 9900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012113870381586917,
      "grad_norm": 1.1231955289840698,
      "learning_rate": 5e-05,
      "loss": 0.9929,
      "step": 10
    },
    {
      "epoch": 0.024227740763173834,
      "grad_norm": 0.36320143938064575,
      "learning_rate": 5e-05,
      "loss": 0.3456,
      "step": 20
    },
    {
      "epoch": 0.03634161114476075,
      "grad_norm": 0.27436184883117676,
      "learning_rate": 5e-05,
      "loss": 0.2248,
      "step": 30
    },
    {
      "epoch": 0.04845548152634767,
      "grad_norm": 0.24473939836025238,
      "learning_rate": 5e-05,
      "loss": 0.1857,
      "step": 40
    },
    {
      "epoch": 0.06056935190793458,
      "grad_norm": 0.1959906816482544,
      "learning_rate": 5e-05,
      "loss": 0.1611,
      "step": 50
    },
    {
      "epoch": 0.0726832222895215,
      "grad_norm": 0.2106345146894455,
      "learning_rate": 5e-05,
      "loss": 0.1447,
      "step": 60
    },
    {
      "epoch": 0.08479709267110842,
      "grad_norm": 0.20398220419883728,
      "learning_rate": 5e-05,
      "loss": 0.1318,
      "step": 70
    },
    {
      "epoch": 0.09691096305269534,
      "grad_norm": 0.1611875295639038,
      "learning_rate": 5e-05,
      "loss": 0.1214,
      "step": 80
    },
    {
      "epoch": 0.10902483343428225,
      "grad_norm": 0.16641052067279816,
      "learning_rate": 5e-05,
      "loss": 0.1182,
      "step": 90
    },
    {
      "epoch": 0.12113870381586916,
      "grad_norm": 0.16283105313777924,
      "learning_rate": 5e-05,
      "loss": 0.1135,
      "step": 100
    },
    {
      "epoch": 0.1332525741974561,
      "grad_norm": 0.13588233292102814,
      "learning_rate": 5e-05,
      "loss": 0.1078,
      "step": 110
    },
    {
      "epoch": 0.145366444579043,
      "grad_norm": 0.16472987830638885,
      "learning_rate": 5e-05,
      "loss": 0.1056,
      "step": 120
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 0.13890142738819122,
      "learning_rate": 5e-05,
      "loss": 0.0999,
      "step": 130
    },
    {
      "epoch": 0.16959418534221685,
      "grad_norm": 0.16446112096309662,
      "learning_rate": 5e-05,
      "loss": 0.0974,
      "step": 140
    },
    {
      "epoch": 0.18170805572380375,
      "grad_norm": 0.14486229419708252,
      "learning_rate": 5e-05,
      "loss": 0.0959,
      "step": 150
    },
    {
      "epoch": 0.19382192610539067,
      "grad_norm": 0.12509874999523163,
      "learning_rate": 5e-05,
      "loss": 0.0912,
      "step": 160
    },
    {
      "epoch": 0.2059357964869776,
      "grad_norm": 0.15885110199451447,
      "learning_rate": 5e-05,
      "loss": 0.0933,
      "step": 170
    },
    {
      "epoch": 0.2180496668685645,
      "grad_norm": 0.1452404409646988,
      "learning_rate": 5e-05,
      "loss": 0.0892,
      "step": 180
    },
    {
      "epoch": 0.23016353725015143,
      "grad_norm": 0.11249794811010361,
      "learning_rate": 5e-05,
      "loss": 0.0864,
      "step": 190
    },
    {
      "epoch": 0.24227740763173833,
      "grad_norm": 0.15895351767539978,
      "learning_rate": 5e-05,
      "loss": 0.0849,
      "step": 200
    },
    {
      "epoch": 0.2543912780133253,
      "grad_norm": 0.1182815358042717,
      "learning_rate": 5e-05,
      "loss": 0.0842,
      "step": 210
    },
    {
      "epoch": 0.2665051483949122,
      "grad_norm": 0.13125218451023102,
      "learning_rate": 5e-05,
      "loss": 0.0827,
      "step": 220
    },
    {
      "epoch": 0.2786190187764991,
      "grad_norm": 0.1106165274977684,
      "learning_rate": 5e-05,
      "loss": 0.0825,
      "step": 230
    },
    {
      "epoch": 0.290732889158086,
      "grad_norm": 0.136031836271286,
      "learning_rate": 5e-05,
      "loss": 0.0789,
      "step": 240
    },
    {
      "epoch": 0.30284675953967294,
      "grad_norm": 0.11000953614711761,
      "learning_rate": 5e-05,
      "loss": 0.0783,
      "step": 250
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 0.1412544846534729,
      "learning_rate": 5e-05,
      "loss": 0.0769,
      "step": 260
    },
    {
      "epoch": 0.32707450030284674,
      "grad_norm": 0.11731408536434174,
      "learning_rate": 5e-05,
      "loss": 0.0783,
      "step": 270
    },
    {
      "epoch": 0.3391883706844337,
      "grad_norm": 0.13158589601516724,
      "learning_rate": 5e-05,
      "loss": 0.0746,
      "step": 280
    },
    {
      "epoch": 0.3513022410660206,
      "grad_norm": 0.10154858231544495,
      "learning_rate": 5e-05,
      "loss": 0.0756,
      "step": 290
    },
    {
      "epoch": 0.3634161114476075,
      "grad_norm": 0.1265193074941635,
      "learning_rate": 5e-05,
      "loss": 0.0757,
      "step": 300
    },
    {
      "epoch": 0.37552998182919445,
      "grad_norm": 0.10101460665464401,
      "learning_rate": 5e-05,
      "loss": 0.0725,
      "step": 310
    },
    {
      "epoch": 0.38764385221078135,
      "grad_norm": 0.11151640862226486,
      "learning_rate": 5e-05,
      "loss": 0.0714,
      "step": 320
    },
    {
      "epoch": 0.39975772259236825,
      "grad_norm": 0.11383434385061264,
      "learning_rate": 5e-05,
      "loss": 0.0726,
      "step": 330
    },
    {
      "epoch": 0.4118715929739552,
      "grad_norm": 0.14345666766166687,
      "learning_rate": 5e-05,
      "loss": 0.0716,
      "step": 340
    },
    {
      "epoch": 0.4239854633555421,
      "grad_norm": 0.10339990258216858,
      "learning_rate": 5e-05,
      "loss": 0.0711,
      "step": 350
    },
    {
      "epoch": 0.436099333737129,
      "grad_norm": 0.11683354526758194,
      "learning_rate": 5e-05,
      "loss": 0.0683,
      "step": 360
    },
    {
      "epoch": 0.4482132041187159,
      "grad_norm": 0.11687421053647995,
      "learning_rate": 5e-05,
      "loss": 0.0698,
      "step": 370
    },
    {
      "epoch": 0.46032707450030286,
      "grad_norm": 0.11910122632980347,
      "learning_rate": 5e-05,
      "loss": 0.0694,
      "step": 380
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 0.12192738801240921,
      "learning_rate": 5e-05,
      "loss": 0.0684,
      "step": 390
    },
    {
      "epoch": 0.48455481526347666,
      "grad_norm": 0.10808023810386658,
      "learning_rate": 5e-05,
      "loss": 0.0679,
      "step": 400
    },
    {
      "epoch": 0.4966686856450636,
      "grad_norm": 0.12642380595207214,
      "learning_rate": 5e-05,
      "loss": 0.0683,
      "step": 410
    },
    {
      "epoch": 0.5087825560266506,
      "grad_norm": 0.11530129611492157,
      "learning_rate": 5e-05,
      "loss": 0.0689,
      "step": 420
    },
    {
      "epoch": 0.5208964264082374,
      "grad_norm": 0.10206703096628189,
      "learning_rate": 5e-05,
      "loss": 0.0649,
      "step": 430
    },
    {
      "epoch": 0.5330102967898244,
      "grad_norm": 0.1202523335814476,
      "learning_rate": 5e-05,
      "loss": 0.0648,
      "step": 440
    },
    {
      "epoch": 0.5451241671714112,
      "grad_norm": 0.11682991683483124,
      "learning_rate": 5e-05,
      "loss": 0.0667,
      "step": 450
    },
    {
      "epoch": 0.5572380375529982,
      "grad_norm": 0.09006969630718231,
      "learning_rate": 5e-05,
      "loss": 0.0664,
      "step": 460
    },
    {
      "epoch": 0.5693519079345851,
      "grad_norm": 0.09241899102926254,
      "learning_rate": 5e-05,
      "loss": 0.0637,
      "step": 470
    },
    {
      "epoch": 0.581465778316172,
      "grad_norm": 0.11405986547470093,
      "learning_rate": 5e-05,
      "loss": 0.0659,
      "step": 480
    },
    {
      "epoch": 0.5935796486977589,
      "grad_norm": 0.09550409018993378,
      "learning_rate": 5e-05,
      "loss": 0.0619,
      "step": 490
    },
    {
      "epoch": 0.6056935190793459,
      "grad_norm": 0.10547858476638794,
      "learning_rate": 5e-05,
      "loss": 0.0616,
      "step": 500
    },
    {
      "epoch": 0.6178073894609327,
      "grad_norm": 0.11573845893144608,
      "learning_rate": 5e-05,
      "loss": 0.0635,
      "step": 510
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 0.14170415699481964,
      "learning_rate": 5e-05,
      "loss": 0.0631,
      "step": 520
    },
    {
      "epoch": 0.6420351302241066,
      "grad_norm": 0.09501916915178299,
      "learning_rate": 5e-05,
      "loss": 0.063,
      "step": 530
    },
    {
      "epoch": 0.6541490006056935,
      "grad_norm": 0.10368267446756363,
      "learning_rate": 5e-05,
      "loss": 0.0628,
      "step": 540
    },
    {
      "epoch": 0.6662628709872804,
      "grad_norm": 0.09848456084728241,
      "learning_rate": 5e-05,
      "loss": 0.0586,
      "step": 550
    },
    {
      "epoch": 0.6783767413688674,
      "grad_norm": 0.09253591299057007,
      "learning_rate": 5e-05,
      "loss": 0.0589,
      "step": 560
    },
    {
      "epoch": 0.6904906117504542,
      "grad_norm": 0.11786065250635147,
      "learning_rate": 5e-05,
      "loss": 0.0588,
      "step": 570
    },
    {
      "epoch": 0.7026044821320412,
      "grad_norm": 0.09436038881540298,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 580
    },
    {
      "epoch": 0.7147183525136281,
      "grad_norm": 0.0981287956237793,
      "learning_rate": 5e-05,
      "loss": 0.0618,
      "step": 590
    },
    {
      "epoch": 0.726832222895215,
      "grad_norm": 0.08872020244598389,
      "learning_rate": 5e-05,
      "loss": 0.0597,
      "step": 600
    },
    {
      "epoch": 0.7389460932768019,
      "grad_norm": 0.0926903709769249,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 610
    },
    {
      "epoch": 0.7510599636583889,
      "grad_norm": 0.09287098050117493,
      "learning_rate": 5e-05,
      "loss": 0.0594,
      "step": 620
    },
    {
      "epoch": 0.7631738340399757,
      "grad_norm": 0.10343164205551147,
      "learning_rate": 5e-05,
      "loss": 0.0596,
      "step": 630
    },
    {
      "epoch": 0.7752877044215627,
      "grad_norm": 0.09680279344320297,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 640
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 0.2001344859600067,
      "learning_rate": 5e-05,
      "loss": 0.0581,
      "step": 650
    },
    {
      "epoch": 0.7995154451847365,
      "grad_norm": 0.10930377244949341,
      "learning_rate": 5e-05,
      "loss": 0.0591,
      "step": 660
    },
    {
      "epoch": 0.8116293155663235,
      "grad_norm": 0.08943571895360947,
      "learning_rate": 5e-05,
      "loss": 0.0576,
      "step": 670
    },
    {
      "epoch": 0.8237431859479104,
      "grad_norm": 0.08038229495286942,
      "learning_rate": 5e-05,
      "loss": 0.0571,
      "step": 680
    },
    {
      "epoch": 0.8358570563294972,
      "grad_norm": 0.09254691004753113,
      "learning_rate": 5e-05,
      "loss": 0.0573,
      "step": 690
    },
    {
      "epoch": 0.8479709267110842,
      "grad_norm": 0.08796384185552597,
      "learning_rate": 5e-05,
      "loss": 0.0575,
      "step": 700
    },
    {
      "epoch": 0.8600847970926712,
      "grad_norm": 0.08372613042593002,
      "learning_rate": 5e-05,
      "loss": 0.0568,
      "step": 710
    },
    {
      "epoch": 0.872198667474258,
      "grad_norm": 0.09298484772443771,
      "learning_rate": 5e-05,
      "loss": 0.0584,
      "step": 720
    },
    {
      "epoch": 0.884312537855845,
      "grad_norm": 0.07293003797531128,
      "learning_rate": 5e-05,
      "loss": 0.0569,
      "step": 730
    },
    {
      "epoch": 0.8964264082374318,
      "grad_norm": 0.09392254054546356,
      "learning_rate": 5e-05,
      "loss": 0.0564,
      "step": 740
    },
    {
      "epoch": 0.9085402786190188,
      "grad_norm": 0.09623833745718002,
      "learning_rate": 5e-05,
      "loss": 0.0552,
      "step": 750
    },
    {
      "epoch": 0.9206541490006057,
      "grad_norm": 0.10208217054605484,
      "learning_rate": 5e-05,
      "loss": 0.057,
      "step": 760
    },
    {
      "epoch": 0.9327680193821926,
      "grad_norm": 0.07811024785041809,
      "learning_rate": 5e-05,
      "loss": 0.0564,
      "step": 770
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 0.08980744332075119,
      "learning_rate": 5e-05,
      "loss": 0.0574,
      "step": 780
    },
    {
      "epoch": 0.9569957601453665,
      "grad_norm": 0.12658275663852692,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 790
    },
    {
      "epoch": 0.9691096305269533,
      "grad_norm": 0.09173087030649185,
      "learning_rate": 5e-05,
      "loss": 0.054,
      "step": 800
    },
    {
      "epoch": 0.9812235009085403,
      "grad_norm": 0.14515607059001923,
      "learning_rate": 5e-05,
      "loss": 0.0565,
      "step": 810
    },
    {
      "epoch": 0.9933373712901272,
      "grad_norm": 0.08116932213306427,
      "learning_rate": 5e-05,
      "loss": 0.0547,
      "step": 820
    },
    {
      "epoch": 1.0,
      "eval_average": 0.46309842657927475,
      "eval_crossner_ai": 0.4479638008557293,
      "eval_crossner_literature": 0.49336870021520196,
      "eval_crossner_music": 0.5691235789745283,
      "eval_crossner_politics": 0.5284360189073253,
      "eval_crossner_science": 0.6050172158874584,
      "eval_mit-movie": 0.34740259735320617,
      "eval_mit-restaurant": 0.2503770738614743,
      "eval_runtime": 171.5737,
      "eval_samples_per_second": 8.16,
      "eval_steps_per_second": 0.256,
      "step": 826
    },
    {
      "epoch": 1.0048455481526348,
      "grad_norm": 0.10511570423841476,
      "learning_rate": 5e-05,
      "loss": 0.0523,
      "step": 830
    },
    {
      "epoch": 1.0169594185342217,
      "grad_norm": 0.09476611018180847,
      "learning_rate": 5e-05,
      "loss": 0.0537,
      "step": 840
    },
    {
      "epoch": 1.0290732889158085,
      "grad_norm": 0.09046481549739838,
      "learning_rate": 5e-05,
      "loss": 0.0543,
      "step": 850
    },
    {
      "epoch": 1.0411871592973956,
      "grad_norm": 0.10475391149520874,
      "learning_rate": 5e-05,
      "loss": 0.0535,
      "step": 860
    },
    {
      "epoch": 1.0533010296789824,
      "grad_norm": 0.12361264228820801,
      "learning_rate": 5e-05,
      "loss": 0.0547,
      "step": 870
    },
    {
      "epoch": 1.0654149000605693,
      "grad_norm": 0.10582255572080612,
      "learning_rate": 5e-05,
      "loss": 0.0535,
      "step": 880
    },
    {
      "epoch": 1.0775287704421563,
      "grad_norm": 0.10560828447341919,
      "learning_rate": 5e-05,
      "loss": 0.0535,
      "step": 890
    },
    {
      "epoch": 1.0896426408237432,
      "grad_norm": 0.10679036378860474,
      "learning_rate": 5e-05,
      "loss": 0.0532,
      "step": 900
    },
    {
      "epoch": 1.10175651120533,
      "grad_norm": 0.10112778097391129,
      "learning_rate": 5e-05,
      "loss": 0.0516,
      "step": 910
    },
    {
      "epoch": 1.113870381586917,
      "grad_norm": 0.09780387580394745,
      "learning_rate": 5e-05,
      "loss": 0.0521,
      "step": 920
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 0.17328669130802155,
      "learning_rate": 5e-05,
      "loss": 0.0534,
      "step": 930
    },
    {
      "epoch": 1.1380981223500908,
      "grad_norm": 0.12370359897613525,
      "learning_rate": 5e-05,
      "loss": 0.0527,
      "step": 940
    },
    {
      "epoch": 1.1502119927316778,
      "grad_norm": 0.08213211596012115,
      "learning_rate": 5e-05,
      "loss": 0.0536,
      "step": 950
    },
    {
      "epoch": 1.1623258631132647,
      "grad_norm": 0.08561252802610397,
      "learning_rate": 5e-05,
      "loss": 0.051,
      "step": 960
    },
    {
      "epoch": 1.1744397334948515,
      "grad_norm": 0.0810365080833435,
      "learning_rate": 5e-05,
      "loss": 0.0521,
      "step": 970
    },
    {
      "epoch": 1.1865536038764386,
      "grad_norm": 0.18420428037643433,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 980
    },
    {
      "epoch": 1.1986674742580254,
      "grad_norm": 0.08886007219552994,
      "learning_rate": 5e-05,
      "loss": 0.0515,
      "step": 990
    },
    {
      "epoch": 1.2107813446396123,
      "grad_norm": 0.0862431451678276,
      "learning_rate": 5e-05,
      "loss": 0.0514,
      "step": 1000
    },
    {
      "epoch": 1.2228952150211994,
      "grad_norm": 0.06725351512432098,
      "learning_rate": 5e-05,
      "loss": 0.0516,
      "step": 1010
    },
    {
      "epoch": 1.2350090854027862,
      "grad_norm": 0.09074319899082184,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 1020
    },
    {
      "epoch": 1.247122955784373,
      "grad_norm": 0.07698699831962585,
      "learning_rate": 5e-05,
      "loss": 0.0504,
      "step": 1030
    },
    {
      "epoch": 1.25923682616596,
      "grad_norm": 0.0926545262336731,
      "learning_rate": 5e-05,
      "loss": 0.0501,
      "step": 1040
    },
    {
      "epoch": 1.271350696547547,
      "grad_norm": 0.07141199707984924,
      "learning_rate": 5e-05,
      "loss": 0.0517,
      "step": 1050
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 0.07899293303489685,
      "learning_rate": 5e-05,
      "loss": 0.0503,
      "step": 1060
    },
    {
      "epoch": 1.2955784373107209,
      "grad_norm": 0.10218171030282974,
      "learning_rate": 5e-05,
      "loss": 0.0495,
      "step": 1070
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.08747633546590805,
      "learning_rate": 5e-05,
      "loss": 0.0528,
      "step": 1080
    },
    {
      "epoch": 1.3198061780738946,
      "grad_norm": 0.07845129817724228,
      "learning_rate": 5e-05,
      "loss": 0.0488,
      "step": 1090
    },
    {
      "epoch": 1.3319200484554816,
      "grad_norm": 0.08114531636238098,
      "learning_rate": 5e-05,
      "loss": 0.0487,
      "step": 1100
    },
    {
      "epoch": 1.3440339188370685,
      "grad_norm": 0.09188779443502426,
      "learning_rate": 5e-05,
      "loss": 0.0488,
      "step": 1110
    },
    {
      "epoch": 1.3561477892186553,
      "grad_norm": 0.07478447258472443,
      "learning_rate": 5e-05,
      "loss": 0.0507,
      "step": 1120
    },
    {
      "epoch": 1.3682616596002424,
      "grad_norm": 0.06730889528989792,
      "learning_rate": 5e-05,
      "loss": 0.0489,
      "step": 1130
    },
    {
      "epoch": 1.3803755299818292,
      "grad_norm": 0.08276066184043884,
      "learning_rate": 5e-05,
      "loss": 0.0515,
      "step": 1140
    },
    {
      "epoch": 1.392489400363416,
      "grad_norm": 0.08336169272661209,
      "learning_rate": 5e-05,
      "loss": 0.0482,
      "step": 1150
    },
    {
      "epoch": 1.4046032707450031,
      "grad_norm": 0.08418937027454376,
      "learning_rate": 5e-05,
      "loss": 0.05,
      "step": 1160
    },
    {
      "epoch": 1.41671714112659,
      "grad_norm": 0.08711593598127365,
      "learning_rate": 5e-05,
      "loss": 0.0493,
      "step": 1170
    },
    {
      "epoch": 1.4288310115081768,
      "grad_norm": 0.07757661491632462,
      "learning_rate": 5e-05,
      "loss": 0.0502,
      "step": 1180
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 0.08124988526105881,
      "learning_rate": 5e-05,
      "loss": 0.0472,
      "step": 1190
    },
    {
      "epoch": 1.4530587522713507,
      "grad_norm": 0.08967266976833344,
      "learning_rate": 5e-05,
      "loss": 0.0491,
      "step": 1200
    },
    {
      "epoch": 1.4651726226529376,
      "grad_norm": 0.09008904546499252,
      "learning_rate": 5e-05,
      "loss": 0.0474,
      "step": 1210
    },
    {
      "epoch": 1.4772864930345246,
      "grad_norm": 0.09094715863466263,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 1220
    },
    {
      "epoch": 1.4894003634161115,
      "grad_norm": 0.08747633546590805,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 1230
    },
    {
      "epoch": 1.5015142337976983,
      "grad_norm": 0.08034519106149673,
      "learning_rate": 5e-05,
      "loss": 0.0484,
      "step": 1240
    },
    {
      "epoch": 1.5136281041792854,
      "grad_norm": 0.07444705069065094,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 1250
    },
    {
      "epoch": 1.5257419745608722,
      "grad_norm": 0.07471435517072678,
      "learning_rate": 5e-05,
      "loss": 0.0485,
      "step": 1260
    },
    {
      "epoch": 1.537855844942459,
      "grad_norm": 0.09149091690778732,
      "learning_rate": 5e-05,
      "loss": 0.0492,
      "step": 1270
    },
    {
      "epoch": 1.5499697153240461,
      "grad_norm": 0.07705176621675491,
      "learning_rate": 5e-05,
      "loss": 0.0496,
      "step": 1280
    },
    {
      "epoch": 1.562083585705633,
      "grad_norm": 0.06553028523921967,
      "learning_rate": 5e-05,
      "loss": 0.0483,
      "step": 1290
    },
    {
      "epoch": 1.5741974560872198,
      "grad_norm": 0.09903009235858917,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 1300
    },
    {
      "epoch": 1.586311326468807,
      "grad_norm": 0.08431965112686157,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 1310
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 0.09428584575653076,
      "learning_rate": 5e-05,
      "loss": 0.0487,
      "step": 1320
    },
    {
      "epoch": 1.6105390672319806,
      "grad_norm": 0.07093799114227295,
      "learning_rate": 5e-05,
      "loss": 0.0468,
      "step": 1330
    },
    {
      "epoch": 1.6226529376135677,
      "grad_norm": 0.07926428318023682,
      "learning_rate": 5e-05,
      "loss": 0.0483,
      "step": 1340
    },
    {
      "epoch": 1.6347668079951545,
      "grad_norm": 0.0769810900092125,
      "learning_rate": 5e-05,
      "loss": 0.0478,
      "step": 1350
    },
    {
      "epoch": 1.6468806783767413,
      "grad_norm": 0.07047799229621887,
      "learning_rate": 5e-05,
      "loss": 0.0481,
      "step": 1360
    },
    {
      "epoch": 1.6589945487583284,
      "grad_norm": 0.08091410994529724,
      "learning_rate": 5e-05,
      "loss": 0.0472,
      "step": 1370
    },
    {
      "epoch": 1.6711084191399153,
      "grad_norm": 0.07779332995414734,
      "learning_rate": 5e-05,
      "loss": 0.0462,
      "step": 1380
    },
    {
      "epoch": 1.683222289521502,
      "grad_norm": 0.10377072542905807,
      "learning_rate": 5e-05,
      "loss": 0.0466,
      "step": 1390
    },
    {
      "epoch": 1.6953361599030892,
      "grad_norm": 0.09074541181325912,
      "learning_rate": 5e-05,
      "loss": 0.0487,
      "step": 1400
    },
    {
      "epoch": 1.7074500302846758,
      "grad_norm": 0.08987781405448914,
      "learning_rate": 5e-05,
      "loss": 0.047,
      "step": 1410
    },
    {
      "epoch": 1.7195639006662629,
      "grad_norm": 0.06394480168819427,
      "learning_rate": 5e-05,
      "loss": 0.0466,
      "step": 1420
    },
    {
      "epoch": 1.73167777104785,
      "grad_norm": 0.07004043459892273,
      "learning_rate": 5e-05,
      "loss": 0.0472,
      "step": 1430
    },
    {
      "epoch": 1.7437916414294365,
      "grad_norm": 0.11847082525491714,
      "learning_rate": 5e-05,
      "loss": 0.0464,
      "step": 1440
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 0.10330227762460709,
      "learning_rate": 5e-05,
      "loss": 0.0482,
      "step": 1450
    },
    {
      "epoch": 1.7680193821926107,
      "grad_norm": 0.051212068647146225,
      "learning_rate": 5e-05,
      "loss": 0.0472,
      "step": 1460
    },
    {
      "epoch": 1.7801332525741973,
      "grad_norm": 0.059796739369630814,
      "learning_rate": 5e-05,
      "loss": 0.0474,
      "step": 1470
    },
    {
      "epoch": 1.7922471229557844,
      "grad_norm": 0.06739230453968048,
      "learning_rate": 5e-05,
      "loss": 0.0444,
      "step": 1480
    },
    {
      "epoch": 1.8043609933373714,
      "grad_norm": 0.0879201665520668,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 1490
    },
    {
      "epoch": 1.816474863718958,
      "grad_norm": 0.06901533901691437,
      "learning_rate": 5e-05,
      "loss": 0.048,
      "step": 1500
    },
    {
      "epoch": 1.8285887341005451,
      "grad_norm": 0.07394358515739441,
      "learning_rate": 5e-05,
      "loss": 0.0459,
      "step": 1510
    },
    {
      "epoch": 1.8407026044821322,
      "grad_norm": 0.1030767485499382,
      "learning_rate": 5e-05,
      "loss": 0.0489,
      "step": 1520
    },
    {
      "epoch": 1.8528164748637188,
      "grad_norm": 0.07667740434408188,
      "learning_rate": 5e-05,
      "loss": 0.0451,
      "step": 1530
    },
    {
      "epoch": 1.8649303452453059,
      "grad_norm": 0.07394170016050339,
      "learning_rate": 5e-05,
      "loss": 0.0462,
      "step": 1540
    },
    {
      "epoch": 1.877044215626893,
      "grad_norm": 0.07473903149366379,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 1550
    },
    {
      "epoch": 1.8891580860084796,
      "grad_norm": 0.10049538314342499,
      "learning_rate": 5e-05,
      "loss": 0.0475,
      "step": 1560
    },
    {
      "epoch": 1.9012719563900666,
      "grad_norm": 0.07185685634613037,
      "learning_rate": 5e-05,
      "loss": 0.0452,
      "step": 1570
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 0.08475207537412643,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 1580
    },
    {
      "epoch": 1.9254996971532403,
      "grad_norm": 0.0891181007027626,
      "learning_rate": 5e-05,
      "loss": 0.0468,
      "step": 1590
    },
    {
      "epoch": 1.9376135675348274,
      "grad_norm": 0.06952392309904099,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 1600
    },
    {
      "epoch": 1.9497274379164145,
      "grad_norm": 0.0880015417933464,
      "learning_rate": 5e-05,
      "loss": 0.0444,
      "step": 1610
    },
    {
      "epoch": 1.961841308298001,
      "grad_norm": 0.07869972288608551,
      "learning_rate": 5e-05,
      "loss": 0.0449,
      "step": 1620
    },
    {
      "epoch": 1.9739551786795881,
      "grad_norm": 0.089900903403759,
      "learning_rate": 5e-05,
      "loss": 0.046,
      "step": 1630
    },
    {
      "epoch": 1.9860690490611752,
      "grad_norm": 0.07255958020687103,
      "learning_rate": 5e-05,
      "loss": 0.0454,
      "step": 1640
    },
    {
      "epoch": 1.9981829194427618,
      "grad_norm": 0.06812942028045654,
      "learning_rate": 5e-05,
      "loss": 0.0463,
      "step": 1650
    },
    {
      "epoch": 2.0,
      "eval_average": 0.5089586966210063,
      "eval_crossner_ai": 0.4917284654383899,
      "eval_crossner_literature": 0.5521339815815232,
      "eval_crossner_music": 0.6717668487659805,
      "eval_crossner_politics": 0.5508170585390421,
      "eval_crossner_science": 0.6596173211989675,
      "eval_mit-movie": 0.3934426229015507,
      "eval_mit-restaurant": 0.24320457792159084,
      "eval_runtime": 170.5181,
      "eval_samples_per_second": 8.21,
      "eval_steps_per_second": 0.258,
      "step": 1652
    },
    {
      "epoch": 2.0096910963052697,
      "grad_norm": 0.11050412803888321,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 1660
    },
    {
      "epoch": 2.0218049666868563,
      "grad_norm": 0.07062633335590363,
      "learning_rate": 5e-05,
      "loss": 0.043,
      "step": 1670
    },
    {
      "epoch": 2.0339188370684433,
      "grad_norm": 0.07602851092815399,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 1680
    },
    {
      "epoch": 2.0460327074500304,
      "grad_norm": 0.08186434954404831,
      "learning_rate": 5e-05,
      "loss": 0.0449,
      "step": 1690
    },
    {
      "epoch": 2.058146577831617,
      "grad_norm": 0.057608507573604584,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 1700
    },
    {
      "epoch": 2.070260448213204,
      "grad_norm": 0.0803079679608345,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 1710
    },
    {
      "epoch": 2.082374318594791,
      "grad_norm": 0.07946044206619263,
      "learning_rate": 5e-05,
      "loss": 0.0438,
      "step": 1720
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 0.07087314128875732,
      "learning_rate": 5e-05,
      "loss": 0.0451,
      "step": 1730
    },
    {
      "epoch": 2.106602059357965,
      "grad_norm": 0.07324565947055817,
      "learning_rate": 5e-05,
      "loss": 0.045,
      "step": 1740
    },
    {
      "epoch": 2.118715929739552,
      "grad_norm": 0.07562920451164246,
      "learning_rate": 5e-05,
      "loss": 0.0442,
      "step": 1750
    },
    {
      "epoch": 2.1308298001211385,
      "grad_norm": 0.08047837764024734,
      "learning_rate": 5e-05,
      "loss": 0.0426,
      "step": 1760
    },
    {
      "epoch": 2.1429436705027256,
      "grad_norm": 0.06774706393480301,
      "learning_rate": 5e-05,
      "loss": 0.0438,
      "step": 1770
    },
    {
      "epoch": 2.1550575408843127,
      "grad_norm": 0.06637302786111832,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 1780
    },
    {
      "epoch": 2.1671714112658993,
      "grad_norm": 0.09290299564599991,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 1790
    },
    {
      "epoch": 2.1792852816474864,
      "grad_norm": 0.06540313363075256,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 1800
    },
    {
      "epoch": 2.1913991520290734,
      "grad_norm": 0.07927723228931427,
      "learning_rate": 5e-05,
      "loss": 0.0434,
      "step": 1810
    },
    {
      "epoch": 2.20351302241066,
      "grad_norm": 0.060770388692617416,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 1820
    },
    {
      "epoch": 2.215626892792247,
      "grad_norm": 0.06599054485559464,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 1830
    },
    {
      "epoch": 2.227740763173834,
      "grad_norm": 0.061366256326436996,
      "learning_rate": 5e-05,
      "loss": 0.0447,
      "step": 1840
    },
    {
      "epoch": 2.239854633555421,
      "grad_norm": 0.101987324655056,
      "learning_rate": 5e-05,
      "loss": 0.0435,
      "step": 1850
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 0.07306468486785889,
      "learning_rate": 5e-05,
      "loss": 0.0442,
      "step": 1860
    },
    {
      "epoch": 2.264082374318595,
      "grad_norm": 0.08013356477022171,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 1870
    },
    {
      "epoch": 2.2761962447001816,
      "grad_norm": 0.09838510304689407,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 1880
    },
    {
      "epoch": 2.2883101150817686,
      "grad_norm": 0.10103243589401245,
      "learning_rate": 5e-05,
      "loss": 0.0422,
      "step": 1890
    },
    {
      "epoch": 2.3004239854633557,
      "grad_norm": 0.10218672454357147,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 1900
    },
    {
      "epoch": 2.3125378558449423,
      "grad_norm": 0.07568024098873138,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 1910
    },
    {
      "epoch": 2.3246517262265294,
      "grad_norm": 0.07286437600851059,
      "learning_rate": 5e-05,
      "loss": 0.0433,
      "step": 1920
    },
    {
      "epoch": 2.3367655966081164,
      "grad_norm": 0.05859380587935448,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 1930
    },
    {
      "epoch": 2.348879466989703,
      "grad_norm": 0.07205978035926819,
      "learning_rate": 5e-05,
      "loss": 0.0444,
      "step": 1940
    },
    {
      "epoch": 2.36099333737129,
      "grad_norm": 0.08377410471439362,
      "learning_rate": 5e-05,
      "loss": 0.0435,
      "step": 1950
    },
    {
      "epoch": 2.373107207752877,
      "grad_norm": 0.09142501652240753,
      "learning_rate": 5e-05,
      "loss": 0.0425,
      "step": 1960
    },
    {
      "epoch": 2.385221078134464,
      "grad_norm": 0.0794559046626091,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 1970
    },
    {
      "epoch": 2.397334948516051,
      "grad_norm": 0.0642211064696312,
      "learning_rate": 5e-05,
      "loss": 0.0415,
      "step": 1980
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 0.09412938356399536,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 1990
    },
    {
      "epoch": 2.4215626892792246,
      "grad_norm": 0.07559072971343994,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 2000
    },
    {
      "epoch": 2.4336765596608116,
      "grad_norm": 0.07242119312286377,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 2010
    },
    {
      "epoch": 2.4457904300423987,
      "grad_norm": 0.09738170355558395,
      "learning_rate": 5e-05,
      "loss": 0.042,
      "step": 2020
    },
    {
      "epoch": 2.4579043004239853,
      "grad_norm": 0.0680798664689064,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 2030
    },
    {
      "epoch": 2.4700181708055724,
      "grad_norm": 0.06891747564077377,
      "learning_rate": 5e-05,
      "loss": 0.0426,
      "step": 2040
    },
    {
      "epoch": 2.4821320411871595,
      "grad_norm": 0.07033143192529678,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 2050
    },
    {
      "epoch": 2.494245911568746,
      "grad_norm": 0.06872926652431488,
      "learning_rate": 5e-05,
      "loss": 0.0433,
      "step": 2060
    },
    {
      "epoch": 2.506359781950333,
      "grad_norm": 0.10200957953929901,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 2070
    },
    {
      "epoch": 2.51847365233192,
      "grad_norm": 0.0879008024930954,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 2080
    },
    {
      "epoch": 2.530587522713507,
      "grad_norm": 0.07831040024757385,
      "learning_rate": 5e-05,
      "loss": 0.0415,
      "step": 2090
    },
    {
      "epoch": 2.542701393095094,
      "grad_norm": 0.08252395689487457,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 2100
    },
    {
      "epoch": 2.554815263476681,
      "grad_norm": 0.070833221077919,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 2110
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 0.07966975122690201,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 2120
    },
    {
      "epoch": 2.5790430042398547,
      "grad_norm": 0.07246079295873642,
      "learning_rate": 5e-05,
      "loss": 0.0405,
      "step": 2130
    },
    {
      "epoch": 2.5911568746214417,
      "grad_norm": 0.05706777423620224,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 2140
    },
    {
      "epoch": 2.6032707450030284,
      "grad_norm": 0.08290386199951172,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 2150
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 0.07806867361068726,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 2160
    },
    {
      "epoch": 2.6274984857662025,
      "grad_norm": 0.07928516715765,
      "learning_rate": 5e-05,
      "loss": 0.042,
      "step": 2170
    },
    {
      "epoch": 2.639612356147789,
      "grad_norm": 0.09527462720870972,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 2180
    },
    {
      "epoch": 2.651726226529376,
      "grad_norm": 0.09133661538362503,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 2190
    },
    {
      "epoch": 2.6638400969109632,
      "grad_norm": 0.0685315728187561,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 2200
    },
    {
      "epoch": 2.67595396729255,
      "grad_norm": 0.0689762756228447,
      "learning_rate": 5e-05,
      "loss": 0.0415,
      "step": 2210
    },
    {
      "epoch": 2.688067837674137,
      "grad_norm": 0.0772443637251854,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 2220
    },
    {
      "epoch": 2.700181708055724,
      "grad_norm": 0.06273241341114044,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 2230
    },
    {
      "epoch": 2.7122955784373106,
      "grad_norm": 0.062179356813430786,
      "learning_rate": 5e-05,
      "loss": 0.042,
      "step": 2240
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 0.0674441009759903,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 2250
    },
    {
      "epoch": 2.7365233192004847,
      "grad_norm": 0.06396576762199402,
      "learning_rate": 5e-05,
      "loss": 0.0408,
      "step": 2260
    },
    {
      "epoch": 2.7486371895820714,
      "grad_norm": 0.05491562560200691,
      "learning_rate": 5e-05,
      "loss": 0.0413,
      "step": 2270
    },
    {
      "epoch": 2.7607510599636584,
      "grad_norm": 0.07199830561876297,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 2280
    },
    {
      "epoch": 2.7728649303452455,
      "grad_norm": 0.07401486486196518,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 2290
    },
    {
      "epoch": 2.784978800726832,
      "grad_norm": 0.062369946390390396,
      "learning_rate": 5e-05,
      "loss": 0.0418,
      "step": 2300
    },
    {
      "epoch": 2.797092671108419,
      "grad_norm": 0.08228761702775955,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 2310
    },
    {
      "epoch": 2.8092065414900063,
      "grad_norm": 0.1075640618801117,
      "learning_rate": 5e-05,
      "loss": 0.041,
      "step": 2320
    },
    {
      "epoch": 2.821320411871593,
      "grad_norm": 0.06543296575546265,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 2330
    },
    {
      "epoch": 2.83343428225318,
      "grad_norm": 0.09066629409790039,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 2340
    },
    {
      "epoch": 2.845548152634767,
      "grad_norm": 0.06460697948932648,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 2350
    },
    {
      "epoch": 2.8576620230163536,
      "grad_norm": 0.07566122710704803,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 2360
    },
    {
      "epoch": 2.8697758933979407,
      "grad_norm": 0.08254899829626083,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 2370
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 0.07433696836233139,
      "learning_rate": 5e-05,
      "loss": 0.042,
      "step": 2380
    },
    {
      "epoch": 2.8940036341611144,
      "grad_norm": 0.09250903129577637,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 2390
    },
    {
      "epoch": 2.9061175045427015,
      "grad_norm": 0.06869848072528839,
      "learning_rate": 5e-05,
      "loss": 0.0398,
      "step": 2400
    },
    {
      "epoch": 2.9182313749242885,
      "grad_norm": 0.08274595439434052,
      "learning_rate": 5e-05,
      "loss": 0.0401,
      "step": 2410
    },
    {
      "epoch": 2.930345245305875,
      "grad_norm": 0.08074983209371567,
      "learning_rate": 5e-05,
      "loss": 0.0412,
      "step": 2420
    },
    {
      "epoch": 2.942459115687462,
      "grad_norm": 0.06028766930103302,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 2430
    },
    {
      "epoch": 2.9545729860690493,
      "grad_norm": 0.0691489726305008,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 2440
    },
    {
      "epoch": 2.966686856450636,
      "grad_norm": 0.06507962942123413,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 2450
    },
    {
      "epoch": 2.978800726832223,
      "grad_norm": 0.07580189406871796,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 2460
    },
    {
      "epoch": 2.99091459721381,
      "grad_norm": 0.08947882056236267,
      "learning_rate": 5e-05,
      "loss": 0.0398,
      "step": 2470
    },
    {
      "epoch": 3.0,
      "eval_average": 0.5440497558323928,
      "eval_crossner_ai": 0.5162037036542109,
      "eval_crossner_literature": 0.5770277626064974,
      "eval_crossner_music": 0.7144416150827244,
      "eval_crossner_politics": 0.5777243589243147,
      "eval_crossner_science": 0.6700404857801497,
      "eval_mit-movie": 0.46964856225069207,
      "eval_mit-restaurant": 0.2832618025281602,
      "eval_runtime": 169.2693,
      "eval_samples_per_second": 8.271,
      "eval_steps_per_second": 0.26,
      "step": 2478
    },
    {
      "epoch": 3.0024227740763174,
      "grad_norm": 0.07894071191549301,
      "learning_rate": 5e-05,
      "loss": 0.0403,
      "step": 2480
    },
    {
      "epoch": 3.0145366444579045,
      "grad_norm": 0.056727904826402664,
      "learning_rate": 5e-05,
      "loss": 0.0393,
      "step": 2490
    },
    {
      "epoch": 3.026650514839491,
      "grad_norm": 0.07224751263856888,
      "learning_rate": 5e-05,
      "loss": 0.0394,
      "step": 2500
    },
    {
      "epoch": 3.038764385221078,
      "grad_norm": 0.06338918954133987,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 2510
    },
    {
      "epoch": 3.0508782556026652,
      "grad_norm": 0.09432613104581833,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 2520
    },
    {
      "epoch": 3.062992125984252,
      "grad_norm": 0.08585280925035477,
      "learning_rate": 5e-05,
      "loss": 0.0401,
      "step": 2530
    },
    {
      "epoch": 3.075105996365839,
      "grad_norm": 0.07135462015867233,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 2540
    },
    {
      "epoch": 3.087219866747426,
      "grad_norm": 0.08015482127666473,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 2550
    },
    {
      "epoch": 3.0993337371290126,
      "grad_norm": 0.0627385750412941,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 2560
    },
    {
      "epoch": 3.1114476075105997,
      "grad_norm": 0.06955601274967194,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 2570
    },
    {
      "epoch": 3.1235614778921867,
      "grad_norm": 0.0584123469889164,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 2580
    },
    {
      "epoch": 3.1356753482737734,
      "grad_norm": 0.08029953390359879,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 2590
    },
    {
      "epoch": 3.1477892186553604,
      "grad_norm": 0.05401528254151344,
      "learning_rate": 5e-05,
      "loss": 0.0413,
      "step": 2600
    },
    {
      "epoch": 3.1599030890369475,
      "grad_norm": 0.057498861104249954,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 2610
    },
    {
      "epoch": 3.172016959418534,
      "grad_norm": 0.08649046719074249,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 2620
    },
    {
      "epoch": 3.184130829800121,
      "grad_norm": 0.06828688085079193,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 2630
    },
    {
      "epoch": 3.1962447001817083,
      "grad_norm": 0.06718344241380692,
      "learning_rate": 5e-05,
      "loss": 0.0392,
      "step": 2640
    },
    {
      "epoch": 3.208358570563295,
      "grad_norm": 0.058513764292001724,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 2650
    },
    {
      "epoch": 3.220472440944882,
      "grad_norm": 0.10257740318775177,
      "learning_rate": 5e-05,
      "loss": 0.0392,
      "step": 2660
    },
    {
      "epoch": 3.232586311326469,
      "grad_norm": 0.07826653867959976,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 2670
    },
    {
      "epoch": 3.2447001817080556,
      "grad_norm": 0.08263114094734192,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 2680
    },
    {
      "epoch": 3.2568140520896427,
      "grad_norm": 0.07392879575490952,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 2690
    },
    {
      "epoch": 3.2689279224712298,
      "grad_norm": 0.07588353753089905,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 2700
    },
    {
      "epoch": 3.2810417928528164,
      "grad_norm": 0.06488131731748581,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 2710
    },
    {
      "epoch": 3.2931556632344035,
      "grad_norm": 0.0634593665599823,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 2720
    },
    {
      "epoch": 3.3052695336159905,
      "grad_norm": 0.05965595319867134,
      "learning_rate": 5e-05,
      "loss": 0.0392,
      "step": 2730
    },
    {
      "epoch": 3.317383403997577,
      "grad_norm": 0.05992872640490532,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 2740
    },
    {
      "epoch": 3.329497274379164,
      "grad_norm": 0.0689573734998703,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 2750
    },
    {
      "epoch": 3.3416111447607513,
      "grad_norm": 0.1088191419839859,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 2760
    },
    {
      "epoch": 3.353725015142338,
      "grad_norm": 0.07797641307115555,
      "learning_rate": 5e-05,
      "loss": 0.0389,
      "step": 2770
    },
    {
      "epoch": 3.365838885523925,
      "grad_norm": 0.07485855370759964,
      "learning_rate": 5e-05,
      "loss": 0.0388,
      "step": 2780
    },
    {
      "epoch": 3.377952755905512,
      "grad_norm": 0.10092201083898544,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 2790
    },
    {
      "epoch": 3.3900666262870987,
      "grad_norm": 0.0956154614686966,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 2800
    },
    {
      "epoch": 3.4021804966686857,
      "grad_norm": 0.06969165802001953,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 2810
    },
    {
      "epoch": 3.414294367050273,
      "grad_norm": 0.06005346029996872,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 2820
    },
    {
      "epoch": 3.4264082374318594,
      "grad_norm": 0.06673241406679153,
      "learning_rate": 5e-05,
      "loss": 0.0399,
      "step": 2830
    },
    {
      "epoch": 3.4385221078134465,
      "grad_norm": 0.06342742592096329,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 2840
    },
    {
      "epoch": 3.4506359781950335,
      "grad_norm": 0.07414077967405319,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 2850
    },
    {
      "epoch": 3.46274984857662,
      "grad_norm": 0.06548458337783813,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 2860
    },
    {
      "epoch": 3.4748637189582072,
      "grad_norm": 0.08048581331968307,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 2870
    },
    {
      "epoch": 3.4869775893397943,
      "grad_norm": 0.06489235907793045,
      "learning_rate": 5e-05,
      "loss": 0.0388,
      "step": 2880
    },
    {
      "epoch": 3.499091459721381,
      "grad_norm": 0.05735320597887039,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 2890
    },
    {
      "epoch": 3.511205330102968,
      "grad_norm": 0.0658232644200325,
      "learning_rate": 5e-05,
      "loss": 0.0393,
      "step": 2900
    },
    {
      "epoch": 3.523319200484555,
      "grad_norm": 0.08666154742240906,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 2910
    },
    {
      "epoch": 3.5354330708661417,
      "grad_norm": 0.08764012157917023,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 2920
    },
    {
      "epoch": 3.5475469412477287,
      "grad_norm": 0.07777571678161621,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 2930
    },
    {
      "epoch": 3.559660811629316,
      "grad_norm": 0.07883165776729584,
      "learning_rate": 5e-05,
      "loss": 0.0403,
      "step": 2940
    },
    {
      "epoch": 3.5717746820109024,
      "grad_norm": 0.07854263484477997,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 2950
    },
    {
      "epoch": 3.5838885523924895,
      "grad_norm": 0.06838522851467133,
      "learning_rate": 5e-05,
      "loss": 0.0386,
      "step": 2960
    },
    {
      "epoch": 3.5960024227740766,
      "grad_norm": 0.06284311413764954,
      "learning_rate": 5e-05,
      "loss": 0.038,
      "step": 2970
    },
    {
      "epoch": 3.608116293155663,
      "grad_norm": 0.07907837629318237,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 2980
    },
    {
      "epoch": 3.6202301635372502,
      "grad_norm": 0.06284821778535843,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 2990
    },
    {
      "epoch": 3.6323440339188373,
      "grad_norm": 0.07565002143383026,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 3000
    },
    {
      "epoch": 3.644457904300424,
      "grad_norm": 0.08774754405021667,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 3010
    },
    {
      "epoch": 3.656571774682011,
      "grad_norm": 0.08861799538135529,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 3020
    },
    {
      "epoch": 3.668685645063598,
      "grad_norm": 0.05416255071759224,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 3030
    },
    {
      "epoch": 3.6807995154451847,
      "grad_norm": 0.09022647142410278,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 3040
    },
    {
      "epoch": 3.6929133858267718,
      "grad_norm": 0.06627310812473297,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 3050
    },
    {
      "epoch": 3.705027256208359,
      "grad_norm": 0.07129941135644913,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 3060
    },
    {
      "epoch": 3.7171411265899454,
      "grad_norm": 0.06290658563375473,
      "learning_rate": 5e-05,
      "loss": 0.0386,
      "step": 3070
    },
    {
      "epoch": 3.7292549969715325,
      "grad_norm": 0.05709993094205856,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 3080
    },
    {
      "epoch": 3.7413688673531196,
      "grad_norm": 0.0699431523680687,
      "learning_rate": 5e-05,
      "loss": 0.0398,
      "step": 3090
    },
    {
      "epoch": 3.753482737734706,
      "grad_norm": 0.0638478621840477,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 3100
    },
    {
      "epoch": 3.7655966081162933,
      "grad_norm": 0.07748083025217056,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 3110
    },
    {
      "epoch": 3.7777104784978803,
      "grad_norm": 0.0627596452832222,
      "learning_rate": 5e-05,
      "loss": 0.0369,
      "step": 3120
    },
    {
      "epoch": 3.789824348879467,
      "grad_norm": 0.07694362103939056,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 3130
    },
    {
      "epoch": 3.801938219261054,
      "grad_norm": 0.16713503003120422,
      "learning_rate": 5e-05,
      "loss": 0.0386,
      "step": 3140
    },
    {
      "epoch": 3.814052089642641,
      "grad_norm": 0.05990184471011162,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 3150
    },
    {
      "epoch": 3.8261659600242277,
      "grad_norm": 0.07232487201690674,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 3160
    },
    {
      "epoch": 3.8382798304058148,
      "grad_norm": 0.06580331921577454,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 3170
    },
    {
      "epoch": 3.850393700787402,
      "grad_norm": 0.05995628237724304,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 3180
    },
    {
      "epoch": 3.8625075711689885,
      "grad_norm": 0.06887157261371613,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 3190
    },
    {
      "epoch": 3.8746214415505755,
      "grad_norm": 0.0566130094230175,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 3200
    },
    {
      "epoch": 3.8867353119321626,
      "grad_norm": 0.08304843306541443,
      "learning_rate": 5e-05,
      "loss": 0.0383,
      "step": 3210
    },
    {
      "epoch": 3.898849182313749,
      "grad_norm": 0.0714212954044342,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 3220
    },
    {
      "epoch": 3.9109630526953363,
      "grad_norm": 0.06923269480466843,
      "learning_rate": 5e-05,
      "loss": 0.0404,
      "step": 3230
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 0.07069318741559982,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 3240
    },
    {
      "epoch": 3.93519079345851,
      "grad_norm": 0.07900255173444748,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3250
    },
    {
      "epoch": 3.947304663840097,
      "grad_norm": 0.06378614902496338,
      "learning_rate": 5e-05,
      "loss": 0.0377,
      "step": 3260
    },
    {
      "epoch": 3.959418534221684,
      "grad_norm": 0.08539889007806778,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3270
    },
    {
      "epoch": 3.9715324046032707,
      "grad_norm": 0.06615135818719864,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 3280
    },
    {
      "epoch": 3.983646274984858,
      "grad_norm": 0.053014446049928665,
      "learning_rate": 5e-05,
      "loss": 0.0382,
      "step": 3290
    },
    {
      "epoch": 3.9957601453664444,
      "grad_norm": 0.10114047676324844,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 3300
    },
    {
      "epoch": 4.0,
      "eval_average": 0.551242713885457,
      "eval_crossner_ai": 0.5135603000082387,
      "eval_crossner_literature": 0.5826086956021602,
      "eval_crossner_music": 0.7254830477078766,
      "eval_crossner_politics": 0.5689172991908696,
      "eval_crossner_science": 0.674723061380235,
      "eval_mit-movie": 0.4857142856645851,
      "eval_mit-restaurant": 0.3076923076442341,
      "eval_runtime": 170.3816,
      "eval_samples_per_second": 8.217,
      "eval_steps_per_second": 0.258,
      "step": 3304
    },
    {
      "epoch": 4.007268322228952,
      "grad_norm": 0.12954266369342804,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 3310
    },
    {
      "epoch": 4.019382192610539,
      "grad_norm": 0.062412962317466736,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 3320
    },
    {
      "epoch": 4.031496062992126,
      "grad_norm": 0.07471810281276703,
      "learning_rate": 5e-05,
      "loss": 0.0371,
      "step": 3330
    },
    {
      "epoch": 4.0436099333737126,
      "grad_norm": 0.07414764910936356,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 3340
    },
    {
      "epoch": 4.0557238037553,
      "grad_norm": 0.058271221816539764,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3350
    },
    {
      "epoch": 4.067837674136887,
      "grad_norm": 0.06045720726251602,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 3360
    },
    {
      "epoch": 4.079951544518473,
      "grad_norm": 0.07223706692457199,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 3370
    },
    {
      "epoch": 4.092065414900061,
      "grad_norm": 0.05894572660326958,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 3380
    },
    {
      "epoch": 4.104179285281647,
      "grad_norm": 0.07682913541793823,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 3390
    },
    {
      "epoch": 4.116293155663234,
      "grad_norm": 0.0772736668586731,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3400
    },
    {
      "epoch": 4.128407026044822,
      "grad_norm": 0.058474257588386536,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 3410
    },
    {
      "epoch": 4.140520896426408,
      "grad_norm": 0.07631388306617737,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3420
    },
    {
      "epoch": 4.152634766807995,
      "grad_norm": 0.07159172743558884,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 3430
    },
    {
      "epoch": 4.164748637189582,
      "grad_norm": 0.09671995043754578,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 3440
    },
    {
      "epoch": 4.176862507571169,
      "grad_norm": 0.08231812715530396,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 3450
    },
    {
      "epoch": 4.188976377952756,
      "grad_norm": 0.07241758704185486,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 3460
    },
    {
      "epoch": 4.201090248334343,
      "grad_norm": 0.06194751709699631,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3470
    },
    {
      "epoch": 4.21320411871593,
      "grad_norm": 0.058275461196899414,
      "learning_rate": 5e-05,
      "loss": 0.0362,
      "step": 3480
    },
    {
      "epoch": 4.225317989097516,
      "grad_norm": 0.07637191563844681,
      "learning_rate": 5e-05,
      "loss": 0.037,
      "step": 3490
    },
    {
      "epoch": 4.237431859479104,
      "grad_norm": 0.06607170403003693,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 3500
    },
    {
      "epoch": 4.2495457298606905,
      "grad_norm": 0.059966616332530975,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 3510
    },
    {
      "epoch": 4.261659600242277,
      "grad_norm": 0.08156190812587738,
      "learning_rate": 5e-05,
      "loss": 0.0362,
      "step": 3520
    },
    {
      "epoch": 4.273773470623865,
      "grad_norm": 0.05832863599061966,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 3530
    },
    {
      "epoch": 4.285887341005451,
      "grad_norm": 0.054892878979444504,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3540
    },
    {
      "epoch": 4.298001211387038,
      "grad_norm": 0.07611411809921265,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 3550
    },
    {
      "epoch": 4.310115081768625,
      "grad_norm": 0.04923466965556145,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 3560
    },
    {
      "epoch": 4.322228952150212,
      "grad_norm": 0.07829022407531738,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 3570
    },
    {
      "epoch": 4.334342822531799,
      "grad_norm": 0.06911627948284149,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 3580
    },
    {
      "epoch": 4.346456692913386,
      "grad_norm": 0.06274263560771942,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 3590
    },
    {
      "epoch": 4.358570563294973,
      "grad_norm": 0.06479807943105698,
      "learning_rate": 5e-05,
      "loss": 0.0369,
      "step": 3600
    },
    {
      "epoch": 4.370684433676559,
      "grad_norm": 0.07635167241096497,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 3610
    },
    {
      "epoch": 4.382798304058147,
      "grad_norm": 0.06996562331914902,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 3620
    },
    {
      "epoch": 4.3949121744397335,
      "grad_norm": 0.060138456523418427,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 3630
    },
    {
      "epoch": 4.40702604482132,
      "grad_norm": 0.08563349395990372,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 3640
    },
    {
      "epoch": 4.419139915202908,
      "grad_norm": 0.08151303976774216,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 3650
    },
    {
      "epoch": 4.431253785584494,
      "grad_norm": 0.07552094012498856,
      "learning_rate": 5e-05,
      "loss": 0.0362,
      "step": 3660
    },
    {
      "epoch": 4.443367655966081,
      "grad_norm": 0.05844743177294731,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 3670
    },
    {
      "epoch": 4.455481526347668,
      "grad_norm": 0.060145895928144455,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 3680
    },
    {
      "epoch": 4.467595396729255,
      "grad_norm": 0.054422527551651,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 3690
    },
    {
      "epoch": 4.479709267110842,
      "grad_norm": 0.06684521585702896,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 3700
    },
    {
      "epoch": 4.491823137492429,
      "grad_norm": 0.07206249237060547,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 3710
    },
    {
      "epoch": 4.503937007874016,
      "grad_norm": 0.04889504238963127,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 3720
    },
    {
      "epoch": 4.516050878255602,
      "grad_norm": 0.07696795463562012,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 3730
    },
    {
      "epoch": 4.52816474863719,
      "grad_norm": 0.0669478103518486,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 3740
    },
    {
      "epoch": 4.5402786190187765,
      "grad_norm": 0.08377574384212494,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 3750
    },
    {
      "epoch": 4.552392489400363,
      "grad_norm": 0.06420420110225677,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 3760
    },
    {
      "epoch": 4.564506359781951,
      "grad_norm": 0.0768052488565445,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 3770
    },
    {
      "epoch": 4.576620230163537,
      "grad_norm": 0.08022000640630722,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 3780
    },
    {
      "epoch": 4.588734100545124,
      "grad_norm": 0.05824322625994682,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 3790
    },
    {
      "epoch": 4.600847970926711,
      "grad_norm": 0.046162206679582596,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 3800
    },
    {
      "epoch": 4.612961841308298,
      "grad_norm": 0.05267072468996048,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 3810
    },
    {
      "epoch": 4.625075711689885,
      "grad_norm": 0.0592661052942276,
      "learning_rate": 5e-05,
      "loss": 0.037,
      "step": 3820
    },
    {
      "epoch": 4.637189582071472,
      "grad_norm": 0.0472833625972271,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 3830
    },
    {
      "epoch": 4.649303452453059,
      "grad_norm": 0.06725344061851501,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 3840
    },
    {
      "epoch": 4.661417322834645,
      "grad_norm": 0.05985833704471588,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 3850
    },
    {
      "epoch": 4.673531193216233,
      "grad_norm": 0.07836378365755081,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 3860
    },
    {
      "epoch": 4.6856450635978195,
      "grad_norm": 0.0697031319141388,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 3870
    },
    {
      "epoch": 4.697758933979406,
      "grad_norm": 0.07394421100616455,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 3880
    },
    {
      "epoch": 4.709872804360994,
      "grad_norm": 0.06997254490852356,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 3890
    },
    {
      "epoch": 4.72198667474258,
      "grad_norm": 0.07305118441581726,
      "learning_rate": 5e-05,
      "loss": 0.0364,
      "step": 3900
    },
    {
      "epoch": 4.734100545124167,
      "grad_norm": 0.06269808858633041,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 3910
    },
    {
      "epoch": 4.746214415505754,
      "grad_norm": 0.28616395592689514,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 3920
    },
    {
      "epoch": 4.758328285887341,
      "grad_norm": 0.06583312898874283,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 3930
    },
    {
      "epoch": 4.770442156268928,
      "grad_norm": 0.050411827862262726,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 3940
    },
    {
      "epoch": 4.782556026650515,
      "grad_norm": 0.08411365747451782,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 3950
    },
    {
      "epoch": 4.794669897032102,
      "grad_norm": 0.05914429575204849,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 3960
    },
    {
      "epoch": 4.806783767413688,
      "grad_norm": 0.070884570479393,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 3970
    },
    {
      "epoch": 4.818897637795276,
      "grad_norm": 0.10113436728715897,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 3980
    },
    {
      "epoch": 4.8310115081768625,
      "grad_norm": 0.0757383331656456,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 3990
    },
    {
      "epoch": 4.843125378558449,
      "grad_norm": 0.064897321164608,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 4000
    },
    {
      "epoch": 4.855239248940037,
      "grad_norm": 0.06663346290588379,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 4010
    },
    {
      "epoch": 4.867353119321623,
      "grad_norm": 0.06509147584438324,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 4020
    },
    {
      "epoch": 4.87946698970321,
      "grad_norm": 0.059125714004039764,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 4030
    },
    {
      "epoch": 4.891580860084797,
      "grad_norm": 0.07721900194883347,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 4040
    },
    {
      "epoch": 4.903694730466384,
      "grad_norm": 0.06524927914142609,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 4050
    },
    {
      "epoch": 4.915808600847971,
      "grad_norm": 0.06464008241891861,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 4060
    },
    {
      "epoch": 4.927922471229558,
      "grad_norm": 0.07111181318759918,
      "learning_rate": 5e-05,
      "loss": 0.037,
      "step": 4070
    },
    {
      "epoch": 4.940036341611145,
      "grad_norm": 0.045346926897764206,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4080
    },
    {
      "epoch": 4.952150211992731,
      "grad_norm": 0.06582721322774887,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 4090
    },
    {
      "epoch": 4.964264082374319,
      "grad_norm": 0.05022529512643814,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 4100
    },
    {
      "epoch": 4.9763779527559056,
      "grad_norm": 0.05304441601037979,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 4110
    },
    {
      "epoch": 4.988491823137492,
      "grad_norm": 0.05448126047849655,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 4120
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.1256738305091858,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 4130
    },
    {
      "epoch": 5.0,
      "eval_average": 0.5616215743942713,
      "eval_crossner_ai": 0.5264997087448551,
      "eval_crossner_literature": 0.5729788387996839,
      "eval_crossner_music": 0.7209894506592972,
      "eval_crossner_politics": 0.5613338625938736,
      "eval_crossner_science": 0.6800602711707346,
      "eval_mit-movie": 0.5055999999503791,
      "eval_mit-restaurant": 0.3638888888410753,
      "eval_runtime": 170.7879,
      "eval_samples_per_second": 8.197,
      "eval_steps_per_second": 0.258,
      "step": 4130
    },
    {
      "epoch": 5.012113870381587,
      "grad_norm": 0.06129307672381401,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 4140
    },
    {
      "epoch": 5.024227740763174,
      "grad_norm": 0.09478029608726501,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 4150
    },
    {
      "epoch": 5.036341611144761,
      "grad_norm": 0.06533701717853546,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 4160
    },
    {
      "epoch": 5.048455481526347,
      "grad_norm": 0.09744574129581451,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 4170
    },
    {
      "epoch": 5.060569351907935,
      "grad_norm": 0.12409879267215729,
      "learning_rate": 5e-05,
      "loss": 0.0364,
      "step": 4180
    },
    {
      "epoch": 5.0726832222895215,
      "grad_norm": 0.056739531457424164,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 4190
    },
    {
      "epoch": 5.084797092671108,
      "grad_norm": 0.06405815482139587,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 4200
    },
    {
      "epoch": 5.096910963052696,
      "grad_norm": 0.08170991390943527,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 4210
    },
    {
      "epoch": 5.109024833434282,
      "grad_norm": 0.06913071870803833,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 4220
    },
    {
      "epoch": 5.121138703815869,
      "grad_norm": 0.17722781002521515,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 4230
    },
    {
      "epoch": 5.133252574197456,
      "grad_norm": 0.062054917216300964,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 4240
    },
    {
      "epoch": 5.145366444579043,
      "grad_norm": 0.06462676078081131,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 4250
    },
    {
      "epoch": 5.15748031496063,
      "grad_norm": 0.05325666069984436,
      "learning_rate": 5e-05,
      "loss": 0.0345,
      "step": 4260
    },
    {
      "epoch": 5.169594185342217,
      "grad_norm": 0.050302352756261826,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 4270
    },
    {
      "epoch": 5.181708055723804,
      "grad_norm": 0.0447535403072834,
      "learning_rate": 5e-05,
      "loss": 0.0347,
      "step": 4280
    },
    {
      "epoch": 5.19382192610539,
      "grad_norm": 0.08079838752746582,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4290
    },
    {
      "epoch": 5.205935796486978,
      "grad_norm": 0.06311183422803879,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 4300
    },
    {
      "epoch": 5.2180496668685645,
      "grad_norm": 0.07728248089551926,
      "learning_rate": 5e-05,
      "loss": 0.0342,
      "step": 4310
    },
    {
      "epoch": 5.230163537250151,
      "grad_norm": 0.0532071515917778,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 4320
    },
    {
      "epoch": 5.242277407631739,
      "grad_norm": 0.07564311474561691,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 4330
    },
    {
      "epoch": 5.254391278013325,
      "grad_norm": 0.07398522645235062,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 4340
    },
    {
      "epoch": 5.266505148394912,
      "grad_norm": 0.05682060867547989,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 4350
    },
    {
      "epoch": 5.278619018776499,
      "grad_norm": 0.0686274841427803,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 4360
    },
    {
      "epoch": 5.290732889158086,
      "grad_norm": 0.08115892112255096,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 4370
    },
    {
      "epoch": 5.302846759539673,
      "grad_norm": 0.05404061824083328,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 4380
    },
    {
      "epoch": 5.31496062992126,
      "grad_norm": 0.06172339618206024,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 4390
    },
    {
      "epoch": 5.327074500302847,
      "grad_norm": 0.0692114382982254,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4400
    },
    {
      "epoch": 5.339188370684433,
      "grad_norm": 0.08099684119224548,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4410
    },
    {
      "epoch": 5.351302241066021,
      "grad_norm": 0.07602494955062866,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 4420
    },
    {
      "epoch": 5.3634161114476075,
      "grad_norm": 0.07669471204280853,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 4430
    },
    {
      "epoch": 5.375529981829194,
      "grad_norm": 0.05999765917658806,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4440
    },
    {
      "epoch": 5.387643852210782,
      "grad_norm": 0.0730520486831665,
      "learning_rate": 5e-05,
      "loss": 0.0345,
      "step": 4450
    },
    {
      "epoch": 5.399757722592368,
      "grad_norm": 0.11417732387781143,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 4460
    },
    {
      "epoch": 5.411871592973955,
      "grad_norm": 0.07672368735074997,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 4470
    },
    {
      "epoch": 5.423985463355542,
      "grad_norm": 0.06527208536863327,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4480
    },
    {
      "epoch": 5.436099333737129,
      "grad_norm": 0.058720916509628296,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 4490
    },
    {
      "epoch": 5.448213204118716,
      "grad_norm": 0.07527316361665726,
      "learning_rate": 5e-05,
      "loss": 0.0347,
      "step": 4500
    },
    {
      "epoch": 5.460327074500303,
      "grad_norm": 0.05996432527899742,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4510
    },
    {
      "epoch": 5.47244094488189,
      "grad_norm": 0.06019777059555054,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4520
    },
    {
      "epoch": 5.484554815263476,
      "grad_norm": 0.05991049110889435,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4530
    },
    {
      "epoch": 5.496668685645064,
      "grad_norm": 0.06244543567299843,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4540
    },
    {
      "epoch": 5.508782556026651,
      "grad_norm": 0.045704763382673264,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 4550
    },
    {
      "epoch": 5.520896426408237,
      "grad_norm": 0.07149963825941086,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4560
    },
    {
      "epoch": 5.533010296789825,
      "grad_norm": 0.05662578344345093,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 4570
    },
    {
      "epoch": 5.545124167171411,
      "grad_norm": 0.10762136429548264,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 4580
    },
    {
      "epoch": 5.557238037552998,
      "grad_norm": 0.05982782319188118,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 4590
    },
    {
      "epoch": 5.5693519079345855,
      "grad_norm": 0.09300129860639572,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4600
    },
    {
      "epoch": 5.581465778316172,
      "grad_norm": 0.06492498517036438,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 4610
    },
    {
      "epoch": 5.593579648697759,
      "grad_norm": 0.07928040623664856,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 4620
    },
    {
      "epoch": 5.605693519079346,
      "grad_norm": 0.08108610659837723,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 4630
    },
    {
      "epoch": 5.617807389460933,
      "grad_norm": 0.068260557949543,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 4640
    },
    {
      "epoch": 5.6299212598425195,
      "grad_norm": 0.12395225465297699,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 4650
    },
    {
      "epoch": 5.642035130224107,
      "grad_norm": 0.049239594489336014,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 4660
    },
    {
      "epoch": 5.654149000605694,
      "grad_norm": 0.06241491809487343,
      "learning_rate": 5e-05,
      "loss": 0.0345,
      "step": 4670
    },
    {
      "epoch": 5.66626287098728,
      "grad_norm": 0.044735029339790344,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 4680
    },
    {
      "epoch": 5.678376741368868,
      "grad_norm": 0.07711976021528244,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 4690
    },
    {
      "epoch": 5.690490611750454,
      "grad_norm": 0.06498511880636215,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 4700
    },
    {
      "epoch": 5.702604482132041,
      "grad_norm": 0.0800684243440628,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 4710
    },
    {
      "epoch": 5.7147183525136285,
      "grad_norm": 0.06324289739131927,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 4720
    },
    {
      "epoch": 5.726832222895215,
      "grad_norm": 0.06476308405399323,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4730
    },
    {
      "epoch": 5.738946093276802,
      "grad_norm": 0.07113835960626602,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 4740
    },
    {
      "epoch": 5.751059963658389,
      "grad_norm": 0.05341620370745659,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 4750
    },
    {
      "epoch": 5.763173834039976,
      "grad_norm": 0.0651073306798935,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 4760
    },
    {
      "epoch": 5.7752877044215625,
      "grad_norm": 0.09340476244688034,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 4770
    },
    {
      "epoch": 5.78740157480315,
      "grad_norm": 0.05922752246260643,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4780
    },
    {
      "epoch": 5.799515445184737,
      "grad_norm": 0.05945443734526634,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4790
    },
    {
      "epoch": 5.811629315566323,
      "grad_norm": 0.06658990681171417,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 4800
    },
    {
      "epoch": 5.823743185947911,
      "grad_norm": 0.0447063185274601,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 4810
    },
    {
      "epoch": 5.835857056329497,
      "grad_norm": 0.06845900416374207,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 4820
    },
    {
      "epoch": 5.847970926711084,
      "grad_norm": 0.059245798736810684,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 4830
    },
    {
      "epoch": 5.8600847970926715,
      "grad_norm": 0.050376392900943756,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4840
    },
    {
      "epoch": 5.872198667474258,
      "grad_norm": 0.08487063646316528,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 4850
    },
    {
      "epoch": 5.884312537855845,
      "grad_norm": 0.06810680031776428,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 4860
    },
    {
      "epoch": 5.896426408237431,
      "grad_norm": 0.05611369013786316,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 4870
    },
    {
      "epoch": 5.908540278619019,
      "grad_norm": 0.07004448026418686,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 4880
    },
    {
      "epoch": 5.9206541490006055,
      "grad_norm": 0.0670810416340828,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 4890
    },
    {
      "epoch": 5.932768019382193,
      "grad_norm": 0.0638681948184967,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4900
    },
    {
      "epoch": 5.94488188976378,
      "grad_norm": 0.06001131236553192,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 4910
    },
    {
      "epoch": 5.956995760145366,
      "grad_norm": 0.060030777007341385,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4920
    },
    {
      "epoch": 5.969109630526953,
      "grad_norm": 0.09282384067773819,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 4930
    },
    {
      "epoch": 5.98122350090854,
      "grad_norm": 0.06895308196544647,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 4940
    },
    {
      "epoch": 5.993337371290127,
      "grad_norm": 0.06941267848014832,
      "learning_rate": 5e-05,
      "loss": 0.0342,
      "step": 4950
    },
    {
      "epoch": 6.0,
      "eval_average": 0.5676605725080801,
      "eval_crossner_ai": 0.560859188494396,
      "eval_crossner_literature": 0.5662650601909797,
      "eval_crossner_music": 0.7478005864602097,
      "eval_crossner_politics": 0.5829702969796726,
      "eval_crossner_science": 0.6625766870666912,
      "eval_mit-movie": 0.5129032257569236,
      "eval_mit-restaurant": 0.3402489626076885,
      "eval_runtime": 167.5835,
      "eval_samples_per_second": 8.354,
      "eval_steps_per_second": 0.263,
      "step": 4956
    },
    {
      "epoch": 6.004845548152635,
      "grad_norm": 0.08352956175804138,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 4960
    },
    {
      "epoch": 6.0169594185342214,
      "grad_norm": 0.10681979358196259,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 4970
    },
    {
      "epoch": 6.029073288915809,
      "grad_norm": 0.057358454912900925,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 4980
    },
    {
      "epoch": 6.041187159297396,
      "grad_norm": 0.05559050664305687,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 4990
    },
    {
      "epoch": 6.053301029678982,
      "grad_norm": 0.08282307535409927,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 5000
    },
    {
      "epoch": 6.06541490006057,
      "grad_norm": 0.050910092890262604,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 5010
    },
    {
      "epoch": 6.077528770442156,
      "grad_norm": 0.056238140910863876,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 5020
    },
    {
      "epoch": 6.089642640823743,
      "grad_norm": 0.0720590278506279,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 5030
    },
    {
      "epoch": 6.1017565112053305,
      "grad_norm": 0.06726886332035065,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 5040
    },
    {
      "epoch": 6.113870381586917,
      "grad_norm": 0.060042012482881546,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 5050
    },
    {
      "epoch": 6.125984251968504,
      "grad_norm": 0.08953242003917694,
      "learning_rate": 5e-05,
      "loss": 0.0325,
      "step": 5060
    },
    {
      "epoch": 6.138098122350091,
      "grad_norm": 0.06093310937285423,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 5070
    },
    {
      "epoch": 6.150211992731678,
      "grad_norm": 0.06907443702220917,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 5080
    },
    {
      "epoch": 6.1623258631132645,
      "grad_norm": 0.06312427669763565,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 5090
    },
    {
      "epoch": 6.174439733494852,
      "grad_norm": 0.07363404333591461,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 5100
    },
    {
      "epoch": 6.186553603876439,
      "grad_norm": 0.050760556012392044,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 5110
    },
    {
      "epoch": 6.198667474258025,
      "grad_norm": 0.06583340466022491,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5120
    },
    {
      "epoch": 6.210781344639613,
      "grad_norm": 0.0472080297768116,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 5130
    },
    {
      "epoch": 6.222895215021199,
      "grad_norm": 0.052835918962955475,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 5140
    },
    {
      "epoch": 6.235009085402786,
      "grad_norm": 0.06391005963087082,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5150
    },
    {
      "epoch": 6.2471229557843735,
      "grad_norm": 0.051652245223522186,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 5160
    },
    {
      "epoch": 6.25923682616596,
      "grad_norm": 0.07545474171638489,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 5170
    },
    {
      "epoch": 6.271350696547547,
      "grad_norm": 0.055184803903102875,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 5180
    },
    {
      "epoch": 6.283464566929134,
      "grad_norm": 0.052502550184726715,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 5190
    },
    {
      "epoch": 6.295578437310721,
      "grad_norm": 0.06483534723520279,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 5200
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 0.060881320387125015,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 5210
    },
    {
      "epoch": 6.319806178073895,
      "grad_norm": 0.06575930118560791,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 5220
    },
    {
      "epoch": 6.331920048455482,
      "grad_norm": 0.05036241561174393,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5230
    },
    {
      "epoch": 6.344033918837068,
      "grad_norm": 0.0704694390296936,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5240
    },
    {
      "epoch": 6.356147789218656,
      "grad_norm": 0.04783518984913826,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 5250
    },
    {
      "epoch": 6.368261659600242,
      "grad_norm": 0.0623050332069397,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 5260
    },
    {
      "epoch": 6.380375529981829,
      "grad_norm": 0.050227828323841095,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 5270
    },
    {
      "epoch": 6.3924894003634165,
      "grad_norm": 0.047027382999658585,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5280
    },
    {
      "epoch": 6.404603270745003,
      "grad_norm": 0.07287853956222534,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 5290
    },
    {
      "epoch": 6.41671714112659,
      "grad_norm": 0.09531179815530777,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 5300
    },
    {
      "epoch": 6.428831011508177,
      "grad_norm": 0.07293086498975754,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5310
    },
    {
      "epoch": 6.440944881889764,
      "grad_norm": 0.07796013355255127,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5320
    },
    {
      "epoch": 6.4530587522713505,
      "grad_norm": 0.04404869303107262,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 5330
    },
    {
      "epoch": 6.465172622652938,
      "grad_norm": 0.0802021473646164,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 5340
    },
    {
      "epoch": 6.477286493034525,
      "grad_norm": 0.04779570549726486,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 5350
    },
    {
      "epoch": 6.489400363416111,
      "grad_norm": 0.04839992895722389,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 5360
    },
    {
      "epoch": 6.501514233797698,
      "grad_norm": 0.04410141333937645,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5370
    },
    {
      "epoch": 6.513628104179285,
      "grad_norm": 0.05803285539150238,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 5380
    },
    {
      "epoch": 6.525741974560872,
      "grad_norm": 0.05955834686756134,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 5390
    },
    {
      "epoch": 6.5378558449424595,
      "grad_norm": 0.061018336564302444,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 5400
    },
    {
      "epoch": 6.549969715324046,
      "grad_norm": 0.07131639122962952,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 5410
    },
    {
      "epoch": 6.562083585705633,
      "grad_norm": 0.05962331220507622,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 5420
    },
    {
      "epoch": 6.574197456087219,
      "grad_norm": 0.14762157201766968,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5430
    },
    {
      "epoch": 6.586311326468807,
      "grad_norm": 0.04618527367711067,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 5440
    },
    {
      "epoch": 6.5984251968503935,
      "grad_norm": 0.06415203958749771,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 5450
    },
    {
      "epoch": 6.610539067231981,
      "grad_norm": 0.06917355209589005,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 5460
    },
    {
      "epoch": 6.622652937613568,
      "grad_norm": 0.08250263333320618,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 5470
    },
    {
      "epoch": 6.634766807995154,
      "grad_norm": 0.0451686717569828,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 5480
    },
    {
      "epoch": 6.646880678376741,
      "grad_norm": 0.054979465901851654,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 5490
    },
    {
      "epoch": 6.658994548758328,
      "grad_norm": 0.0633658617734909,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 5500
    },
    {
      "epoch": 6.671108419139915,
      "grad_norm": 0.05434800311923027,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 5510
    },
    {
      "epoch": 6.6832222895215025,
      "grad_norm": 0.0504140742123127,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 5520
    },
    {
      "epoch": 6.695336159903089,
      "grad_norm": 0.05414586141705513,
      "learning_rate": 5e-05,
      "loss": 0.0325,
      "step": 5530
    },
    {
      "epoch": 6.707450030284676,
      "grad_norm": 0.1377338171005249,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5540
    },
    {
      "epoch": 6.719563900666262,
      "grad_norm": 0.04775308817625046,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 5550
    },
    {
      "epoch": 6.73167777104785,
      "grad_norm": 0.05901984125375748,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 5560
    },
    {
      "epoch": 6.7437916414294365,
      "grad_norm": 0.060304395854473114,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 5570
    },
    {
      "epoch": 6.755905511811024,
      "grad_norm": 0.06886904686689377,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 5580
    },
    {
      "epoch": 6.768019382192611,
      "grad_norm": 0.055291205644607544,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 5590
    },
    {
      "epoch": 6.780133252574197,
      "grad_norm": 0.04906377196311951,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 5600
    },
    {
      "epoch": 6.792247122955784,
      "grad_norm": 0.05902537330985069,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 5610
    },
    {
      "epoch": 6.804360993337371,
      "grad_norm": 0.06632525473833084,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 5620
    },
    {
      "epoch": 6.816474863718958,
      "grad_norm": 0.05011603981256485,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 5630
    },
    {
      "epoch": 6.828588734100546,
      "grad_norm": 0.050137005746364594,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 5640
    },
    {
      "epoch": 6.840702604482132,
      "grad_norm": 0.050149932503700256,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 5650
    },
    {
      "epoch": 6.852816474863719,
      "grad_norm": 0.06009037792682648,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 5660
    },
    {
      "epoch": 6.864930345245305,
      "grad_norm": 0.06605114787817001,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 5670
    },
    {
      "epoch": 6.877044215626893,
      "grad_norm": 0.07313747704029083,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5680
    },
    {
      "epoch": 6.88915808600848,
      "grad_norm": 0.04906207323074341,
      "learning_rate": 5e-05,
      "loss": 0.0325,
      "step": 5690
    },
    {
      "epoch": 6.901271956390067,
      "grad_norm": 0.06437793374061584,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 5700
    },
    {
      "epoch": 6.913385826771654,
      "grad_norm": 0.08078691363334656,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 5710
    },
    {
      "epoch": 6.92549969715324,
      "grad_norm": 0.06454647332429886,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 5720
    },
    {
      "epoch": 6.937613567534827,
      "grad_norm": 0.07426005601882935,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 5730
    },
    {
      "epoch": 6.9497274379164145,
      "grad_norm": 0.04567117244005203,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 5740
    },
    {
      "epoch": 6.961841308298001,
      "grad_norm": 0.07619211077690125,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 5750
    },
    {
      "epoch": 6.973955178679589,
      "grad_norm": 0.0665295347571373,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 5760
    },
    {
      "epoch": 6.986069049061175,
      "grad_norm": 0.051913294941186905,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 5770
    },
    {
      "epoch": 6.998182919442762,
      "grad_norm": 0.10181151330471039,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 5780
    },
    {
      "epoch": 7.0,
      "eval_average": 0.5707061611767995,
      "eval_crossner_ai": 0.5477031801623609,
      "eval_crossner_literature": 0.5769020251278997,
      "eval_crossner_music": 0.7209047792275833,
      "eval_crossner_politics": 0.5759368835791694,
      "eval_crossner_science": 0.6784274193050561,
      "eval_mit-movie": 0.5358851674144488,
      "eval_mit-restaurant": 0.35918367342107826,
      "eval_runtime": 168.609,
      "eval_samples_per_second": 8.303,
      "eval_steps_per_second": 0.261,
      "step": 5782
    },
    {
      "epoch": 7.00969109630527,
      "grad_norm": 0.05022187530994415,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 5790
    },
    {
      "epoch": 7.021804966686856,
      "grad_norm": 0.06988824158906937,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5800
    },
    {
      "epoch": 7.033918837068444,
      "grad_norm": 0.05783960223197937,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 5810
    },
    {
      "epoch": 7.04603270745003,
      "grad_norm": 0.05420483276247978,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 5820
    },
    {
      "epoch": 7.058146577831617,
      "grad_norm": 0.06967790424823761,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 5830
    },
    {
      "epoch": 7.0702604482132045,
      "grad_norm": 0.06097285822033882,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 5840
    },
    {
      "epoch": 7.082374318594791,
      "grad_norm": 0.06064319238066673,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 5850
    },
    {
      "epoch": 7.094488188976378,
      "grad_norm": 0.06298394501209259,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 5860
    },
    {
      "epoch": 7.106602059357965,
      "grad_norm": 0.055080655962228775,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 5870
    },
    {
      "epoch": 7.118715929739552,
      "grad_norm": 0.06508302688598633,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 5880
    },
    {
      "epoch": 7.1308298001211385,
      "grad_norm": 0.062456369400024414,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 5890
    },
    {
      "epoch": 7.142943670502726,
      "grad_norm": 0.05779886990785599,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 5900
    },
    {
      "epoch": 7.155057540884313,
      "grad_norm": 0.06707116216421127,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 5910
    },
    {
      "epoch": 7.167171411265899,
      "grad_norm": 0.047113001346588135,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 5920
    },
    {
      "epoch": 7.179285281647486,
      "grad_norm": 0.05739705264568329,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 5930
    },
    {
      "epoch": 7.191399152029073,
      "grad_norm": 0.05677136778831482,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 5940
    },
    {
      "epoch": 7.20351302241066,
      "grad_norm": 0.05309337377548218,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 5950
    },
    {
      "epoch": 7.215626892792248,
      "grad_norm": 0.05647359788417816,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 5960
    },
    {
      "epoch": 7.227740763173834,
      "grad_norm": 0.11731040477752686,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 5970
    },
    {
      "epoch": 7.239854633555421,
      "grad_norm": 0.05298127606511116,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 5980
    },
    {
      "epoch": 7.251968503937007,
      "grad_norm": 0.044412899762392044,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 5990
    },
    {
      "epoch": 7.264082374318595,
      "grad_norm": 0.0559035986661911,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6000
    },
    {
      "epoch": 7.276196244700182,
      "grad_norm": 0.05873805657029152,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 6010
    },
    {
      "epoch": 7.288310115081769,
      "grad_norm": 0.059954799711704254,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6020
    },
    {
      "epoch": 7.300423985463356,
      "grad_norm": 0.060368526726961136,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 6030
    },
    {
      "epoch": 7.312537855844942,
      "grad_norm": 0.07216101139783859,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 6040
    },
    {
      "epoch": 7.324651726226529,
      "grad_norm": 0.08456137031316757,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 6050
    },
    {
      "epoch": 7.3367655966081164,
      "grad_norm": 0.06396958231925964,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 6060
    },
    {
      "epoch": 7.348879466989703,
      "grad_norm": 0.06370623409748077,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6070
    },
    {
      "epoch": 7.360993337371291,
      "grad_norm": 0.05153396725654602,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 6080
    },
    {
      "epoch": 7.373107207752877,
      "grad_norm": 0.07219531387090683,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 6090
    },
    {
      "epoch": 7.385221078134464,
      "grad_norm": 0.050704386085271835,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6100
    },
    {
      "epoch": 7.3973349485160504,
      "grad_norm": 0.06485896557569504,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 6110
    },
    {
      "epoch": 7.409448818897638,
      "grad_norm": 0.06030704826116562,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 6120
    },
    {
      "epoch": 7.421562689279225,
      "grad_norm": 0.08214344829320908,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6130
    },
    {
      "epoch": 7.433676559660812,
      "grad_norm": 0.065457783639431,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 6140
    },
    {
      "epoch": 7.445790430042399,
      "grad_norm": 0.04205619916319847,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6150
    },
    {
      "epoch": 7.457904300423985,
      "grad_norm": 0.0998958945274353,
      "learning_rate": 5e-05,
      "loss": 0.0325,
      "step": 6160
    },
    {
      "epoch": 7.470018170805572,
      "grad_norm": 0.05666937306523323,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6170
    },
    {
      "epoch": 7.4821320411871595,
      "grad_norm": 0.07074624300003052,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6180
    },
    {
      "epoch": 7.494245911568746,
      "grad_norm": 0.10848774760961533,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 6190
    },
    {
      "epoch": 7.506359781950334,
      "grad_norm": 0.06036131829023361,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6200
    },
    {
      "epoch": 7.51847365233192,
      "grad_norm": 0.06661076098680496,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6210
    },
    {
      "epoch": 7.530587522713507,
      "grad_norm": 0.05654647573828697,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 6220
    },
    {
      "epoch": 7.5427013930950935,
      "grad_norm": 0.0464090071618557,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6230
    },
    {
      "epoch": 7.554815263476681,
      "grad_norm": 0.047339797019958496,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6240
    },
    {
      "epoch": 7.566929133858268,
      "grad_norm": 0.05059123784303665,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 6250
    },
    {
      "epoch": 7.579043004239855,
      "grad_norm": 0.08243439346551895,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 6260
    },
    {
      "epoch": 7.591156874621442,
      "grad_norm": 0.05937214940786362,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 6270
    },
    {
      "epoch": 7.603270745003028,
      "grad_norm": 0.0631866306066513,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6280
    },
    {
      "epoch": 7.615384615384615,
      "grad_norm": 0.06370532512664795,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6290
    },
    {
      "epoch": 7.6274984857662025,
      "grad_norm": 0.09502455592155457,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 6300
    },
    {
      "epoch": 7.639612356147789,
      "grad_norm": 0.06021163612604141,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 6310
    },
    {
      "epoch": 7.651726226529377,
      "grad_norm": 0.06290420889854431,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6320
    },
    {
      "epoch": 7.663840096910963,
      "grad_norm": 0.046574730426073074,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6330
    },
    {
      "epoch": 7.67595396729255,
      "grad_norm": 0.055415838956832886,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6340
    },
    {
      "epoch": 7.6880678376741365,
      "grad_norm": 0.08191889524459839,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 6350
    },
    {
      "epoch": 7.700181708055724,
      "grad_norm": 0.0596172921359539,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6360
    },
    {
      "epoch": 7.712295578437311,
      "grad_norm": 0.04681958630681038,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 6370
    },
    {
      "epoch": 7.724409448818898,
      "grad_norm": 0.07729113101959229,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 6380
    },
    {
      "epoch": 7.736523319200485,
      "grad_norm": 0.061912648379802704,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 6390
    },
    {
      "epoch": 7.748637189582071,
      "grad_norm": 0.06327230483293533,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 6400
    },
    {
      "epoch": 7.760751059963658,
      "grad_norm": 0.06090974435210228,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 6410
    },
    {
      "epoch": 7.7728649303452455,
      "grad_norm": 0.09368443489074707,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 6420
    },
    {
      "epoch": 7.784978800726832,
      "grad_norm": 0.053674787282943726,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6430
    },
    {
      "epoch": 7.79709267110842,
      "grad_norm": 0.055964864790439606,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 6440
    },
    {
      "epoch": 7.809206541490006,
      "grad_norm": 0.07669354975223541,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 6450
    },
    {
      "epoch": 7.821320411871593,
      "grad_norm": 0.06755132973194122,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 6460
    },
    {
      "epoch": 7.8334342822531795,
      "grad_norm": 0.06304248422384262,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6470
    },
    {
      "epoch": 7.845548152634767,
      "grad_norm": 0.0687779113650322,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 6480
    },
    {
      "epoch": 7.857662023016354,
      "grad_norm": 0.06430011242628098,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 6490
    },
    {
      "epoch": 7.86977589339794,
      "grad_norm": 0.07722048461437225,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6500
    },
    {
      "epoch": 7.881889763779528,
      "grad_norm": 0.06165015324950218,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6510
    },
    {
      "epoch": 7.894003634161114,
      "grad_norm": 0.0747261494398117,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 6520
    },
    {
      "epoch": 7.906117504542701,
      "grad_norm": 0.047679174691438675,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6530
    },
    {
      "epoch": 7.9182313749242885,
      "grad_norm": 0.09093746542930603,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 6540
    },
    {
      "epoch": 7.930345245305875,
      "grad_norm": 0.0564582385122776,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 6550
    },
    {
      "epoch": 7.942459115687462,
      "grad_norm": 0.05036993697285652,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 6560
    },
    {
      "epoch": 7.954572986069049,
      "grad_norm": 0.05777345970273018,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6570
    },
    {
      "epoch": 7.966686856450636,
      "grad_norm": 0.05738065391778946,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 6580
    },
    {
      "epoch": 7.9788007268322225,
      "grad_norm": 0.06299673020839691,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6590
    },
    {
      "epoch": 7.99091459721381,
      "grad_norm": 0.051668304949998856,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 6600
    },
    {
      "epoch": 8.0,
      "eval_average": 0.5706772630370756,
      "eval_crossner_ai": 0.5552274069201977,
      "eval_crossner_literature": 0.5830583057806157,
      "eval_crossner_music": 0.7262813521854989,
      "eval_crossner_politics": 0.577075098764203,
      "eval_crossner_science": 0.6690211906666592,
      "eval_mit-movie": 0.49840255586090654,
      "eval_mit-restaurant": 0.38567493108144824,
      "eval_runtime": 169.0647,
      "eval_samples_per_second": 8.281,
      "eval_steps_per_second": 0.26,
      "step": 6608
    },
    {
      "epoch": 8.002422774076317,
      "grad_norm": 0.06700786203145981,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 6610
    },
    {
      "epoch": 8.014536644457904,
      "grad_norm": 0.06700427085161209,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6620
    },
    {
      "epoch": 8.026650514839492,
      "grad_norm": 0.08378256857395172,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 6630
    },
    {
      "epoch": 8.038764385221079,
      "grad_norm": 0.0703229084610939,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6640
    },
    {
      "epoch": 8.050878255602665,
      "grad_norm": 0.04397948458790779,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 6650
    },
    {
      "epoch": 8.062992125984252,
      "grad_norm": 0.07199078053236008,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 6660
    },
    {
      "epoch": 8.075105996365838,
      "grad_norm": 0.101606085896492,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 6670
    },
    {
      "epoch": 8.087219866747425,
      "grad_norm": 0.05550051108002663,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 6680
    },
    {
      "epoch": 8.099333737129013,
      "grad_norm": 0.11989198625087738,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 6690
    },
    {
      "epoch": 8.1114476075106,
      "grad_norm": 0.07167259603738785,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 6700
    },
    {
      "epoch": 8.123561477892187,
      "grad_norm": 0.0753958597779274,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 6710
    },
    {
      "epoch": 8.135675348273773,
      "grad_norm": 0.05182762071490288,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 6720
    },
    {
      "epoch": 8.14778921865536,
      "grad_norm": 0.04691226780414581,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 6730
    },
    {
      "epoch": 8.159903089036947,
      "grad_norm": 0.044810518622398376,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 6740
    },
    {
      "epoch": 8.172016959418535,
      "grad_norm": 0.05539979413151741,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 6750
    },
    {
      "epoch": 8.184130829800122,
      "grad_norm": 0.06409293413162231,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 6760
    },
    {
      "epoch": 8.196244700181708,
      "grad_norm": 0.05935652181506157,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 6770
    },
    {
      "epoch": 8.208358570563295,
      "grad_norm": 0.0569990798830986,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 6780
    },
    {
      "epoch": 8.220472440944881,
      "grad_norm": 0.08169563859701157,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 6790
    },
    {
      "epoch": 8.232586311326468,
      "grad_norm": 0.07409108430147171,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6800
    },
    {
      "epoch": 8.244700181708057,
      "grad_norm": 0.04857584834098816,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 6810
    },
    {
      "epoch": 8.256814052089643,
      "grad_norm": 0.07924052327871323,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 6820
    },
    {
      "epoch": 8.26892792247123,
      "grad_norm": 0.06577718257904053,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 6830
    },
    {
      "epoch": 8.281041792852816,
      "grad_norm": 0.05895228311419487,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 6840
    },
    {
      "epoch": 8.293155663234403,
      "grad_norm": 0.06021483242511749,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 6850
    },
    {
      "epoch": 8.30526953361599,
      "grad_norm": 0.0699157565832138,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6860
    },
    {
      "epoch": 8.317383403997578,
      "grad_norm": 0.059648506343364716,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6870
    },
    {
      "epoch": 8.329497274379165,
      "grad_norm": 0.04722623527050018,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 6880
    },
    {
      "epoch": 8.341611144760751,
      "grad_norm": 0.05075819417834282,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 6890
    },
    {
      "epoch": 8.353725015142338,
      "grad_norm": 0.04939611256122589,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 6900
    },
    {
      "epoch": 8.365838885523925,
      "grad_norm": 0.05258653312921524,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 6910
    },
    {
      "epoch": 8.377952755905511,
      "grad_norm": 0.060906458646059036,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 6920
    },
    {
      "epoch": 8.3900666262871,
      "grad_norm": 0.0640186071395874,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 6930
    },
    {
      "epoch": 8.402180496668686,
      "grad_norm": 0.07799302786588669,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 6940
    },
    {
      "epoch": 8.414294367050273,
      "grad_norm": 0.053230367600917816,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 6950
    },
    {
      "epoch": 8.42640823743186,
      "grad_norm": 0.04478249326348305,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 6960
    },
    {
      "epoch": 8.438522107813446,
      "grad_norm": 0.04417639970779419,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 6970
    },
    {
      "epoch": 8.450635978195033,
      "grad_norm": 0.05914773419499397,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 6980
    },
    {
      "epoch": 8.462749848576621,
      "grad_norm": 0.047092534601688385,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 6990
    },
    {
      "epoch": 8.474863718958208,
      "grad_norm": 0.051935773342847824,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7000
    },
    {
      "epoch": 8.486977589339794,
      "grad_norm": 0.06251001358032227,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 7010
    },
    {
      "epoch": 8.499091459721381,
      "grad_norm": 0.05952072516083717,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 7020
    },
    {
      "epoch": 8.511205330102968,
      "grad_norm": 0.09805408865213394,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 7030
    },
    {
      "epoch": 8.523319200484554,
      "grad_norm": 0.07051268965005875,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 7040
    },
    {
      "epoch": 8.535433070866143,
      "grad_norm": 0.0410262793302536,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7050
    },
    {
      "epoch": 8.54754694124773,
      "grad_norm": 0.05124836415052414,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 7060
    },
    {
      "epoch": 8.559660811629316,
      "grad_norm": 0.05929224565625191,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 7070
    },
    {
      "epoch": 8.571774682010902,
      "grad_norm": 0.04988015070557594,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7080
    },
    {
      "epoch": 8.583888552392489,
      "grad_norm": 0.0507039837539196,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 7090
    },
    {
      "epoch": 8.596002422774076,
      "grad_norm": 0.051685549318790436,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 7100
    },
    {
      "epoch": 8.608116293155664,
      "grad_norm": 0.06147461757063866,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 7110
    },
    {
      "epoch": 8.62023016353725,
      "grad_norm": 0.049178969115018845,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 7120
    },
    {
      "epoch": 8.632344033918837,
      "grad_norm": 0.06668394804000854,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7130
    },
    {
      "epoch": 8.644457904300424,
      "grad_norm": 0.0628393292427063,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 7140
    },
    {
      "epoch": 8.65657177468201,
      "grad_norm": 0.04876089096069336,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 7150
    },
    {
      "epoch": 8.668685645063597,
      "grad_norm": 0.04743991047143936,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 7160
    },
    {
      "epoch": 8.680799515445184,
      "grad_norm": 0.05808515101671219,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 7170
    },
    {
      "epoch": 8.692913385826772,
      "grad_norm": 0.05189051106572151,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 7180
    },
    {
      "epoch": 8.705027256208359,
      "grad_norm": 0.05041467770934105,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 7190
    },
    {
      "epoch": 8.717141126589945,
      "grad_norm": 0.07299977540969849,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7200
    },
    {
      "epoch": 8.729254996971532,
      "grad_norm": 0.055000752210617065,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 7210
    },
    {
      "epoch": 8.741368867353119,
      "grad_norm": 0.04921656847000122,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7220
    },
    {
      "epoch": 8.753482737734707,
      "grad_norm": 0.05259346589446068,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 7230
    },
    {
      "epoch": 8.765596608116294,
      "grad_norm": 0.05732627585530281,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 7240
    },
    {
      "epoch": 8.77771047849788,
      "grad_norm": 0.06741902977228165,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 7250
    },
    {
      "epoch": 8.789824348879467,
      "grad_norm": 0.061125390231609344,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7260
    },
    {
      "epoch": 8.801938219261054,
      "grad_norm": 0.04781479015946388,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7270
    },
    {
      "epoch": 8.81405208964264,
      "grad_norm": 0.05695708841085434,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 7280
    },
    {
      "epoch": 8.826165960024227,
      "grad_norm": 0.04562579095363617,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 7290
    },
    {
      "epoch": 8.838279830405815,
      "grad_norm": 0.08123734593391418,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 7300
    },
    {
      "epoch": 8.850393700787402,
      "grad_norm": 0.05103147774934769,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7310
    },
    {
      "epoch": 8.862507571168988,
      "grad_norm": 0.04667806997895241,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7320
    },
    {
      "epoch": 8.874621441550575,
      "grad_norm": 0.08352319896221161,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 7330
    },
    {
      "epoch": 8.886735311932162,
      "grad_norm": 0.0625789538025856,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7340
    },
    {
      "epoch": 8.89884918231375,
      "grad_norm": 0.059078510850667953,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 7350
    },
    {
      "epoch": 8.910963052695337,
      "grad_norm": 0.0515921525657177,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 7360
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 0.05051165819168091,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7370
    },
    {
      "epoch": 8.93519079345851,
      "grad_norm": 0.05595686659216881,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7380
    },
    {
      "epoch": 8.947304663840097,
      "grad_norm": 0.14417368173599243,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 7390
    },
    {
      "epoch": 8.959418534221683,
      "grad_norm": 0.08111144602298737,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 7400
    },
    {
      "epoch": 8.97153240460327,
      "grad_norm": 0.05264715850353241,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 7410
    },
    {
      "epoch": 8.983646274984858,
      "grad_norm": 0.05041257292032242,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7420
    },
    {
      "epoch": 8.995760145366445,
      "grad_norm": 0.06630532443523407,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 7430
    },
    {
      "epoch": 9.0,
      "eval_average": 0.5828563108636823,
      "eval_crossner_ai": 0.5465393794251852,
      "eval_crossner_literature": 0.593238822196452,
      "eval_crossner_music": 0.7552804078159323,
      "eval_crossner_politics": 0.5756369426251216,
      "eval_crossner_science": 0.6981227802647318,
      "eval_mit-movie": 0.5224358973862893,
      "eval_mit-restaurant": 0.38873994633206493,
      "eval_runtime": 169.0979,
      "eval_samples_per_second": 8.279,
      "eval_steps_per_second": 0.26,
      "step": 7434
    },
    {
      "epoch": 9.007268322228953,
      "grad_norm": 0.059768542647361755,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 7440
    },
    {
      "epoch": 9.01938219261054,
      "grad_norm": 0.04706709459424019,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7450
    },
    {
      "epoch": 9.031496062992126,
      "grad_norm": 0.06963618844747543,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 7460
    },
    {
      "epoch": 9.043609933373713,
      "grad_norm": 0.04931259900331497,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 7470
    },
    {
      "epoch": 9.0557238037553,
      "grad_norm": 0.0598042793571949,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7480
    },
    {
      "epoch": 9.067837674136888,
      "grad_norm": 0.06277851015329361,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7490
    },
    {
      "epoch": 9.079951544518474,
      "grad_norm": 0.06703856587409973,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7500
    },
    {
      "epoch": 9.09206541490006,
      "grad_norm": 0.0828629806637764,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7510
    },
    {
      "epoch": 9.104179285281647,
      "grad_norm": 0.06443225592374802,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7520
    },
    {
      "epoch": 9.116293155663234,
      "grad_norm": 0.10066930949687958,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 7530
    },
    {
      "epoch": 9.12840702604482,
      "grad_norm": 0.04960065335035324,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 7540
    },
    {
      "epoch": 9.140520896426409,
      "grad_norm": 0.04525882378220558,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 7550
    },
    {
      "epoch": 9.152634766807996,
      "grad_norm": 0.04373876750469208,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7560
    },
    {
      "epoch": 9.164748637189582,
      "grad_norm": 0.09469714015722275,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 7570
    },
    {
      "epoch": 9.176862507571169,
      "grad_norm": 0.0690685510635376,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 7580
    },
    {
      "epoch": 9.188976377952756,
      "grad_norm": 0.057684559375047684,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 7590
    },
    {
      "epoch": 9.201090248334342,
      "grad_norm": 0.06555525213479996,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7600
    },
    {
      "epoch": 9.21320411871593,
      "grad_norm": 0.08424431830644608,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 7610
    },
    {
      "epoch": 9.225317989097517,
      "grad_norm": 0.0560125894844532,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 7620
    },
    {
      "epoch": 9.237431859479104,
      "grad_norm": 0.07228877395391464,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7630
    },
    {
      "epoch": 9.24954572986069,
      "grad_norm": 0.06906953454017639,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 7640
    },
    {
      "epoch": 9.261659600242277,
      "grad_norm": 0.07885479182004929,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7650
    },
    {
      "epoch": 9.273773470623864,
      "grad_norm": 0.05399014428257942,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 7660
    },
    {
      "epoch": 9.285887341005452,
      "grad_norm": 0.07080085575580597,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 7670
    },
    {
      "epoch": 9.298001211387039,
      "grad_norm": 0.05002142861485481,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 7680
    },
    {
      "epoch": 9.310115081768625,
      "grad_norm": 0.061370112001895905,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 7690
    },
    {
      "epoch": 9.322228952150212,
      "grad_norm": 0.0510115884244442,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 7700
    },
    {
      "epoch": 9.334342822531799,
      "grad_norm": 0.05997750535607338,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7710
    },
    {
      "epoch": 9.346456692913385,
      "grad_norm": 0.07004708796739578,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 7720
    },
    {
      "epoch": 9.358570563294972,
      "grad_norm": 0.07536523789167404,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7730
    },
    {
      "epoch": 9.37068443367656,
      "grad_norm": 0.05565881356596947,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 7740
    },
    {
      "epoch": 9.382798304058147,
      "grad_norm": 0.07349418848752975,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7750
    },
    {
      "epoch": 9.394912174439733,
      "grad_norm": 0.045222654938697815,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 7760
    },
    {
      "epoch": 9.40702604482132,
      "grad_norm": 0.054430924355983734,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 7770
    },
    {
      "epoch": 9.419139915202907,
      "grad_norm": 0.06075428053736687,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7780
    },
    {
      "epoch": 9.431253785584495,
      "grad_norm": 0.059471968561410904,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 7790
    },
    {
      "epoch": 9.443367655966082,
      "grad_norm": 0.04322509467601776,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 7800
    },
    {
      "epoch": 9.455481526347668,
      "grad_norm": 0.05267166346311569,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 7810
    },
    {
      "epoch": 9.467595396729255,
      "grad_norm": 0.09709232300519943,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7820
    },
    {
      "epoch": 9.479709267110842,
      "grad_norm": 0.05909568816423416,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 7830
    },
    {
      "epoch": 9.491823137492428,
      "grad_norm": 0.04929182678461075,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 7840
    },
    {
      "epoch": 9.503937007874015,
      "grad_norm": 0.06155452877283096,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 7850
    },
    {
      "epoch": 9.516050878255603,
      "grad_norm": 0.04989394545555115,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 7860
    },
    {
      "epoch": 9.52816474863719,
      "grad_norm": 0.14524312317371368,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 7870
    },
    {
      "epoch": 9.540278619018776,
      "grad_norm": 0.06092321500182152,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 7880
    },
    {
      "epoch": 9.552392489400363,
      "grad_norm": 0.04497021436691284,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7890
    },
    {
      "epoch": 9.56450635978195,
      "grad_norm": 0.06449387222528458,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 7900
    },
    {
      "epoch": 9.576620230163538,
      "grad_norm": 0.06502891331911087,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 7910
    },
    {
      "epoch": 9.588734100545125,
      "grad_norm": 0.05299033224582672,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7920
    },
    {
      "epoch": 9.600847970926711,
      "grad_norm": 0.05005867779254913,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 7930
    },
    {
      "epoch": 9.612961841308298,
      "grad_norm": 0.0447520911693573,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 7940
    },
    {
      "epoch": 9.625075711689885,
      "grad_norm": 0.05678754299879074,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 7950
    },
    {
      "epoch": 9.637189582071471,
      "grad_norm": 0.0551735982298851,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 7960
    },
    {
      "epoch": 9.649303452453058,
      "grad_norm": 0.04358578473329544,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 7970
    },
    {
      "epoch": 9.661417322834646,
      "grad_norm": 0.07490818947553635,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 7980
    },
    {
      "epoch": 9.673531193216233,
      "grad_norm": 0.046905290335416794,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 7990
    },
    {
      "epoch": 9.68564506359782,
      "grad_norm": 0.05402452126145363,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 8000
    },
    {
      "epoch": 9.697758933979406,
      "grad_norm": 0.07567144185304642,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 8010
    },
    {
      "epoch": 9.709872804360993,
      "grad_norm": 0.04987477511167526,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8020
    },
    {
      "epoch": 9.721986674742581,
      "grad_norm": 0.052680738270282745,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 8030
    },
    {
      "epoch": 9.734100545124168,
      "grad_norm": 0.07211039960384369,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 8040
    },
    {
      "epoch": 9.746214415505754,
      "grad_norm": 0.05110478773713112,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8050
    },
    {
      "epoch": 9.758328285887341,
      "grad_norm": 0.06693224608898163,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 8060
    },
    {
      "epoch": 9.770442156268928,
      "grad_norm": 0.05163189396262169,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8070
    },
    {
      "epoch": 9.782556026650514,
      "grad_norm": 0.05866417661309242,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 8080
    },
    {
      "epoch": 9.794669897032101,
      "grad_norm": 0.05354995280504227,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 8090
    },
    {
      "epoch": 9.80678376741369,
      "grad_norm": 0.041625119745731354,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 8100
    },
    {
      "epoch": 9.818897637795276,
      "grad_norm": 0.0473911352455616,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 8110
    },
    {
      "epoch": 9.831011508176863,
      "grad_norm": 0.07647092640399933,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 8120
    },
    {
      "epoch": 9.84312537855845,
      "grad_norm": 0.08443570882081985,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 8130
    },
    {
      "epoch": 9.855239248940036,
      "grad_norm": 0.09376140683889389,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 8140
    },
    {
      "epoch": 9.867353119321624,
      "grad_norm": 0.056864019483327866,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 8150
    },
    {
      "epoch": 9.87946698970321,
      "grad_norm": 0.07343296706676483,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 8160
    },
    {
      "epoch": 9.891580860084797,
      "grad_norm": 0.07169260084629059,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 8170
    },
    {
      "epoch": 9.903694730466384,
      "grad_norm": 0.06157853826880455,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8180
    },
    {
      "epoch": 9.91580860084797,
      "grad_norm": 0.05319369211792946,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 8190
    },
    {
      "epoch": 9.927922471229557,
      "grad_norm": 0.06568072736263275,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 8200
    },
    {
      "epoch": 9.940036341611144,
      "grad_norm": 0.04634074494242668,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 8210
    },
    {
      "epoch": 9.952150211992732,
      "grad_norm": 0.044791869819164276,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 8220
    },
    {
      "epoch": 9.964264082374319,
      "grad_norm": 0.05091824755072594,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 8230
    },
    {
      "epoch": 9.976377952755906,
      "grad_norm": 0.056516580283641815,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 8240
    },
    {
      "epoch": 9.988491823137492,
      "grad_norm": 0.057373423129320145,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 8250
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.07875057309865952,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 8260
    },
    {
      "epoch": 10.0,
      "eval_average": 0.5765234643622271,
      "eval_crossner_ai": 0.5622343654631813,
      "eval_crossner_literature": 0.5772402418411798,
      "eval_crossner_music": 0.7358767424297709,
      "eval_crossner_politics": 0.5663858804152587,
      "eval_crossner_science": 0.673511293584599,
      "eval_mit-movie": 0.5316045380380479,
      "eval_mit-restaurant": 0.38881118876355264,
      "eval_runtime": 168.1965,
      "eval_samples_per_second": 8.324,
      "eval_steps_per_second": 0.262,
      "step": 8260
    },
    {
      "epoch": 10.012113870381587,
      "grad_norm": 0.06303677707910538,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8270
    },
    {
      "epoch": 10.024227740763173,
      "grad_norm": 0.06867120414972305,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8280
    },
    {
      "epoch": 10.03634161114476,
      "grad_norm": 0.0864197388291359,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 8290
    },
    {
      "epoch": 10.048455481526348,
      "grad_norm": 0.04117531701922417,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 8300
    },
    {
      "epoch": 10.060569351907935,
      "grad_norm": 0.08530303835868835,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 8310
    },
    {
      "epoch": 10.072683222289522,
      "grad_norm": 0.04962395504117012,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 8320
    },
    {
      "epoch": 10.084797092671108,
      "grad_norm": 0.056805167347192764,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 8330
    },
    {
      "epoch": 10.096910963052695,
      "grad_norm": 0.045884180814027786,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 8340
    },
    {
      "epoch": 10.109024833434281,
      "grad_norm": 0.060730941593647,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 8350
    },
    {
      "epoch": 10.12113870381587,
      "grad_norm": 0.06972973793745041,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 8360
    },
    {
      "epoch": 10.133252574197456,
      "grad_norm": 0.05251840874552727,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 8370
    },
    {
      "epoch": 10.145366444579043,
      "grad_norm": 0.05124010145664215,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8380
    },
    {
      "epoch": 10.15748031496063,
      "grad_norm": 0.046299781650304794,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 8390
    },
    {
      "epoch": 10.169594185342216,
      "grad_norm": 0.05180433392524719,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 8400
    },
    {
      "epoch": 10.181708055723803,
      "grad_norm": 0.05863704904913902,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 8410
    },
    {
      "epoch": 10.193821926105391,
      "grad_norm": 0.04453059285879135,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 8420
    },
    {
      "epoch": 10.205935796486978,
      "grad_norm": 0.06268448382616043,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 8430
    },
    {
      "epoch": 10.218049666868565,
      "grad_norm": 0.05532728135585785,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 8440
    },
    {
      "epoch": 10.230163537250151,
      "grad_norm": 0.05421324446797371,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 8450
    },
    {
      "epoch": 10.242277407631738,
      "grad_norm": 0.04528725892305374,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 8460
    },
    {
      "epoch": 10.254391278013326,
      "grad_norm": 0.04511852189898491,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 8470
    },
    {
      "epoch": 10.266505148394913,
      "grad_norm": 0.05329641327261925,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8480
    },
    {
      "epoch": 10.2786190187765,
      "grad_norm": 0.07954806089401245,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 8490
    },
    {
      "epoch": 10.290732889158086,
      "grad_norm": 0.042671237140893936,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 8500
    },
    {
      "epoch": 10.302846759539673,
      "grad_norm": 0.15268574655056,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 8510
    },
    {
      "epoch": 10.31496062992126,
      "grad_norm": 0.07283715903759003,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 8520
    },
    {
      "epoch": 10.327074500302846,
      "grad_norm": 0.07318396866321564,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8530
    },
    {
      "epoch": 10.339188370684434,
      "grad_norm": 0.1035086065530777,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8540
    },
    {
      "epoch": 10.351302241066021,
      "grad_norm": 0.050597257912158966,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8550
    },
    {
      "epoch": 10.363416111447608,
      "grad_norm": 0.05948174372315407,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 8560
    },
    {
      "epoch": 10.375529981829194,
      "grad_norm": 0.05145808309316635,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 8570
    },
    {
      "epoch": 10.38764385221078,
      "grad_norm": 0.051998551934957504,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8580
    },
    {
      "epoch": 10.399757722592367,
      "grad_norm": 0.04177211970090866,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8590
    },
    {
      "epoch": 10.411871592973956,
      "grad_norm": 0.05431985482573509,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 8600
    },
    {
      "epoch": 10.423985463355542,
      "grad_norm": 0.05620291829109192,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 8610
    },
    {
      "epoch": 10.436099333737129,
      "grad_norm": 0.04623458907008171,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 8620
    },
    {
      "epoch": 10.448213204118716,
      "grad_norm": 0.05064861476421356,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 8630
    },
    {
      "epoch": 10.460327074500302,
      "grad_norm": 0.0752241238951683,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8640
    },
    {
      "epoch": 10.472440944881889,
      "grad_norm": 0.054772522300481796,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8650
    },
    {
      "epoch": 10.484554815263477,
      "grad_norm": 0.07285093516111374,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8660
    },
    {
      "epoch": 10.496668685645064,
      "grad_norm": 0.059336066246032715,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 8670
    },
    {
      "epoch": 10.50878255602665,
      "grad_norm": 0.06207685172557831,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 8680
    },
    {
      "epoch": 10.520896426408237,
      "grad_norm": 0.09555963426828384,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 8690
    },
    {
      "epoch": 10.533010296789824,
      "grad_norm": 0.05124190077185631,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8700
    },
    {
      "epoch": 10.54512416717141,
      "grad_norm": 0.06234337016940117,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 8710
    },
    {
      "epoch": 10.557238037552999,
      "grad_norm": 0.05557698756456375,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8720
    },
    {
      "epoch": 10.569351907934585,
      "grad_norm": 0.06291544437408447,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 8730
    },
    {
      "epoch": 10.581465778316172,
      "grad_norm": 0.04625774174928665,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 8740
    },
    {
      "epoch": 10.593579648697759,
      "grad_norm": 0.04623044282197952,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8750
    },
    {
      "epoch": 10.605693519079345,
      "grad_norm": 0.06276363879442215,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 8760
    },
    {
      "epoch": 10.617807389460932,
      "grad_norm": 0.05646049231290817,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 8770
    },
    {
      "epoch": 10.62992125984252,
      "grad_norm": 0.058535922318696976,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8780
    },
    {
      "epoch": 10.642035130224107,
      "grad_norm": 0.05295998230576515,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 8790
    },
    {
      "epoch": 10.654149000605694,
      "grad_norm": 0.059769097715616226,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 8800
    },
    {
      "epoch": 10.66626287098728,
      "grad_norm": 0.08117330819368362,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 8810
    },
    {
      "epoch": 10.678376741368867,
      "grad_norm": 0.062146615236997604,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 8820
    },
    {
      "epoch": 10.690490611750453,
      "grad_norm": 0.08524048328399658,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 8830
    },
    {
      "epoch": 10.702604482132042,
      "grad_norm": 0.047561053186655045,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 8840
    },
    {
      "epoch": 10.714718352513628,
      "grad_norm": 0.04072849079966545,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 8850
    },
    {
      "epoch": 10.726832222895215,
      "grad_norm": 0.06835158169269562,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8860
    },
    {
      "epoch": 10.738946093276802,
      "grad_norm": 0.08589872717857361,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 8870
    },
    {
      "epoch": 10.751059963658388,
      "grad_norm": 0.10396254807710648,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 8880
    },
    {
      "epoch": 10.763173834039975,
      "grad_norm": 0.041733793914318085,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8890
    },
    {
      "epoch": 10.775287704421563,
      "grad_norm": 0.08845353126525879,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8900
    },
    {
      "epoch": 10.78740157480315,
      "grad_norm": 0.06044287234544754,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 8910
    },
    {
      "epoch": 10.799515445184737,
      "grad_norm": 0.05085516348481178,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8920
    },
    {
      "epoch": 10.811629315566323,
      "grad_norm": 0.04839715361595154,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 8930
    },
    {
      "epoch": 10.82374318594791,
      "grad_norm": 0.05414149537682533,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 8940
    },
    {
      "epoch": 10.835857056329496,
      "grad_norm": 0.05011343955993652,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 8950
    },
    {
      "epoch": 10.847970926711085,
      "grad_norm": 0.052940137684345245,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 8960
    },
    {
      "epoch": 10.860084797092671,
      "grad_norm": 0.05667440593242645,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 8970
    },
    {
      "epoch": 10.872198667474258,
      "grad_norm": 0.06683414429426193,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 8980
    },
    {
      "epoch": 10.884312537855845,
      "grad_norm": 0.0930878147482872,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 8990
    },
    {
      "epoch": 10.896426408237431,
      "grad_norm": 0.05005992203950882,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 9000
    },
    {
      "epoch": 10.908540278619018,
      "grad_norm": 0.04341979697346687,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 9010
    },
    {
      "epoch": 10.920654149000606,
      "grad_norm": 0.043811678886413574,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 9020
    },
    {
      "epoch": 10.932768019382193,
      "grad_norm": 0.05393432825803757,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9030
    },
    {
      "epoch": 10.94488188976378,
      "grad_norm": 0.04112107306718826,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 9040
    },
    {
      "epoch": 10.956995760145366,
      "grad_norm": 0.052045788615942,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 9050
    },
    {
      "epoch": 10.969109630526953,
      "grad_norm": 0.05054125562310219,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9060
    },
    {
      "epoch": 10.98122350090854,
      "grad_norm": 0.04798123612999916,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 9070
    },
    {
      "epoch": 10.993337371290128,
      "grad_norm": 0.05855538323521614,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9080
    },
    {
      "epoch": 11.0,
      "eval_average": 0.5751047216115918,
      "eval_crossner_ai": 0.5478468899023805,
      "eval_crossner_literature": 0.5826513911120288,
      "eval_crossner_music": 0.7378427787433647,
      "eval_crossner_politics": 0.572908366483826,
      "eval_crossner_science": 0.6575342465255173,
      "eval_mit-movie": 0.5457413248713536,
      "eval_mit-restaurant": 0.38120805364267196,
      "eval_runtime": 168.3956,
      "eval_samples_per_second": 8.314,
      "eval_steps_per_second": 0.261,
      "step": 9086
    },
    {
      "epoch": 11.004845548152634,
      "grad_norm": 0.06679485738277435,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 9090
    },
    {
      "epoch": 11.016959418534222,
      "grad_norm": 0.06430775672197342,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 9100
    },
    {
      "epoch": 11.029073288915809,
      "grad_norm": 0.051751215010881424,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 9110
    },
    {
      "epoch": 11.041187159297396,
      "grad_norm": 0.056830521672964096,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9120
    },
    {
      "epoch": 11.053301029678982,
      "grad_norm": 0.086794413626194,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 9130
    },
    {
      "epoch": 11.065414900060569,
      "grad_norm": 0.0654119923710823,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 9140
    },
    {
      "epoch": 11.077528770442155,
      "grad_norm": 0.0702187791466713,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 9150
    },
    {
      "epoch": 11.089642640823744,
      "grad_norm": 0.0479588657617569,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9160
    },
    {
      "epoch": 11.10175651120533,
      "grad_norm": 0.05811053887009621,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 9170
    },
    {
      "epoch": 11.113870381586917,
      "grad_norm": 0.0466933399438858,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9180
    },
    {
      "epoch": 11.125984251968504,
      "grad_norm": 0.038522813469171524,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9190
    },
    {
      "epoch": 11.13809812235009,
      "grad_norm": 0.05497270077466965,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9200
    },
    {
      "epoch": 11.150211992731677,
      "grad_norm": 0.04191945120692253,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9210
    },
    {
      "epoch": 11.162325863113265,
      "grad_norm": 0.05162179470062256,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9220
    },
    {
      "epoch": 11.174439733494852,
      "grad_norm": 0.048655007034540176,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 9230
    },
    {
      "epoch": 11.186553603876439,
      "grad_norm": 0.07145821303129196,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 9240
    },
    {
      "epoch": 11.198667474258025,
      "grad_norm": 0.05232417955994606,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 9250
    },
    {
      "epoch": 11.210781344639612,
      "grad_norm": 0.04854045435786247,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9260
    },
    {
      "epoch": 11.222895215021198,
      "grad_norm": 0.052684761583805084,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9270
    },
    {
      "epoch": 11.235009085402787,
      "grad_norm": 0.04502826929092407,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9280
    },
    {
      "epoch": 11.247122955784373,
      "grad_norm": 0.042252182960510254,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9290
    },
    {
      "epoch": 11.25923682616596,
      "grad_norm": 0.047412771731615067,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9300
    },
    {
      "epoch": 11.271350696547547,
      "grad_norm": 0.07154317200183868,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 9310
    },
    {
      "epoch": 11.283464566929133,
      "grad_norm": 0.047423381358385086,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 9320
    },
    {
      "epoch": 11.29557843731072,
      "grad_norm": 0.056423306465148926,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9330
    },
    {
      "epoch": 11.307692307692308,
      "grad_norm": 0.0567290335893631,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 9340
    },
    {
      "epoch": 11.319806178073895,
      "grad_norm": 0.04591021314263344,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 9350
    },
    {
      "epoch": 11.331920048455482,
      "grad_norm": 0.06360827386379242,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 9360
    },
    {
      "epoch": 11.344033918837068,
      "grad_norm": 0.05219729244709015,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9370
    },
    {
      "epoch": 11.356147789218655,
      "grad_norm": 0.05326950550079346,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9380
    },
    {
      "epoch": 11.368261659600241,
      "grad_norm": 0.057746194303035736,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 9390
    },
    {
      "epoch": 11.38037552998183,
      "grad_norm": 0.03981640562415123,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9400
    },
    {
      "epoch": 11.392489400363417,
      "grad_norm": 0.05255325511097908,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 9410
    },
    {
      "epoch": 11.404603270745003,
      "grad_norm": 0.057779520750045776,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9420
    },
    {
      "epoch": 11.41671714112659,
      "grad_norm": 0.05461664870381355,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9430
    },
    {
      "epoch": 11.428831011508176,
      "grad_norm": 0.043569426983594894,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9440
    },
    {
      "epoch": 11.440944881889763,
      "grad_norm": 0.11333983391523361,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 9450
    },
    {
      "epoch": 11.453058752271351,
      "grad_norm": 0.048881057649850845,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9460
    },
    {
      "epoch": 11.465172622652938,
      "grad_norm": 0.07504715025424957,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 9470
    },
    {
      "epoch": 11.477286493034525,
      "grad_norm": 0.06036854162812233,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9480
    },
    {
      "epoch": 11.489400363416111,
      "grad_norm": 0.03806626424193382,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 9490
    },
    {
      "epoch": 11.501514233797698,
      "grad_norm": 0.05462774634361267,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9500
    },
    {
      "epoch": 11.513628104179285,
      "grad_norm": 0.04361744970083237,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9510
    },
    {
      "epoch": 11.525741974560873,
      "grad_norm": 0.03996638208627701,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9520
    },
    {
      "epoch": 11.53785584494246,
      "grad_norm": 0.04758962243795395,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9530
    },
    {
      "epoch": 11.549969715324046,
      "grad_norm": 0.053586117923259735,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 9540
    },
    {
      "epoch": 11.562083585705633,
      "grad_norm": 0.0542464554309845,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9550
    },
    {
      "epoch": 11.57419745608722,
      "grad_norm": 0.07889345288276672,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 9560
    },
    {
      "epoch": 11.586311326468806,
      "grad_norm": 0.04022034630179405,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 9570
    },
    {
      "epoch": 11.598425196850394,
      "grad_norm": 0.04941993206739426,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9580
    },
    {
      "epoch": 11.610539067231981,
      "grad_norm": 0.04931218922138214,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 9590
    },
    {
      "epoch": 11.622652937613568,
      "grad_norm": 0.05874072387814522,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 9600
    },
    {
      "epoch": 11.634766807995154,
      "grad_norm": 0.07603458315134048,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 9610
    },
    {
      "epoch": 11.646880678376741,
      "grad_norm": 0.04467570036649704,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 9620
    },
    {
      "epoch": 11.658994548758328,
      "grad_norm": 0.044660888612270355,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 9630
    },
    {
      "epoch": 11.671108419139916,
      "grad_norm": 0.05107070505619049,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9640
    },
    {
      "epoch": 11.683222289521503,
      "grad_norm": 0.06049723923206329,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9650
    },
    {
      "epoch": 11.69533615990309,
      "grad_norm": 0.05224834382534027,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 9660
    },
    {
      "epoch": 11.707450030284676,
      "grad_norm": 0.06710938364267349,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9670
    },
    {
      "epoch": 11.719563900666262,
      "grad_norm": 0.038973160088062286,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 9680
    },
    {
      "epoch": 11.731677771047849,
      "grad_norm": 0.0509997233748436,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9690
    },
    {
      "epoch": 11.743791641429437,
      "grad_norm": 0.07427551597356796,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9700
    },
    {
      "epoch": 11.755905511811024,
      "grad_norm": 0.07728783041238785,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9710
    },
    {
      "epoch": 11.76801938219261,
      "grad_norm": 0.04182935878634453,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 9720
    },
    {
      "epoch": 11.780133252574197,
      "grad_norm": 0.05221060290932655,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 9730
    },
    {
      "epoch": 11.792247122955784,
      "grad_norm": 0.04384114593267441,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 9740
    },
    {
      "epoch": 11.80436099333737,
      "grad_norm": 0.046457644551992416,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 9750
    },
    {
      "epoch": 11.816474863718959,
      "grad_norm": 0.046483114361763,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 9760
    },
    {
      "epoch": 11.828588734100546,
      "grad_norm": 0.036909691989421844,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9770
    },
    {
      "epoch": 11.840702604482132,
      "grad_norm": 0.05201311409473419,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 9780
    },
    {
      "epoch": 11.852816474863719,
      "grad_norm": 0.038678597658872604,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 9790
    },
    {
      "epoch": 11.864930345245305,
      "grad_norm": 0.05434257537126541,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 9800
    },
    {
      "epoch": 11.877044215626892,
      "grad_norm": 0.06901085376739502,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 9810
    },
    {
      "epoch": 11.88915808600848,
      "grad_norm": 0.061999596655368805,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 9820
    },
    {
      "epoch": 11.901271956390067,
      "grad_norm": 0.06864991039037704,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 9830
    },
    {
      "epoch": 11.913385826771654,
      "grad_norm": 0.04287511110305786,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 9840
    },
    {
      "epoch": 11.92549969715324,
      "grad_norm": 0.0670277401804924,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 9850
    },
    {
      "epoch": 11.937613567534827,
      "grad_norm": 0.0439721941947937,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 9860
    },
    {
      "epoch": 11.949727437916414,
      "grad_norm": 0.05644761770963669,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 9870
    },
    {
      "epoch": 11.961841308298002,
      "grad_norm": 0.06848804652690887,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 9880
    },
    {
      "epoch": 11.973955178679589,
      "grad_norm": 0.04554004222154617,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 9890
    },
    {
      "epoch": 11.986069049061175,
      "grad_norm": 0.0520789660513401,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 9900
    },
    {
      "epoch": 11.986069049061175,
      "eval_average": 0.5718373161328332,
      "eval_crossner_ai": 0.558734023078537,
      "eval_crossner_literature": 0.5882352940676766,
      "eval_crossner_music": 0.7221006564050897,
      "eval_crossner_politics": 0.571201272821881,
      "eval_crossner_science": 0.643877550970549,
      "eval_mit-movie": 0.5181674565063239,
      "eval_mit-restaurant": 0.4005449590797753,
      "eval_runtime": 167.3265,
      "eval_samples_per_second": 8.367,
      "eval_steps_per_second": 0.263,
      "step": 9900
    },
    {
      "epoch": 11.986069049061175,
      "step": 9900,
      "total_flos": 6.18323766618882e+17,
      "train_loss": 0.03913711475904542,
      "train_runtime": 23579.31,
      "train_samples_per_second": 53.772,
      "train_steps_per_second": 0.42
    }
  ],
  "logging_steps": 10,
  "max_steps": 9900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.18323766618882e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
