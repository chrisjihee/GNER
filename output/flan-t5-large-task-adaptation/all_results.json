{
    "epoch": 9.995305875227135,
    "eval_average_f1": 0.6288851766173617,
    "eval_crossner_ai_f1": 0.603916613973959,
    "eval_crossner_ai_precision": 0.5893958076448101,
    "eval_crossner_ai_recall": 0.6191709844558783,
    "eval_crossner_literature_f1": 0.5298588490270788,
    "eval_crossner_literature_precision": 0.5464725643896364,
    "eval_crossner_literature_recall": 0.5142255005268161,
    "eval_crossner_music_f1": 0.7779411764205333,
    "eval_crossner_music_precision": 0.7831236121390982,
    "eval_crossner_music_recall": 0.7728268809349326,
    "eval_crossner_politics_f1": 0.7005325685693736,
    "eval_crossner_politics_precision": 0.7119067443796243,
    "eval_crossner_politics_recall": 0.6895161290322024,
    "eval_crossner_science_f1": 0.7140649149423504,
    "eval_crossner_science_precision": 0.6767578124999338,
    "eval_crossner_science_recall": 0.7557251908396122,
    "eval_mit-movie_f1": 0.6192733016879665,
    "eval_mit-movie_precision": 0.6805555555553193,
    "eval_mit-movie_recall": 0.5681159420288209,
    "eval_mit-restaurant_f1": 0.4566088117002693,
    "eval_mit-restaurant_precision": 0.5480769230767474,
    "eval_mit-restaurant_recall": 0.39130434782599743,
    "eval_runtime": 319.4357,
    "eval_samples": 1400,
    "eval_samples_per_second": 4.383,
    "eval_steps_per_second": 0.138,
    "predict_average_f1": 0.6370584598968405,
    "predict_crossner_ai_f1": 0.6061502069282071,
    "predict_crossner_ai_precision": 0.5774647887323618,
    "predict_crossner_ai_recall": 0.6378344741754426,
    "predict_crossner_literature_f1": 0.5591731265649877,
    "predict_crossner_literature_precision": 0.5730932203389527,
    "predict_crossner_literature_recall": 0.5459132189707091,
    "predict_crossner_music_f1": 0.7724005749379967,
    "predict_crossner_music_precision": 0.7708001275103357,
    "predict_crossner_music_recall": 0.7740076824583619,
    "predict_crossner_politics_f1": 0.7026335974953672,
    "predict_crossner_politics_precision": 0.7006629270780036,
    "predict_crossner_politics_recall": 0.7046153846153665,
    "predict_crossner_science_f1": 0.7261287934364821,
    "predict_crossner_science_precision": 0.6826722338204355,
    "predict_crossner_science_recall": 0.7754940711462144,
    "predict_mit-movie_f1": 0.5997205030446351,
    "predict_mit-movie_precision": 0.6420175251121897,
    "predict_mit-movie_recall": 0.5626521820565543,
    "predict_mit-restaurant_f1": 0.4932024168702073,
    "predict_mit-restaurant_precision": 0.6085740913326837,
    "predict_mit-restaurant_recall": 0.41460317460316143,
    "predict_runtime": 1012.7528,
    "predict_samples": 6470,
    "predict_samples_per_second": 6.389,
    "predict_steps_per_second": 0.2,
    "total_flos": 1.5973528293773148e+18,
    "train_loss": 0.02749265919309674,
    "train_runtime": 49974.6002,
    "train_samples": 105659,
    "train_samples_per_second": 21.143,
    "train_steps_per_second": 0.165
}