{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 3.9492346879314892,
      "learning_rate": 1.593279540393825e-05,
      "loss": 1.1068,
      "step": 10
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.5845837021507283,
      "learning_rate": 1.9950980392156866e-05,
      "loss": 0.1841,
      "step": 20
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.2682227562937713,
      "learning_rate": 1.946078431372549e-05,
      "loss": 0.0866,
      "step": 30
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.19427640760331774,
      "learning_rate": 1.897058823529412e-05,
      "loss": 0.0635,
      "step": 40
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.232267416592029,
      "learning_rate": 1.8480392156862748e-05,
      "loss": 0.0521,
      "step": 50
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.12864419280356232,
      "learning_rate": 1.7990196078431373e-05,
      "loss": 0.0465,
      "step": 60
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.1529954761462761,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.041,
      "step": 70
    },
    {
      "epoch": 1.0,
      "eval_average_f1": 0.6164073688673742,
      "eval_crossner_ai_f1": 0.4530612244401993,
      "eval_crossner_ai_precision": 0.4996249062265191,
      "eval_crossner_ai_recall": 0.4144368388300924,
      "eval_crossner_literature_f1": 0.5664913598297638,
      "eval_crossner_literature_precision": 0.5624067628045468,
      "eval_crossner_literature_recall": 0.5706357214934121,
      "eval_crossner_music_f1": 0.6105063889703413,
      "eval_crossner_music_precision": 0.6018662519439937,
      "eval_crossner_music_recall": 0.6193982074263567,
      "eval_crossner_politics_f1": 0.6259381171323767,
      "eval_crossner_politics_precision": 0.6433017591339474,
      "eval_crossner_politics_recall": 0.6094871794871638,
      "eval_crossner_science_f1": 0.558666117693991,
      "eval_crossner_science_precision": 0.5829037800687035,
      "eval_crossner_science_recall": 0.5363636363636152,
      "eval_mit-movie_f1": 0.810855872495378,
      "eval_mit-movie_precision": 0.8020890599230199,
      "eval_mit-movie_recall": 0.8198164450271432,
      "eval_mit-restaurant_f1": 0.6893325015095695,
      "eval_mit-restaurant_precision": 0.6774984671980172,
      "eval_mit-restaurant_recall": 0.7015873015872793,
      "eval_runtime": 440.7,
      "eval_samples_per_second": 14.681,
      "eval_steps_per_second": 0.059,
      "step": 71
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 0.17177179879525814,
      "learning_rate": 1.7009803921568627e-05,
      "loss": 0.0361,
      "step": 80
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 0.15963631645766338,
      "learning_rate": 1.6519607843137256e-05,
      "loss": 0.034,
      "step": 90
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 0.12188020398571513,
      "learning_rate": 1.6029411764705884e-05,
      "loss": 0.0335,
      "step": 100
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 0.13149948279248264,
      "learning_rate": 1.5539215686274513e-05,
      "loss": 0.0314,
      "step": 110
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 0.20774460199609962,
      "learning_rate": 1.5049019607843138e-05,
      "loss": 0.0315,
      "step": 120
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 0.22642559815235447,
      "learning_rate": 1.4558823529411765e-05,
      "loss": 0.031,
      "step": 130
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 0.16544901702259723,
      "learning_rate": 1.4068627450980394e-05,
      "loss": 0.0318,
      "step": 140
    },
    {
      "epoch": 2.0,
      "eval_average_f1": 0.6818014926379133,
      "eval_crossner_ai_f1": 0.5874079035001517,
      "eval_crossner_ai_precision": 0.6359680928208385,
      "eval_crossner_ai_recall": 0.5457373988798665,
      "eval_crossner_literature_f1": 0.6240167802332118,
      "eval_crossner_literature_precision": 0.6495633187772571,
      "eval_crossner_literature_recall": 0.6004036326942179,
      "eval_crossner_music_f1": 0.6653626731366293,
      "eval_crossner_music_precision": 0.6778478910660685,
      "eval_crossner_music_recall": 0.6533290653008753,
      "eval_crossner_politics_f1": 0.675017205731395,
      "eval_crossner_politics_precision": 0.7286775631500526,
      "eval_crossner_politics_recall": 0.6287179487179326,
      "eval_crossner_science_f1": 0.60775066634332,
      "eval_crossner_science_precision": 0.6314443971026573,
      "eval_crossner_science_recall": 0.5857707509881191,
      "eval_mit-movie_f1": 0.8485813277896694,
      "eval_mit-movie_precision": 0.8485018726591601,
      "eval_mit-movie_recall": 0.8486607979022129,
      "eval_mit-restaurant_f1": 0.7644738917310159,
      "eval_mit-restaurant_precision": 0.7597993101285431,
      "eval_mit-restaurant_recall": 0.7692063492063248,
      "eval_runtime": 302.2018,
      "eval_samples_per_second": 21.41,
      "eval_steps_per_second": 0.086,
      "step": 142
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 0.12245843601866072,
      "learning_rate": 1.357843137254902e-05,
      "loss": 0.0265,
      "step": 150
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 0.1093200349639966,
      "learning_rate": 1.3088235294117648e-05,
      "loss": 0.0244,
      "step": 160
    },
    {
      "epoch": 2.3943661971830985,
      "grad_norm": 0.13405583925306538,
      "learning_rate": 1.2598039215686275e-05,
      "loss": 0.0241,
      "step": 170
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 0.13153192293933463,
      "learning_rate": 1.2107843137254901e-05,
      "loss": 0.0247,
      "step": 180
    },
    {
      "epoch": 2.676056338028169,
      "grad_norm": 0.1203783537795097,
      "learning_rate": 1.1617647058823532e-05,
      "loss": 0.0242,
      "step": 190
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 0.1504195792329191,
      "learning_rate": 1.1127450980392159e-05,
      "loss": 0.0253,
      "step": 200
    },
    {
      "epoch": 2.9577464788732395,
      "grad_norm": 0.122208254123524,
      "learning_rate": 1.0637254901960786e-05,
      "loss": 0.0241,
      "step": 210
    },
    {
      "epoch": 3.0,
      "eval_average_f1": 0.7233676057254919,
      "eval_crossner_ai_f1": 0.6088820826452275,
      "eval_crossner_ai_precision": 0.5995174909529192,
      "eval_crossner_ai_recall": 0.618543870566234,
      "eval_crossner_literature_f1": 0.6470151101803533,
      "eval_crossner_literature_precision": 0.6355231143552003,
      "eval_crossner_literature_recall": 0.6589303733602089,
      "eval_crossner_music_f1": 0.7260985981544313,
      "eval_crossner_music_precision": 0.7147286821705204,
      "eval_crossner_music_recall": 0.7378361075543938,
      "eval_crossner_politics_f1": 0.739750445582782,
      "eval_crossner_politics_precision": 0.7346990389478822,
      "eval_crossner_politics_recall": 0.7448717948717758,
      "eval_crossner_science_f1": 0.6994387458373564,
      "eval_crossner_science_precision": 0.6852483883200347,
      "eval_crossner_science_recall": 0.7142292490118295,
      "eval_mit-movie_f1": 0.8623323397413416,
      "eval_mit-movie_precision": 0.8576987215119352,
      "eval_mit-movie_recall": 0.8670162951863483,
      "eval_mit-restaurant_f1": 0.7800559179369511,
      "eval_mit-restaurant_precision": 0.763686131386838,
      "eval_mit-restaurant_recall": 0.7971428571428318,
      "eval_runtime": 354.5292,
      "eval_samples_per_second": 18.25,
      "eval_steps_per_second": 0.073,
      "step": 213
    },
    {
      "epoch": 3.0985915492957745,
      "grad_norm": 0.11707928747549229,
      "learning_rate": 1.0147058823529413e-05,
      "loss": 0.0217,
      "step": 220
    },
    {
      "epoch": 3.23943661971831,
      "grad_norm": 0.1718151139920496,
      "learning_rate": 9.65686274509804e-06,
      "loss": 0.0203,
      "step": 230
    },
    {
      "epoch": 3.380281690140845,
      "grad_norm": 0.11906200300573513,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0191,
      "step": 240
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 0.1130727007470104,
      "learning_rate": 8.676470588235295e-06,
      "loss": 0.0202,
      "step": 250
    },
    {
      "epoch": 3.6619718309859155,
      "grad_norm": 0.1385618633114542,
      "learning_rate": 8.186274509803922e-06,
      "loss": 0.0201,
      "step": 260
    },
    {
      "epoch": 3.802816901408451,
      "grad_norm": 0.14197937777744457,
      "learning_rate": 7.69607843137255e-06,
      "loss": 0.02,
      "step": 270
    },
    {
      "epoch": 3.943661971830986,
      "grad_norm": 0.10185346738580854,
      "learning_rate": 7.205882352941177e-06,
      "loss": 0.019,
      "step": 280
    },
    {
      "epoch": 4.0,
      "eval_average_f1": 0.7431780781323535,
      "eval_crossner_ai_f1": 0.6437978559989773,
      "eval_crossner_ai_precision": 0.6338962605548472,
      "eval_crossner_ai_recall": 0.6540136901057465,
      "eval_crossner_literature_f1": 0.6880040322080299,
      "eval_crossner_literature_precision": 0.6873111782476995,
      "eval_crossner_literature_recall": 0.6886982845610147,
      "eval_crossner_music_f1": 0.7163987137763446,
      "eval_crossner_music_precision": 0.7196382428940336,
      "eval_crossner_music_recall": 0.7131882202304509,
      "eval_crossner_politics_f1": 0.7809082721743469,
      "eval_crossner_politics_precision": 0.7836302607797371,
      "eval_crossner_politics_recall": 0.7782051282051082,
      "eval_crossner_science_f1": 0.7163257429063876,
      "eval_crossner_science_precision": 0.6998491704373794,
      "eval_crossner_science_recall": 0.733596837944635,
      "eval_mit-movie_f1": 0.8698014237041988,
      "eval_mit-movie_precision": 0.8699643994753444,
      "eval_mit-movie_recall": 0.8696385090840818,
      "eval_mit-restaurant_f1": 0.7870105061581892,
      "eval_mit-restaurant_precision": 0.7892720306513158,
      "eval_mit-restaurant_recall": 0.7847619047618798,
      "eval_runtime": 279.4212,
      "eval_samples_per_second": 23.155,
      "eval_steps_per_second": 0.093,
      "step": 284
    },
    {
      "epoch": 4.084507042253521,
      "grad_norm": 0.09757216171142132,
      "learning_rate": 6.715686274509804e-06,
      "loss": 0.0182,
      "step": 290
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 0.12649364217181255,
      "learning_rate": 6.225490196078432e-06,
      "loss": 0.0173,
      "step": 300
    },
    {
      "epoch": 4.366197183098592,
      "grad_norm": 0.11725332874862851,
      "learning_rate": 5.735294117647059e-06,
      "loss": 0.0169,
      "step": 310
    },
    {
      "epoch": 4.507042253521127,
      "grad_norm": 0.11489859653180987,
      "learning_rate": 5.245098039215687e-06,
      "loss": 0.0173,
      "step": 320
    },
    {
      "epoch": 4.647887323943662,
      "grad_norm": 0.0949843869978779,
      "learning_rate": 4.754901960784314e-06,
      "loss": 0.016,
      "step": 330
    },
    {
      "epoch": 4.788732394366197,
      "grad_norm": 0.12219108329333447,
      "learning_rate": 4.264705882352942e-06,
      "loss": 0.017,
      "step": 340
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 0.10208410877870716,
      "learning_rate": 3.774509803921569e-06,
      "loss": 0.0166,
      "step": 350
    },
    {
      "epoch": 5.0,
      "eval_average_f1": 0.7457727993489084,
      "eval_crossner_ai_f1": 0.6387567395630451,
      "eval_crossner_ai_precision": 0.6513583441138,
      "eval_crossner_ai_recall": 0.626633478531386,
      "eval_crossner_literature_f1": 0.694689151724443,
      "eval_crossner_literature_precision": 0.6931190356604373,
      "eval_crossner_literature_recall": 0.6962663975781687,
      "eval_crossner_music_f1": 0.7271551026509362,
      "eval_crossner_music_precision": 0.7348806799607475,
      "eval_crossner_music_recall": 0.7195902688860205,
      "eval_crossner_politics_f1": 0.7779077322436864,
      "eval_crossner_politics_precision": 0.7886693017127592,
      "eval_crossner_politics_recall": 0.7674358974358777,
      "eval_crossner_science_f1": 0.7181221403915887,
      "eval_crossner_science_precision": 0.722867440929086,
      "eval_crossner_science_recall": 0.7134387351778374,
      "eval_mit-movie_f1": 0.8698009159234394,
      "eval_mit-movie_precision": 0.868097014925357,
      "eval_mit-movie_recall": 0.8715115190110344,
      "eval_mit-restaurant_f1": 0.7939778129452205,
      "eval_mit-restaurant_precision": 0.7927215189873167,
      "eval_mit-restaurant_recall": 0.79523809523807,
      "eval_runtime": 288.5751,
      "eval_samples_per_second": 22.421,
      "eval_steps_per_second": 0.09,
      "step": 355
    },
    {
      "epoch": 5.070422535211268,
      "grad_norm": 0.10602249373446772,
      "learning_rate": 3.2843137254901964e-06,
      "loss": 0.0156,
      "step": 360
    },
    {
      "epoch": 5.211267605633803,
      "grad_norm": 0.13243489018623605,
      "learning_rate": 2.7941176470588237e-06,
      "loss": 0.0144,
      "step": 370
    },
    {
      "epoch": 5.352112676056338,
      "grad_norm": 0.13482054683145245,
      "learning_rate": 2.303921568627451e-06,
      "loss": 0.0147,
      "step": 380
    },
    {
      "epoch": 5.492957746478873,
      "grad_norm": 0.10439833767038391,
      "learning_rate": 1.8137254901960786e-06,
      "loss": 0.0136,
      "step": 390
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 0.12398296732184537,
      "learning_rate": 1.323529411764706e-06,
      "loss": 0.0145,
      "step": 400
    },
    {
      "epoch": 5.774647887323944,
      "grad_norm": 0.11451033864498242,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0147,
      "step": 410
    },
    {
      "epoch": 5.915492957746479,
      "grad_norm": 0.1317316349125995,
      "learning_rate": 3.4313725490196084e-07,
      "loss": 0.0138,
      "step": 420
    },
    {
      "epoch": 6.0,
      "eval_average_f1": 0.7458759108976292,
      "eval_crossner_ai_f1": 0.6491616576532314,
      "eval_crossner_ai_precision": 0.6602316602316177,
      "eval_crossner_ai_recall": 0.6384567517112235,
      "eval_crossner_literature_f1": 0.6843434342933998,
      "eval_crossner_literature_precision": 0.6850353892820685,
      "eval_crossner_literature_recall": 0.683652875882912,
      "eval_crossner_music_f1": 0.7349242179441744,
      "eval_crossner_music_precision": 0.7404158544509181,
      "eval_crossner_music_recall": 0.7295134443021534,
      "eval_crossner_politics_f1": 0.7661290322080552,
      "eval_crossner_politics_precision": 0.7774551214360935,
      "eval_crossner_politics_recall": 0.7551282051281858,
      "eval_crossner_science_f1": 0.7170411612146383,
      "eval_crossner_science_precision": 0.7214885954381464,
      "eval_crossner_science_recall": 0.7126482213438453,
      "eval_mit-movie_f1": 0.8748592870043926,
      "eval_mit-movie_precision": 0.8763390340161459,
      "eval_mit-movie_recall": 0.873384528937987,
      "eval_mit-restaurant_f1": 0.7946725859655132,
      "eval_mit-restaurant_precision": 0.793791574279354,
      "eval_mit-restaurant_recall": 0.7955555555555303,
      "eval_runtime": 251.0594,
      "eval_samples_per_second": 25.771,
      "eval_steps_per_second": 0.104,
      "step": 426
    },
    {
      "epoch": 6.0,
      "step": 426,
      "total_flos": 6.410297412220355e+17,
      "train_loss": 0.055277152013191035,
      "train_runtime": 2966.5777,
      "train_samples_per_second": 36.679,
      "train_steps_per_second": 0.144
    }
  ],
  "logging_steps": 10,
  "max_steps": 426,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.410297412220355e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
