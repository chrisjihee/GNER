{'loss': 0.0313, 'grad_norm': tensor(0.3704, device='cuda:0'), 'learning_rate': 1.9848229342327153e-05, 'epoch': 0.15}
{'loss': 0.0338, 'grad_norm': tensor(0.5819, device='cuda:0'), 'learning_rate': 1.9679595278246207e-05, 'epoch': 0.17}
{'loss': 0.0294, 'grad_norm': tensor(0.1490, device='cuda:0'), 'learning_rate': 1.9510961214165264e-05, 'epoch': 0.19}
{'loss': 0.0309, 'grad_norm': tensor(0.8542, device='cuda:0'), 'learning_rate': 1.9342327150084318e-05, 'epoch': 0.22}
{'loss': 0.0289, 'grad_norm': tensor(0.4013, device='cuda:0'), 'learning_rate': 1.917369308600337e-05, 'epoch': 0.24}
  8%|██████████████████████▎                                                                                                                                                                                                                                                             | 100/1236 [24:47<4:42:06, 14.90s/it][INFO|trainer.py:3067] 2024-11-07 19:48:56,934 >> Saving model checkpoint to output/llama-7b-task-adaptation/tmp-checkpoint-100
[INFO|configuration_utils.py:473] 2024-11-07 19:48:56,935 >> Configuration saved in output/llama-7b-task-adaptation/tmp-checkpoint-100/config.json
[INFO|configuration_utils.py:614] 2024-11-07 19:48:56,936 >> Configuration saved in output/llama-7b-task-adaptation/tmp-checkpoint-100/generation_config.json
[INFO|modeling_utils.py:2462] 2024-11-07 19:49:11,513 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at output/llama-7b-task-adaptation/tmp-checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-11-07 19:49:11,514 >> tokenizer config file saved in output/llama-7b-task-adaptation/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-11-07 19:49:11,514 >> Special tokens file saved in output/llama-7b-task-adaptation/tmp-checkpoint-100/special_tokens_map.json
[2024-11-07 19:49:12,140] [INFO] [logging.py:129:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-11-07 19:49:12,148] [INFO] [logging.py:129:log_dist] [Rank 0] Saving model checkpoint: output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2024-11-07 19:49:12,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2024-11-07 19:49:29,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2024-11-07 19:49:29,075] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-11-07 19:49:48,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-11-07 19:49:48,511] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved output/llama-7b-task-adaptation/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-11-07 19:49:48,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[rank0]: Traceback (most recent call last):
[rank0]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank0]:     main()
[rank0]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 2029, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 2423, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 2525, in _save_checkpoint
[rank0]:     self.state.save_to_json(os.path.join(staging_output_dir, TRAINER_STATE_NAME))
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer_callback.py", line 113, in save_to_json
[rank0]:     json_string = json.dumps(dataclasses.asdict(self), indent=2, sort_keys=True) + "\n"
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/__init__.py", line 238, in dumps
[rank0]:     **kw).encode(obj)
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 202, in encode
[rank0]:     chunks = list(chunks)
[rank0]:              ^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 432, in _iterencode
[rank0]:     yield from _iterencode_dict(o, _current_indent_level)
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 406, in _iterencode_dict
[rank0]:     yield from chunks
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 326, in _iterencode_list
[rank0]:     yield from chunks
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 406, in _iterencode_dict
[rank0]:     yield from chunks
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 439, in _iterencode
[rank0]:     o = _default(o)
[rank0]:         ^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/json/encoder.py", line 180, in default
[rank0]:     raise TypeError(f'Object of type {o.__class__.__name__} '
[rank0]: TypeError: Object of type Tensor is not JSON serializable
  8%|██████████████████████▎                                                                                                                                                                                                                                                             | 100/1236 [25:45<4:52:38, 15.46s/it]
[rank0]:[W1107 19:49:49.918094542 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[2024-11-07 19:49:51,349] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331126
[2024-11-07 19:49:51,349] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331127
[2024-11-07 19:49:53,261] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331128
[2024-11-07 19:49:53,875] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331129
[2024-11-07 19:49:54,528] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331130
[2024-11-07 19:49:55,181] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331131
[2024-11-07 19:49:56,078] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331132
[2024-11-07 19:49:58,280] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2331133
[2024-11-07 19:49:59,055] [ERROR] [launch.py:325:sigkill_handler] ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=7', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '24', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--gradient_checkpointing', 'True', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero2_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234'] exits with return code = 1
(GNER) chrisjihee@dgx-a100:~/proj/GNER$
