+ CUDA_VISIBLE_DEVICES=0,1,2,3
++ shuf -i25000-30000 -n1
+ MASTER_PORT=26211
++ hostname
+ RUN_NAME=ZSE-yuyang-BL-lirs-b1
+ DATA_DIR=data
+ OUTPUT_DIR=output-lfs
+ TRAIN_JSON_DIR=data/pile-ner.json
+ DATA_CONFIG_DIR=configs/dataset/ZSE
+ INSTRUCTION_FILE=configs/instruction/GNER-paper.json
+ DEEPSPEED_CONFIG=configs/deepspeed/ds2_t5.json
+ MODEL_NAME_OR_PATH=google/flan-t5-base
+ deepspeed --include=localhost:0,1,2,3 --master_port 26211 gner/run.py --do_train --do_predict --predict_with_generate --run_name ZSE-yuyang-BL-lirs-b1 --data_dir data --output_dir output-lfs/ZSE-yuyang-BL-lirs-b1 --train_json_dir data/pile-ner.json --data_config_dir configs/dataset/ZSE --instruction_file configs/instruction/GNER-paper.json --deepspeed configs/deepspeed/ds2_t5.json --model_name_or_path google/flan-t5-base --metric_for_best_model eval_average --load_best_model_at_end True --greater_is_better True --gradient_accumulation_steps 4 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --preprocessing_num_workers 4 --overwrite_output_dir --overwrite_cache --num_train_epochs 12 --max_source_length 640 --max_target_length 640 --generation_max_length 640 --lr_scheduler_type constant --learning_rate 5e-05 --warmup_steps 0 --logging_steps 10 --logging_strategy steps --eval_strategy epoch --save_strategy epoch --seed 1234
[2025-02-24 02:40:18,136] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:21,098] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-02-24 02:40:21,099] [INFO] [runner.py:607:main] cmd = /home/chrisjihee/miniforge3/envs/GNER/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=26211 --enable_each_rank_log=None gner/run.py --do_train --do_predict --predict_with_generate --run_name ZSE-yuyang-BL-lirs-b1 --data_dir data --output_dir output-lfs/ZSE-yuyang-BL-lirs-b1 --train_json_dir data/pile-ner.json --data_config_dir configs/dataset/ZSE --instruction_file configs/instruction/GNER-paper.json --deepspeed configs/deepspeed/ds2_t5.json --model_name_or_path google/flan-t5-base --metric_for_best_model eval_average --load_best_model_at_end True --greater_is_better True --gradient_accumulation_steps 4 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --preprocessing_num_workers 4 --overwrite_output_dir --overwrite_cache --num_train_epochs 12 --max_source_length 640 --max_target_length 640 --generation_max_length 640 --lr_scheduler_type constant --learning_rate 5e-05 --warmup_steps 0 --logging_steps 10 --logging_strategy steps --eval_strategy epoch --save_strategy epoch --seed 1234
[2025-02-24 02:40:22,555] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:25,312] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-02-24 02:40:25,312] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-02-24 02:40:25,312] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-02-24 02:40:25,312] [INFO] [launch.py:164:main] dist_world_size=4
[2025-02-24 02:40:25,312] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-02-24 02:40:25,314] [INFO] [launch.py:256:main] process 2938319 spawned with command: ['/home/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=0', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'ZSE-yuyang-BL-lirs-b1', '--data_dir', 'data', '--output_dir', 'output-lfs/ZSE-yuyang-BL-lirs-b1', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset/ZSE', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234']
[2025-02-24 02:40:25,315] [INFO] [launch.py:256:main] process 2938320 spawned with command: ['/home/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=1', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'ZSE-yuyang-BL-lirs-b1', '--data_dir', 'data', '--output_dir', 'output-lfs/ZSE-yuyang-BL-lirs-b1', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset/ZSE', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234']
[2025-02-24 02:40:25,315] [INFO] [launch.py:256:main] process 2938321 spawned with command: ['/home/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=2', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'ZSE-yuyang-BL-lirs-b1', '--data_dir', 'data', '--output_dir', 'output-lfs/ZSE-yuyang-BL-lirs-b1', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset/ZSE', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234']
[2025-02-24 02:40:25,316] [INFO] [launch.py:256:main] process 2938322 spawned with command: ['/home/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=3', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'ZSE-yuyang-BL-lirs-b1', '--data_dir', 'data', '--output_dir', 'output-lfs/ZSE-yuyang-BL-lirs-b1', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset/ZSE', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234']
[2025-02-24 02:40:29,743] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:29,846] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:29,959] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:30,072] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-24 02:40:30,420] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-02-24 02:40:30,421] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-02-24 02:40:30,520] [INFO] [comm.py:652:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
02/24/2025 02:40:30 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
[2025-02-24 02:40:30,621] [INFO] [comm.py:652:init_distributed] cdb=None
Generating validation split: 0 examples [00:00, ? examples/s]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
02/24/2025 02:40:30 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False
Generating validation split: 1400 examples [00:00, 8908.13 examples/s]
Generating test split: 0 examples [00:00, ? examples/s][2025-02-24 02:40:30,824] [INFO] [comm.py:652:init_distributed] cdb=None
Generating test split: 6470 examples [00:00, 16851.03 examples/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
02/24/2025 02:40:32 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False
02/24/2025 02:40:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/24/2025 02:40:32 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds2_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=640,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output-lfs/ZSE-yuyang-BL-lirs-b1/runs/Feb24_02-40-29_lirs-b1,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.CONSTANT,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_average,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=12.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output-lfs/ZSE-yuyang-BL-lirs-b1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=ZSE-yuyang-BL-lirs-b1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=1234,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-e3ca1a81b5caf981
02/24/2025 02:40:32 - INFO - datasets.builder - Using custom data configuration default-e3ca1a81b5caf981
Loading Dataset Infos from /home/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
02/24/2025 02:40:32 - INFO - datasets.info - Loading Dataset Infos from /home/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
Overwrite dataset info from restored data version if exists.
02/24/2025 02:40:32 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
02/24/2025 02:40:32 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
Found cached dataset gner_dataset (/home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8)
02/24/2025 02:40:32 - INFO - datasets.builder - Found cached dataset gner_dataset (/home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8)
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
02/24/2025 02:40:32 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
02/24/2025 02:40:32 - INFO - datasets.arrow_dataset - Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
02/24/2025 02:40:32 - INFO - datasets.arrow_dataset - Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8
[INFO|configuration_utils.py:699] 2025-02-24 02:40:32,745 >> loading configuration file config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json
[INFO|configuration_utils.py:771] 2025-02-24 02:40:32,750 >> Model config T5Config {
  "_name_or_path": "google/flan-t5-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.50.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

Generating train split: 0 examples [00:00, ? examples/s][INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file spiece.model from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model
[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file tokenizer.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json
[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file special_tokens_map.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json
[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file tokenizer_config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:40:32,950 >> loading file chat_template.jinja from cache at None
[INFO|modeling_utils.py:4024] 2025-02-24 02:40:33,031 >> loading weights file model.safetensors from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors
[INFO|configuration_utils.py:1140] 2025-02-24 02:40:33,040 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

[INFO|modeling_utils.py:5017] 2025-02-24 02:40:33,356 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.

[INFO|modeling_utils.py:5025] 2025-02-24 02:40:33,357 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-02-24 02:40:33,571 >> loading configuration file generation_config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json
[INFO|configuration_utils.py:1140] 2025-02-24 02:40:33,572 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

Using custom data configuration default-51fcbc13d937ff86
02/24/2025 02:40:34 - INFO - datasets.builder - Using custom data configuration default-51fcbc13d937ff86
Loading Dataset Infos from /home/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
02/24/2025 02:40:34 - INFO - datasets.info - Loading Dataset Infos from /home/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Generating train split: 105659 examples [00:01, 62034.27 examples/s]
Found cached dataset json (/home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
02/24/2025 02:40:34 - INFO - datasets.builder - Found cached dataset json (/home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
02/24/2025 02:40:34 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
02/24/2025 02:40:34 - INFO - __main__ - Use data/pile-ner.json as train dataset, len(dataset) = 105659
02/24/2025 02:40:34 - INFO - __main__ - len(dataset) = 105659
Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00000_of_00004.arrow
02/24/2025 02:40:34 - INFO - datasets.arrow_dataset - Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00000_of_00004.arrow
Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00001_of_00004.arrow
02/24/2025 02:40:34 - INFO - datasets.arrow_dataset - Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00001_of_00004.arrow
Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00002_of_00004.arrow
02/24/2025 02:40:34 - INFO - datasets.arrow_dataset - Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00002_of_00004.arrow
Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00003_of_00004.arrow
02/24/2025 02:40:34 - INFO - datasets.arrow_dataset - Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00003_of_00004.arrow
Spawning 4 processes
02/24/2025 02:40:35 - INFO - datasets.arrow_dataset - Spawning 4 processes
Running tokenizer on train dataset (num_proc=4):   0%|          | 0/105659 [00:00<?, ? examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00000_of_00004.arrow
02/24/2025 02:40:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00000_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00001_of_00004.arrow
02/24/2025 02:40:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00001_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00002_of_00004.arrow
02/24/2025 02:40:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00002_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00003_of_00004.arrow
02/24/2025 02:40:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-51fcbc13d937ff86/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b05b968845243deb_00003_of_00004.arrow
Running tokenizer on train dataset (num_proc=4):   3%|▎         | 3374/105659 [00:00<00:25, 3947.34 examples/s][rank3]:[W224 02:40:36.085385209 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W224 02:40:36.109709426 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W224 02:40:36.125557631 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on train dataset (num_proc=4): 100%|██████████| 105659/105659 [00:26<00:00, 3940.07 examples/s]
Concatenating 4 shards
02/24/2025 02:41:02 - INFO - datasets.arrow_dataset - Concatenating 4 shards
[rank0]:[W224 02:41:02.429504344 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00000_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00000_of_00004.arrow
Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00001_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00001_of_00004.arrow
Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00002_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00002_of_00004.arrow
Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00003_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00003_of_00004.arrow
Running tokenizer on train dataset (num_proc=4):   0%|          | 0/105659 [00:00<?, ? examples/s]Spawning 4 processes
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Spawning 4 processes
Running tokenizer on validation dataset (num_proc=4):   0%|          | 0/1400 [00:00<?, ? examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00000_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00000_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00001_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00001_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00002_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00002_of_00004.arrow
Running tokenizer on train dataset (num_proc=4):   0%|          | 45/105659 [00:00<05:46, 304.48 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00003_of_00004.arrow
02/24/2025 02:41:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-fc0aafaa95844ca5_00003_of_00004.arrow
Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 1400/1400 [00:00<00:00, 3193.18 examples/s]
Running tokenizer on train dataset (num_proc=4):   1%|          | 1189/105659 [00:00<00:34, 3060.47 examples/s]Concatenating 4 shards
02/24/2025 02:41:04 - INFO - datasets.arrow_dataset - Concatenating 4 shards
Running tokenizer on train dataset (num_proc=4): 100%|██████████| 105659/105659 [00:29<00:00, 3587.29 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|██████████| 105659/105659 [00:29<00:00, 3586.17 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|██████████| 105659/105659 [00:29<00:00, 3556.44 examples/s]
Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00000_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00000_of_00004.arrow
Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00001_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00001_of_00004.arrow
Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00002_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00002_of_00004.arrow
Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00003_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00003_of_00004.arrow
Running tokenizer on validation dataset (num_proc=4):   0%|          | 0/1400 [00:00<?, ? examples/s]Spawning 4 processes
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Spawning 4 processes
Running tokenizer on validation dataset (num_proc=4):   0%|          | 0/1400 [00:00<?, ? examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00000_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00000_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00001_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00001_of_00004.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00002_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00002_of_00004.arrow
Running tokenizer on validation dataset (num_proc=4):   9%|▉         | 126/1400 [00:00<00:01, 812.08 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00003_of_00004.arrow
02/24/2025 02:41:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e3ca1a81b5caf981/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-cf9141b6d30f5af5_00003_of_00004.arrow
Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 1400/1400 [00:00<00:00, 3269.53 examples/s]
Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 1400/1400 [00:00<00:00, 3224.99 examples/s]
Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 1400/1400 [00:00<00:00, 2873.60 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 6470/6470 [00:01<00:00, 5367.99 examples/s]
Concatenating 4 shards
02/24/2025 02:41:34 - INFO - datasets.arrow_dataset - Concatenating 4 shards
Running tokenizer on prediction dataset (num_proc=4):  21%|██        | 1351/6470 [00:00<00:00, 5917.78 examples/s][2025-02-24 02:41:35,249] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown
[2025-02-24 02:41:35,249] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 6470/6470 [00:01<00:00, 5466.38 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 6470/6470 [00:01<00:00, 5440.83 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 6470/6470 [00:02<00:00, 2887.79 examples/s]
[2025-02-24 02:41:38,893] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-02-24 02:41:38,896] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-02-24 02:41:38,897] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-02-24 02:41:38,910] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-02-24 02:41:38,910] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-02-24 02:41:38,910] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-02-24 02:41:38,910] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-02-24 02:41:38,910] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-02-24 02:41:38,910] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-02-24 02:41:38,910] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-02-24 02:41:51,485] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[WARNING|logging.py:329] 2025-02-24 02:41:51,734 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
[2025-02-24 02:41:51,913] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-02-24 02:41:51,945] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[WARNING|logging.py:329] 2025-02-24 02:41:52,210 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
[WARNING|logging.py:329] 2025-02-24 02:41:52,212 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
[2025-02-24 02:41:52,249] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-02-24 02:41:52,250] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.15 GB         CA 1.16 GB         Max_CA 1 GB
[2025-02-24 02:41:52,250] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 45.45 GB, percent = 4.5%
[2025-02-24 02:41:52,405] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-02-24 02:41:52,406] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.38 GB         CA 1.39 GB         Max_CA 1 GB
[2025-02-24 02:41:52,406] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 45.48 GB, percent = 4.5%
[2025-02-24 02:41:52,406] [INFO] [stage_1_and_2.py:545:__init__] optimizer state initialized
[2025-02-24 02:41:52,558] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-02-24 02:41:52,558] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.15 GB         CA 1.39 GB         Max_CA 1 GB
[2025-02-24 02:41:52,558] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 45.51 GB, percent = 4.5%
[2025-02-24 02:41:52,560] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-02-24 02:41:52,560] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-02-24 02:41:52,560] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-02-24 02:41:52,560] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7d93378bc0>
[2025-02-24 02:41:52,560] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-02-24 02:41:52,561] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-02-24 02:41:52,562] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7d93356240>
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-02-24 02:41:52,563] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   optimizer_name ............... adamw
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-02-24 02:41:52,564] [INFO] [config.py:1003:print]   scheduler_name ............... WarmupLR
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   world_size ................... 4
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-02-24 02:41:52,565] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2025-02-24 02:41:52,565] [INFO] [config.py:989:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": false
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 5e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 5e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 128,
    "train_micro_batch_size_per_gpu": 8,
    "steps_per_print": inf
}
[INFO|trainer.py:2407] 2025-02-24 02:41:52,567 >> ***** Running training *****
[INFO|trainer.py:2408] 2025-02-24 02:41:52,567 >>   Num examples = 105,659
[INFO|trainer.py:2409] 2025-02-24 02:41:52,567 >>   Num Epochs = 12
[INFO|trainer.py:2410] 2025-02-24 02:41:52,567 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2413] 2025-02-24 02:41:52,567 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2414] 2025-02-24 02:41:52,567 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2415] 2025-02-24 02:41:52,567 >>   Total optimization steps = 9,900
[INFO|trainer.py:2416] 2025-02-24 02:41:52,568 >>   Number of trainable parameters = 247,577,856
  0%|          | 0/9900 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-02-24 02:41:53,832 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
{'loss': 0.9929, 'grad_norm': 1.1231955289840698, 'learning_rate': 5e-05, 'epoch': 0.01}
{'loss': 0.3456, 'grad_norm': 0.36320143938064575, 'learning_rate': 5e-05, 'epoch': 0.02}
{'loss': 0.2248, 'grad_norm': 0.27436184883117676, 'learning_rate': 5e-05, 'epoch': 0.04}
{'loss': 0.1857, 'grad_norm': 0.24473939836025238, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 0.1611, 'grad_norm': 0.1959906816482544, 'learning_rate': 5e-05, 'epoch': 0.06}
{'loss': 0.1447, 'grad_norm': 0.2106345146894455, 'learning_rate': 5e-05, 'epoch': 0.07}
{'loss': 0.1318, 'grad_norm': 0.20398220419883728, 'learning_rate': 5e-05, 'epoch': 0.08}
{'loss': 0.1214, 'grad_norm': 0.1611875295639038, 'learning_rate': 5e-05, 'epoch': 0.1}
{'loss': 0.1182, 'grad_norm': 0.16641052067279816, 'learning_rate': 5e-05, 'epoch': 0.11}
{'loss': 0.1135, 'grad_norm': 0.16283105313777924, 'learning_rate': 5e-05, 'epoch': 0.12}
{'loss': 0.1078, 'grad_norm': 0.13588233292102814, 'learning_rate': 5e-05, 'epoch': 0.13}
{'loss': 0.1056, 'grad_norm': 0.16472987830638885, 'learning_rate': 5e-05, 'epoch': 0.15}
{'loss': 0.0999, 'grad_norm': 0.13890142738819122, 'learning_rate': 5e-05, 'epoch': 0.16}
{'loss': 0.0974, 'grad_norm': 0.16446112096309662, 'learning_rate': 5e-05, 'epoch': 0.17}
{'loss': 0.0959, 'grad_norm': 0.14486229419708252, 'learning_rate': 5e-05, 'epoch': 0.18}
{'loss': 0.0912, 'grad_norm': 0.12509874999523163, 'learning_rate': 5e-05, 'epoch': 0.19}
{'loss': 0.0933, 'grad_norm': 0.15885110199451447, 'learning_rate': 5e-05, 'epoch': 0.21}
{'loss': 0.0892, 'grad_norm': 0.1452404409646988, 'learning_rate': 5e-05, 'epoch': 0.22}
{'loss': 0.0864, 'grad_norm': 0.11249794811010361, 'learning_rate': 5e-05, 'epoch': 0.23}
{'loss': 0.0849, 'grad_norm': 0.15895351767539978, 'learning_rate': 5e-05, 'epoch': 0.24}
{'loss': 0.0842, 'grad_norm': 0.1182815358042717, 'learning_rate': 5e-05, 'epoch': 0.25}
{'loss': 0.0827, 'grad_norm': 0.13125218451023102, 'learning_rate': 5e-05, 'epoch': 0.27}
{'loss': 0.0825, 'grad_norm': 0.1106165274977684, 'learning_rate': 5e-05, 'epoch': 0.28}
{'loss': 0.0789, 'grad_norm': 0.136031836271286, 'learning_rate': 5e-05, 'epoch': 0.29}
{'loss': 0.0783, 'grad_norm': 0.11000953614711761, 'learning_rate': 5e-05, 'epoch': 0.3}
{'loss': 0.0769, 'grad_norm': 0.1412544846534729, 'learning_rate': 5e-05, 'epoch': 0.31}
{'loss': 0.0783, 'grad_norm': 0.11731408536434174, 'learning_rate': 5e-05, 'epoch': 0.33}
{'loss': 0.0746, 'grad_norm': 0.13158589601516724, 'learning_rate': 5e-05, 'epoch': 0.34}
{'loss': 0.0756, 'grad_norm': 0.10154858231544495, 'learning_rate': 5e-05, 'epoch': 0.35}
{'loss': 0.0757, 'grad_norm': 0.1265193074941635, 'learning_rate': 5e-05, 'epoch': 0.36}
{'loss': 0.0725, 'grad_norm': 0.10101460665464401, 'learning_rate': 5e-05, 'epoch': 0.38}
{'loss': 0.0714, 'grad_norm': 0.11151640862226486, 'learning_rate': 5e-05, 'epoch': 0.39}
{'loss': 0.0726, 'grad_norm': 0.11383434385061264, 'learning_rate': 5e-05, 'epoch': 0.4}
{'loss': 0.0716, 'grad_norm': 0.14345666766166687, 'learning_rate': 5e-05, 'epoch': 0.41}
{'loss': 0.0711, 'grad_norm': 0.10339990258216858, 'learning_rate': 5e-05, 'epoch': 0.42}
{'loss': 0.0683, 'grad_norm': 0.11683354526758194, 'learning_rate': 5e-05, 'epoch': 0.44}
{'loss': 0.0698, 'grad_norm': 0.11687421053647995, 'learning_rate': 5e-05, 'epoch': 0.45}
{'loss': 0.0694, 'grad_norm': 0.11910122632980347, 'learning_rate': 5e-05, 'epoch': 0.46}
{'loss': 0.0684, 'grad_norm': 0.12192738801240921, 'learning_rate': 5e-05, 'epoch': 0.47}
{'loss': 0.0679, 'grad_norm': 0.10808023810386658, 'learning_rate': 5e-05, 'epoch': 0.48}
{'loss': 0.0683, 'grad_norm': 0.12642380595207214, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.0689, 'grad_norm': 0.11530129611492157, 'learning_rate': 5e-05, 'epoch': 0.51}
{'loss': 0.0649, 'grad_norm': 0.10206703096628189, 'learning_rate': 5e-05, 'epoch': 0.52}
{'loss': 0.0648, 'grad_norm': 0.1202523335814476, 'learning_rate': 5e-05, 'epoch': 0.53}
{'loss': 0.0667, 'grad_norm': 0.11682991683483124, 'learning_rate': 5e-05, 'epoch': 0.55}
{'loss': 0.0664, 'grad_norm': 0.09006969630718231, 'learning_rate': 5e-05, 'epoch': 0.56}
{'loss': 0.0637, 'grad_norm': 0.09241899102926254, 'learning_rate': 5e-05, 'epoch': 0.57}
{'loss': 0.0659, 'grad_norm': 0.11405986547470093, 'learning_rate': 5e-05, 'epoch': 0.58}
{'loss': 0.0619, 'grad_norm': 0.09550409018993378, 'learning_rate': 5e-05, 'epoch': 0.59}
{'loss': 0.0616, 'grad_norm': 0.10547858476638794, 'learning_rate': 5e-05, 'epoch': 0.61}
{'loss': 0.0635, 'grad_norm': 0.11573845893144608, 'learning_rate': 5e-05, 'epoch': 0.62}
{'loss': 0.0631, 'grad_norm': 0.14170415699481964, 'learning_rate': 5e-05, 'epoch': 0.63}
{'loss': 0.063, 'grad_norm': 0.09501916915178299, 'learning_rate': 5e-05, 'epoch': 0.64}
{'loss': 0.0628, 'grad_norm': 0.10368267446756363, 'learning_rate': 5e-05, 'epoch': 0.65}
{'loss': 0.0586, 'grad_norm': 0.09848456084728241, 'learning_rate': 5e-05, 'epoch': 0.67}
{'loss': 0.0589, 'grad_norm': 0.09253591299057007, 'learning_rate': 5e-05, 'epoch': 0.68}
{'loss': 0.0588, 'grad_norm': 0.11786065250635147, 'learning_rate': 5e-05, 'epoch': 0.69}
{'loss': 0.0594, 'grad_norm': 0.09436038881540298, 'learning_rate': 5e-05, 'epoch': 0.7}
{'loss': 0.0618, 'grad_norm': 0.0981287956237793, 'learning_rate': 5e-05, 'epoch': 0.71}
{'loss': 0.0597, 'grad_norm': 0.08872020244598389, 'learning_rate': 5e-05, 'epoch': 0.73}
{'loss': 0.0594, 'grad_norm': 0.0926903709769249, 'learning_rate': 5e-05, 'epoch': 0.74}
{'loss': 0.0594, 'grad_norm': 0.09287098050117493, 'learning_rate': 5e-05, 'epoch': 0.75}
{'loss': 0.0596, 'grad_norm': 0.10343164205551147, 'learning_rate': 5e-05, 'epoch': 0.76}
{'loss': 0.0565, 'grad_norm': 0.09680279344320297, 'learning_rate': 5e-05, 'epoch': 0.78}
{'loss': 0.0581, 'grad_norm': 0.2001344859600067, 'learning_rate': 5e-05, 'epoch': 0.79}
{'loss': 0.0591, 'grad_norm': 0.10930377244949341, 'learning_rate': 5e-05, 'epoch': 0.8}
{'loss': 0.0576, 'grad_norm': 0.08943571895360947, 'learning_rate': 5e-05, 'epoch': 0.81}
{'loss': 0.0571, 'grad_norm': 0.08038229495286942, 'learning_rate': 5e-05, 'epoch': 0.82}
{'loss': 0.0573, 'grad_norm': 0.09254691004753113, 'learning_rate': 5e-05, 'epoch': 0.84}
{'loss': 0.0575, 'grad_norm': 0.08796384185552597, 'learning_rate': 5e-05, 'epoch': 0.85}
{'loss': 0.0568, 'grad_norm': 0.08372613042593002, 'learning_rate': 5e-05, 'epoch': 0.86}
{'loss': 0.0584, 'grad_norm': 0.09298484772443771, 'learning_rate': 5e-05, 'epoch': 0.87}
{'loss': 0.0569, 'grad_norm': 0.07293003797531128, 'learning_rate': 5e-05, 'epoch': 0.88}
{'loss': 0.0564, 'grad_norm': 0.09392254054546356, 'learning_rate': 5e-05, 'epoch': 0.9}
{'loss': 0.0552, 'grad_norm': 0.09623833745718002, 'learning_rate': 5e-05, 'epoch': 0.91}
{'loss': 0.057, 'grad_norm': 0.10208217054605484, 'learning_rate': 5e-05, 'epoch': 0.92}
{'loss': 0.0564, 'grad_norm': 0.07811024785041809, 'learning_rate': 5e-05, 'epoch': 0.93}
{'loss': 0.0574, 'grad_norm': 0.08980744332075119, 'learning_rate': 5e-05, 'epoch': 0.94}
{'loss': 0.0565, 'grad_norm': 0.12658275663852692, 'learning_rate': 5e-05, 'epoch': 0.96}
{'loss': 0.054, 'grad_norm': 0.09173087030649185, 'learning_rate': 5e-05, 'epoch': 0.97}
{'loss': 0.0565, 'grad_norm': 0.14515607059001923, 'learning_rate': 5e-05, 'epoch': 0.98}
{'loss': 0.0547, 'grad_norm': 0.08116932213306427, 'learning_rate': 5e-05, 'epoch': 0.99}
{'eval_mit-movie': 0.34740259735320617, 'eval_mit-restaurant': 0.2503770738614743, 'eval_crossner_ai': 0.4479638008557293, 'eval_crossner_literature': 0.49336870021520196, 'eval_crossner_music': 0.5691235789745283, 'eval_crossner_politics': 0.5284360189073253, 'eval_crossner_science': 0.6050172158874584, 'eval_average': 0.46309842657927475, 'eval_runtime': 171.5737, 'eval_samples_per_second': 8.16, 'eval_steps_per_second': 0.256, 'epoch': 1.0}
  8%|▊         | 826/9900 [32:47<4:38:32,  1.84[INFO|trainer.py:3948] 2025-02-24 03:14:40,865 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826
[INFO|configuration_utils.py:423] 2025-02-24 03:14:40,867 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/config.json
[INFO|configuration_utils.py:909] 2025-02-24 03:14:40,868 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 03:14:42,515 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 03:14:42,516 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 03:14:42,516 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 03:14:42,518 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/spiece.model
[2025-02-24 03:14:42,541] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step825 is about to be saved!
[2025-02-24 03:14:42,547] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/mp_rank_00_model_states.pt
[2025-02-24 03:14:42,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/mp_rank_00_model_states.pt...
[2025-02-24 03:14:44,239] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/mp_rank_00_model_states.pt.
[2025-02-24 03:14:44,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 03:14:45,577] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 03:14:45,577] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-826/global_step825/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 03:14:45,577] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step825 is ready now!
{'loss': 0.0523, 'grad_norm': 0.10511570423841476, 'learning_rate': 5e-05, 'epoch': 1.0}
{'loss': 0.0537, 'grad_norm': 0.09476611018180847, 'learning_rate': 5e-05, 'epoch': 1.02}
{'loss': 0.0543, 'grad_norm': 0.09046481549739838, 'learning_rate': 5e-05, 'epoch': 1.03}
{'loss': 0.0535, 'grad_norm': 0.10475391149520874, 'learning_rate': 5e-05, 'epoch': 1.04}
{'loss': 0.0547, 'grad_norm': 0.12361264228820801, 'learning_rate': 5e-05, 'epoch': 1.05}
{'loss': 0.0535, 'grad_norm': 0.10582255572080612, 'learning_rate': 5e-05, 'epoch': 1.07}
{'loss': 0.0535, 'grad_norm': 0.10560828447341919, 'learning_rate': 5e-05, 'epoch': 1.08}
{'loss': 0.0532, 'grad_norm': 0.10679036378860474, 'learning_rate': 5e-05, 'epoch': 1.09}
{'loss': 0.0516, 'grad_norm': 0.10112778097391129, 'learning_rate': 5e-05, 'epoch': 1.1}
{'loss': 0.0521, 'grad_norm': 0.09780387580394745, 'learning_rate': 5e-05, 'epoch': 1.11}
{'loss': 0.0534, 'grad_norm': 0.17328669130802155, 'learning_rate': 5e-05, 'epoch': 1.13}
{'loss': 0.0527, 'grad_norm': 0.12370359897613525, 'learning_rate': 5e-05, 'epoch': 1.14}
{'loss': 0.0536, 'grad_norm': 0.08213211596012115, 'learning_rate': 5e-05, 'epoch': 1.15}
{'loss': 0.051, 'grad_norm': 0.08561252802610397, 'learning_rate': 5e-05, 'epoch': 1.16}
{'loss': 0.0521, 'grad_norm': 0.0810365080833435, 'learning_rate': 5e-05, 'epoch': 1.17}
{'loss': 0.0514, 'grad_norm': 0.18420428037643433, 'learning_rate': 5e-05, 'epoch': 1.19}
{'loss': 0.0515, 'grad_norm': 0.08886007219552994, 'learning_rate': 5e-05, 'epoch': 1.2}
{'loss': 0.0514, 'grad_norm': 0.0862431451678276, 'learning_rate': 5e-05, 'epoch': 1.21}
{'loss': 0.0516, 'grad_norm': 0.06725351512432098, 'learning_rate': 5e-05, 'epoch': 1.22}
{'loss': 0.0498, 'grad_norm': 0.09074319899082184, 'learning_rate': 5e-05, 'epoch': 1.24}
{'loss': 0.0504, 'grad_norm': 0.07698699831962585, 'learning_rate': 5e-05, 'epoch': 1.25}
{'loss': 0.0501, 'grad_norm': 0.0926545262336731, 'learning_rate': 5e-05, 'epoch': 1.26}
{'loss': 0.0517, 'grad_norm': 0.07141199707984924, 'learning_rate': 5e-05, 'epoch': 1.27}
{'loss': 0.0503, 'grad_norm': 0.07899293303489685, 'learning_rate': 5e-05, 'epoch': 1.28}
{'loss': 0.0495, 'grad_norm': 0.10218171030282974, 'learning_rate': 5e-05, 'epoch': 1.3}
{'loss': 0.0528, 'grad_norm': 0.08747633546590805, 'learning_rate': 5e-05, 'epoch': 1.31}
{'loss': 0.0488, 'grad_norm': 0.07845129817724228, 'learning_rate': 5e-05, 'epoch': 1.32}
{'loss': 0.0487, 'grad_norm': 0.08114531636238098, 'learning_rate': 5e-05, 'epoch': 1.33}
{'loss': 0.0488, 'grad_norm': 0.09188779443502426, 'learning_rate': 5e-05, 'epoch': 1.34}
{'loss': 0.0507, 'grad_norm': 0.07478447258472443, 'learning_rate': 5e-05, 'epoch': 1.36}
{'loss': 0.0489, 'grad_norm': 0.06730889528989792, 'learning_rate': 5e-05, 'epoch': 1.37}
{'loss': 0.0515, 'grad_norm': 0.08276066184043884, 'learning_rate': 5e-05, 'epoch': 1.38}
{'loss': 0.0482, 'grad_norm': 0.08336169272661209, 'learning_rate': 5e-05, 'epoch': 1.39}
{'loss': 0.05, 'grad_norm': 0.08418937027454376, 'learning_rate': 5e-05, 'epoch': 1.4}
{'loss': 0.0493, 'grad_norm': 0.08711593598127365, 'learning_rate': 5e-05, 'epoch': 1.42}
{'loss': 0.0502, 'grad_norm': 0.07757661491632462, 'learning_rate': 5e-05, 'epoch': 1.43}
{'loss': 0.0472, 'grad_norm': 0.08124988526105881, 'learning_rate': 5e-05, 'epoch': 1.44}
{'loss': 0.0491, 'grad_norm': 0.08967266976833344, 'learning_rate': 5e-05, 'epoch': 1.45}
{'loss': 0.0474, 'grad_norm': 0.09008904546499252, 'learning_rate': 5e-05, 'epoch': 1.47}
{'loss': 0.0498, 'grad_norm': 0.09094715863466263, 'learning_rate': 5e-05, 'epoch': 1.48}
{'loss': 0.048, 'grad_norm': 0.08747633546590805, 'learning_rate': 5e-05, 'epoch': 1.49}
{'loss': 0.0484, 'grad_norm': 0.08034519106149673, 'learning_rate': 5e-05, 'epoch': 1.5}
{'loss': 0.048, 'grad_norm': 0.07444705069065094, 'learning_rate': 5e-05, 'epoch': 1.51}
{'loss': 0.0485, 'grad_norm': 0.07471435517072678, 'learning_rate': 5e-05, 'epoch': 1.53}
{'loss': 0.0492, 'grad_norm': 0.09149091690778732, 'learning_rate': 5e-05, 'epoch': 1.54}
{'loss': 0.0496, 'grad_norm': 0.07705176621675491, 'learning_rate': 5e-05, 'epoch': 1.55}
{'loss': 0.0483, 'grad_norm': 0.06553028523921967, 'learning_rate': 5e-05, 'epoch': 1.56}
{'loss': 0.0498, 'grad_norm': 0.09903009235858917, 'learning_rate': 5e-05, 'epoch': 1.57}
{'loss': 0.048, 'grad_norm': 0.08431965112686157, 'learning_rate': 5e-05, 'epoch': 1.59}
{'loss': 0.0487, 'grad_norm': 0.09428584575653076, 'learning_rate': 5e-05, 'epoch': 1.6}
{'loss': 0.0468, 'grad_norm': 0.07093799114227295, 'learning_rate': 5e-05, 'epoch': 1.61}
{'loss': 0.0483, 'grad_norm': 0.07926428318023682, 'learning_rate': 5e-05, 'epoch': 1.62}
{'loss': 0.0478, 'grad_norm': 0.0769810900092125, 'learning_rate': 5e-05, 'epoch': 1.63}
{'loss': 0.0481, 'grad_norm': 0.07047799229621887, 'learning_rate': 5e-05, 'epoch': 1.65}
{'loss': 0.0472, 'grad_norm': 0.08091410994529724, 'learning_rate': 5e-05, 'epoch': 1.66}
{'loss': 0.0462, 'grad_norm': 0.07779332995414734, 'learning_rate': 5e-05, 'epoch': 1.67}
{'loss': 0.0466, 'grad_norm': 0.10377072542905807, 'learning_rate': 5e-05, 'epoch': 1.68}
{'loss': 0.0487, 'grad_norm': 0.09074541181325912, 'learning_rate': 5e-05, 'epoch': 1.7}
{'loss': 0.047, 'grad_norm': 0.08987781405448914, 'learning_rate': 5e-05, 'epoch': 1.71}
{'loss': 0.0466, 'grad_norm': 0.06394480168819427, 'learning_rate': 5e-05, 'epoch': 1.72}
{'loss': 0.0472, 'grad_norm': 0.07004043459892273, 'learning_rate': 5e-05, 'epoch': 1.73}
{'loss': 0.0464, 'grad_norm': 0.11847082525491714, 'learning_rate': 5e-05, 'epoch': 1.74}
{'loss': 0.0482, 'grad_norm': 0.10330227762460709, 'learning_rate': 5e-05, 'epoch': 1.76}
{'loss': 0.0472, 'grad_norm': 0.051212068647146225, 'learning_rate': 5e-05, 'epoch': 1.77}
{'loss': 0.0474, 'grad_norm': 0.059796739369630814, 'learning_rate': 5e-05, 'epoch': 1.78}
{'loss': 0.0444, 'grad_norm': 0.06739230453968048, 'learning_rate': 5e-05, 'epoch': 1.79}
{'loss': 0.0473, 'grad_norm': 0.0879201665520668, 'learning_rate': 5e-05, 'epoch': 1.8}
{'loss': 0.048, 'grad_norm': 0.06901533901691437, 'learning_rate': 5e-05, 'epoch': 1.82}
{'loss': 0.0459, 'grad_norm': 0.07394358515739441, 'learning_rate': 5e-05, 'epoch': 1.83}
{'loss': 0.0489, 'grad_norm': 0.1030767485499382, 'learning_rate': 5e-05, 'epoch': 1.84}
{'loss': 0.0451, 'grad_norm': 0.07667740434408188, 'learning_rate': 5e-05, 'epoch': 1.85}
{'loss': 0.0462, 'grad_norm': 0.07394170016050339, 'learning_rate': 5e-05, 'epoch': 1.86}
{'loss': 0.0473, 'grad_norm': 0.07473903149366379, 'learning_rate': 5e-05, 'epoch': 1.88}
{'loss': 0.0475, 'grad_norm': 0.10049538314342499, 'learning_rate': 5e-05, 'epoch': 1.89}
{'loss': 0.0452, 'grad_norm': 0.07185685634613037, 'learning_rate': 5e-05, 'epoch': 1.9}
{'loss': 0.0473, 'grad_norm': 0.08475207537412643, 'learning_rate': 5e-05, 'epoch': 1.91}
{'loss': 0.0468, 'grad_norm': 0.0891181007027626, 'learning_rate': 5e-05, 'epoch': 1.93}
{'loss': 0.0455, 'grad_norm': 0.06952392309904099, 'learning_rate': 5e-05, 'epoch': 1.94}
{'loss': 0.0444, 'grad_norm': 0.0880015417933464, 'learning_rate': 5e-05, 'epoch': 1.95}
{'loss': 0.0449, 'grad_norm': 0.07869972288608551, 'learning_rate': 5e-05, 'epoch': 1.96}
{'loss': 0.046, 'grad_norm': 0.089900903403759, 'learning_rate': 5e-05, 'epoch': 1.97}
{'loss': 0.0454, 'grad_norm': 0.07255958020687103, 'learning_rate': 5e-05, 'epoch': 1.99}
{'loss': 0.0463, 'grad_norm': 0.06812942028045654, 'learning_rate': 5e-05, 'epoch': 2.0}
{'eval_mit-movie': 0.3934426229015507, 'eval_mit-restaurant': 0.24320457792159084, 'eval_crossner_ai': 0.4917284654383899, 'eval_crossner_literature': 0.5521339815815232, 'eval_crossner_music': 0.6717668487659805, 'eval_crossner_politics': 0.5508170585390421, 'eval_crossner_science': 0.6596173211989675, 'eval_average': 0.5089586966210063, 'eval_runtime': 170.5181, 'eval_samples_per_second': 8.21, 'eval_steps_per_second': 0.258, 'epoch': 2.0}
 17%|█▋        | 1652/9900 [1:05:36<4:15:45,  1[INFO|trainer.py:3948] 2025-02-24 03:47:29,512 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652
[INFO|configuration_utils.py:423] 2025-02-24 03:47:29,514 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/config.json
[INFO|configuration_utils.py:909] 2025-02-24 03:47:29,514 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 03:47:31,148 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 03:47:31,150 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 03:47:31,150 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 03:47:31,152 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/spiece.model
[2025-02-24 03:47:31,192] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1651 is about to be saved!
[2025-02-24 03:47:31,199] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/mp_rank_00_model_states.pt
[2025-02-24 03:47:31,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/mp_rank_00_model_states.pt...
[2025-02-24 03:47:32,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/mp_rank_00_model_states.pt.
[2025-02-24 03:47:32,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 03:47:34,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 03:47:34,332] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-1652/global_step1651/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 03:47:34,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1651 is ready now!
{'loss': 0.0439, 'grad_norm': 0.11050412803888321, 'learning_rate': 5e-05, 'epoch': 2.01}
{'loss': 0.043, 'grad_norm': 0.07062633335590363, 'learning_rate': 5e-05, 'epoch': 2.02}
{'loss': 0.0455, 'grad_norm': 0.07602851092815399, 'learning_rate': 5e-05, 'epoch': 2.03}
{'loss': 0.0449, 'grad_norm': 0.08186434954404831, 'learning_rate': 5e-05, 'epoch': 2.05}
{'loss': 0.0439, 'grad_norm': 0.057608507573604584, 'learning_rate': 5e-05, 'epoch': 2.06}
{'loss': 0.0455, 'grad_norm': 0.0803079679608345, 'learning_rate': 5e-05, 'epoch': 2.07}
{'loss': 0.0438, 'grad_norm': 0.07946044206619263, 'learning_rate': 5e-05, 'epoch': 2.08}
{'loss': 0.0451, 'grad_norm': 0.07087314128875732, 'learning_rate': 5e-05, 'epoch': 2.09}
{'loss': 0.045, 'grad_norm': 0.07324565947055817, 'learning_rate': 5e-05, 'epoch': 2.11}
{'loss': 0.0442, 'grad_norm': 0.07562920451164246, 'learning_rate': 5e-05, 'epoch': 2.12}
{'loss': 0.0426, 'grad_norm': 0.08047837764024734, 'learning_rate': 5e-05, 'epoch': 2.13}
{'loss': 0.0438, 'grad_norm': 0.06774706393480301, 'learning_rate': 5e-05, 'epoch': 2.14}
{'loss': 0.0436, 'grad_norm': 0.06637302786111832, 'learning_rate': 5e-05, 'epoch': 2.16}
{'loss': 0.0427, 'grad_norm': 0.09290299564599991, 'learning_rate': 5e-05, 'epoch': 2.17}
{'loss': 0.0436, 'grad_norm': 0.06540313363075256, 'learning_rate': 5e-05, 'epoch': 2.18}
{'loss': 0.0434, 'grad_norm': 0.07927723228931427, 'learning_rate': 5e-05, 'epoch': 2.19}
{'loss': 0.0437, 'grad_norm': 0.060770388692617416, 'learning_rate': 5e-05, 'epoch': 2.2}
{'loss': 0.0431, 'grad_norm': 0.06599054485559464, 'learning_rate': 5e-05, 'epoch': 2.22}
{'loss': 0.0447, 'grad_norm': 0.061366256326436996, 'learning_rate': 5e-05, 'epoch': 2.23}
{'loss': 0.0435, 'grad_norm': 0.101987324655056, 'learning_rate': 5e-05, 'epoch': 2.24}
{'loss': 0.0442, 'grad_norm': 0.07306468486785889, 'learning_rate': 5e-05, 'epoch': 2.25}
{'loss': 0.0428, 'grad_norm': 0.08013356477022171, 'learning_rate': 5e-05, 'epoch': 2.26}
{'loss': 0.0436, 'grad_norm': 0.09838510304689407, 'learning_rate': 5e-05, 'epoch': 2.28}
{'loss': 0.0422, 'grad_norm': 0.10103243589401245, 'learning_rate': 5e-05, 'epoch': 2.29}
{'loss': 0.0417, 'grad_norm': 0.10218672454357147, 'learning_rate': 5e-05, 'epoch': 2.3}
{'loss': 0.0437, 'grad_norm': 0.07568024098873138, 'learning_rate': 5e-05, 'epoch': 2.31}
{'loss': 0.0433, 'grad_norm': 0.07286437600851059, 'learning_rate': 5e-05, 'epoch': 2.32}
{'loss': 0.0431, 'grad_norm': 0.05859380587935448, 'learning_rate': 5e-05, 'epoch': 2.34}
{'loss': 0.0444, 'grad_norm': 0.07205978035926819, 'learning_rate': 5e-05, 'epoch': 2.35}
{'loss': 0.0435, 'grad_norm': 0.08377410471439362, 'learning_rate': 5e-05, 'epoch': 2.36}
{'loss': 0.0425, 'grad_norm': 0.09142501652240753, 'learning_rate': 5e-05, 'epoch': 2.37}
{'loss': 0.0436, 'grad_norm': 0.0794559046626091, 'learning_rate': 5e-05, 'epoch': 2.39}
{'loss': 0.0415, 'grad_norm': 0.0642211064696312, 'learning_rate': 5e-05, 'epoch': 2.4}
{'loss': 0.0432, 'grad_norm': 0.09412938356399536, 'learning_rate': 5e-05, 'epoch': 2.41}
{'loss': 0.0439, 'grad_norm': 0.07559072971343994, 'learning_rate': 5e-05, 'epoch': 2.42}
{'loss': 0.0419, 'grad_norm': 0.07242119312286377, 'learning_rate': 5e-05, 'epoch': 2.43}
{'loss': 0.042, 'grad_norm': 0.09738170355558395, 'learning_rate': 5e-05, 'epoch': 2.45}
{'loss': 0.0424, 'grad_norm': 0.0680798664689064, 'learning_rate': 5e-05, 'epoch': 2.46}
{'loss': 0.0426, 'grad_norm': 0.06891747564077377, 'learning_rate': 5e-05, 'epoch': 2.47}
{'loss': 0.0437, 'grad_norm': 0.07033143192529678, 'learning_rate': 5e-05, 'epoch': 2.48}
{'loss': 0.0433, 'grad_norm': 0.06872926652431488, 'learning_rate': 5e-05, 'epoch': 2.49}
{'loss': 0.0427, 'grad_norm': 0.10200957953929901, 'learning_rate': 5e-05, 'epoch': 2.51}
{'loss': 0.0432, 'grad_norm': 0.0879008024930954, 'learning_rate': 5e-05, 'epoch': 2.52}
{'loss': 0.0415, 'grad_norm': 0.07831040024757385, 'learning_rate': 5e-05, 'epoch': 2.53}
{'loss': 0.0411, 'grad_norm': 0.08252395689487457, 'learning_rate': 5e-05, 'epoch': 2.54}
{'loss': 0.0419, 'grad_norm': 0.070833221077919, 'learning_rate': 5e-05, 'epoch': 2.55}
{'loss': 0.0418, 'grad_norm': 0.07966975122690201, 'learning_rate': 5e-05, 'epoch': 2.57}
{'loss': 0.0405, 'grad_norm': 0.07246079295873642, 'learning_rate': 5e-05, 'epoch': 2.58}
{'loss': 0.0431, 'grad_norm': 0.05706777423620224, 'learning_rate': 5e-05, 'epoch': 2.59}
{'loss': 0.0424, 'grad_norm': 0.08290386199951172, 'learning_rate': 5e-05, 'epoch': 2.6}
{'loss': 0.0427, 'grad_norm': 0.07806867361068726, 'learning_rate': 5e-05, 'epoch': 2.62}
{'loss': 0.042, 'grad_norm': 0.07928516715765, 'learning_rate': 5e-05, 'epoch': 2.63}
{'loss': 0.0421, 'grad_norm': 0.09527462720870972, 'learning_rate': 5e-05, 'epoch': 2.64}
{'loss': 0.0418, 'grad_norm': 0.09133661538362503, 'learning_rate': 5e-05, 'epoch': 2.65}
{'loss': 0.0427, 'grad_norm': 0.0685315728187561, 'learning_rate': 5e-05, 'epoch': 2.66}
{'loss': 0.0415, 'grad_norm': 0.0689762756228447, 'learning_rate': 5e-05, 'epoch': 2.68}
{'loss': 0.0411, 'grad_norm': 0.0772443637251854, 'learning_rate': 5e-05, 'epoch': 2.69}
{'loss': 0.0417, 'grad_norm': 0.06273241341114044, 'learning_rate': 5e-05, 'epoch': 2.7}
{'loss': 0.042, 'grad_norm': 0.062179356813430786, 'learning_rate': 5e-05, 'epoch': 2.71}
{'loss': 0.0416, 'grad_norm': 0.0674441009759903, 'learning_rate': 5e-05, 'epoch': 2.72}
{'loss': 0.0408, 'grad_norm': 0.06396576762199402, 'learning_rate': 5e-05, 'epoch': 2.74}
{'loss': 0.0413, 'grad_norm': 0.05491562560200691, 'learning_rate': 5e-05, 'epoch': 2.75}
{'loss': 0.0419, 'grad_norm': 0.07199830561876297, 'learning_rate': 5e-05, 'epoch': 2.76}
{'loss': 0.0418, 'grad_norm': 0.07401486486196518, 'learning_rate': 5e-05, 'epoch': 2.77}
{'loss': 0.0418, 'grad_norm': 0.062369946390390396, 'learning_rate': 5e-05, 'epoch': 2.78}
{'loss': 0.0431, 'grad_norm': 0.08228761702775955, 'learning_rate': 5e-05, 'epoch': 2.8}
{'loss': 0.041, 'grad_norm': 0.1075640618801117, 'learning_rate': 5e-05, 'epoch': 2.81}
{'loss': 0.0416, 'grad_norm': 0.06543296575546265, 'learning_rate': 5e-05, 'epoch': 2.82}
{'loss': 0.0416, 'grad_norm': 0.09066629409790039, 'learning_rate': 5e-05, 'epoch': 2.83}
{'loss': 0.0402, 'grad_norm': 0.06460697948932648, 'learning_rate': 5e-05, 'epoch': 2.85}
{'loss': 0.0407, 'grad_norm': 0.07566122710704803, 'learning_rate': 5e-05, 'epoch': 2.86}
{'loss': 0.0424, 'grad_norm': 0.08254899829626083, 'learning_rate': 5e-05, 'epoch': 2.87}
{'loss': 0.042, 'grad_norm': 0.07433696836233139, 'learning_rate': 5e-05, 'epoch': 2.88}
{'loss': 0.0423, 'grad_norm': 0.09250903129577637, 'learning_rate': 5e-05, 'epoch': 2.89}
{'loss': 0.0398, 'grad_norm': 0.06869848072528839, 'learning_rate': 5e-05, 'epoch': 2.91}
{'loss': 0.0401, 'grad_norm': 0.08274595439434052, 'learning_rate': 5e-05, 'epoch': 2.92}
{'loss': 0.0412, 'grad_norm': 0.08074983209371567, 'learning_rate': 5e-05, 'epoch': 2.93}
{'loss': 0.0407, 'grad_norm': 0.06028766930103302, 'learning_rate': 5e-05, 'epoch': 2.94}
{'loss': 0.0407, 'grad_norm': 0.0691489726305008, 'learning_rate': 5e-05, 'epoch': 2.95}
{'loss': 0.0416, 'grad_norm': 0.06507962942123413, 'learning_rate': 5e-05, 'epoch': 2.97}
{'loss': 0.0397, 'grad_norm': 0.07580189406871796, 'learning_rate': 5e-05, 'epoch': 2.98}
{'loss': 0.0398, 'grad_norm': 0.08947882056236267, 'learning_rate': 5e-05, 'epoch': 2.99}
{'eval_mit-movie': 0.46964856225069207, 'eval_mit-restaurant': 0.2832618025281602, 'eval_crossner_ai': 0.5162037036542109, 'eval_crossner_literature': 0.5770277626064974, 'eval_crossner_music': 0.7144416150827244, 'eval_crossner_politics': 0.5777243589243147, 'eval_crossner_science': 0.6700404857801497, 'eval_average': 0.5440497558323928, 'eval_runtime': 169.2693, 'eval_samples_per_second': 8.271, 'eval_steps_per_second': 0.26, 'epoch': 3.0}
 25%|██▌       | 2478/9900 [1:38:27<3:47:37,  1[INFO|trainer.py:3948] 2025-02-24 04:20:20,113 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478
[INFO|configuration_utils.py:423] 2025-02-24 04:20:20,115 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/config.json
[INFO|configuration_utils.py:909] 2025-02-24 04:20:20,115 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 04:20:21,756 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 04:20:21,758 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 04:20:21,758 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 04:20:21,759 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/spiece.model
[2025-02-24 04:20:21,787] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2476 is about to be saved!
[2025-02-24 04:20:21,794] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/mp_rank_00_model_states.pt
[2025-02-24 04:20:21,794] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/mp_rank_00_model_states.pt...
[2025-02-24 04:20:23,472] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/mp_rank_00_model_states.pt.
[2025-02-24 04:20:23,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 04:20:24,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 04:20:24,914] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-2478/global_step2476/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 04:20:24,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2476 is ready now!
{'loss': 0.0403, 'grad_norm': 0.07894071191549301, 'learning_rate': 5e-05, 'epoch': 3.0}
{'loss': 0.0393, 'grad_norm': 0.056727904826402664, 'learning_rate': 5e-05, 'epoch': 3.01}
{'loss': 0.0394, 'grad_norm': 0.07224751263856888, 'learning_rate': 5e-05, 'epoch': 3.03}
{'loss': 0.0396, 'grad_norm': 0.06338918954133987, 'learning_rate': 5e-05, 'epoch': 3.04}
{'loss': 0.0396, 'grad_norm': 0.09432613104581833, 'learning_rate': 5e-05, 'epoch': 3.05}
{'loss': 0.0401, 'grad_norm': 0.08585280925035477, 'learning_rate': 5e-05, 'epoch': 3.06}
{'loss': 0.039, 'grad_norm': 0.07135462015867233, 'learning_rate': 5e-05, 'epoch': 3.08}
{'loss': 0.0397, 'grad_norm': 0.08015482127666473, 'learning_rate': 5e-05, 'epoch': 3.09}
{'loss': 0.0402, 'grad_norm': 0.0627385750412941, 'learning_rate': 5e-05, 'epoch': 3.1}
{'loss': 0.0395, 'grad_norm': 0.06955601274967194, 'learning_rate': 5e-05, 'epoch': 3.11}
{'loss': 0.0395, 'grad_norm': 0.0584123469889164, 'learning_rate': 5e-05, 'epoch': 3.12}
{'loss': 0.0402, 'grad_norm': 0.08029953390359879, 'learning_rate': 5e-05, 'epoch': 3.14}
{'loss': 0.0413, 'grad_norm': 0.05401528254151344, 'learning_rate': 5e-05, 'epoch': 3.15}
{'loss': 0.039, 'grad_norm': 0.057498861104249954, 'learning_rate': 5e-05, 'epoch': 3.16}
{'loss': 0.0391, 'grad_norm': 0.08649046719074249, 'learning_rate': 5e-05, 'epoch': 3.17}
{'loss': 0.0411, 'grad_norm': 0.06828688085079193, 'learning_rate': 5e-05, 'epoch': 3.18}
{'loss': 0.0392, 'grad_norm': 0.06718344241380692, 'learning_rate': 5e-05, 'epoch': 3.2}
{'loss': 0.0411, 'grad_norm': 0.058513764292001724, 'learning_rate': 5e-05, 'epoch': 3.21}
{'loss': 0.0392, 'grad_norm': 0.10257740318775177, 'learning_rate': 5e-05, 'epoch': 3.22}
{'loss': 0.0396, 'grad_norm': 0.07826653867959976, 'learning_rate': 5e-05, 'epoch': 3.23}
{'loss': 0.0387, 'grad_norm': 0.08263114094734192, 'learning_rate': 5e-05, 'epoch': 3.24}
{'loss': 0.0395, 'grad_norm': 0.07392879575490952, 'learning_rate': 5e-05, 'epoch': 3.26}
{'loss': 0.0409, 'grad_norm': 0.07588353753089905, 'learning_rate': 5e-05, 'epoch': 3.27}
{'loss': 0.0387, 'grad_norm': 0.06488131731748581, 'learning_rate': 5e-05, 'epoch': 3.28}
{'loss': 0.039, 'grad_norm': 0.0634593665599823, 'learning_rate': 5e-05, 'epoch': 3.29}
{'loss': 0.0392, 'grad_norm': 0.05965595319867134, 'learning_rate': 5e-05, 'epoch': 3.31}
{'loss': 0.0384, 'grad_norm': 0.05992872640490532, 'learning_rate': 5e-05, 'epoch': 3.32}
{'loss': 0.0384, 'grad_norm': 0.0689573734998703, 'learning_rate': 5e-05, 'epoch': 3.33}
{'loss': 0.0384, 'grad_norm': 0.1088191419839859, 'learning_rate': 5e-05, 'epoch': 3.34}
{'loss': 0.0389, 'grad_norm': 0.07797641307115555, 'learning_rate': 5e-05, 'epoch': 3.35}
{'loss': 0.0388, 'grad_norm': 0.07485855370759964, 'learning_rate': 5e-05, 'epoch': 3.37}
{'loss': 0.0384, 'grad_norm': 0.10092201083898544, 'learning_rate': 5e-05, 'epoch': 3.38}
{'loss': 0.0377, 'grad_norm': 0.0956154614686966, 'learning_rate': 5e-05, 'epoch': 3.39}
{'loss': 0.0391, 'grad_norm': 0.06969165802001953, 'learning_rate': 5e-05, 'epoch': 3.4}
{'loss': 0.039, 'grad_norm': 0.06005346029996872, 'learning_rate': 5e-05, 'epoch': 3.41}
{'loss': 0.0399, 'grad_norm': 0.06673241406679153, 'learning_rate': 5e-05, 'epoch': 3.43}
{'loss': 0.039, 'grad_norm': 0.06342742592096329, 'learning_rate': 5e-05, 'epoch': 3.44}
{'loss': 0.0385, 'grad_norm': 0.07414077967405319, 'learning_rate': 5e-05, 'epoch': 3.45}
{'loss': 0.0387, 'grad_norm': 0.06548458337783813, 'learning_rate': 5e-05, 'epoch': 3.46}
{'loss': 0.0381, 'grad_norm': 0.08048581331968307, 'learning_rate': 5e-05, 'epoch': 3.47}
{'loss': 0.0388, 'grad_norm': 0.06489235907793045, 'learning_rate': 5e-05, 'epoch': 3.49}
{'loss': 0.0379, 'grad_norm': 0.05735320597887039, 'learning_rate': 5e-05, 'epoch': 3.5}
{'loss': 0.0393, 'grad_norm': 0.0658232644200325, 'learning_rate': 5e-05, 'epoch': 3.51}
{'loss': 0.0377, 'grad_norm': 0.08666154742240906, 'learning_rate': 5e-05, 'epoch': 3.52}
{'loss': 0.0385, 'grad_norm': 0.08764012157917023, 'learning_rate': 5e-05, 'epoch': 3.54}
{'loss': 0.0366, 'grad_norm': 0.07777571678161621, 'learning_rate': 5e-05, 'epoch': 3.55}
{'loss': 0.0403, 'grad_norm': 0.07883165776729584, 'learning_rate': 5e-05, 'epoch': 3.56}
{'loss': 0.0381, 'grad_norm': 0.07854263484477997, 'learning_rate': 5e-05, 'epoch': 3.57}
{'loss': 0.0386, 'grad_norm': 0.06838522851467133, 'learning_rate': 5e-05, 'epoch': 3.58}
{'loss': 0.038, 'grad_norm': 0.06284311413764954, 'learning_rate': 5e-05, 'epoch': 3.6}
{'loss': 0.0391, 'grad_norm': 0.07907837629318237, 'learning_rate': 5e-05, 'epoch': 3.61}
{'loss': 0.0396, 'grad_norm': 0.06284821778535843, 'learning_rate': 5e-05, 'epoch': 3.62}
{'loss': 0.0374, 'grad_norm': 0.07565002143383026, 'learning_rate': 5e-05, 'epoch': 3.63}
{'loss': 0.0373, 'grad_norm': 0.08774754405021667, 'learning_rate': 5e-05, 'epoch': 3.64}
{'loss': 0.0385, 'grad_norm': 0.08861799538135529, 'learning_rate': 5e-05, 'epoch': 3.66}
{'loss': 0.0379, 'grad_norm': 0.05416255071759224, 'learning_rate': 5e-05, 'epoch': 3.67}
{'loss': 0.0381, 'grad_norm': 0.09022647142410278, 'learning_rate': 5e-05, 'epoch': 3.68}
{'loss': 0.0384, 'grad_norm': 0.06627310812473297, 'learning_rate': 5e-05, 'epoch': 3.69}
{'loss': 0.0391, 'grad_norm': 0.07129941135644913, 'learning_rate': 5e-05, 'epoch': 3.71}
{'loss': 0.0386, 'grad_norm': 0.06290658563375473, 'learning_rate': 5e-05, 'epoch': 3.72}
{'loss': 0.0387, 'grad_norm': 0.05709993094205856, 'learning_rate': 5e-05, 'epoch': 3.73}
{'loss': 0.0398, 'grad_norm': 0.0699431523680687, 'learning_rate': 5e-05, 'epoch': 3.74}
{'loss': 0.0379, 'grad_norm': 0.0638478621840477, 'learning_rate': 5e-05, 'epoch': 3.75}
{'loss': 0.0373, 'grad_norm': 0.07748083025217056, 'learning_rate': 5e-05, 'epoch': 3.77}
{'loss': 0.0369, 'grad_norm': 0.0627596452832222, 'learning_rate': 5e-05, 'epoch': 3.78}
{'loss': 0.0377, 'grad_norm': 0.07694362103939056, 'learning_rate': 5e-05, 'epoch': 3.79}
{'loss': 0.0386, 'grad_norm': 0.16713503003120422, 'learning_rate': 5e-05, 'epoch': 3.8}
{'loss': 0.0377, 'grad_norm': 0.05990184471011162, 'learning_rate': 5e-05, 'epoch': 3.81}
{'loss': 0.0373, 'grad_norm': 0.07232487201690674, 'learning_rate': 5e-05, 'epoch': 3.83}
{'loss': 0.0378, 'grad_norm': 0.06580331921577454, 'learning_rate': 5e-05, 'epoch': 3.84}
{'loss': 0.0384, 'grad_norm': 0.05995628237724304, 'learning_rate': 5e-05, 'epoch': 3.85}
{'loss': 0.0375, 'grad_norm': 0.06887157261371613, 'learning_rate': 5e-05, 'epoch': 3.86}
{'loss': 0.0375, 'grad_norm': 0.0566130094230175, 'learning_rate': 5e-05, 'epoch': 3.87}
{'loss': 0.0383, 'grad_norm': 0.08304843306541443, 'learning_rate': 5e-05, 'epoch': 3.89}
{'loss': 0.0374, 'grad_norm': 0.0714212954044342, 'learning_rate': 5e-05, 'epoch': 3.9}
{'loss': 0.0404, 'grad_norm': 0.06923269480466843, 'learning_rate': 5e-05, 'epoch': 3.91}
{'loss': 0.0397, 'grad_norm': 0.07069318741559982, 'learning_rate': 5e-05, 'epoch': 3.92}
{'loss': 0.0361, 'grad_norm': 0.07900255173444748, 'learning_rate': 5e-05, 'epoch': 3.94}
{'loss': 0.0377, 'grad_norm': 0.06378614902496338, 'learning_rate': 5e-05, 'epoch': 3.95}
{'loss': 0.0361, 'grad_norm': 0.08539889007806778, 'learning_rate': 5e-05, 'epoch': 3.96}
{'loss': 0.0373, 'grad_norm': 0.06615135818719864, 'learning_rate': 5e-05, 'epoch': 3.97}
{'loss': 0.0382, 'grad_norm': 0.053014446049928665, 'learning_rate': 5e-05, 'epoch': 3.98}
{'loss': 0.0391, 'grad_norm': 0.10114047676324844, 'learning_rate': 5e-05, 'epoch': 4.0}
{'eval_mit-movie': 0.4857142856645851, 'eval_mit-restaurant': 0.3076923076442341, 'eval_crossner_ai': 0.5135603000082387, 'eval_crossner_literature': 0.5826086956021602, 'eval_crossner_music': 0.7254830477078766, 'eval_crossner_politics': 0.5689172991908696, 'eval_crossner_science': 0.674723061380235, 'eval_average': 0.551242713885457, 'eval_runtime': 170.3816, 'eval_samples_per_second': 8.217, 'eval_steps_per_second': 0.258, 'epoch': 4.0}
 33%|███▎      | 3304/9900 [2:11:16<3:26:58,  1[INFO|trainer.py:3948] 2025-02-24 04:53:09,405 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304
[INFO|configuration_utils.py:423] 2025-02-24 04:53:09,406 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/config.json
[INFO|configuration_utils.py:909] 2025-02-24 04:53:09,407 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 04:53:11,036 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 04:53:11,038 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 04:53:11,039 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 04:53:11,040 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/spiece.model
[2025-02-24 04:53:11,094] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3302 is about to be saved!
[2025-02-24 04:53:11,100] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/mp_rank_00_model_states.pt
[2025-02-24 04:53:11,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/mp_rank_00_model_states.pt...
[2025-02-24 04:53:12,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/mp_rank_00_model_states.pt.
[2025-02-24 04:53:12,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 04:53:14,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 04:53:14,164] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-3304/global_step3302/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 04:53:14,165] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3302 is ready now!
{'loss': 0.0363, 'grad_norm': 0.12954266369342804, 'learning_rate': 5e-05, 'epoch': 4.01}
{'loss': 0.0384, 'grad_norm': 0.062412962317466736, 'learning_rate': 5e-05, 'epoch': 4.02}
{'loss': 0.0371, 'grad_norm': 0.07471810281276703, 'learning_rate': 5e-05, 'epoch': 4.03}
{'loss': 0.0358, 'grad_norm': 0.07414764910936356, 'learning_rate': 5e-05, 'epoch': 4.04}
{'loss': 0.0361, 'grad_norm': 0.058271221816539764, 'learning_rate': 5e-05, 'epoch': 4.06}
{'loss': 0.0363, 'grad_norm': 0.06045720726251602, 'learning_rate': 5e-05, 'epoch': 4.07}
{'loss': 0.0367, 'grad_norm': 0.07223706692457199, 'learning_rate': 5e-05, 'epoch': 4.08}
{'loss': 0.0363, 'grad_norm': 0.05894572660326958, 'learning_rate': 5e-05, 'epoch': 4.09}
{'loss': 0.0366, 'grad_norm': 0.07682913541793823, 'learning_rate': 5e-05, 'epoch': 4.1}
{'loss': 0.0361, 'grad_norm': 0.0772736668586731, 'learning_rate': 5e-05, 'epoch': 4.12}
{'loss': 0.0385, 'grad_norm': 0.058474257588386536, 'learning_rate': 5e-05, 'epoch': 4.13}
{'loss': 0.0361, 'grad_norm': 0.07631388306617737, 'learning_rate': 5e-05, 'epoch': 4.14}
{'loss': 0.0375, 'grad_norm': 0.07159172743558884, 'learning_rate': 5e-05, 'epoch': 4.15}
{'loss': 0.0353, 'grad_norm': 0.09671995043754578, 'learning_rate': 5e-05, 'epoch': 4.16}
{'loss': 0.0373, 'grad_norm': 0.08231812715530396, 'learning_rate': 5e-05, 'epoch': 4.18}
{'loss': 0.0379, 'grad_norm': 0.07241758704185486, 'learning_rate': 5e-05, 'epoch': 4.19}
{'loss': 0.0361, 'grad_norm': 0.06194751709699631, 'learning_rate': 5e-05, 'epoch': 4.2}
{'loss': 0.0362, 'grad_norm': 0.058275461196899414, 'learning_rate': 5e-05, 'epoch': 4.21}
{'loss': 0.037, 'grad_norm': 0.07637191563844681, 'learning_rate': 5e-05, 'epoch': 4.23}
{'loss': 0.0367, 'grad_norm': 0.06607170403003693, 'learning_rate': 5e-05, 'epoch': 4.24}
{'loss': 0.0363, 'grad_norm': 0.059966616332530975, 'learning_rate': 5e-05, 'epoch': 4.25}
{'loss': 0.0362, 'grad_norm': 0.08156190812587738, 'learning_rate': 5e-05, 'epoch': 4.26}
{'loss': 0.0365, 'grad_norm': 0.05832863599061966, 'learning_rate': 5e-05, 'epoch': 4.27}
{'loss': 0.0361, 'grad_norm': 0.054892878979444504, 'learning_rate': 5e-05, 'epoch': 4.29}
{'loss': 0.0356, 'grad_norm': 0.07611411809921265, 'learning_rate': 5e-05, 'epoch': 4.3}
{'loss': 0.0374, 'grad_norm': 0.04923466965556145, 'learning_rate': 5e-05, 'epoch': 4.31}
{'loss': 0.035, 'grad_norm': 0.07829022407531738, 'learning_rate': 5e-05, 'epoch': 4.32}
{'loss': 0.0356, 'grad_norm': 0.06911627948284149, 'learning_rate': 5e-05, 'epoch': 4.33}
{'loss': 0.0357, 'grad_norm': 0.06274263560771942, 'learning_rate': 5e-05, 'epoch': 4.35}
{'loss': 0.0369, 'grad_norm': 0.06479807943105698, 'learning_rate': 5e-05, 'epoch': 4.36}
{'loss': 0.0365, 'grad_norm': 0.07635167241096497, 'learning_rate': 5e-05, 'epoch': 4.37}
{'loss': 0.0355, 'grad_norm': 0.06996562331914902, 'learning_rate': 5e-05, 'epoch': 4.38}
{'loss': 0.0348, 'grad_norm': 0.060138456523418427, 'learning_rate': 5e-05, 'epoch': 4.39}
{'loss': 0.0358, 'grad_norm': 0.08563349395990372, 'learning_rate': 5e-05, 'epoch': 4.41}
{'loss': 0.0378, 'grad_norm': 0.08151303976774216, 'learning_rate': 5e-05, 'epoch': 4.42}
{'loss': 0.0362, 'grad_norm': 0.07552094012498856, 'learning_rate': 5e-05, 'epoch': 4.43}
{'loss': 0.0357, 'grad_norm': 0.05844743177294731, 'learning_rate': 5e-05, 'epoch': 4.44}
{'loss': 0.0372, 'grad_norm': 0.060145895928144455, 'learning_rate': 5e-05, 'epoch': 4.46}
{'loss': 0.0358, 'grad_norm': 0.054422527551651, 'learning_rate': 5e-05, 'epoch': 4.47}
{'loss': 0.0368, 'grad_norm': 0.06684521585702896, 'learning_rate': 5e-05, 'epoch': 4.48}
{'loss': 0.0363, 'grad_norm': 0.07206249237060547, 'learning_rate': 5e-05, 'epoch': 4.49}
{'loss': 0.0353, 'grad_norm': 0.04889504238963127, 'learning_rate': 5e-05, 'epoch': 4.5}
{'loss': 0.0352, 'grad_norm': 0.07696795463562012, 'learning_rate': 5e-05, 'epoch': 4.52}
{'loss': 0.0365, 'grad_norm': 0.0669478103518486, 'learning_rate': 5e-05, 'epoch': 4.53}
{'loss': 0.0358, 'grad_norm': 0.08377574384212494, 'learning_rate': 5e-05, 'epoch': 4.54}
{'loss': 0.0352, 'grad_norm': 0.06420420110225677, 'learning_rate': 5e-05, 'epoch': 4.55}
{'loss': 0.0359, 'grad_norm': 0.0768052488565445, 'learning_rate': 5e-05, 'epoch': 4.56}
{'loss': 0.0378, 'grad_norm': 0.08022000640630722, 'learning_rate': 5e-05, 'epoch': 4.58}
{'loss': 0.0365, 'grad_norm': 0.05824322625994682, 'learning_rate': 5e-05, 'epoch': 4.59}
{'loss': 0.0357, 'grad_norm': 0.046162206679582596, 'learning_rate': 5e-05, 'epoch': 4.6}
{'loss': 0.0354, 'grad_norm': 0.05267072468996048, 'learning_rate': 5e-05, 'epoch': 4.61}
{'loss': 0.037, 'grad_norm': 0.0592661052942276, 'learning_rate': 5e-05, 'epoch': 4.63}
{'loss': 0.0357, 'grad_norm': 0.0472833625972271, 'learning_rate': 5e-05, 'epoch': 4.64}
{'loss': 0.0365, 'grad_norm': 0.06725344061851501, 'learning_rate': 5e-05, 'epoch': 4.65}
{'loss': 0.0361, 'grad_norm': 0.05985833704471588, 'learning_rate': 5e-05, 'epoch': 4.66}
{'loss': 0.0353, 'grad_norm': 0.07836378365755081, 'learning_rate': 5e-05, 'epoch': 4.67}
{'loss': 0.0355, 'grad_norm': 0.0697031319141388, 'learning_rate': 5e-05, 'epoch': 4.69}
{'loss': 0.0354, 'grad_norm': 0.07394421100616455, 'learning_rate': 5e-05, 'epoch': 4.7}
{'loss': 0.0359, 'grad_norm': 0.06997254490852356, 'learning_rate': 5e-05, 'epoch': 4.71}
{'loss': 0.0364, 'grad_norm': 0.07305118441581726, 'learning_rate': 5e-05, 'epoch': 4.72}
{'loss': 0.0368, 'grad_norm': 0.06269808858633041, 'learning_rate': 5e-05, 'epoch': 4.73}
{'loss': 0.0367, 'grad_norm': 0.28616395592689514, 'learning_rate': 5e-05, 'epoch': 4.75}
{'loss': 0.0351, 'grad_norm': 0.06583312898874283, 'learning_rate': 5e-05, 'epoch': 4.76}
{'loss': 0.0359, 'grad_norm': 0.050411827862262726, 'learning_rate': 5e-05, 'epoch': 4.77}
{'loss': 0.0343, 'grad_norm': 0.08411365747451782, 'learning_rate': 5e-05, 'epoch': 4.78}
{'loss': 0.0358, 'grad_norm': 0.05914429575204849, 'learning_rate': 5e-05, 'epoch': 4.79}
{'loss': 0.036, 'grad_norm': 0.070884570479393, 'learning_rate': 5e-05, 'epoch': 4.81}
{'loss': 0.0359, 'grad_norm': 0.10113436728715897, 'learning_rate': 5e-05, 'epoch': 4.82}
{'loss': 0.0348, 'grad_norm': 0.0757383331656456, 'learning_rate': 5e-05, 'epoch': 4.83}
{'loss': 0.0368, 'grad_norm': 0.064897321164608, 'learning_rate': 5e-05, 'epoch': 4.84}
{'loss': 0.0353, 'grad_norm': 0.06663346290588379, 'learning_rate': 5e-05, 'epoch': 4.86}
{'loss': 0.0372, 'grad_norm': 0.06509147584438324, 'learning_rate': 5e-05, 'epoch': 4.87}
{'loss': 0.0354, 'grad_norm': 0.059125714004039764, 'learning_rate': 5e-05, 'epoch': 4.88}
{'loss': 0.0354, 'grad_norm': 0.07721900194883347, 'learning_rate': 5e-05, 'epoch': 4.89}
{'loss': 0.0356, 'grad_norm': 0.06524927914142609, 'learning_rate': 5e-05, 'epoch': 4.9}
{'loss': 0.0353, 'grad_norm': 0.06464008241891861, 'learning_rate': 5e-05, 'epoch': 4.92}
{'loss': 0.037, 'grad_norm': 0.07111181318759918, 'learning_rate': 5e-05, 'epoch': 4.93}
{'loss': 0.0339, 'grad_norm': 0.045346926897764206, 'learning_rate': 5e-05, 'epoch': 4.94}
{'loss': 0.0348, 'grad_norm': 0.06582721322774887, 'learning_rate': 5e-05, 'epoch': 4.95}
{'loss': 0.0352, 'grad_norm': 0.05022529512643814, 'learning_rate': 5e-05, 'epoch': 4.96}
{'loss': 0.0357, 'grad_norm': 0.05304441601037979, 'learning_rate': 5e-05, 'epoch': 4.98}
{'loss': 0.0343, 'grad_norm': 0.05448126047849655, 'learning_rate': 5e-05, 'epoch': 4.99}
{'loss': 0.0331, 'grad_norm': 0.1256738305091858, 'learning_rate': 5e-05, 'epoch': 5.0}
{'eval_mit-movie': 0.5055999999503791, 'eval_mit-restaurant': 0.3638888888410753, 'eval_crossner_ai': 0.5264997087448551, 'eval_crossner_literature': 0.5729788387996839, 'eval_crossner_music': 0.7209894506592972, 'eval_crossner_politics': 0.5613338625938736, 'eval_crossner_science': 0.6800602711707346, 'eval_average': 0.5616215743942713, 'eval_runtime': 170.7879, 'eval_samples_per_second': 8.197, 'eval_steps_per_second': 0.258, 'epoch': 5.0}
 42%|████▏     | 4130/9900 [2:44:04<3:00:34,  1[INFO|trainer.py:3948] 2025-02-24 05:25:57,767 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130
[INFO|configuration_utils.py:423] 2025-02-24 05:25:57,770 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/config.json
[INFO|configuration_utils.py:909] 2025-02-24 05:25:57,770 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 05:25:59,402 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 05:25:59,404 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 05:25:59,404 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 05:25:59,406 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/spiece.model
[2025-02-24 05:25:59,446] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4127 is about to be saved!
[2025-02-24 05:25:59,453] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/mp_rank_00_model_states.pt
[2025-02-24 05:25:59,453] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/mp_rank_00_model_states.pt...
[2025-02-24 05:26:01,098] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/mp_rank_00_model_states.pt.
[2025-02-24 05:26:01,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 05:26:02,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 05:26:02,532] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4130/global_step4127/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 05:26:02,533] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4127 is ready now!
{'loss': 0.0352, 'grad_norm': 0.06129307672381401, 'learning_rate': 5e-05, 'epoch': 5.01}
{'loss': 0.0346, 'grad_norm': 0.09478029608726501, 'learning_rate': 5e-05, 'epoch': 5.02}
{'loss': 0.0341, 'grad_norm': 0.06533701717853546, 'learning_rate': 5e-05, 'epoch': 5.04}
{'loss': 0.0357, 'grad_norm': 0.09744574129581451, 'learning_rate': 5e-05, 'epoch': 5.05}
{'loss': 0.0364, 'grad_norm': 0.12409879267215729, 'learning_rate': 5e-05, 'epoch': 5.06}
{'loss': 0.0352, 'grad_norm': 0.056739531457424164, 'learning_rate': 5e-05, 'epoch': 5.07}
{'loss': 0.0337, 'grad_norm': 0.06405815482139587, 'learning_rate': 5e-05, 'epoch': 5.08}
{'loss': 0.0355, 'grad_norm': 0.08170991390943527, 'learning_rate': 5e-05, 'epoch': 5.1}
{'loss': 0.0352, 'grad_norm': 0.06913071870803833, 'learning_rate': 5e-05, 'epoch': 5.11}
{'loss': 0.0351, 'grad_norm': 0.17722781002521515, 'learning_rate': 5e-05, 'epoch': 5.12}
{'loss': 0.0346, 'grad_norm': 0.062054917216300964, 'learning_rate': 5e-05, 'epoch': 5.13}
{'loss': 0.0336, 'grad_norm': 0.06462676078081131, 'learning_rate': 5e-05, 'epoch': 5.15}
{'loss': 0.0345, 'grad_norm': 0.05325666069984436, 'learning_rate': 5e-05, 'epoch': 5.16}
{'loss': 0.0334, 'grad_norm': 0.050302352756261826, 'learning_rate': 5e-05, 'epoch': 5.17}
{'loss': 0.0347, 'grad_norm': 0.0447535403072834, 'learning_rate': 5e-05, 'epoch': 5.18}
{'loss': 0.0339, 'grad_norm': 0.08079838752746582, 'learning_rate': 5e-05, 'epoch': 5.19}
{'loss': 0.0358, 'grad_norm': 0.06311183422803879, 'learning_rate': 5e-05, 'epoch': 5.21}
{'loss': 0.0342, 'grad_norm': 0.07728248089551926, 'learning_rate': 5e-05, 'epoch': 5.22}
{'loss': 0.0349, 'grad_norm': 0.0532071515917778, 'learning_rate': 5e-05, 'epoch': 5.23}
{'loss': 0.0351, 'grad_norm': 0.07564311474561691, 'learning_rate': 5e-05, 'epoch': 5.24}
{'loss': 0.0334, 'grad_norm': 0.07398522645235062, 'learning_rate': 5e-05, 'epoch': 5.25}
{'loss': 0.0331, 'grad_norm': 0.05682060867547989, 'learning_rate': 5e-05, 'epoch': 5.27}
{'loss': 0.0329, 'grad_norm': 0.0686274841427803, 'learning_rate': 5e-05, 'epoch': 5.28}
{'loss': 0.0337, 'grad_norm': 0.08115892112255096, 'learning_rate': 5e-05, 'epoch': 5.29}
{'loss': 0.0334, 'grad_norm': 0.05404061824083328, 'learning_rate': 5e-05, 'epoch': 5.3}
{'loss': 0.035, 'grad_norm': 0.06172339618206024, 'learning_rate': 5e-05, 'epoch': 5.31}
{'loss': 0.0344, 'grad_norm': 0.0692114382982254, 'learning_rate': 5e-05, 'epoch': 5.33}
{'loss': 0.0339, 'grad_norm': 0.08099684119224548, 'learning_rate': 5e-05, 'epoch': 5.34}
{'loss': 0.0341, 'grad_norm': 0.07602494955062866, 'learning_rate': 5e-05, 'epoch': 5.35}
{'loss': 0.0332, 'grad_norm': 0.07669471204280853, 'learning_rate': 5e-05, 'epoch': 5.36}
{'loss': 0.034, 'grad_norm': 0.05999765917658806, 'learning_rate': 5e-05, 'epoch': 5.38}
{'loss': 0.0345, 'grad_norm': 0.0730520486831665, 'learning_rate': 5e-05, 'epoch': 5.39}
{'loss': 0.0346, 'grad_norm': 0.11417732387781143, 'learning_rate': 5e-05, 'epoch': 5.4}
{'loss': 0.0343, 'grad_norm': 0.07672368735074997, 'learning_rate': 5e-05, 'epoch': 5.41}
{'loss': 0.0344, 'grad_norm': 0.06527208536863327, 'learning_rate': 5e-05, 'epoch': 5.42}
{'loss': 0.0348, 'grad_norm': 0.058720916509628296, 'learning_rate': 5e-05, 'epoch': 5.44}
{'loss': 0.0347, 'grad_norm': 0.07527316361665726, 'learning_rate': 5e-05, 'epoch': 5.45}
{'loss': 0.0339, 'grad_norm': 0.05996432527899742, 'learning_rate': 5e-05, 'epoch': 5.46}
{'loss': 0.0344, 'grad_norm': 0.06019777059555054, 'learning_rate': 5e-05, 'epoch': 5.47}
{'loss': 0.034, 'grad_norm': 0.05991049110889435, 'learning_rate': 5e-05, 'epoch': 5.48}
{'loss': 0.034, 'grad_norm': 0.06244543567299843, 'learning_rate': 5e-05, 'epoch': 5.5}
{'loss': 0.0335, 'grad_norm': 0.045704763382673264, 'learning_rate': 5e-05, 'epoch': 5.51}
{'loss': 0.0339, 'grad_norm': 0.07149963825941086, 'learning_rate': 5e-05, 'epoch': 5.52}
{'loss': 0.0336, 'grad_norm': 0.05662578344345093, 'learning_rate': 5e-05, 'epoch': 5.53}
{'loss': 0.0341, 'grad_norm': 0.10762136429548264, 'learning_rate': 5e-05, 'epoch': 5.55}
{'loss': 0.0338, 'grad_norm': 0.05982782319188118, 'learning_rate': 5e-05, 'epoch': 5.56}
{'loss': 0.0344, 'grad_norm': 0.09300129860639572, 'learning_rate': 5e-05, 'epoch': 5.57}
{'loss': 0.0337, 'grad_norm': 0.06492498517036438, 'learning_rate': 5e-05, 'epoch': 5.58}
{'loss': 0.0331, 'grad_norm': 0.07928040623664856, 'learning_rate': 5e-05, 'epoch': 5.59}
{'loss': 0.0343, 'grad_norm': 0.08108610659837723, 'learning_rate': 5e-05, 'epoch': 5.61}
{'loss': 0.0331, 'grad_norm': 0.068260557949543, 'learning_rate': 5e-05, 'epoch': 5.62}
{'loss': 0.0349, 'grad_norm': 0.12395225465297699, 'learning_rate': 5e-05, 'epoch': 5.63}
{'loss': 0.0348, 'grad_norm': 0.049239594489336014, 'learning_rate': 5e-05, 'epoch': 5.64}
{'loss': 0.0345, 'grad_norm': 0.06241491809487343, 'learning_rate': 5e-05, 'epoch': 5.65}
{'loss': 0.0334, 'grad_norm': 0.044735029339790344, 'learning_rate': 5e-05, 'epoch': 5.67}
{'loss': 0.0328, 'grad_norm': 0.07711976021528244, 'learning_rate': 5e-05, 'epoch': 5.68}
{'loss': 0.0338, 'grad_norm': 0.06498511880636215, 'learning_rate': 5e-05, 'epoch': 5.69}
{'loss': 0.0334, 'grad_norm': 0.0800684243440628, 'learning_rate': 5e-05, 'epoch': 5.7}
{'loss': 0.0349, 'grad_norm': 0.06324289739131927, 'learning_rate': 5e-05, 'epoch': 5.71}
{'loss': 0.0344, 'grad_norm': 0.06476308405399323, 'learning_rate': 5e-05, 'epoch': 5.73}
{'loss': 0.0343, 'grad_norm': 0.07113835960626602, 'learning_rate': 5e-05, 'epoch': 5.74}
{'loss': 0.0346, 'grad_norm': 0.05341620370745659, 'learning_rate': 5e-05, 'epoch': 5.75}
{'loss': 0.0333, 'grad_norm': 0.0651073306798935, 'learning_rate': 5e-05, 'epoch': 5.76}
{'loss': 0.0328, 'grad_norm': 0.09340476244688034, 'learning_rate': 5e-05, 'epoch': 5.78}
{'loss': 0.034, 'grad_norm': 0.05922752246260643, 'learning_rate': 5e-05, 'epoch': 5.79}
{'loss': 0.034, 'grad_norm': 0.05945443734526634, 'learning_rate': 5e-05, 'epoch': 5.8}
{'loss': 0.034, 'grad_norm': 0.06658990681171417, 'learning_rate': 5e-05, 'epoch': 5.81}
{'loss': 0.0335, 'grad_norm': 0.0447063185274601, 'learning_rate': 5e-05, 'epoch': 5.82}
{'loss': 0.035, 'grad_norm': 0.06845900416374207, 'learning_rate': 5e-05, 'epoch': 5.84}
{'loss': 0.0327, 'grad_norm': 0.059245798736810684, 'learning_rate': 5e-05, 'epoch': 5.85}
{'loss': 0.0339, 'grad_norm': 0.050376392900943756, 'learning_rate': 5e-05, 'epoch': 5.86}
{'loss': 0.0333, 'grad_norm': 0.08487063646316528, 'learning_rate': 5e-05, 'epoch': 5.87}
{'loss': 0.0332, 'grad_norm': 0.06810680031776428, 'learning_rate': 5e-05, 'epoch': 5.88}
{'loss': 0.0336, 'grad_norm': 0.05611369013786316, 'learning_rate': 5e-05, 'epoch': 5.9}
{'loss': 0.0344, 'grad_norm': 0.07004448026418686, 'learning_rate': 5e-05, 'epoch': 5.91}
{'loss': 0.0341, 'grad_norm': 0.0670810416340828, 'learning_rate': 5e-05, 'epoch': 5.92}
{'loss': 0.0339, 'grad_norm': 0.0638681948184967, 'learning_rate': 5e-05, 'epoch': 5.93}
{'loss': 0.0326, 'grad_norm': 0.06001131236553192, 'learning_rate': 5e-05, 'epoch': 5.94}
{'loss': 0.0339, 'grad_norm': 0.060030777007341385, 'learning_rate': 5e-05, 'epoch': 5.96}
{'loss': 0.0335, 'grad_norm': 0.09282384067773819, 'learning_rate': 5e-05, 'epoch': 5.97}
{'loss': 0.0339, 'grad_norm': 0.06895308196544647, 'learning_rate': 5e-05, 'epoch': 5.98}
{'loss': 0.0342, 'grad_norm': 0.06941267848014832, 'learning_rate': 5e-05, 'epoch': 5.99}
{'eval_mit-movie': 0.5129032257569236, 'eval_mit-restaurant': 0.3402489626076885, 'eval_crossner_ai': 0.560859188494396, 'eval_crossner_literature': 0.5662650601909797, 'eval_crossner_music': 0.7478005864602097, 'eval_crossner_politics': 0.5829702969796726, 'eval_crossner_science': 0.6625766870666912, 'eval_average': 0.5676605725080801, 'eval_runtime': 167.5835, 'eval_samples_per_second': 8.354, 'eval_steps_per_second': 0.263, 'epoch': 6.0}
 50%|█████     | 4956/9900 [3:16:46<2:32:07,  1[INFO|trainer.py:3948] 2025-02-24 05:58:40,151 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956
[INFO|configuration_utils.py:423] 2025-02-24 05:58:40,153 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/config.json
[INFO|configuration_utils.py:909] 2025-02-24 05:58:40,154 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 05:58:41,797 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 05:58:41,799 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 05:58:41,799 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 05:58:41,801 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/spiece.model
[2025-02-24 05:58:41,856] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4953 is about to be saved!
[2025-02-24 05:58:41,862] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/mp_rank_00_model_states.pt
[2025-02-24 05:58:41,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/mp_rank_00_model_states.pt...
[2025-02-24 05:58:43,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/mp_rank_00_model_states.pt.
[2025-02-24 05:58:43,478] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 05:58:44,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 05:58:44,923] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-4956/global_step4953/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 05:58:44,923] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4953 is ready now!
{'loss': 0.0319, 'grad_norm': 0.08352956175804138, 'learning_rate': 5e-05, 'epoch': 6.0}
{'loss': 0.0337, 'grad_norm': 0.10681979358196259, 'learning_rate': 5e-05, 'epoch': 6.02}
{'loss': 0.0327, 'grad_norm': 0.057358454912900925, 'learning_rate': 5e-05, 'epoch': 6.03}
{'loss': 0.033, 'grad_norm': 0.05559050664305687, 'learning_rate': 5e-05, 'epoch': 6.04}
{'loss': 0.0326, 'grad_norm': 0.08282307535409927, 'learning_rate': 5e-05, 'epoch': 6.05}
{'loss': 0.0337, 'grad_norm': 0.050910092890262604, 'learning_rate': 5e-05, 'epoch': 6.07}
{'loss': 0.0333, 'grad_norm': 0.056238140910863876, 'learning_rate': 5e-05, 'epoch': 6.08}
{'loss': 0.0337, 'grad_norm': 0.0720590278506279, 'learning_rate': 5e-05, 'epoch': 6.09}
{'loss': 0.0341, 'grad_norm': 0.06726886332035065, 'learning_rate': 5e-05, 'epoch': 6.1}
{'loss': 0.0331, 'grad_norm': 0.060042012482881546, 'learning_rate': 5e-05, 'epoch': 6.11}
{'loss': 0.0325, 'grad_norm': 0.08953242003917694, 'learning_rate': 5e-05, 'epoch': 6.13}
{'loss': 0.0324, 'grad_norm': 0.06093310937285423, 'learning_rate': 5e-05, 'epoch': 6.14}
{'loss': 0.0333, 'grad_norm': 0.06907443702220917, 'learning_rate': 5e-05, 'epoch': 6.15}
{'loss': 0.0327, 'grad_norm': 0.06312427669763565, 'learning_rate': 5e-05, 'epoch': 6.16}
{'loss': 0.0329, 'grad_norm': 0.07363404333591461, 'learning_rate': 5e-05, 'epoch': 6.17}
{'loss': 0.0338, 'grad_norm': 0.050760556012392044, 'learning_rate': 5e-05, 'epoch': 6.19}
{'loss': 0.032, 'grad_norm': 0.06583340466022491, 'learning_rate': 5e-05, 'epoch': 6.2}
{'loss': 0.0327, 'grad_norm': 0.0472080297768116, 'learning_rate': 5e-05, 'epoch': 6.21}
{'loss': 0.034, 'grad_norm': 0.052835918962955475, 'learning_rate': 5e-05, 'epoch': 6.22}
{'loss': 0.0323, 'grad_norm': 0.06391005963087082, 'learning_rate': 5e-05, 'epoch': 6.24}
{'loss': 0.0339, 'grad_norm': 0.051652245223522186, 'learning_rate': 5e-05, 'epoch': 6.25}
{'loss': 0.0318, 'grad_norm': 0.07545474171638489, 'learning_rate': 5e-05, 'epoch': 6.26}
{'loss': 0.0319, 'grad_norm': 0.055184803903102875, 'learning_rate': 5e-05, 'epoch': 6.27}
{'loss': 0.0334, 'grad_norm': 0.052502550184726715, 'learning_rate': 5e-05, 'epoch': 6.28}
{'loss': 0.0329, 'grad_norm': 0.06483534723520279, 'learning_rate': 5e-05, 'epoch': 6.3}
{'loss': 0.0324, 'grad_norm': 0.060881320387125015, 'learning_rate': 5e-05, 'epoch': 6.31}
{'loss': 0.0331, 'grad_norm': 0.06575930118560791, 'learning_rate': 5e-05, 'epoch': 6.32}
{'loss': 0.0323, 'grad_norm': 0.05036241561174393, 'learning_rate': 5e-05, 'epoch': 6.33}
{'loss': 0.032, 'grad_norm': 0.0704694390296936, 'learning_rate': 5e-05, 'epoch': 6.34}
{'loss': 0.0314, 'grad_norm': 0.04783518984913826, 'learning_rate': 5e-05, 'epoch': 6.36}
{'loss': 0.0333, 'grad_norm': 0.0623050332069397, 'learning_rate': 5e-05, 'epoch': 6.37}
{'loss': 0.0308, 'grad_norm': 0.050227828323841095, 'learning_rate': 5e-05, 'epoch': 6.38}
{'loss': 0.032, 'grad_norm': 0.047027382999658585, 'learning_rate': 5e-05, 'epoch': 6.39}
{'loss': 0.0324, 'grad_norm': 0.07287853956222534, 'learning_rate': 5e-05, 'epoch': 6.4}
{'loss': 0.0327, 'grad_norm': 0.09531179815530777, 'learning_rate': 5e-05, 'epoch': 6.42}
{'loss': 0.032, 'grad_norm': 0.07293086498975754, 'learning_rate': 5e-05, 'epoch': 6.43}
{'loss': 0.0323, 'grad_norm': 0.07796013355255127, 'learning_rate': 5e-05, 'epoch': 6.44}
{'loss': 0.0318, 'grad_norm': 0.04404869303107262, 'learning_rate': 5e-05, 'epoch': 6.45}
{'loss': 0.033, 'grad_norm': 0.0802021473646164, 'learning_rate': 5e-05, 'epoch': 6.47}
{'loss': 0.0315, 'grad_norm': 0.04779570549726486, 'learning_rate': 5e-05, 'epoch': 6.48}
{'loss': 0.0328, 'grad_norm': 0.04839992895722389, 'learning_rate': 5e-05, 'epoch': 6.49}
{'loss': 0.0323, 'grad_norm': 0.04410141333937645, 'learning_rate': 5e-05, 'epoch': 6.5}
{'loss': 0.0326, 'grad_norm': 0.05803285539150238, 'learning_rate': 5e-05, 'epoch': 6.51}
{'loss': 0.0329, 'grad_norm': 0.05955834686756134, 'learning_rate': 5e-05, 'epoch': 6.53}
{'loss': 0.034, 'grad_norm': 0.061018336564302444, 'learning_rate': 5e-05, 'epoch': 6.54}
{'loss': 0.0326, 'grad_norm': 0.07131639122962952, 'learning_rate': 5e-05, 'epoch': 6.55}
{'loss': 0.0322, 'grad_norm': 0.05962331220507622, 'learning_rate': 5e-05, 'epoch': 6.56}
{'loss': 0.0323, 'grad_norm': 0.14762157201766968, 'learning_rate': 5e-05, 'epoch': 6.57}
{'loss': 0.0318, 'grad_norm': 0.04618527367711067, 'learning_rate': 5e-05, 'epoch': 6.59}
{'loss': 0.0337, 'grad_norm': 0.06415203958749771, 'learning_rate': 5e-05, 'epoch': 6.6}
{'loss': 0.0314, 'grad_norm': 0.06917355209589005, 'learning_rate': 5e-05, 'epoch': 6.61}
{'loss': 0.0322, 'grad_norm': 0.08250263333320618, 'learning_rate': 5e-05, 'epoch': 6.62}
{'loss': 0.0326, 'grad_norm': 0.0451686717569828, 'learning_rate': 5e-05, 'epoch': 6.63}
{'loss': 0.0326, 'grad_norm': 0.054979465901851654, 'learning_rate': 5e-05, 'epoch': 6.65}
{'loss': 0.0334, 'grad_norm': 0.0633658617734909, 'learning_rate': 5e-05, 'epoch': 6.66}
{'loss': 0.0314, 'grad_norm': 0.05434800311923027, 'learning_rate': 5e-05, 'epoch': 6.67}
{'loss': 0.0319, 'grad_norm': 0.0504140742123127, 'learning_rate': 5e-05, 'epoch': 6.68}
{'loss': 0.0325, 'grad_norm': 0.05414586141705513, 'learning_rate': 5e-05, 'epoch': 6.7}
{'loss': 0.032, 'grad_norm': 0.1377338171005249, 'learning_rate': 5e-05, 'epoch': 6.71}
{'loss': 0.0315, 'grad_norm': 0.04775308817625046, 'learning_rate': 5e-05, 'epoch': 6.72}
{'loss': 0.0323, 'grad_norm': 0.05901984125375748, 'learning_rate': 5e-05, 'epoch': 6.73}
{'loss': 0.0328, 'grad_norm': 0.060304395854473114, 'learning_rate': 5e-05, 'epoch': 6.74}
{'loss': 0.0312, 'grad_norm': 0.06886904686689377, 'learning_rate': 5e-05, 'epoch': 6.76}
{'loss': 0.0317, 'grad_norm': 0.055291205644607544, 'learning_rate': 5e-05, 'epoch': 6.77}
{'loss': 0.0314, 'grad_norm': 0.04906377196311951, 'learning_rate': 5e-05, 'epoch': 6.78}
{'loss': 0.0306, 'grad_norm': 0.05902537330985069, 'learning_rate': 5e-05, 'epoch': 6.79}
{'loss': 0.0313, 'grad_norm': 0.06632525473833084, 'learning_rate': 5e-05, 'epoch': 6.8}
{'loss': 0.0334, 'grad_norm': 0.05011603981256485, 'learning_rate': 5e-05, 'epoch': 6.82}
{'loss': 0.0315, 'grad_norm': 0.050137005746364594, 'learning_rate': 5e-05, 'epoch': 6.83}
{'loss': 0.0312, 'grad_norm': 0.050149932503700256, 'learning_rate': 5e-05, 'epoch': 6.84}
{'loss': 0.0343, 'grad_norm': 0.06009037792682648, 'learning_rate': 5e-05, 'epoch': 6.85}
{'loss': 0.0317, 'grad_norm': 0.06605114787817001, 'learning_rate': 5e-05, 'epoch': 6.86}
{'loss': 0.032, 'grad_norm': 0.07313747704029083, 'learning_rate': 5e-05, 'epoch': 6.88}
{'loss': 0.0325, 'grad_norm': 0.04906207323074341, 'learning_rate': 5e-05, 'epoch': 6.89}
{'loss': 0.0314, 'grad_norm': 0.06437793374061584, 'learning_rate': 5e-05, 'epoch': 6.9}
{'loss': 0.0321, 'grad_norm': 0.08078691363334656, 'learning_rate': 5e-05, 'epoch': 6.91}
{'loss': 0.0329, 'grad_norm': 0.06454647332429886, 'learning_rate': 5e-05, 'epoch': 6.93}
{'loss': 0.0331, 'grad_norm': 0.07426005601882935, 'learning_rate': 5e-05, 'epoch': 6.94}
{'loss': 0.0321, 'grad_norm': 0.04567117244005203, 'learning_rate': 5e-05, 'epoch': 6.95}
{'loss': 0.0324, 'grad_norm': 0.07619211077690125, 'learning_rate': 5e-05, 'epoch': 6.96}
{'loss': 0.0322, 'grad_norm': 0.0665295347571373, 'learning_rate': 5e-05, 'epoch': 6.97}
{'loss': 0.0332, 'grad_norm': 0.051913294941186905, 'learning_rate': 5e-05, 'epoch': 6.99}
{'loss': 0.0334, 'grad_norm': 0.10181151330471039, 'learning_rate': 5e-05, 'epoch': 7.0}
{'eval_mit-movie': 0.5358851674144488, 'eval_mit-restaurant': 0.35918367342107826, 'eval_crossner_ai': 0.5477031801623609, 'eval_crossner_literature': 0.5769020251278997, 'eval_crossner_music': 0.7209047792275833, 'eval_crossner_politics': 0.5759368835791694, 'eval_crossner_science': 0.6784274193050561, 'eval_average': 0.5707061611767995, 'eval_runtime': 168.609, 'eval_samples_per_second': 8.303, 'eval_steps_per_second': 0.261, 'epoch': 7.0}
 58%|█████▊    | 5782/9900 [3:49:30<2:11:26,  1[INFO|trainer.py:3948] 2025-02-24 06:31:23,830 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782
[INFO|configuration_utils.py:423] 2025-02-24 06:31:23,832 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/config.json
[INFO|configuration_utils.py:909] 2025-02-24 06:31:23,832 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 06:31:25,473 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 06:31:25,475 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 06:31:25,475 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 06:31:25,476 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/spiece.model
[2025-02-24 06:31:25,523] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5778 is about to be saved!
[2025-02-24 06:31:25,529] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/mp_rank_00_model_states.pt
[2025-02-24 06:31:25,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/mp_rank_00_model_states.pt...
[2025-02-24 06:31:27,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/mp_rank_00_model_states.pt.
[2025-02-24 06:31:27,153] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 06:31:28,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 06:31:28,590] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-5782/global_step5778/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 06:31:28,591] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5778 is ready now!
{'loss': 0.0304, 'grad_norm': 0.05022187530994415, 'learning_rate': 5e-05, 'epoch': 7.01}
{'loss': 0.032, 'grad_norm': 0.06988824158906937, 'learning_rate': 5e-05, 'epoch': 7.02}
{'loss': 0.0317, 'grad_norm': 0.05783960223197937, 'learning_rate': 5e-05, 'epoch': 7.03}
{'loss': 0.0324, 'grad_norm': 0.05420483276247978, 'learning_rate': 5e-05, 'epoch': 7.05}
{'loss': 0.0332, 'grad_norm': 0.06967790424823761, 'learning_rate': 5e-05, 'epoch': 7.06}
{'loss': 0.0317, 'grad_norm': 0.06097285822033882, 'learning_rate': 5e-05, 'epoch': 7.07}
{'loss': 0.0307, 'grad_norm': 0.06064319238066673, 'learning_rate': 5e-05, 'epoch': 7.08}
{'loss': 0.0322, 'grad_norm': 0.06298394501209259, 'learning_rate': 5e-05, 'epoch': 7.09}
{'loss': 0.0316, 'grad_norm': 0.055080655962228775, 'learning_rate': 5e-05, 'epoch': 7.11}
{'loss': 0.0308, 'grad_norm': 0.06508302688598633, 'learning_rate': 5e-05, 'epoch': 7.12}
{'loss': 0.0312, 'grad_norm': 0.062456369400024414, 'learning_rate': 5e-05, 'epoch': 7.13}
{'loss': 0.0318, 'grad_norm': 0.05779886990785599, 'learning_rate': 5e-05, 'epoch': 7.14}
{'loss': 0.0307, 'grad_norm': 0.06707116216421127, 'learning_rate': 5e-05, 'epoch': 7.16}
{'loss': 0.0313, 'grad_norm': 0.047113001346588135, 'learning_rate': 5e-05, 'epoch': 7.17}
{'loss': 0.032, 'grad_norm': 0.05739705264568329, 'learning_rate': 5e-05, 'epoch': 7.18}
{'loss': 0.0317, 'grad_norm': 0.05677136778831482, 'learning_rate': 5e-05, 'epoch': 7.19}
{'loss': 0.0312, 'grad_norm': 0.05309337377548218, 'learning_rate': 5e-05, 'epoch': 7.2}
{'loss': 0.0319, 'grad_norm': 0.05647359788417816, 'learning_rate': 5e-05, 'epoch': 7.22}
{'loss': 0.031, 'grad_norm': 0.11731040477752686, 'learning_rate': 5e-05, 'epoch': 7.23}
{'loss': 0.0313, 'grad_norm': 0.05298127606511116, 'learning_rate': 5e-05, 'epoch': 7.24}
{'loss': 0.0306, 'grad_norm': 0.044412899762392044, 'learning_rate': 5e-05, 'epoch': 7.25}
{'loss': 0.031, 'grad_norm': 0.0559035986661911, 'learning_rate': 5e-05, 'epoch': 7.26}
{'loss': 0.0321, 'grad_norm': 0.05873805657029152, 'learning_rate': 5e-05, 'epoch': 7.28}
{'loss': 0.0302, 'grad_norm': 0.059954799711704254, 'learning_rate': 5e-05, 'epoch': 7.29}
{'loss': 0.0306, 'grad_norm': 0.060368526726961136, 'learning_rate': 5e-05, 'epoch': 7.3}
{'loss': 0.0319, 'grad_norm': 0.07216101139783859, 'learning_rate': 5e-05, 'epoch': 7.31}
{'loss': 0.0309, 'grad_norm': 0.08456137031316757, 'learning_rate': 5e-05, 'epoch': 7.32}
{'loss': 0.0323, 'grad_norm': 0.06396958231925964, 'learning_rate': 5e-05, 'epoch': 7.34}
{'loss': 0.0315, 'grad_norm': 0.06370623409748077, 'learning_rate': 5e-05, 'epoch': 7.35}
{'loss': 0.0308, 'grad_norm': 0.05153396725654602, 'learning_rate': 5e-05, 'epoch': 7.36}
{'loss': 0.0309, 'grad_norm': 0.07219531387090683, 'learning_rate': 5e-05, 'epoch': 7.37}
{'loss': 0.0315, 'grad_norm': 0.050704386085271835, 'learning_rate': 5e-05, 'epoch': 7.39}
{'loss': 0.0303, 'grad_norm': 0.06485896557569504, 'learning_rate': 5e-05, 'epoch': 7.4}
{'loss': 0.0304, 'grad_norm': 0.06030704826116562, 'learning_rate': 5e-05, 'epoch': 7.41}
{'loss': 0.031, 'grad_norm': 0.08214344829320908, 'learning_rate': 5e-05, 'epoch': 7.42}
{'loss': 0.0318, 'grad_norm': 0.065457783639431, 'learning_rate': 5e-05, 'epoch': 7.43}
{'loss': 0.031, 'grad_norm': 0.04205619916319847, 'learning_rate': 5e-05, 'epoch': 7.45}
{'loss': 0.0325, 'grad_norm': 0.0998958945274353, 'learning_rate': 5e-05, 'epoch': 7.46}
{'loss': 0.0311, 'grad_norm': 0.05666937306523323, 'learning_rate': 5e-05, 'epoch': 7.47}
{'loss': 0.031, 'grad_norm': 0.07074624300003052, 'learning_rate': 5e-05, 'epoch': 7.48}
{'loss': 0.0299, 'grad_norm': 0.10848774760961533, 'learning_rate': 5e-05, 'epoch': 7.49}
{'loss': 0.0314, 'grad_norm': 0.06036131829023361, 'learning_rate': 5e-05, 'epoch': 7.51}
{'loss': 0.031, 'grad_norm': 0.06661076098680496, 'learning_rate': 5e-05, 'epoch': 7.52}
{'loss': 0.032, 'grad_norm': 0.05654647573828697, 'learning_rate': 5e-05, 'epoch': 7.53}
{'loss': 0.0314, 'grad_norm': 0.0464090071618557, 'learning_rate': 5e-05, 'epoch': 7.54}
{'loss': 0.031, 'grad_norm': 0.047339797019958496, 'learning_rate': 5e-05, 'epoch': 7.55}
{'loss': 0.0306, 'grad_norm': 0.05059123784303665, 'learning_rate': 5e-05, 'epoch': 7.57}
{'loss': 0.0313, 'grad_norm': 0.08243439346551895, 'learning_rate': 5e-05, 'epoch': 7.58}
{'loss': 0.0309, 'grad_norm': 0.05937214940786362, 'learning_rate': 5e-05, 'epoch': 7.59}
{'loss': 0.0311, 'grad_norm': 0.0631866306066513, 'learning_rate': 5e-05, 'epoch': 7.6}
{'loss': 0.031, 'grad_norm': 0.06370532512664795, 'learning_rate': 5e-05, 'epoch': 7.62}
{'loss': 0.0317, 'grad_norm': 0.09502455592155457, 'learning_rate': 5e-05, 'epoch': 7.63}
{'loss': 0.0304, 'grad_norm': 0.06021163612604141, 'learning_rate': 5e-05, 'epoch': 7.64}
{'loss': 0.0314, 'grad_norm': 0.06290420889854431, 'learning_rate': 5e-05, 'epoch': 7.65}
{'loss': 0.0314, 'grad_norm': 0.046574730426073074, 'learning_rate': 5e-05, 'epoch': 7.66}
{'loss': 0.0315, 'grad_norm': 0.055415838956832886, 'learning_rate': 5e-05, 'epoch': 7.68}
{'loss': 0.0324, 'grad_norm': 0.08191889524459839, 'learning_rate': 5e-05, 'epoch': 7.69}
{'loss': 0.031, 'grad_norm': 0.0596172921359539, 'learning_rate': 5e-05, 'epoch': 7.7}
{'loss': 0.0309, 'grad_norm': 0.04681958630681038, 'learning_rate': 5e-05, 'epoch': 7.71}
{'loss': 0.0298, 'grad_norm': 0.07729113101959229, 'learning_rate': 5e-05, 'epoch': 7.72}
{'loss': 0.0319, 'grad_norm': 0.061912648379802704, 'learning_rate': 5e-05, 'epoch': 7.74}
{'loss': 0.0307, 'grad_norm': 0.06327230483293533, 'learning_rate': 5e-05, 'epoch': 7.75}
{'loss': 0.0299, 'grad_norm': 0.06090974435210228, 'learning_rate': 5e-05, 'epoch': 7.76}
{'loss': 0.0312, 'grad_norm': 0.09368443489074707, 'learning_rate': 5e-05, 'epoch': 7.77}
{'loss': 0.0315, 'grad_norm': 0.053674787282943726, 'learning_rate': 5e-05, 'epoch': 7.78}
{'loss': 0.0308, 'grad_norm': 0.055964864790439606, 'learning_rate': 5e-05, 'epoch': 7.8}
{'loss': 0.0305, 'grad_norm': 0.07669354975223541, 'learning_rate': 5e-05, 'epoch': 7.81}
{'loss': 0.0313, 'grad_norm': 0.06755132973194122, 'learning_rate': 5e-05, 'epoch': 7.82}
{'loss': 0.0311, 'grad_norm': 0.06304248422384262, 'learning_rate': 5e-05, 'epoch': 7.83}
{'loss': 0.0306, 'grad_norm': 0.0687779113650322, 'learning_rate': 5e-05, 'epoch': 7.85}
{'loss': 0.0306, 'grad_norm': 0.06430011242628098, 'learning_rate': 5e-05, 'epoch': 7.86}
{'loss': 0.0302, 'grad_norm': 0.07722048461437225, 'learning_rate': 5e-05, 'epoch': 7.87}
{'loss': 0.0315, 'grad_norm': 0.06165015324950218, 'learning_rate': 5e-05, 'epoch': 7.88}
{'loss': 0.0297, 'grad_norm': 0.0747261494398117, 'learning_rate': 5e-05, 'epoch': 7.89}
{'loss': 0.0311, 'grad_norm': 0.047679174691438675, 'learning_rate': 5e-05, 'epoch': 7.91}
{'loss': 0.0305, 'grad_norm': 0.09093746542930603, 'learning_rate': 5e-05, 'epoch': 7.92}
{'loss': 0.0324, 'grad_norm': 0.0564582385122776, 'learning_rate': 5e-05, 'epoch': 7.93}
{'loss': 0.0312, 'grad_norm': 0.05036993697285652, 'learning_rate': 5e-05, 'epoch': 7.94}
{'loss': 0.0314, 'grad_norm': 0.05777345970273018, 'learning_rate': 5e-05, 'epoch': 7.95}
{'loss': 0.031, 'grad_norm': 0.05738065391778946, 'learning_rate': 5e-05, 'epoch': 7.97}
{'loss': 0.0314, 'grad_norm': 0.06299673020839691, 'learning_rate': 5e-05, 'epoch': 7.98}
{'loss': 0.0314, 'grad_norm': 0.051668304949998856, 'learning_rate': 5e-05, 'epoch': 7.99}
{'eval_mit-movie': 0.49840255586090654, 'eval_mit-restaurant': 0.38567493108144824, 'eval_crossner_ai': 0.5552274069201977, 'eval_crossner_literature': 0.5830583057806157, 'eval_crossner_music': 0.7262813521854989, 'eval_crossner_politics': 0.577075098764203, 'eval_crossner_science': 0.6690211906666592, 'eval_average': 0.5706772630370756, 'eval_runtime': 169.0647, 'eval_samples_per_second': 8.281, 'eval_steps_per_second': 0.26, 'epoch': 8.0}
 67%|██████▋   | 6608/9900 [4:22:16<1:45:16,  1[INFO|trainer.py:3948] 2025-02-24 07:04:09,606 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608
[INFO|configuration_utils.py:423] 2025-02-24 07:04:09,608 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/config.json
[INFO|configuration_utils.py:909] 2025-02-24 07:04:09,608 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 07:04:11,230 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 07:04:11,232 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 07:04:11,232 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 07:04:11,234 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/spiece.model
[2025-02-24 07:04:11,278] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6604 is about to be saved!
[2025-02-24 07:04:11,285] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/mp_rank_00_model_states.pt
[2025-02-24 07:04:11,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/mp_rank_00_model_states.pt...
[2025-02-24 07:04:12,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/mp_rank_00_model_states.pt.
[2025-02-24 07:04:12,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 07:04:14,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 07:04:14,431] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-6608/global_step6604/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 07:04:14,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6604 is ready now!
{'loss': 0.0283, 'grad_norm': 0.06700786203145981, 'learning_rate': 5e-05, 'epoch': 8.0}
{'loss': 0.0311, 'grad_norm': 0.06700427085161209, 'learning_rate': 5e-05, 'epoch': 8.01}
{'loss': 0.0292, 'grad_norm': 0.08378256857395172, 'learning_rate': 5e-05, 'epoch': 8.03}
{'loss': 0.0302, 'grad_norm': 0.0703229084610939, 'learning_rate': 5e-05, 'epoch': 8.04}
{'loss': 0.0307, 'grad_norm': 0.04397948458790779, 'learning_rate': 5e-05, 'epoch': 8.05}
{'loss': 0.0296, 'grad_norm': 0.07199078053236008, 'learning_rate': 5e-05, 'epoch': 8.06}
{'loss': 0.0301, 'grad_norm': 0.101606085896492, 'learning_rate': 5e-05, 'epoch': 8.08}
{'loss': 0.0308, 'grad_norm': 0.05550051108002663, 'learning_rate': 5e-05, 'epoch': 8.09}
{'loss': 0.0303, 'grad_norm': 0.11989198625087738, 'learning_rate': 5e-05, 'epoch': 8.1}
{'loss': 0.0295, 'grad_norm': 0.07167259603738785, 'learning_rate': 5e-05, 'epoch': 8.11}
{'loss': 0.032, 'grad_norm': 0.0753958597779274, 'learning_rate': 5e-05, 'epoch': 8.12}
{'loss': 0.0303, 'grad_norm': 0.05182762071490288, 'learning_rate': 5e-05, 'epoch': 8.14}
{'loss': 0.0301, 'grad_norm': 0.04691226780414581, 'learning_rate': 5e-05, 'epoch': 8.15}
{'loss': 0.0311, 'grad_norm': 0.044810518622398376, 'learning_rate': 5e-05, 'epoch': 8.16}
{'loss': 0.0298, 'grad_norm': 0.05539979413151741, 'learning_rate': 5e-05, 'epoch': 8.17}
{'loss': 0.0287, 'grad_norm': 0.06409293413162231, 'learning_rate': 5e-05, 'epoch': 8.18}
{'loss': 0.03, 'grad_norm': 0.05935652181506157, 'learning_rate': 5e-05, 'epoch': 8.2}
{'loss': 0.0309, 'grad_norm': 0.0569990798830986, 'learning_rate': 5e-05, 'epoch': 8.21}
{'loss': 0.0315, 'grad_norm': 0.08169563859701157, 'learning_rate': 5e-05, 'epoch': 8.22}
{'loss': 0.0302, 'grad_norm': 0.07409108430147171, 'learning_rate': 5e-05, 'epoch': 8.23}
{'loss': 0.0291, 'grad_norm': 0.04857584834098816, 'learning_rate': 5e-05, 'epoch': 8.24}
{'loss': 0.03, 'grad_norm': 0.07924052327871323, 'learning_rate': 5e-05, 'epoch': 8.26}
{'loss': 0.0305, 'grad_norm': 0.06577718257904053, 'learning_rate': 5e-05, 'epoch': 8.27}
{'loss': 0.0313, 'grad_norm': 0.05895228311419487, 'learning_rate': 5e-05, 'epoch': 8.28}
{'loss': 0.0304, 'grad_norm': 0.06021483242511749, 'learning_rate': 5e-05, 'epoch': 8.29}
{'loss': 0.0302, 'grad_norm': 0.0699157565832138, 'learning_rate': 5e-05, 'epoch': 8.31}
{'loss': 0.0302, 'grad_norm': 0.059648506343364716, 'learning_rate': 5e-05, 'epoch': 8.32}
{'loss': 0.0298, 'grad_norm': 0.04722623527050018, 'learning_rate': 5e-05, 'epoch': 8.33}
{'loss': 0.0298, 'grad_norm': 0.05075819417834282, 'learning_rate': 5e-05, 'epoch': 8.34}
{'loss': 0.0292, 'grad_norm': 0.04939611256122589, 'learning_rate': 5e-05, 'epoch': 8.35}
{'loss': 0.0303, 'grad_norm': 0.05258653312921524, 'learning_rate': 5e-05, 'epoch': 8.37}
{'loss': 0.0298, 'grad_norm': 0.060906458646059036, 'learning_rate': 5e-05, 'epoch': 8.38}
{'loss': 0.0308, 'grad_norm': 0.0640186071395874, 'learning_rate': 5e-05, 'epoch': 8.39}
{'loss': 0.0307, 'grad_norm': 0.07799302786588669, 'learning_rate': 5e-05, 'epoch': 8.4}
{'loss': 0.0302, 'grad_norm': 0.053230367600917816, 'learning_rate': 5e-05, 'epoch': 8.41}
{'loss': 0.0287, 'grad_norm': 0.04478249326348305, 'learning_rate': 5e-05, 'epoch': 8.43}
{'loss': 0.0289, 'grad_norm': 0.04417639970779419, 'learning_rate': 5e-05, 'epoch': 8.44}
{'loss': 0.0313, 'grad_norm': 0.05914773419499397, 'learning_rate': 5e-05, 'epoch': 8.45}
{'loss': 0.03, 'grad_norm': 0.047092534601688385, 'learning_rate': 5e-05, 'epoch': 8.46}
{'loss': 0.0296, 'grad_norm': 0.051935773342847824, 'learning_rate': 5e-05, 'epoch': 8.47}
{'loss': 0.0304, 'grad_norm': 0.06251001358032227, 'learning_rate': 5e-05, 'epoch': 8.49}
{'loss': 0.03, 'grad_norm': 0.05952072516083717, 'learning_rate': 5e-05, 'epoch': 8.5}
{'loss': 0.0314, 'grad_norm': 0.09805408865213394, 'learning_rate': 5e-05, 'epoch': 8.51}
{'loss': 0.0298, 'grad_norm': 0.07051268965005875, 'learning_rate': 5e-05, 'epoch': 8.52}
{'loss': 0.029, 'grad_norm': 0.0410262793302536, 'learning_rate': 5e-05, 'epoch': 8.54}
{'loss': 0.0303, 'grad_norm': 0.05124836415052414, 'learning_rate': 5e-05, 'epoch': 8.55}
{'loss': 0.0295, 'grad_norm': 0.05929224565625191, 'learning_rate': 5e-05, 'epoch': 8.56}
{'loss': 0.0296, 'grad_norm': 0.04988015070557594, 'learning_rate': 5e-05, 'epoch': 8.57}
{'loss': 0.0294, 'grad_norm': 0.0507039837539196, 'learning_rate': 5e-05, 'epoch': 8.58}
{'loss': 0.03, 'grad_norm': 0.051685549318790436, 'learning_rate': 5e-05, 'epoch': 8.6}
{'loss': 0.0308, 'grad_norm': 0.06147461757063866, 'learning_rate': 5e-05, 'epoch': 8.61}
{'loss': 0.0299, 'grad_norm': 0.049178969115018845, 'learning_rate': 5e-05, 'epoch': 8.62}
{'loss': 0.0289, 'grad_norm': 0.06668394804000854, 'learning_rate': 5e-05, 'epoch': 8.63}
{'loss': 0.0305, 'grad_norm': 0.0628393292427063, 'learning_rate': 5e-05, 'epoch': 8.64}
{'loss': 0.0302, 'grad_norm': 0.04876089096069336, 'learning_rate': 5e-05, 'epoch': 8.66}
{'loss': 0.0293, 'grad_norm': 0.04743991047143936, 'learning_rate': 5e-05, 'epoch': 8.67}
{'loss': 0.0314, 'grad_norm': 0.05808515101671219, 'learning_rate': 5e-05, 'epoch': 8.68}
{'loss': 0.0304, 'grad_norm': 0.05189051106572151, 'learning_rate': 5e-05, 'epoch': 8.69}
{'loss': 0.0295, 'grad_norm': 0.05041467770934105, 'learning_rate': 5e-05, 'epoch': 8.71}
{'loss': 0.0289, 'grad_norm': 0.07299977540969849, 'learning_rate': 5e-05, 'epoch': 8.72}
{'loss': 0.0299, 'grad_norm': 0.055000752210617065, 'learning_rate': 5e-05, 'epoch': 8.73}
{'loss': 0.0289, 'grad_norm': 0.04921656847000122, 'learning_rate': 5e-05, 'epoch': 8.74}
{'loss': 0.0305, 'grad_norm': 0.05259346589446068, 'learning_rate': 5e-05, 'epoch': 8.75}
{'loss': 0.0304, 'grad_norm': 0.05732627585530281, 'learning_rate': 5e-05, 'epoch': 8.77}
{'loss': 0.0304, 'grad_norm': 0.06741902977228165, 'learning_rate': 5e-05, 'epoch': 8.78}
{'loss': 0.0296, 'grad_norm': 0.061125390231609344, 'learning_rate': 5e-05, 'epoch': 8.79}
{'loss': 0.0289, 'grad_norm': 0.04781479015946388, 'learning_rate': 5e-05, 'epoch': 8.8}
{'loss': 0.0297, 'grad_norm': 0.05695708841085434, 'learning_rate': 5e-05, 'epoch': 8.81}
{'loss': 0.0309, 'grad_norm': 0.04562579095363617, 'learning_rate': 5e-05, 'epoch': 8.83}
{'loss': 0.0291, 'grad_norm': 0.08123734593391418, 'learning_rate': 5e-05, 'epoch': 8.84}
{'loss': 0.0296, 'grad_norm': 0.05103147774934769, 'learning_rate': 5e-05, 'epoch': 8.85}
{'loss': 0.0301, 'grad_norm': 0.04667806997895241, 'learning_rate': 5e-05, 'epoch': 8.86}
{'loss': 0.0306, 'grad_norm': 0.08352319896221161, 'learning_rate': 5e-05, 'epoch': 8.87}
{'loss': 0.0301, 'grad_norm': 0.0625789538025856, 'learning_rate': 5e-05, 'epoch': 8.89}
{'loss': 0.0303, 'grad_norm': 0.059078510850667953, 'learning_rate': 5e-05, 'epoch': 8.9}
{'loss': 0.0307, 'grad_norm': 0.0515921525657177, 'learning_rate': 5e-05, 'epoch': 8.91}
{'loss': 0.0301, 'grad_norm': 0.05051165819168091, 'learning_rate': 5e-05, 'epoch': 8.92}
{'loss': 0.0296, 'grad_norm': 0.05595686659216881, 'learning_rate': 5e-05, 'epoch': 8.94}
{'loss': 0.0295, 'grad_norm': 0.14417368173599243, 'learning_rate': 5e-05, 'epoch': 8.95}
{'loss': 0.0298, 'grad_norm': 0.08111144602298737, 'learning_rate': 5e-05, 'epoch': 8.96}
{'loss': 0.0296, 'grad_norm': 0.05264715850353241, 'learning_rate': 5e-05, 'epoch': 8.97}
{'loss': 0.0292, 'grad_norm': 0.05041257292032242, 'learning_rate': 5e-05, 'epoch': 8.98}
{'loss': 0.0284, 'grad_norm': 0.06630532443523407, 'learning_rate': 5e-05, 'epoch': 9.0}
{'eval_mit-movie': 0.5224358973862893, 'eval_mit-restaurant': 0.38873994633206493, 'eval_crossner_ai': 0.5465393794251852, 'eval_crossner_literature': 0.593238822196452, 'eval_crossner_music': 0.7552804078159323, 'eval_crossner_politics': 0.5756369426251216, 'eval_crossner_science': 0.6981227802647318, 'eval_average': 0.5828563108636823, 'eval_runtime': 169.0979, 'eval_samples_per_second': 8.279, 'eval_steps_per_second': 0.26, 'epoch': 9.0}
 75%|███████▌  | 7434/9900 [4:55:01<1:16:55,  1[INFO|trainer.py:3948] 2025-02-24 07:36:54,444 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434
[INFO|configuration_utils.py:423] 2025-02-24 07:36:54,451 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/config.json
[INFO|configuration_utils.py:909] 2025-02-24 07:36:54,451 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 07:36:56,060 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 07:36:56,062 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 07:36:56,062 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 07:36:56,064 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/spiece.model
[2025-02-24 07:36:56,099] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7429 is about to be saved!
[2025-02-24 07:36:56,113] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt
[2025-02-24 07:36:56,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt...
[2025-02-24 07:36:57,706] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt.
[2025-02-24 07:36:57,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 07:36:58,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 07:36:58,946] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 07:36:58,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7429 is ready now!
{'loss': 0.0283, 'grad_norm': 0.059768542647361755, 'learning_rate': 5e-05, 'epoch': 9.01}
{'loss': 0.0292, 'grad_norm': 0.04706709459424019, 'learning_rate': 5e-05, 'epoch': 9.02}
{'loss': 0.0295, 'grad_norm': 0.06963618844747543, 'learning_rate': 5e-05, 'epoch': 9.03}
{'loss': 0.0297, 'grad_norm': 0.04931259900331497, 'learning_rate': 5e-05, 'epoch': 9.04}
{'loss': 0.0301, 'grad_norm': 0.0598042793571949, 'learning_rate': 5e-05, 'epoch': 9.06}
{'loss': 0.0288, 'grad_norm': 0.06277851015329361, 'learning_rate': 5e-05, 'epoch': 9.07}
{'loss': 0.029, 'grad_norm': 0.06703856587409973, 'learning_rate': 5e-05, 'epoch': 9.08}
{'loss': 0.0288, 'grad_norm': 0.0828629806637764, 'learning_rate': 5e-05, 'epoch': 9.09}
{'loss': 0.029, 'grad_norm': 0.06443225592374802, 'learning_rate': 5e-05, 'epoch': 9.1}
{'loss': 0.0287, 'grad_norm': 0.10066930949687958, 'learning_rate': 5e-05, 'epoch': 9.12}
{'loss': 0.03, 'grad_norm': 0.04960065335035324, 'learning_rate': 5e-05, 'epoch': 9.13}
{'loss': 0.0281, 'grad_norm': 0.04525882378220558, 'learning_rate': 5e-05, 'epoch': 9.14}
{'loss': 0.029, 'grad_norm': 0.04373876750469208, 'learning_rate': 5e-05, 'epoch': 9.15}
{'loss': 0.0298, 'grad_norm': 0.09469714015722275, 'learning_rate': 5e-05, 'epoch': 9.16}
{'loss': 0.0284, 'grad_norm': 0.0690685510635376, 'learning_rate': 5e-05, 'epoch': 9.18}
{'loss': 0.0294, 'grad_norm': 0.057684559375047684, 'learning_rate': 5e-05, 'epoch': 9.19}
{'loss': 0.029, 'grad_norm': 0.06555525213479996, 'learning_rate': 5e-05, 'epoch': 9.2}
{'loss': 0.0293, 'grad_norm': 0.08424431830644608, 'learning_rate': 5e-05, 'epoch': 9.21}
{'loss': 0.0287, 'grad_norm': 0.0560125894844532, 'learning_rate': 5e-05, 'epoch': 9.23}
{'loss': 0.0301, 'grad_norm': 0.07228877395391464, 'learning_rate': 5e-05, 'epoch': 9.24}
{'loss': 0.0282, 'grad_norm': 0.06906953454017639, 'learning_rate': 5e-05, 'epoch': 9.25}
{'loss': 0.0292, 'grad_norm': 0.07885479182004929, 'learning_rate': 5e-05, 'epoch': 9.26}
{'loss': 0.0286, 'grad_norm': 0.05399014428257942, 'learning_rate': 5e-05, 'epoch': 9.27}
{'loss': 0.0291, 'grad_norm': 0.07080085575580597, 'learning_rate': 5e-05, 'epoch': 9.29}
{'loss': 0.0284, 'grad_norm': 0.05002142861485481, 'learning_rate': 5e-05, 'epoch': 9.3}
{'loss': 0.0294, 'grad_norm': 0.061370112001895905, 'learning_rate': 5e-05, 'epoch': 9.31}
{'loss': 0.0291, 'grad_norm': 0.0510115884244442, 'learning_rate': 5e-05, 'epoch': 9.32}
{'loss': 0.0292, 'grad_norm': 0.05997750535607338, 'learning_rate': 5e-05, 'epoch': 9.33}
{'loss': 0.0297, 'grad_norm': 0.07004708796739578, 'learning_rate': 5e-05, 'epoch': 9.35}
{'loss': 0.0289, 'grad_norm': 0.07536523789167404, 'learning_rate': 5e-05, 'epoch': 9.36}
{'loss': 0.0295, 'grad_norm': 0.05565881356596947, 'learning_rate': 5e-05, 'epoch': 9.37}
{'loss': 0.0292, 'grad_norm': 0.07349418848752975, 'learning_rate': 5e-05, 'epoch': 9.38}
{'loss': 0.0276, 'grad_norm': 0.045222654938697815, 'learning_rate': 5e-05, 'epoch': 9.39}
{'loss': 0.0298, 'grad_norm': 0.054430924355983734, 'learning_rate': 5e-05, 'epoch': 9.41}
{'loss': 0.0292, 'grad_norm': 0.06075428053736687, 'learning_rate': 5e-05, 'epoch': 9.42}
{'loss': 0.0286, 'grad_norm': 0.059471968561410904, 'learning_rate': 5e-05, 'epoch': 9.43}
{'loss': 0.0287, 'grad_norm': 0.04322509467601776, 'learning_rate': 5e-05, 'epoch': 9.44}
{'loss': 0.0301, 'grad_norm': 0.05267166346311569, 'learning_rate': 5e-05, 'epoch': 9.46}
{'loss': 0.0288, 'grad_norm': 0.09709232300519943, 'learning_rate': 5e-05, 'epoch': 9.47}
{'loss': 0.0294, 'grad_norm': 0.05909568816423416, 'learning_rate': 5e-05, 'epoch': 9.48}
{'loss': 0.0289, 'grad_norm': 0.04929182678461075, 'learning_rate': 5e-05, 'epoch': 9.49}
{'loss': 0.0281, 'grad_norm': 0.06155452877283096, 'learning_rate': 5e-05, 'epoch': 9.5}
{'loss': 0.029, 'grad_norm': 0.04989394545555115, 'learning_rate': 5e-05, 'epoch': 9.52}
{'loss': 0.0297, 'grad_norm': 0.14524312317371368, 'learning_rate': 5e-05, 'epoch': 9.53}
{'loss': 0.0272, 'grad_norm': 0.06092321500182152, 'learning_rate': 5e-05, 'epoch': 9.54}
{'loss': 0.0288, 'grad_norm': 0.04497021436691284, 'learning_rate': 5e-05, 'epoch': 9.55}
{'loss': 0.0279, 'grad_norm': 0.06449387222528458, 'learning_rate': 5e-05, 'epoch': 9.56}
{'loss': 0.0279, 'grad_norm': 0.06502891331911087, 'learning_rate': 5e-05, 'epoch': 9.58}
{'loss': 0.0288, 'grad_norm': 0.05299033224582672, 'learning_rate': 5e-05, 'epoch': 9.59}
{'loss': 0.0293, 'grad_norm': 0.05005867779254913, 'learning_rate': 5e-05, 'epoch': 9.6}
{'loss': 0.0293, 'grad_norm': 0.0447520911693573, 'learning_rate': 5e-05, 'epoch': 9.61}
{'loss': 0.0288, 'grad_norm': 0.05678754299879074, 'learning_rate': 5e-05, 'epoch': 9.63}
{'loss': 0.0292, 'grad_norm': 0.0551735982298851, 'learning_rate': 5e-05, 'epoch': 9.64}
{'loss': 0.028, 'grad_norm': 0.04358578473329544, 'learning_rate': 5e-05, 'epoch': 9.65}
{'loss': 0.0298, 'grad_norm': 0.07490818947553635, 'learning_rate': 5e-05, 'epoch': 9.66}
{'loss': 0.0303, 'grad_norm': 0.046905290335416794, 'learning_rate': 5e-05, 'epoch': 9.67}
{'loss': 0.0288, 'grad_norm': 0.05402452126145363, 'learning_rate': 5e-05, 'epoch': 9.69}
{'loss': 0.0293, 'grad_norm': 0.07567144185304642, 'learning_rate': 5e-05, 'epoch': 9.7}
{'loss': 0.0279, 'grad_norm': 0.04987477511167526, 'learning_rate': 5e-05, 'epoch': 9.71}
{'loss': 0.0294, 'grad_norm': 0.052680738270282745, 'learning_rate': 5e-05, 'epoch': 9.72}
{'loss': 0.0295, 'grad_norm': 0.07211039960384369, 'learning_rate': 5e-05, 'epoch': 9.73}
{'loss': 0.0276, 'grad_norm': 0.05110478773713112, 'learning_rate': 5e-05, 'epoch': 9.75}
{'loss': 0.0293, 'grad_norm': 0.06693224608898163, 'learning_rate': 5e-05, 'epoch': 9.76}
{'loss': 0.0279, 'grad_norm': 0.05163189396262169, 'learning_rate': 5e-05, 'epoch': 9.77}
{'loss': 0.0295, 'grad_norm': 0.05866417661309242, 'learning_rate': 5e-05, 'epoch': 9.78}
{'loss': 0.0288, 'grad_norm': 0.05354995280504227, 'learning_rate': 5e-05, 'epoch': 9.79}
{'loss': 0.0294, 'grad_norm': 0.041625119745731354, 'learning_rate': 5e-05, 'epoch': 9.81}
{'loss': 0.0292, 'grad_norm': 0.0473911352455616, 'learning_rate': 5e-05, 'epoch': 9.82}
{'loss': 0.0292, 'grad_norm': 0.07647092640399933, 'learning_rate': 5e-05, 'epoch': 9.83}
{'loss': 0.0299, 'grad_norm': 0.08443570882081985, 'learning_rate': 5e-05, 'epoch': 9.84}
{'loss': 0.0283, 'grad_norm': 0.09376140683889389, 'learning_rate': 5e-05, 'epoch': 9.86}
{'loss': 0.0297, 'grad_norm': 0.056864019483327866, 'learning_rate': 5e-05, 'epoch': 9.87}
{'loss': 0.0285, 'grad_norm': 0.07343296706676483, 'learning_rate': 5e-05, 'epoch': 9.88}
{'loss': 0.0295, 'grad_norm': 0.07169260084629059, 'learning_rate': 5e-05, 'epoch': 9.89}
{'loss': 0.0276, 'grad_norm': 0.06157853826880455, 'learning_rate': 5e-05, 'epoch': 9.9}
{'loss': 0.0287, 'grad_norm': 0.05319369211792946, 'learning_rate': 5e-05, 'epoch': 9.92}
{'loss': 0.0285, 'grad_norm': 0.06568072736263275, 'learning_rate': 5e-05, 'epoch': 9.93}
{'loss': 0.0287, 'grad_norm': 0.04634074494242668, 'learning_rate': 5e-05, 'epoch': 9.94}
{'loss': 0.0296, 'grad_norm': 0.044791869819164276, 'learning_rate': 5e-05, 'epoch': 9.95}
{'loss': 0.0283, 'grad_norm': 0.05091824755072594, 'learning_rate': 5e-05, 'epoch': 9.96}
{'loss': 0.0294, 'grad_norm': 0.056516580283641815, 'learning_rate': 5e-05, 'epoch': 9.98}
{'loss': 0.0283, 'grad_norm': 0.057373423129320145, 'learning_rate': 5e-05, 'epoch': 9.99}
{'loss': 0.0273, 'grad_norm': 0.07875057309865952, 'learning_rate': 5e-05, 'epoch': 10.0}
{'eval_mit-movie': 0.5316045380380479, 'eval_mit-restaurant': 0.38881118876355264, 'eval_crossner_ai': 0.5622343654631813, 'eval_crossner_literature': 0.5772402418411798, 'eval_crossner_music': 0.7358767424297709, 'eval_crossner_politics': 0.5663858804152587, 'eval_crossner_science': 0.673511293584599, 'eval_average': 0.5765234643622271, 'eval_runtime': 168.1965, 'eval_samples_per_second': 8.324, 'eval_steps_per_second': 0.262, 'epoch': 10.0}
 83%|████████▎ | 8260/9900 [5:27:48<50:40,  1.8[INFO|trainer.py:3948] 2025-02-24 08:09:41,189 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260
[INFO|configuration_utils.py:423] 2025-02-24 08:09:41,191 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/config.json
[INFO|configuration_utils.py:909] 2025-02-24 08:09:41,191 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 08:09:42,799 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 08:09:42,801 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 08:09:42,802 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 08:09:42,803 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/spiece.model
[2025-02-24 08:09:42,838] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8255 is about to be saved!
[2025-02-24 08:09:42,844] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/mp_rank_00_model_states.pt
[2025-02-24 08:09:42,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/mp_rank_00_model_states.pt...
[2025-02-24 08:09:44,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/mp_rank_00_model_states.pt.
[2025-02-24 08:09:44,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 08:09:45,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 08:09:45,673] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-8260/global_step8255/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 08:09:45,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8255 is ready now!
{'loss': 0.0276, 'grad_norm': 0.06303677707910538, 'learning_rate': 5e-05, 'epoch': 10.01}
{'loss': 0.0279, 'grad_norm': 0.06867120414972305, 'learning_rate': 5e-05, 'epoch': 10.02}
{'loss': 0.0285, 'grad_norm': 0.0864197388291359, 'learning_rate': 5e-05, 'epoch': 10.04}
{'loss': 0.0274, 'grad_norm': 0.04117531701922417, 'learning_rate': 5e-05, 'epoch': 10.05}
{'loss': 0.0278, 'grad_norm': 0.08530303835868835, 'learning_rate': 5e-05, 'epoch': 10.06}
{'loss': 0.0273, 'grad_norm': 0.04962395504117012, 'learning_rate': 5e-05, 'epoch': 10.07}
{'loss': 0.0293, 'grad_norm': 0.056805167347192764, 'learning_rate': 5e-05, 'epoch': 10.08}
{'loss': 0.0292, 'grad_norm': 0.045884180814027786, 'learning_rate': 5e-05, 'epoch': 10.1}
{'loss': 0.029, 'grad_norm': 0.060730941593647, 'learning_rate': 5e-05, 'epoch': 10.11}
{'loss': 0.0277, 'grad_norm': 0.06972973793745041, 'learning_rate': 5e-05, 'epoch': 10.12}
{'loss': 0.0286, 'grad_norm': 0.05251840874552727, 'learning_rate': 5e-05, 'epoch': 10.13}
{'loss': 0.0284, 'grad_norm': 0.05124010145664215, 'learning_rate': 5e-05, 'epoch': 10.15}
{'loss': 0.028, 'grad_norm': 0.046299781650304794, 'learning_rate': 5e-05, 'epoch': 10.16}
{'loss': 0.0283, 'grad_norm': 0.05180433392524719, 'learning_rate': 5e-05, 'epoch': 10.17}
{'loss': 0.0277, 'grad_norm': 0.05863704904913902, 'learning_rate': 5e-05, 'epoch': 10.18}
{'loss': 0.029, 'grad_norm': 0.04453059285879135, 'learning_rate': 5e-05, 'epoch': 10.19}
{'loss': 0.0283, 'grad_norm': 0.06268448382616043, 'learning_rate': 5e-05, 'epoch': 10.21}
{'loss': 0.0286, 'grad_norm': 0.05532728135585785, 'learning_rate': 5e-05, 'epoch': 10.22}
{'loss': 0.0288, 'grad_norm': 0.05421324446797371, 'learning_rate': 5e-05, 'epoch': 10.23}
{'loss': 0.0271, 'grad_norm': 0.04528725892305374, 'learning_rate': 5e-05, 'epoch': 10.24}
{'loss': 0.0274, 'grad_norm': 0.04511852189898491, 'learning_rate': 5e-05, 'epoch': 10.25}
{'loss': 0.0282, 'grad_norm': 0.05329641327261925, 'learning_rate': 5e-05, 'epoch': 10.27}
{'loss': 0.0278, 'grad_norm': 0.07954806089401245, 'learning_rate': 5e-05, 'epoch': 10.28}
{'loss': 0.0272, 'grad_norm': 0.042671237140893936, 'learning_rate': 5e-05, 'epoch': 10.29}
{'loss': 0.0294, 'grad_norm': 0.15268574655056, 'learning_rate': 5e-05, 'epoch': 10.3}
{'loss': 0.0275, 'grad_norm': 0.07283715903759003, 'learning_rate': 5e-05, 'epoch': 10.31}
{'loss': 0.0284, 'grad_norm': 0.07318396866321564, 'learning_rate': 5e-05, 'epoch': 10.33}
{'loss': 0.0279, 'grad_norm': 0.1035086065530777, 'learning_rate': 5e-05, 'epoch': 10.34}
{'loss': 0.0279, 'grad_norm': 0.050597257912158966, 'learning_rate': 5e-05, 'epoch': 10.35}
{'loss': 0.0293, 'grad_norm': 0.05948174372315407, 'learning_rate': 5e-05, 'epoch': 10.36}
{'loss': 0.0274, 'grad_norm': 0.05145808309316635, 'learning_rate': 5e-05, 'epoch': 10.38}
{'loss': 0.0282, 'grad_norm': 0.051998551934957504, 'learning_rate': 5e-05, 'epoch': 10.39}
{'loss': 0.0284, 'grad_norm': 0.04177211970090866, 'learning_rate': 5e-05, 'epoch': 10.4}
{'loss': 0.0285, 'grad_norm': 0.05431985482573509, 'learning_rate': 5e-05, 'epoch': 10.41}
{'loss': 0.0271, 'grad_norm': 0.05620291829109192, 'learning_rate': 5e-05, 'epoch': 10.42}
{'loss': 0.0275, 'grad_norm': 0.04623458907008171, 'learning_rate': 5e-05, 'epoch': 10.44}
{'loss': 0.0277, 'grad_norm': 0.05064861476421356, 'learning_rate': 5e-05, 'epoch': 10.45}
{'loss': 0.0284, 'grad_norm': 0.0752241238951683, 'learning_rate': 5e-05, 'epoch': 10.46}
{'loss': 0.0276, 'grad_norm': 0.054772522300481796, 'learning_rate': 5e-05, 'epoch': 10.47}
{'loss': 0.0276, 'grad_norm': 0.07285093516111374, 'learning_rate': 5e-05, 'epoch': 10.48}
{'loss': 0.028, 'grad_norm': 0.059336066246032715, 'learning_rate': 5e-05, 'epoch': 10.5}
{'loss': 0.0278, 'grad_norm': 0.06207685172557831, 'learning_rate': 5e-05, 'epoch': 10.51}
{'loss': 0.0277, 'grad_norm': 0.09555963426828384, 'learning_rate': 5e-05, 'epoch': 10.52}
{'loss': 0.0284, 'grad_norm': 0.05124190077185631, 'learning_rate': 5e-05, 'epoch': 10.53}
{'loss': 0.028, 'grad_norm': 0.06234337016940117, 'learning_rate': 5e-05, 'epoch': 10.55}
{'loss': 0.0282, 'grad_norm': 0.05557698756456375, 'learning_rate': 5e-05, 'epoch': 10.56}
{'loss': 0.0277, 'grad_norm': 0.06291544437408447, 'learning_rate': 5e-05, 'epoch': 10.57}
{'loss': 0.0286, 'grad_norm': 0.04625774174928665, 'learning_rate': 5e-05, 'epoch': 10.58}
{'loss': 0.0279, 'grad_norm': 0.04623044282197952, 'learning_rate': 5e-05, 'epoch': 10.59}
{'loss': 0.0274, 'grad_norm': 0.06276363879442215, 'learning_rate': 5e-05, 'epoch': 10.61}
{'loss': 0.0287, 'grad_norm': 0.05646049231290817, 'learning_rate': 5e-05, 'epoch': 10.62}
{'loss': 0.0284, 'grad_norm': 0.058535922318696976, 'learning_rate': 5e-05, 'epoch': 10.63}
{'loss': 0.0286, 'grad_norm': 0.05295998230576515, 'learning_rate': 5e-05, 'epoch': 10.64}
{'loss': 0.0286, 'grad_norm': 0.059769097715616226, 'learning_rate': 5e-05, 'epoch': 10.65}
{'loss': 0.0276, 'grad_norm': 0.08117330819368362, 'learning_rate': 5e-05, 'epoch': 10.67}
{'loss': 0.0281, 'grad_norm': 0.062146615236997604, 'learning_rate': 5e-05, 'epoch': 10.68}
{'loss': 0.0273, 'grad_norm': 0.08524048328399658, 'learning_rate': 5e-05, 'epoch': 10.69}
{'loss': 0.0284, 'grad_norm': 0.047561053186655045, 'learning_rate': 5e-05, 'epoch': 10.7}
{'loss': 0.0281, 'grad_norm': 0.04072849079966545, 'learning_rate': 5e-05, 'epoch': 10.71}
{'loss': 0.0282, 'grad_norm': 0.06835158169269562, 'learning_rate': 5e-05, 'epoch': 10.73}
{'loss': 0.029, 'grad_norm': 0.08589872717857361, 'learning_rate': 5e-05, 'epoch': 10.74}
{'loss': 0.0281, 'grad_norm': 0.10396254807710648, 'learning_rate': 5e-05, 'epoch': 10.75}
{'loss': 0.0279, 'grad_norm': 0.041733793914318085, 'learning_rate': 5e-05, 'epoch': 10.76}
{'loss': 0.0282, 'grad_norm': 0.08845353126525879, 'learning_rate': 5e-05, 'epoch': 10.78}
{'loss': 0.0271, 'grad_norm': 0.06044287234544754, 'learning_rate': 5e-05, 'epoch': 10.79}
{'loss': 0.0282, 'grad_norm': 0.05085516348481178, 'learning_rate': 5e-05, 'epoch': 10.8}
{'loss': 0.0274, 'grad_norm': 0.04839715361595154, 'learning_rate': 5e-05, 'epoch': 10.81}
{'loss': 0.027, 'grad_norm': 0.05414149537682533, 'learning_rate': 5e-05, 'epoch': 10.82}
{'loss': 0.0278, 'grad_norm': 0.05011343955993652, 'learning_rate': 5e-05, 'epoch': 10.84}
{'loss': 0.0282, 'grad_norm': 0.052940137684345245, 'learning_rate': 5e-05, 'epoch': 10.85}
{'loss': 0.0287, 'grad_norm': 0.05667440593242645, 'learning_rate': 5e-05, 'epoch': 10.86}
{'loss': 0.0279, 'grad_norm': 0.06683414429426193, 'learning_rate': 5e-05, 'epoch': 10.87}
{'loss': 0.0271, 'grad_norm': 0.0930878147482872, 'learning_rate': 5e-05, 'epoch': 10.88}
{'loss': 0.0289, 'grad_norm': 0.05005992203950882, 'learning_rate': 5e-05, 'epoch': 10.9}
{'loss': 0.0279, 'grad_norm': 0.04341979697346687, 'learning_rate': 5e-05, 'epoch': 10.91}
{'loss': 0.0284, 'grad_norm': 0.043811678886413574, 'learning_rate': 5e-05, 'epoch': 10.92}
{'loss': 0.0269, 'grad_norm': 0.05393432825803757, 'learning_rate': 5e-05, 'epoch': 10.93}
{'loss': 0.0279, 'grad_norm': 0.04112107306718826, 'learning_rate': 5e-05, 'epoch': 10.94}
{'loss': 0.0284, 'grad_norm': 0.052045788615942, 'learning_rate': 5e-05, 'epoch': 10.96}
{'loss': 0.0268, 'grad_norm': 0.05054125562310219, 'learning_rate': 5e-05, 'epoch': 10.97}
{'loss': 0.0282, 'grad_norm': 0.04798123612999916, 'learning_rate': 5e-05, 'epoch': 10.98}
{'loss': 0.0274, 'grad_norm': 0.05855538323521614, 'learning_rate': 5e-05, 'epoch': 10.99}
{'eval_mit-movie': 0.5457413248713536, 'eval_mit-restaurant': 0.38120805364267196, 'eval_crossner_ai': 0.5478468899023805, 'eval_crossner_literature': 0.5826513911120288, 'eval_crossner_music': 0.7378427787433647, 'eval_crossner_politics': 0.572908366483826, 'eval_crossner_science': 0.6575342465255173, 'eval_average': 0.5751047216115918, 'eval_runtime': 168.3956, 'eval_samples_per_second': 8.314, 'eval_steps_per_second': 0.261, 'epoch': 11.0}
 92%|█████████▏| 9086/9900 [6:00:32<26:08,  1.9[INFO|trainer.py:3948] 2025-02-24 08:42:25,628 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086
[INFO|configuration_utils.py:423] 2025-02-24 08:42:25,630 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/config.json
[INFO|configuration_utils.py:909] 2025-02-24 08:42:25,630 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 08:42:27,237 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 08:42:27,238 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 08:42:27,239 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 08:42:27,240 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/spiece.model
[2025-02-24 08:42:27,283] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9080 is about to be saved!
[2025-02-24 08:42:27,289] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/mp_rank_00_model_states.pt
[2025-02-24 08:42:27,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/mp_rank_00_model_states.pt...
[2025-02-24 08:42:28,928] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/mp_rank_00_model_states.pt.
[2025-02-24 08:42:28,930] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 08:42:30,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 08:42:30,314] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9086/global_step9080/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 08:42:30,314] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9080 is ready now!
{'loss': 0.0265, 'grad_norm': 0.06679485738277435, 'learning_rate': 5e-05, 'epoch': 11.0}
{'loss': 0.0279, 'grad_norm': 0.06430775672197342, 'learning_rate': 5e-05, 'epoch': 11.02}
{'loss': 0.0261, 'grad_norm': 0.051751215010881424, 'learning_rate': 5e-05, 'epoch': 11.03}
{'loss': 0.0273, 'grad_norm': 0.056830521672964096, 'learning_rate': 5e-05, 'epoch': 11.04}
{'loss': 0.0267, 'grad_norm': 0.086794413626194, 'learning_rate': 5e-05, 'epoch': 11.05}
{'loss': 0.0277, 'grad_norm': 0.0654119923710823, 'learning_rate': 5e-05, 'epoch': 11.07}
{'loss': 0.0277, 'grad_norm': 0.0702187791466713, 'learning_rate': 5e-05, 'epoch': 11.08}
{'loss': 0.0268, 'grad_norm': 0.0479588657617569, 'learning_rate': 5e-05, 'epoch': 11.09}
{'loss': 0.0276, 'grad_norm': 0.05811053887009621, 'learning_rate': 5e-05, 'epoch': 11.1}
{'loss': 0.027, 'grad_norm': 0.0466933399438858, 'learning_rate': 5e-05, 'epoch': 11.11}
{'loss': 0.0273, 'grad_norm': 0.038522813469171524, 'learning_rate': 5e-05, 'epoch': 11.13}
{'loss': 0.0268, 'grad_norm': 0.05497270077466965, 'learning_rate': 5e-05, 'epoch': 11.14}
{'loss': 0.0268, 'grad_norm': 0.04191945120692253, 'learning_rate': 5e-05, 'epoch': 11.15}
{'loss': 0.0269, 'grad_norm': 0.05162179470062256, 'learning_rate': 5e-05, 'epoch': 11.16}
{'loss': 0.0284, 'grad_norm': 0.048655007034540176, 'learning_rate': 5e-05, 'epoch': 11.17}
{'loss': 0.0266, 'grad_norm': 0.07145821303129196, 'learning_rate': 5e-05, 'epoch': 11.19}
{'loss': 0.028, 'grad_norm': 0.05232417955994606, 'learning_rate': 5e-05, 'epoch': 11.2}
{'loss': 0.0274, 'grad_norm': 0.04854045435786247, 'learning_rate': 5e-05, 'epoch': 11.21}
{'loss': 0.027, 'grad_norm': 0.052684761583805084, 'learning_rate': 5e-05, 'epoch': 11.22}
{'loss': 0.0269, 'grad_norm': 0.04502826929092407, 'learning_rate': 5e-05, 'epoch': 11.24}
{'loss': 0.027, 'grad_norm': 0.042252182960510254, 'learning_rate': 5e-05, 'epoch': 11.25}
{'loss': 0.0271, 'grad_norm': 0.047412771731615067, 'learning_rate': 5e-05, 'epoch': 11.26}
{'loss': 0.0258, 'grad_norm': 0.07154317200183868, 'learning_rate': 5e-05, 'epoch': 11.27}
{'loss': 0.0282, 'grad_norm': 0.047423381358385086, 'learning_rate': 5e-05, 'epoch': 11.28}
{'loss': 0.0274, 'grad_norm': 0.056423306465148926, 'learning_rate': 5e-05, 'epoch': 11.3}
{'loss': 0.0266, 'grad_norm': 0.0567290335893631, 'learning_rate': 5e-05, 'epoch': 11.31}
{'loss': 0.0264, 'grad_norm': 0.04591021314263344, 'learning_rate': 5e-05, 'epoch': 11.32}
{'loss': 0.0272, 'grad_norm': 0.06360827386379242, 'learning_rate': 5e-05, 'epoch': 11.33}
{'loss': 0.027, 'grad_norm': 0.05219729244709015, 'learning_rate': 5e-05, 'epoch': 11.34}
{'loss': 0.0271, 'grad_norm': 0.05326950550079346, 'learning_rate': 5e-05, 'epoch': 11.36}
{'loss': 0.0282, 'grad_norm': 0.057746194303035736, 'learning_rate': 5e-05, 'epoch': 11.37}
{'loss': 0.0269, 'grad_norm': 0.03981640562415123, 'learning_rate': 5e-05, 'epoch': 11.38}
{'loss': 0.0277, 'grad_norm': 0.05255325511097908, 'learning_rate': 5e-05, 'epoch': 11.39}
{'loss': 0.0268, 'grad_norm': 0.057779520750045776, 'learning_rate': 5e-05, 'epoch': 11.4}
{'loss': 0.0273, 'grad_norm': 0.05461664870381355, 'learning_rate': 5e-05, 'epoch': 11.42}
{'loss': 0.027, 'grad_norm': 0.043569426983594894, 'learning_rate': 5e-05, 'epoch': 11.43}
{'loss': 0.0282, 'grad_norm': 0.11333983391523361, 'learning_rate': 5e-05, 'epoch': 11.44}
{'loss': 0.0274, 'grad_norm': 0.048881057649850845, 'learning_rate': 5e-05, 'epoch': 11.45}
{'loss': 0.0277, 'grad_norm': 0.07504715025424957, 'learning_rate': 5e-05, 'epoch': 11.47}
{'loss': 0.027, 'grad_norm': 0.06036854162812233, 'learning_rate': 5e-05, 'epoch': 11.48}
{'loss': 0.027, 'grad_norm': 0.03806626424193382, 'learning_rate': 5e-05, 'epoch': 11.49}
{'loss': 0.0268, 'grad_norm': 0.05462774634361267, 'learning_rate': 5e-05, 'epoch': 11.5}
{'loss': 0.0268, 'grad_norm': 0.04361744970083237, 'learning_rate': 5e-05, 'epoch': 11.51}
{'loss': 0.0274, 'grad_norm': 0.03996638208627701, 'learning_rate': 5e-05, 'epoch': 11.53}
{'loss': 0.0273, 'grad_norm': 0.04758962243795395, 'learning_rate': 5e-05, 'epoch': 11.54}
{'loss': 0.0278, 'grad_norm': 0.053586117923259735, 'learning_rate': 5e-05, 'epoch': 11.55}
{'loss': 0.0273, 'grad_norm': 0.0542464554309845, 'learning_rate': 5e-05, 'epoch': 11.56}
{'loss': 0.0267, 'grad_norm': 0.07889345288276672, 'learning_rate': 5e-05, 'epoch': 11.57}
{'loss': 0.0265, 'grad_norm': 0.04022034630179405, 'learning_rate': 5e-05, 'epoch': 11.59}
{'loss': 0.0273, 'grad_norm': 0.04941993206739426, 'learning_rate': 5e-05, 'epoch': 11.6}
{'loss': 0.0273, 'grad_norm': 0.04931218922138214, 'learning_rate': 5e-05, 'epoch': 11.61}
{'loss': 0.0288, 'grad_norm': 0.05874072387814522, 'learning_rate': 5e-05, 'epoch': 11.62}
{'loss': 0.028, 'grad_norm': 0.07603458315134048, 'learning_rate': 5e-05, 'epoch': 11.63}
{'loss': 0.0264, 'grad_norm': 0.04467570036649704, 'learning_rate': 5e-05, 'epoch': 11.65}
{'loss': 0.0277, 'grad_norm': 0.044660888612270355, 'learning_rate': 5e-05, 'epoch': 11.66}
{'loss': 0.0274, 'grad_norm': 0.05107070505619049, 'learning_rate': 5e-05, 'epoch': 11.67}
{'loss': 0.0269, 'grad_norm': 0.06049723923206329, 'learning_rate': 5e-05, 'epoch': 11.68}
{'loss': 0.0281, 'grad_norm': 0.05224834382534027, 'learning_rate': 5e-05, 'epoch': 11.7}
{'loss': 0.0271, 'grad_norm': 0.06710938364267349, 'learning_rate': 5e-05, 'epoch': 11.71}
{'loss': 0.0278, 'grad_norm': 0.038973160088062286, 'learning_rate': 5e-05, 'epoch': 11.72}
{'loss': 0.0271, 'grad_norm': 0.0509997233748436, 'learning_rate': 5e-05, 'epoch': 11.73}
{'loss': 0.0268, 'grad_norm': 0.07427551597356796, 'learning_rate': 5e-05, 'epoch': 11.74}
{'loss': 0.0268, 'grad_norm': 0.07728783041238785, 'learning_rate': 5e-05, 'epoch': 11.76}
{'loss': 0.0281, 'grad_norm': 0.04182935878634453, 'learning_rate': 5e-05, 'epoch': 11.77}
{'loss': 0.0272, 'grad_norm': 0.05221060290932655, 'learning_rate': 5e-05, 'epoch': 11.78}
{'loss': 0.0272, 'grad_norm': 0.04384114593267441, 'learning_rate': 5e-05, 'epoch': 11.79}
{'loss': 0.028, 'grad_norm': 0.046457644551992416, 'learning_rate': 5e-05, 'epoch': 11.8}
{'loss': 0.0275, 'grad_norm': 0.046483114361763, 'learning_rate': 5e-05, 'epoch': 11.82}
{'loss': 0.0271, 'grad_norm': 0.036909691989421844, 'learning_rate': 5e-05, 'epoch': 11.83}
{'loss': 0.0267, 'grad_norm': 0.05201311409473419, 'learning_rate': 5e-05, 'epoch': 11.84}
{'loss': 0.0272, 'grad_norm': 0.038678597658872604, 'learning_rate': 5e-05, 'epoch': 11.85}
{'loss': 0.0276, 'grad_norm': 0.05434257537126541, 'learning_rate': 5e-05, 'epoch': 11.86}
{'loss': 0.0269, 'grad_norm': 0.06901085376739502, 'learning_rate': 5e-05, 'epoch': 11.88}
{'loss': 0.0272, 'grad_norm': 0.061999596655368805, 'learning_rate': 5e-05, 'epoch': 11.89}
{'loss': 0.0274, 'grad_norm': 0.06864991039037704, 'learning_rate': 5e-05, 'epoch': 11.9}
{'loss': 0.0268, 'grad_norm': 0.04287511110305786, 'learning_rate': 5e-05, 'epoch': 11.91}
{'loss': 0.0262, 'grad_norm': 0.0670277401804924, 'learning_rate': 5e-05, 'epoch': 11.93}
{'loss': 0.028, 'grad_norm': 0.0439721941947937, 'learning_rate': 5e-05, 'epoch': 11.94}
{'loss': 0.0275, 'grad_norm': 0.05644761770963669, 'learning_rate': 5e-05, 'epoch': 11.95}
{'loss': 0.0261, 'grad_norm': 0.06848804652690887, 'learning_rate': 5e-05, 'epoch': 11.96}
{'loss': 0.0275, 'grad_norm': 0.04554004222154617, 'learning_rate': 5e-05, 'epoch': 11.97}
{'loss': 0.0271, 'grad_norm': 0.0520789660513401, 'learning_rate': 5e-05, 'epoch': 11.99}
{'eval_mit-movie': 0.5181674565063239, 'eval_mit-restaurant': 0.4005449590797753, 'eval_crossner_ai': 0.558734023078537, 'eval_crossner_literature': 0.5882352940676766, 'eval_crossner_music': 0.7221006564050897, 'eval_crossner_politics': 0.571201272821881, 'eval_crossner_science': 0.643877550970549, 'eval_average': 0.5718373161328332, 'eval_runtime': 167.3265, 'eval_samples_per_second': 8.367, 'eval_steps_per_second': 0.263, 'epoch': 11.99}
100%|██████████| 9900/9900 [6:32:52<00:00,  2.1[INFO|trainer.py:3948] 2025-02-24 09:14:45,413 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900
[INFO|configuration_utils.py:423] 2025-02-24 09:14:45,415 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/config.json
[INFO|configuration_utils.py:909] 2025-02-24 09:14:45,416 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 09:14:47,017 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 09:14:47,019 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 09:14:47,019 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 09:14:47,020 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/spiece.model
[2025-02-24 09:14:47,074] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9894 is about to be saved!
[2025-02-24 09:14:47,080] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/mp_rank_00_model_states.pt
[2025-02-24 09:14:47,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/mp_rank_00_model_states.pt...
[2025-02-24 09:14:48,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/mp_rank_00_model_states.pt.
[2025-02-24 09:14:48,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 09:14:50,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 09:14:50,043] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-9900/global_step9894/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-02-24 09:14:50,043] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9894 is ready now!
[INFO|trainer.py:2663] 2025-02-24 09:14:50,195 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2901] 2025-02-24 09:14:50,195 >> Loading best model from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434 (score: 0.5828563108636823).
[INFO|deepspeed.py:436] 2025-02-24 09:14:50,196 >> Attempting to resume from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434
[2025-02-24 09:14:50,196] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt...
[2025-02-24 09:14:50,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt.
[2025-02-24 09:14:50,805] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt...
[2025-02-24 09:14:51,251] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/mp_rank_00_model_states.pt.
[2025-02-24 09:14:51,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-02-24 09:14:51,772] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/ZSE-yuyang-BL-lirs-b1/checkpoint-7434/global_step7429/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-02-24 09:14:51,772] [INFO] [engine.py:3112:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0
[2025-02-24 09:14:51,821] [INFO] [engine.py:3062:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0
{'train_runtime': 23579.31, 'train_samples_per_second': 53.772, 'train_steps_per_second': 0.42, 'train_loss': 0.03913711475904542, 'epoch': 11.99}
100%|██████████| 9900/9900 [6:32:59<00:00,  2.38s/it]
[INFO|trainer.py:3948] 2025-02-24 09:14:52,758 >> Saving model checkpoint to output-lfs/ZSE-yuyang-BL-lirs-b1
[INFO|configuration_utils.py:423] 2025-02-24 09:14:52,760 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/config.json
[INFO|configuration_utils.py:909] 2025-02-24 09:14:52,760 >> Configuration saved in output-lfs/ZSE-yuyang-BL-lirs-b1/generation_config.json
[INFO|modeling_utils.py:3077] 2025-02-24 09:14:54,371 >> Model weights saved in output-lfs/ZSE-yuyang-BL-lirs-b1/model.safetensors
[INFO|tokenization_utils_base.py:2500] 2025-02-24 09:14:54,372 >> tokenizer config file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-24 09:14:54,372 >> Special tokens file saved in output-lfs/ZSE-yuyang-BL-lirs-b1/special_tokens_map.json
[INFO|tokenization_t5_fast.py:176] 2025-02-24 09:14:54,374 >> Copy vocab file to output-lfs/ZSE-yuyang-BL-lirs-b1/spiece.model
***** train metrics *****
  epoch                    =     11.9861
  total_flos               = 575858882GF
  train_loss               =      0.0391
  train_runtime            =  6:32:59.31
  train_samples            =      105659
  train_samples_per_second =      53.772
  train_steps_per_second   =        0.42
02/24/2025 09:14:54 - INFO - __main__ - Metrics {'train_runtime': 23579.31, 'train_samples_per_second': 53.772, 'train_steps_per_second': 0.42, 'total_flos': 6.18323766618882e+17, 'train_loss': 0.03913711475904542, 'epoch': 11.986069049061175, 'train_samples': 105659}
02/24/2025 09:14:54 - INFO - __main__ - *** Evaluate ***
100%|██████████| 44/44 [02:46<00:00,  3.79s/it]
***** eval metrics *****
  epoch                    =    11.9861
  eval_average             =     0.5829
  eval_crossner_ai         =     0.5465
  eval_crossner_literature =     0.5932
  eval_crossner_music      =     0.7553
  eval_crossner_politics   =     0.5756
  eval_crossner_science    =     0.6981
  eval_mit-movie           =     0.5224
  eval_mit-restaurant      =     0.3887
  eval_runtime             = 0:02:47.71
  eval_samples             =       1400
  eval_samples_per_second  =      8.348
  eval_steps_per_second    =      0.262
02/24/2025 09:17:42 - INFO - __main__ - *** Predict ***
100%|██████████| 203/203 [08:47<00:00,  2.60s/it]
***** predict metrics *****
  predict_average             =     0.5987
  predict_crossner_ai         =     0.5827
  predict_crossner_literature =     0.5845
  predict_crossner_music      =     0.7375
  predict_crossner_politics   =     0.6318
  predict_crossner_science    =     0.6789
  predict_mit-movie           =     0.5697
  predict_mit-restaurant      =     0.4057
  predict_runtime             = 0:08:48.78
  predict_samples             =       6470
  predict_samples_per_second  =     12.235
  predict_steps_per_second    =      0.384
[rank0]:[W224 09:26:31.462491494 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-02-24 09:26:33,644] [INFO] [launch.py:351:main] Process 2938320 exits successfully.
[2025-02-24 09:26:33,644] [INFO] [launch.py:351:main] Process 2938321 exits successfully.
[2025-02-24 09:26:33,645] [INFO] [launch.py:351:main] Process 2938319 exits successfully.
[2025-02-24 09:26:33,645] [INFO] [launch.py:351:main] Process 2938322 exits successfully.