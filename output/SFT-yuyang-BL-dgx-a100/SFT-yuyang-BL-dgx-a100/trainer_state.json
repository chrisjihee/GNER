{
  "best_metric": 0.7613216436784657,
  "best_model_checkpoint": "output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309",
  "epoch": 11.989497206703911,
  "eval_steps": 500,
  "global_step": 13416,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008938547486033519,
      "grad_norm": 1.3608158826828003,
      "learning_rate": 5e-05,
      "loss": 1.0643,
      "step": 10
    },
    {
      "epoch": 0.017877094972067038,
      "grad_norm": 0.45210137963294983,
      "learning_rate": 5e-05,
      "loss": 0.3152,
      "step": 20
    },
    {
      "epoch": 0.026815642458100558,
      "grad_norm": 0.34399768710136414,
      "learning_rate": 5e-05,
      "loss": 0.2056,
      "step": 30
    },
    {
      "epoch": 0.035754189944134075,
      "grad_norm": 0.32977738976478577,
      "learning_rate": 5e-05,
      "loss": 0.1636,
      "step": 40
    },
    {
      "epoch": 0.0446927374301676,
      "grad_norm": 0.23794935643672943,
      "learning_rate": 5e-05,
      "loss": 0.1391,
      "step": 50
    },
    {
      "epoch": 0.053631284916201116,
      "grad_norm": 0.21703729033470154,
      "learning_rate": 5e-05,
      "loss": 0.1296,
      "step": 60
    },
    {
      "epoch": 0.06256983240223464,
      "grad_norm": 0.21630258858203888,
      "learning_rate": 5e-05,
      "loss": 0.1149,
      "step": 70
    },
    {
      "epoch": 0.07150837988826815,
      "grad_norm": 0.23872493207454681,
      "learning_rate": 5e-05,
      "loss": 0.1107,
      "step": 80
    },
    {
      "epoch": 0.08044692737430167,
      "grad_norm": 0.1507975161075592,
      "learning_rate": 5e-05,
      "loss": 0.0983,
      "step": 90
    },
    {
      "epoch": 0.0893854748603352,
      "grad_norm": 0.19742536544799805,
      "learning_rate": 5e-05,
      "loss": 0.1008,
      "step": 100
    },
    {
      "epoch": 0.09832402234636871,
      "grad_norm": 0.20530596375465393,
      "learning_rate": 5e-05,
      "loss": 0.1,
      "step": 110
    },
    {
      "epoch": 0.10726256983240223,
      "grad_norm": 0.20546752214431763,
      "learning_rate": 5e-05,
      "loss": 0.0881,
      "step": 120
    },
    {
      "epoch": 0.11620111731843576,
      "grad_norm": 0.28300124406814575,
      "learning_rate": 5e-05,
      "loss": 0.087,
      "step": 130
    },
    {
      "epoch": 0.12513966480446928,
      "grad_norm": 0.1980736255645752,
      "learning_rate": 5e-05,
      "loss": 0.0835,
      "step": 140
    },
    {
      "epoch": 0.1340782122905028,
      "grad_norm": 0.1548335999250412,
      "learning_rate": 5e-05,
      "loss": 0.0788,
      "step": 150
    },
    {
      "epoch": 0.1430167597765363,
      "grad_norm": 0.16939006745815277,
      "learning_rate": 5e-05,
      "loss": 0.0806,
      "step": 160
    },
    {
      "epoch": 0.15195530726256984,
      "grad_norm": 0.17983846366405487,
      "learning_rate": 5e-05,
      "loss": 0.0762,
      "step": 170
    },
    {
      "epoch": 0.16089385474860335,
      "grad_norm": 0.17687354981899261,
      "learning_rate": 5e-05,
      "loss": 0.0733,
      "step": 180
    },
    {
      "epoch": 0.16983240223463686,
      "grad_norm": 0.33722972869873047,
      "learning_rate": 5e-05,
      "loss": 0.0731,
      "step": 190
    },
    {
      "epoch": 0.1787709497206704,
      "grad_norm": 0.1837114840745926,
      "learning_rate": 5e-05,
      "loss": 0.0705,
      "step": 200
    },
    {
      "epoch": 0.1877094972067039,
      "grad_norm": 0.14230914413928986,
      "learning_rate": 5e-05,
      "loss": 0.0719,
      "step": 210
    },
    {
      "epoch": 0.19664804469273742,
      "grad_norm": 0.1280413269996643,
      "learning_rate": 5e-05,
      "loss": 0.0669,
      "step": 220
    },
    {
      "epoch": 0.20558659217877095,
      "grad_norm": 0.19885091483592987,
      "learning_rate": 5e-05,
      "loss": 0.0644,
      "step": 230
    },
    {
      "epoch": 0.21452513966480447,
      "grad_norm": 0.1613454520702362,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 240
    },
    {
      "epoch": 0.22346368715083798,
      "grad_norm": 0.14053606986999512,
      "learning_rate": 5e-05,
      "loss": 0.0672,
      "step": 250
    },
    {
      "epoch": 0.2324022346368715,
      "grad_norm": 0.1670684814453125,
      "learning_rate": 5e-05,
      "loss": 0.0636,
      "step": 260
    },
    {
      "epoch": 0.24134078212290502,
      "grad_norm": 0.10619278997182846,
      "learning_rate": 5e-05,
      "loss": 0.0626,
      "step": 270
    },
    {
      "epoch": 0.25027932960893856,
      "grad_norm": 0.23772716522216797,
      "learning_rate": 5e-05,
      "loss": 0.0598,
      "step": 280
    },
    {
      "epoch": 0.25921787709497207,
      "grad_norm": 0.1508154571056366,
      "learning_rate": 5e-05,
      "loss": 0.0637,
      "step": 290
    },
    {
      "epoch": 0.2681564245810056,
      "grad_norm": 0.14329862594604492,
      "learning_rate": 5e-05,
      "loss": 0.0587,
      "step": 300
    },
    {
      "epoch": 0.2770949720670391,
      "grad_norm": 0.15412208437919617,
      "learning_rate": 5e-05,
      "loss": 0.0598,
      "step": 310
    },
    {
      "epoch": 0.2860335195530726,
      "grad_norm": 0.14805443584918976,
      "learning_rate": 5e-05,
      "loss": 0.0587,
      "step": 320
    },
    {
      "epoch": 0.29497206703910617,
      "grad_norm": 0.11470546573400497,
      "learning_rate": 5e-05,
      "loss": 0.056,
      "step": 330
    },
    {
      "epoch": 0.3039106145251397,
      "grad_norm": 0.12347155064344406,
      "learning_rate": 5e-05,
      "loss": 0.0569,
      "step": 340
    },
    {
      "epoch": 0.3128491620111732,
      "grad_norm": 0.12979483604431152,
      "learning_rate": 5e-05,
      "loss": 0.0552,
      "step": 350
    },
    {
      "epoch": 0.3217877094972067,
      "grad_norm": 0.14177380502223969,
      "learning_rate": 5e-05,
      "loss": 0.054,
      "step": 360
    },
    {
      "epoch": 0.3307262569832402,
      "grad_norm": 0.13859522342681885,
      "learning_rate": 5e-05,
      "loss": 0.0554,
      "step": 370
    },
    {
      "epoch": 0.3396648044692737,
      "grad_norm": 0.13452379405498505,
      "learning_rate": 5e-05,
      "loss": 0.0519,
      "step": 380
    },
    {
      "epoch": 0.3486033519553073,
      "grad_norm": 0.1670239120721817,
      "learning_rate": 5e-05,
      "loss": 0.0536,
      "step": 390
    },
    {
      "epoch": 0.3575418994413408,
      "grad_norm": 0.13676439225673676,
      "learning_rate": 5e-05,
      "loss": 0.0547,
      "step": 400
    },
    {
      "epoch": 0.3664804469273743,
      "grad_norm": 0.16163888573646545,
      "learning_rate": 5e-05,
      "loss": 0.0524,
      "step": 410
    },
    {
      "epoch": 0.3754189944134078,
      "grad_norm": 0.11922567337751389,
      "learning_rate": 5e-05,
      "loss": 0.0507,
      "step": 420
    },
    {
      "epoch": 0.3843575418994413,
      "grad_norm": 0.13087250292301178,
      "learning_rate": 5e-05,
      "loss": 0.0522,
      "step": 430
    },
    {
      "epoch": 0.39329608938547483,
      "grad_norm": 0.10397513210773468,
      "learning_rate": 5e-05,
      "loss": 0.0499,
      "step": 440
    },
    {
      "epoch": 0.4022346368715084,
      "grad_norm": 0.11999259889125824,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 450
    },
    {
      "epoch": 0.4111731843575419,
      "grad_norm": 0.13263726234436035,
      "learning_rate": 5e-05,
      "loss": 0.0496,
      "step": 460
    },
    {
      "epoch": 0.4201117318435754,
      "grad_norm": 0.11318276822566986,
      "learning_rate": 5e-05,
      "loss": 0.0491,
      "step": 470
    },
    {
      "epoch": 0.42905027932960893,
      "grad_norm": 0.1488083004951477,
      "learning_rate": 5e-05,
      "loss": 0.0494,
      "step": 480
    },
    {
      "epoch": 0.43798882681564244,
      "grad_norm": 0.12324687093496323,
      "learning_rate": 5e-05,
      "loss": 0.0476,
      "step": 490
    },
    {
      "epoch": 0.44692737430167595,
      "grad_norm": 0.18245026469230652,
      "learning_rate": 5e-05,
      "loss": 0.0491,
      "step": 500
    },
    {
      "epoch": 0.4558659217877095,
      "grad_norm": 0.11413589119911194,
      "learning_rate": 5e-05,
      "loss": 0.047,
      "step": 510
    },
    {
      "epoch": 0.464804469273743,
      "grad_norm": 0.11385564506053925,
      "learning_rate": 5e-05,
      "loss": 0.0473,
      "step": 520
    },
    {
      "epoch": 0.47374301675977654,
      "grad_norm": 0.13580097258090973,
      "learning_rate": 5e-05,
      "loss": 0.0471,
      "step": 530
    },
    {
      "epoch": 0.48268156424581005,
      "grad_norm": 0.11891704052686691,
      "learning_rate": 5e-05,
      "loss": 0.0426,
      "step": 540
    },
    {
      "epoch": 0.49162011173184356,
      "grad_norm": 0.14956477284431458,
      "learning_rate": 5e-05,
      "loss": 0.0458,
      "step": 550
    },
    {
      "epoch": 0.5005586592178771,
      "grad_norm": 0.09692291170358658,
      "learning_rate": 5e-05,
      "loss": 0.0447,
      "step": 560
    },
    {
      "epoch": 0.5094972067039106,
      "grad_norm": 0.12099242955446243,
      "learning_rate": 5e-05,
      "loss": 0.0445,
      "step": 570
    },
    {
      "epoch": 0.5184357541899441,
      "grad_norm": 0.11015671491622925,
      "learning_rate": 5e-05,
      "loss": 0.0427,
      "step": 580
    },
    {
      "epoch": 0.5273743016759777,
      "grad_norm": 0.1056525707244873,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 590
    },
    {
      "epoch": 0.5363128491620112,
      "grad_norm": 0.09845397621393204,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 600
    },
    {
      "epoch": 0.5452513966480447,
      "grad_norm": 0.14905968308448792,
      "learning_rate": 5e-05,
      "loss": 0.0444,
      "step": 610
    },
    {
      "epoch": 0.5541899441340782,
      "grad_norm": 0.12235887348651886,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 620
    },
    {
      "epoch": 0.5631284916201117,
      "grad_norm": 0.10665979236364365,
      "learning_rate": 5e-05,
      "loss": 0.0433,
      "step": 630
    },
    {
      "epoch": 0.5720670391061452,
      "grad_norm": 0.11393610388040543,
      "learning_rate": 5e-05,
      "loss": 0.043,
      "step": 640
    },
    {
      "epoch": 0.5810055865921788,
      "grad_norm": 0.1350659728050232,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 650
    },
    {
      "epoch": 0.5899441340782123,
      "grad_norm": 0.13946954905986786,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 660
    },
    {
      "epoch": 0.5988826815642458,
      "grad_norm": 0.10625141859054565,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 670
    },
    {
      "epoch": 0.6078212290502794,
      "grad_norm": 0.12473705410957336,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 680
    },
    {
      "epoch": 0.6167597765363129,
      "grad_norm": 0.10658219456672668,
      "learning_rate": 5e-05,
      "loss": 0.0403,
      "step": 690
    },
    {
      "epoch": 0.6256983240223464,
      "grad_norm": 0.10108156502246857,
      "learning_rate": 5e-05,
      "loss": 0.0396,
      "step": 700
    },
    {
      "epoch": 0.6346368715083799,
      "grad_norm": 0.10756638646125793,
      "learning_rate": 5e-05,
      "loss": 0.0388,
      "step": 710
    },
    {
      "epoch": 0.6435754189944134,
      "grad_norm": 0.09791532903909683,
      "learning_rate": 5e-05,
      "loss": 0.0412,
      "step": 720
    },
    {
      "epoch": 0.6525139664804469,
      "grad_norm": 0.0995614305138588,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 730
    },
    {
      "epoch": 0.6614525139664804,
      "grad_norm": 0.15138044953346252,
      "learning_rate": 5e-05,
      "loss": 0.04,
      "step": 740
    },
    {
      "epoch": 0.6703910614525139,
      "grad_norm": 0.09500790387392044,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 750
    },
    {
      "epoch": 0.6793296089385474,
      "grad_norm": 0.12435303628444672,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 760
    },
    {
      "epoch": 0.6882681564245811,
      "grad_norm": 0.12718519568443298,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 770
    },
    {
      "epoch": 0.6972067039106146,
      "grad_norm": 0.1373368501663208,
      "learning_rate": 5e-05,
      "loss": 0.0405,
      "step": 780
    },
    {
      "epoch": 0.7061452513966481,
      "grad_norm": 0.08546178042888641,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 790
    },
    {
      "epoch": 0.7150837988826816,
      "grad_norm": 0.11154385656118393,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 800
    },
    {
      "epoch": 0.7240223463687151,
      "grad_norm": 0.11031264811754227,
      "learning_rate": 5e-05,
      "loss": 0.0385,
      "step": 810
    },
    {
      "epoch": 0.7329608938547486,
      "grad_norm": 0.10928153991699219,
      "learning_rate": 5e-05,
      "loss": 0.0369,
      "step": 820
    },
    {
      "epoch": 0.7418994413407821,
      "grad_norm": 0.10808121412992477,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 830
    },
    {
      "epoch": 0.7508379888268156,
      "grad_norm": 0.1271839141845703,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 840
    },
    {
      "epoch": 0.7597765363128491,
      "grad_norm": 0.10396832972764969,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 850
    },
    {
      "epoch": 0.7687150837988826,
      "grad_norm": 0.12680551409721375,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 860
    },
    {
      "epoch": 0.7776536312849162,
      "grad_norm": 0.20622310042381287,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 870
    },
    {
      "epoch": 0.7865921787709497,
      "grad_norm": 0.11346101760864258,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 880
    },
    {
      "epoch": 0.7955307262569833,
      "grad_norm": 0.1240425780415535,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 890
    },
    {
      "epoch": 0.8044692737430168,
      "grad_norm": 0.11950046569108963,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 900
    },
    {
      "epoch": 0.8134078212290503,
      "grad_norm": 0.08711783587932587,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 910
    },
    {
      "epoch": 0.8223463687150838,
      "grad_norm": 0.17376133799552917,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 920
    },
    {
      "epoch": 0.8312849162011173,
      "grad_norm": 0.1176934540271759,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 930
    },
    {
      "epoch": 0.8402234636871508,
      "grad_norm": 0.10209446400403976,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 940
    },
    {
      "epoch": 0.8491620111731844,
      "grad_norm": 0.0930309146642685,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 950
    },
    {
      "epoch": 0.8581005586592179,
      "grad_norm": 0.13660287857055664,
      "learning_rate": 5e-05,
      "loss": 0.0364,
      "step": 960
    },
    {
      "epoch": 0.8670391061452514,
      "grad_norm": 0.09471837431192398,
      "learning_rate": 5e-05,
      "loss": 0.0364,
      "step": 970
    },
    {
      "epoch": 0.8759776536312849,
      "grad_norm": 0.12352073192596436,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 980
    },
    {
      "epoch": 0.8849162011173184,
      "grad_norm": 0.10920679569244385,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 990
    },
    {
      "epoch": 0.8938547486033519,
      "grad_norm": 0.09083276987075806,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 1000
    },
    {
      "epoch": 0.9027932960893855,
      "grad_norm": 0.13556431233882904,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1010
    },
    {
      "epoch": 0.911731843575419,
      "grad_norm": 0.15446995198726654,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1020
    },
    {
      "epoch": 0.9206703910614525,
      "grad_norm": 0.10300192981958389,
      "learning_rate": 5e-05,
      "loss": 0.0335,
      "step": 1030
    },
    {
      "epoch": 0.929608938547486,
      "grad_norm": 0.0983043983578682,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 1040
    },
    {
      "epoch": 0.9385474860335196,
      "grad_norm": 0.09812165051698685,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 1050
    },
    {
      "epoch": 0.9474860335195531,
      "grad_norm": 0.09566853195428848,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1060
    },
    {
      "epoch": 0.9564245810055866,
      "grad_norm": 0.0970584824681282,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1070
    },
    {
      "epoch": 0.9653631284916201,
      "grad_norm": 0.07697637379169464,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 1080
    },
    {
      "epoch": 0.9743016759776536,
      "grad_norm": 0.08765705674886703,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1090
    },
    {
      "epoch": 0.9832402234636871,
      "grad_norm": 0.11163213849067688,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 1100
    },
    {
      "epoch": 0.9921787709497206,
      "grad_norm": 0.0979473739862442,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 1110
    },
    {
      "epoch": 1.0,
      "eval_AnatEM": 0.291262135873843,
      "eval_FabNER": 0.005769230742271265,
      "eval_FindVehicle": 0.7201834861884989,
      "eval_HarveyNER": 0.3048780487303019,
      "eval_MultiNERD": 0.8306010928459203,
      "eval_Ontonotes": 0.8303655107277456,
      "eval_TweetNER7": 0.5903151421482892,
      "eval_WikiANN-en": 0.7536231883555266,
      "eval_WikiNeural": 0.8273894436016949,
      "eval_average": 0.6307886421142708,
      "eval_bc2gm": 0.5070422534711269,
      "eval_bc4chemd": 0.3942652329356508,
      "eval_bc5cdr": 0.7386091126600072,
      "eval_broad_twitter_corpus": 0.6113207546675736,
      "eval_conll2003": 0.8908857509125442,
      "eval_conllpp": 0.8934531450075371,
      "eval_mit-movie": 0.7031019201861475,
      "eval_mit-restaurant": 0.7191011235453602,
      "eval_ncbi": 0.7420289854568368,
      "eval_runtime": 406.4851,
      "eval_samples_per_second": 8.856,
      "eval_steps_per_second": 0.278,
      "step": 1119
    },
    {
      "epoch": 1.0008938547486033,
      "grad_norm": 0.1508300006389618,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1120
    },
    {
      "epoch": 1.0098324022346368,
      "grad_norm": 0.08522915095090866,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 1130
    },
    {
      "epoch": 1.0187709497206703,
      "grad_norm": 0.09854909777641296,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 1140
    },
    {
      "epoch": 1.027709497206704,
      "grad_norm": 0.09986922889947891,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 1150
    },
    {
      "epoch": 1.0366480446927375,
      "grad_norm": 0.12007535994052887,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1160
    },
    {
      "epoch": 1.045586592178771,
      "grad_norm": 0.09712943434715271,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1170
    },
    {
      "epoch": 1.0545251396648045,
      "grad_norm": 0.10448198765516281,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 1180
    },
    {
      "epoch": 1.063463687150838,
      "grad_norm": 0.09175737202167511,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 1190
    },
    {
      "epoch": 1.0724022346368716,
      "grad_norm": 0.1079220175743103,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1200
    },
    {
      "epoch": 1.081340782122905,
      "grad_norm": 0.09090516716241837,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 1210
    },
    {
      "epoch": 1.0902793296089386,
      "grad_norm": 0.08954821527004242,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 1220
    },
    {
      "epoch": 1.099217877094972,
      "grad_norm": 0.11446047574281693,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 1230
    },
    {
      "epoch": 1.1081564245810056,
      "grad_norm": 0.09049084037542343,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 1240
    },
    {
      "epoch": 1.1170949720670391,
      "grad_norm": 0.1349681168794632,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 1250
    },
    {
      "epoch": 1.1260335195530726,
      "grad_norm": 0.09404056519269943,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 1260
    },
    {
      "epoch": 1.1349720670391061,
      "grad_norm": 0.10767237097024918,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 1270
    },
    {
      "epoch": 1.1439106145251396,
      "grad_norm": 0.1136915311217308,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1280
    },
    {
      "epoch": 1.1528491620111732,
      "grad_norm": 0.13066191971302032,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 1290
    },
    {
      "epoch": 1.1617877094972067,
      "grad_norm": 0.09551674127578735,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 1300
    },
    {
      "epoch": 1.1707262569832402,
      "grad_norm": 0.11234297603368759,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 1310
    },
    {
      "epoch": 1.1796648044692737,
      "grad_norm": 0.11956655979156494,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 1320
    },
    {
      "epoch": 1.1886033519553072,
      "grad_norm": 0.14179959893226624,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1330
    },
    {
      "epoch": 1.1975418994413407,
      "grad_norm": 0.09096759557723999,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 1340
    },
    {
      "epoch": 1.2064804469273742,
      "grad_norm": 0.13477696478366852,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 1350
    },
    {
      "epoch": 1.2154189944134077,
      "grad_norm": 0.09984178841114044,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 1360
    },
    {
      "epoch": 1.2243575418994412,
      "grad_norm": 0.10375247150659561,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 1370
    },
    {
      "epoch": 1.2332960893854747,
      "grad_norm": 0.09686172008514404,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 1380
    },
    {
      "epoch": 1.2422346368715083,
      "grad_norm": 0.0706484392285347,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 1390
    },
    {
      "epoch": 1.2511731843575418,
      "grad_norm": 0.1369088590145111,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 1400
    },
    {
      "epoch": 1.2601117318435755,
      "grad_norm": 0.15020212531089783,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 1410
    },
    {
      "epoch": 1.269050279329609,
      "grad_norm": 0.1457827240228653,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 1420
    },
    {
      "epoch": 1.2779888268156425,
      "grad_norm": 0.11641058325767517,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 1430
    },
    {
      "epoch": 1.286927374301676,
      "grad_norm": 0.10550510138273239,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 1440
    },
    {
      "epoch": 1.2958659217877095,
      "grad_norm": 0.13687492907047272,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 1450
    },
    {
      "epoch": 1.304804469273743,
      "grad_norm": 0.11215363442897797,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 1460
    },
    {
      "epoch": 1.3137430167597766,
      "grad_norm": 0.09749653190374374,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 1470
    },
    {
      "epoch": 1.32268156424581,
      "grad_norm": 0.07411917299032211,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 1480
    },
    {
      "epoch": 1.3316201117318436,
      "grad_norm": 0.10616487264633179,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 1490
    },
    {
      "epoch": 1.340558659217877,
      "grad_norm": 0.08675713837146759,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 1500
    },
    {
      "epoch": 1.3494972067039106,
      "grad_norm": 0.11582214385271072,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 1510
    },
    {
      "epoch": 1.3584357541899441,
      "grad_norm": 0.08086715638637543,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 1520
    },
    {
      "epoch": 1.3673743016759776,
      "grad_norm": 0.08152089267969131,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 1530
    },
    {
      "epoch": 1.3763128491620111,
      "grad_norm": 0.08737868815660477,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 1540
    },
    {
      "epoch": 1.3852513966480446,
      "grad_norm": 0.09282663464546204,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 1550
    },
    {
      "epoch": 1.3941899441340782,
      "grad_norm": 0.08959618210792542,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 1560
    },
    {
      "epoch": 1.4031284916201117,
      "grad_norm": 0.12422776967287064,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 1570
    },
    {
      "epoch": 1.4120670391061452,
      "grad_norm": 0.06420591473579407,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 1580
    },
    {
      "epoch": 1.421005586592179,
      "grad_norm": 0.0820733904838562,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 1590
    },
    {
      "epoch": 1.4299441340782124,
      "grad_norm": 0.08058261126279831,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 1600
    },
    {
      "epoch": 1.438882681564246,
      "grad_norm": 0.0982515960931778,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 1610
    },
    {
      "epoch": 1.4478212290502794,
      "grad_norm": 0.09529227763414383,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 1620
    },
    {
      "epoch": 1.456759776536313,
      "grad_norm": 0.08704535663127899,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 1630
    },
    {
      "epoch": 1.4656983240223465,
      "grad_norm": 0.11137789487838745,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 1640
    },
    {
      "epoch": 1.47463687150838,
      "grad_norm": 0.0889832004904747,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 1650
    },
    {
      "epoch": 1.4835754189944135,
      "grad_norm": 0.1158461943268776,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 1660
    },
    {
      "epoch": 1.492513966480447,
      "grad_norm": 0.06763870269060135,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 1670
    },
    {
      "epoch": 1.5014525139664805,
      "grad_norm": 0.16856549680233002,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 1680
    },
    {
      "epoch": 1.510391061452514,
      "grad_norm": 0.0754920169711113,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 1690
    },
    {
      "epoch": 1.5193296089385475,
      "grad_norm": 0.12911169230937958,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 1700
    },
    {
      "epoch": 1.528268156424581,
      "grad_norm": 0.06482373923063278,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 1710
    },
    {
      "epoch": 1.5372067039106145,
      "grad_norm": 0.08769845217466354,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 1720
    },
    {
      "epoch": 1.546145251396648,
      "grad_norm": 0.06961068511009216,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 1730
    },
    {
      "epoch": 1.5550837988826816,
      "grad_norm": 0.1080135628581047,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 1740
    },
    {
      "epoch": 1.564022346368715,
      "grad_norm": 0.08524374663829803,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 1750
    },
    {
      "epoch": 1.5729608938547486,
      "grad_norm": 0.0834922045469284,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 1760
    },
    {
      "epoch": 1.581899441340782,
      "grad_norm": 0.11286009103059769,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 1770
    },
    {
      "epoch": 1.5908379888268156,
      "grad_norm": 0.08881642669439316,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 1780
    },
    {
      "epoch": 1.599776536312849,
      "grad_norm": 0.0863368958234787,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 1790
    },
    {
      "epoch": 1.6087150837988826,
      "grad_norm": 0.07552596181631088,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 1800
    },
    {
      "epoch": 1.6176536312849161,
      "grad_norm": 0.07613509148359299,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 1810
    },
    {
      "epoch": 1.6265921787709496,
      "grad_norm": 0.06286927312612534,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 1820
    },
    {
      "epoch": 1.6355307262569831,
      "grad_norm": 0.07220866531133652,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 1830
    },
    {
      "epoch": 1.6444692737430167,
      "grad_norm": 0.06455791741609573,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 1840
    },
    {
      "epoch": 1.6534078212290502,
      "grad_norm": 0.07678110152482986,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 1850
    },
    {
      "epoch": 1.6623463687150837,
      "grad_norm": 0.08650211244821548,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 1860
    },
    {
      "epoch": 1.6712849162011172,
      "grad_norm": 0.10006625205278397,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 1870
    },
    {
      "epoch": 1.6802234636871507,
      "grad_norm": 0.09318188577890396,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 1880
    },
    {
      "epoch": 1.6891620111731842,
      "grad_norm": 0.08340974152088165,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 1890
    },
    {
      "epoch": 1.6981005586592177,
      "grad_norm": 0.0885632336139679,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 1900
    },
    {
      "epoch": 1.7070391061452515,
      "grad_norm": 0.07894979417324066,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 1910
    },
    {
      "epoch": 1.715977653631285,
      "grad_norm": 0.06937915831804276,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 1920
    },
    {
      "epoch": 1.7249162011173185,
      "grad_norm": 0.07017017155885696,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 1930
    },
    {
      "epoch": 1.733854748603352,
      "grad_norm": 0.07401300966739655,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 1940
    },
    {
      "epoch": 1.7427932960893855,
      "grad_norm": 0.06589407473802567,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 1950
    },
    {
      "epoch": 1.751731843575419,
      "grad_norm": 0.10907035320997238,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 1960
    },
    {
      "epoch": 1.7606703910614525,
      "grad_norm": 0.0623776838183403,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 1970
    },
    {
      "epoch": 1.769608938547486,
      "grad_norm": 0.10180862993001938,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 1980
    },
    {
      "epoch": 1.7785474860335195,
      "grad_norm": 0.08448014408349991,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 1990
    },
    {
      "epoch": 1.787486033519553,
      "grad_norm": 0.07123516499996185,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 2000
    },
    {
      "epoch": 1.7964245810055866,
      "grad_norm": 0.0730782002210617,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 2010
    },
    {
      "epoch": 1.80536312849162,
      "grad_norm": 0.08645619451999664,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 2020
    },
    {
      "epoch": 1.8143016759776538,
      "grad_norm": 0.09724660962820053,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 2030
    },
    {
      "epoch": 1.8232402234636873,
      "grad_norm": 0.069844089448452,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 2040
    },
    {
      "epoch": 1.8321787709497208,
      "grad_norm": 0.07002456486225128,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 2050
    },
    {
      "epoch": 1.8411173184357543,
      "grad_norm": 0.08539856970310211,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 2060
    },
    {
      "epoch": 1.8500558659217878,
      "grad_norm": 0.0809856429696083,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 2070
    },
    {
      "epoch": 1.8589944134078213,
      "grad_norm": 0.0948903039097786,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 2080
    },
    {
      "epoch": 1.8679329608938549,
      "grad_norm": 0.10183800756931305,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 2090
    },
    {
      "epoch": 1.8768715083798884,
      "grad_norm": 0.06816703081130981,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 2100
    },
    {
      "epoch": 1.8858100558659219,
      "grad_norm": 0.07817968726158142,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 2110
    },
    {
      "epoch": 1.8947486033519554,
      "grad_norm": 0.1420930027961731,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2120
    },
    {
      "epoch": 1.903687150837989,
      "grad_norm": 0.09106304496526718,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 2130
    },
    {
      "epoch": 1.9126256983240224,
      "grad_norm": 0.12210578471422195,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 2140
    },
    {
      "epoch": 1.921564245810056,
      "grad_norm": 0.07335058599710464,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 2150
    },
    {
      "epoch": 1.9305027932960894,
      "grad_norm": 0.09029153734445572,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 2160
    },
    {
      "epoch": 1.939441340782123,
      "grad_norm": 0.06431115418672562,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 2170
    },
    {
      "epoch": 1.9483798882681564,
      "grad_norm": 0.08800571411848068,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 2180
    },
    {
      "epoch": 1.95731843575419,
      "grad_norm": 0.1121697723865509,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 2190
    },
    {
      "epoch": 1.9662569832402235,
      "grad_norm": 0.10417275130748749,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 2200
    },
    {
      "epoch": 1.975195530726257,
      "grad_norm": 0.09014527499675751,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 2210
    },
    {
      "epoch": 1.9841340782122905,
      "grad_norm": 0.09343826770782471,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 2220
    },
    {
      "epoch": 1.993072625698324,
      "grad_norm": 0.1620931625366211,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 2230
    },
    {
      "epoch": 2.0,
      "eval_AnatEM": 0.4848484847989797,
      "eval_FabNER": 0.015693112430939585,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.5149700598297107,
      "eval_MultiNERD": 0.8621908126705434,
      "eval_Ontonotes": 0.8682464454474692,
      "eval_TweetNER7": 0.6020710058670707,
      "eval_WikiANN-en": 0.7700729926504489,
      "eval_WikiNeural": 0.8707865168036896,
      "eval_average": 0.69378086125244,
      "eval_bc2gm": 0.6149068322496084,
      "eval_bc4chemd": 0.5570291776689205,
      "eval_bc5cdr": 0.761104441726871,
      "eval_broad_twitter_corpus": 0.6923076922574608,
      "eval_conll2003": 0.9230769230266868,
      "eval_conllpp": 0.9242618741474522,
      "eval_mit-movie": 0.7286135692713347,
      "eval_mit-restaurant": 0.7723669308671576,
      "eval_ncbi": 0.8044077134484166,
      "eval_runtime": 404.2341,
      "eval_samples_per_second": 8.906,
      "eval_steps_per_second": 0.28,
      "step": 2238
    },
    {
      "epoch": 2.0017877094972065,
      "grad_norm": 0.07295242697000504,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 2240
    },
    {
      "epoch": 2.01072625698324,
      "grad_norm": 0.08282249420881271,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 2250
    },
    {
      "epoch": 2.0196648044692735,
      "grad_norm": 0.10827043652534485,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 2260
    },
    {
      "epoch": 2.028603351955307,
      "grad_norm": 0.06632424145936966,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 2270
    },
    {
      "epoch": 2.0375418994413406,
      "grad_norm": 0.05865640938282013,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 2280
    },
    {
      "epoch": 2.0464804469273745,
      "grad_norm": 0.06918152421712875,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 2290
    },
    {
      "epoch": 2.055418994413408,
      "grad_norm": 0.07738669961690903,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 2300
    },
    {
      "epoch": 2.0643575418994415,
      "grad_norm": 0.06028103828430176,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 2310
    },
    {
      "epoch": 2.073296089385475,
      "grad_norm": 0.09276604652404785,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 2320
    },
    {
      "epoch": 2.0822346368715086,
      "grad_norm": 0.09783940017223358,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 2330
    },
    {
      "epoch": 2.091173184357542,
      "grad_norm": 0.07749822735786438,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 2340
    },
    {
      "epoch": 2.1001117318435756,
      "grad_norm": 0.08176068216562271,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 2350
    },
    {
      "epoch": 2.109050279329609,
      "grad_norm": 0.1319935917854309,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2360
    },
    {
      "epoch": 2.1179888268156426,
      "grad_norm": 0.06932628899812698,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 2370
    },
    {
      "epoch": 2.126927374301676,
      "grad_norm": 0.09528433531522751,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 2380
    },
    {
      "epoch": 2.1358659217877096,
      "grad_norm": 0.0997343510389328,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 2390
    },
    {
      "epoch": 2.144804469273743,
      "grad_norm": 0.10258489102125168,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 2400
    },
    {
      "epoch": 2.1537430167597766,
      "grad_norm": 0.0885210931301117,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 2410
    },
    {
      "epoch": 2.16268156424581,
      "grad_norm": 0.059652842581272125,
      "learning_rate": 5e-05,
      "loss": 0.0211,
      "step": 2420
    },
    {
      "epoch": 2.1716201117318437,
      "grad_norm": 0.06880370527505875,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 2430
    },
    {
      "epoch": 2.180558659217877,
      "grad_norm": 0.06287036836147308,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 2440
    },
    {
      "epoch": 2.1894972067039107,
      "grad_norm": 0.10189153254032135,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 2450
    },
    {
      "epoch": 2.198435754189944,
      "grad_norm": 0.07999865710735321,
      "learning_rate": 5e-05,
      "loss": 0.0211,
      "step": 2460
    },
    {
      "epoch": 2.2073743016759777,
      "grad_norm": 0.08220622688531876,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 2470
    },
    {
      "epoch": 2.216312849162011,
      "grad_norm": 0.11143281310796738,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 2480
    },
    {
      "epoch": 2.2252513966480447,
      "grad_norm": 0.06572776287794113,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 2490
    },
    {
      "epoch": 2.2341899441340782,
      "grad_norm": 0.0528181828558445,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 2500
    },
    {
      "epoch": 2.2431284916201117,
      "grad_norm": 0.0935136005282402,
      "learning_rate": 5e-05,
      "loss": 0.0211,
      "step": 2510
    },
    {
      "epoch": 2.2520670391061453,
      "grad_norm": 0.06667246669530869,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2520
    },
    {
      "epoch": 2.2610055865921788,
      "grad_norm": 0.08003605157136917,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 2530
    },
    {
      "epoch": 2.2699441340782123,
      "grad_norm": 0.11532309651374817,
      "learning_rate": 5e-05,
      "loss": 0.0191,
      "step": 2540
    },
    {
      "epoch": 2.278882681564246,
      "grad_norm": 0.09388979524374008,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 2550
    },
    {
      "epoch": 2.2878212290502793,
      "grad_norm": 0.11965835094451904,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 2560
    },
    {
      "epoch": 2.296759776536313,
      "grad_norm": 0.12147701531648636,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 2570
    },
    {
      "epoch": 2.3056983240223463,
      "grad_norm": 0.08655991405248642,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 2580
    },
    {
      "epoch": 2.31463687150838,
      "grad_norm": 0.05370333790779114,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 2590
    },
    {
      "epoch": 2.3235754189944133,
      "grad_norm": 0.061878908425569534,
      "learning_rate": 5e-05,
      "loss": 0.0204,
      "step": 2600
    },
    {
      "epoch": 2.332513966480447,
      "grad_norm": 0.08124025911092758,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 2610
    },
    {
      "epoch": 2.3414525139664804,
      "grad_norm": 0.05364851653575897,
      "learning_rate": 5e-05,
      "loss": 0.021,
      "step": 2620
    },
    {
      "epoch": 2.350391061452514,
      "grad_norm": 0.09796231240034103,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 2630
    },
    {
      "epoch": 2.3593296089385474,
      "grad_norm": 0.08311126381158829,
      "learning_rate": 5e-05,
      "loss": 0.0208,
      "step": 2640
    },
    {
      "epoch": 2.368268156424581,
      "grad_norm": 0.08263669908046722,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 2650
    },
    {
      "epoch": 2.3772067039106144,
      "grad_norm": 0.06732891499996185,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 2660
    },
    {
      "epoch": 2.386145251396648,
      "grad_norm": 0.06959524750709534,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 2670
    },
    {
      "epoch": 2.3950837988826814,
      "grad_norm": 0.06103057414293289,
      "learning_rate": 5e-05,
      "loss": 0.0213,
      "step": 2680
    },
    {
      "epoch": 2.404022346368715,
      "grad_norm": 0.08510878682136536,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 2690
    },
    {
      "epoch": 2.4129608938547484,
      "grad_norm": 0.09547621011734009,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 2700
    },
    {
      "epoch": 2.421899441340782,
      "grad_norm": 0.06075263395905495,
      "learning_rate": 5e-05,
      "loss": 0.0187,
      "step": 2710
    },
    {
      "epoch": 2.4308379888268155,
      "grad_norm": 0.06113536283373833,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 2720
    },
    {
      "epoch": 2.4397765363128494,
      "grad_norm": 0.07309572398662567,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 2730
    },
    {
      "epoch": 2.4487150837988825,
      "grad_norm": 0.09811059385538101,
      "learning_rate": 5e-05,
      "loss": 0.0211,
      "step": 2740
    },
    {
      "epoch": 2.4576536312849164,
      "grad_norm": 0.08523035794496536,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 2750
    },
    {
      "epoch": 2.4665921787709495,
      "grad_norm": 0.06966949999332428,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 2760
    },
    {
      "epoch": 2.4755307262569834,
      "grad_norm": 0.07057671993970871,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 2770
    },
    {
      "epoch": 2.4844692737430165,
      "grad_norm": 0.06904683262109756,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 2780
    },
    {
      "epoch": 2.4934078212290505,
      "grad_norm": 0.07364901155233383,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 2790
    },
    {
      "epoch": 2.5023463687150835,
      "grad_norm": 0.09494862705469131,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 2800
    },
    {
      "epoch": 2.5112849162011175,
      "grad_norm": 0.06009683385491371,
      "learning_rate": 5e-05,
      "loss": 0.0204,
      "step": 2810
    },
    {
      "epoch": 2.520223463687151,
      "grad_norm": 0.07366452366113663,
      "learning_rate": 5e-05,
      "loss": 0.0208,
      "step": 2820
    },
    {
      "epoch": 2.5291620111731845,
      "grad_norm": 0.19381015002727509,
      "learning_rate": 5e-05,
      "loss": 0.0208,
      "step": 2830
    },
    {
      "epoch": 2.538100558659218,
      "grad_norm": 0.06649117916822433,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2840
    },
    {
      "epoch": 2.5470391061452515,
      "grad_norm": 0.08848918229341507,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 2850
    },
    {
      "epoch": 2.555977653631285,
      "grad_norm": 0.08407234400510788,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 2860
    },
    {
      "epoch": 2.5649162011173186,
      "grad_norm": 0.07764944434165955,
      "learning_rate": 5e-05,
      "loss": 0.0213,
      "step": 2870
    },
    {
      "epoch": 2.573854748603352,
      "grad_norm": 0.060752023011446,
      "learning_rate": 5e-05,
      "loss": 0.0194,
      "step": 2880
    },
    {
      "epoch": 2.5827932960893856,
      "grad_norm": 0.05982639268040657,
      "learning_rate": 5e-05,
      "loss": 0.0197,
      "step": 2890
    },
    {
      "epoch": 2.591731843575419,
      "grad_norm": 0.08323317766189575,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 2900
    },
    {
      "epoch": 2.6006703910614526,
      "grad_norm": 0.08636189252138138,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 2910
    },
    {
      "epoch": 2.609608938547486,
      "grad_norm": 0.0952286422252655,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 2920
    },
    {
      "epoch": 2.6185474860335196,
      "grad_norm": 0.06431462615728378,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2930
    },
    {
      "epoch": 2.627486033519553,
      "grad_norm": 0.08118824660778046,
      "learning_rate": 5e-05,
      "loss": 0.0193,
      "step": 2940
    },
    {
      "epoch": 2.6364245810055866,
      "grad_norm": 0.07015486806631088,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 2950
    },
    {
      "epoch": 2.64536312849162,
      "grad_norm": 0.0705484002828598,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 2960
    },
    {
      "epoch": 2.6543016759776537,
      "grad_norm": 0.06199301406741142,
      "learning_rate": 5e-05,
      "loss": 0.0203,
      "step": 2970
    },
    {
      "epoch": 2.663240223463687,
      "grad_norm": 0.09719175845384598,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 2980
    },
    {
      "epoch": 2.6721787709497207,
      "grad_norm": 0.06595801562070847,
      "learning_rate": 5e-05,
      "loss": 0.0185,
      "step": 2990
    },
    {
      "epoch": 2.681117318435754,
      "grad_norm": 0.0761902779340744,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 3000
    },
    {
      "epoch": 2.6900558659217877,
      "grad_norm": 0.06012892350554466,
      "learning_rate": 5e-05,
      "loss": 0.0208,
      "step": 3010
    },
    {
      "epoch": 2.698994413407821,
      "grad_norm": 0.06983626633882523,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 3020
    },
    {
      "epoch": 2.7079329608938547,
      "grad_norm": 0.07424576580524445,
      "learning_rate": 5e-05,
      "loss": 0.0208,
      "step": 3030
    },
    {
      "epoch": 2.7168715083798882,
      "grad_norm": 0.07890335470438004,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 3040
    },
    {
      "epoch": 2.7258100558659217,
      "grad_norm": 0.12625639140605927,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 3050
    },
    {
      "epoch": 2.7347486033519552,
      "grad_norm": 0.06045696511864662,
      "learning_rate": 5e-05,
      "loss": 0.0187,
      "step": 3060
    },
    {
      "epoch": 2.7436871508379888,
      "grad_norm": 0.06754139065742493,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 3070
    },
    {
      "epoch": 2.7526256983240223,
      "grad_norm": 0.06542805582284927,
      "learning_rate": 5e-05,
      "loss": 0.0193,
      "step": 3080
    },
    {
      "epoch": 2.7615642458100558,
      "grad_norm": 0.10007724910974503,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 3090
    },
    {
      "epoch": 2.7705027932960893,
      "grad_norm": 0.048854146152734756,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 3100
    },
    {
      "epoch": 2.779441340782123,
      "grad_norm": 0.06700440496206284,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 3110
    },
    {
      "epoch": 2.7883798882681563,
      "grad_norm": 0.08089668303728104,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3120
    },
    {
      "epoch": 2.79731843575419,
      "grad_norm": 0.078087218105793,
      "learning_rate": 5e-05,
      "loss": 0.0202,
      "step": 3130
    },
    {
      "epoch": 2.8062569832402233,
      "grad_norm": 0.08642126619815826,
      "learning_rate": 5e-05,
      "loss": 0.0191,
      "step": 3140
    },
    {
      "epoch": 2.815195530726257,
      "grad_norm": 0.06957536935806274,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 3150
    },
    {
      "epoch": 2.8241340782122903,
      "grad_norm": 0.052545685321092606,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 3160
    },
    {
      "epoch": 2.833072625698324,
      "grad_norm": 0.08133012801408768,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 3170
    },
    {
      "epoch": 2.842011173184358,
      "grad_norm": 0.2003796547651291,
      "learning_rate": 5e-05,
      "loss": 0.0191,
      "step": 3180
    },
    {
      "epoch": 2.850949720670391,
      "grad_norm": 0.0922231674194336,
      "learning_rate": 5e-05,
      "loss": 0.0201,
      "step": 3190
    },
    {
      "epoch": 2.859888268156425,
      "grad_norm": 0.07898340374231339,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3200
    },
    {
      "epoch": 2.868826815642458,
      "grad_norm": 0.07189009338617325,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 3210
    },
    {
      "epoch": 2.877765363128492,
      "grad_norm": 0.057267043739557266,
      "learning_rate": 5e-05,
      "loss": 0.0189,
      "step": 3220
    },
    {
      "epoch": 2.886703910614525,
      "grad_norm": 0.08078800141811371,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 3230
    },
    {
      "epoch": 2.895642458100559,
      "grad_norm": 0.05960184708237648,
      "learning_rate": 5e-05,
      "loss": 0.0193,
      "step": 3240
    },
    {
      "epoch": 2.904581005586592,
      "grad_norm": 0.08033890277147293,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 3250
    },
    {
      "epoch": 2.913519553072626,
      "grad_norm": 0.0758681669831276,
      "learning_rate": 5e-05,
      "loss": 0.0175,
      "step": 3260
    },
    {
      "epoch": 2.922458100558659,
      "grad_norm": 0.071729376912117,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 3270
    },
    {
      "epoch": 2.931396648044693,
      "grad_norm": 0.060297079384326935,
      "learning_rate": 5e-05,
      "loss": 0.02,
      "step": 3280
    },
    {
      "epoch": 2.9403351955307264,
      "grad_norm": 0.07588402181863785,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3290
    },
    {
      "epoch": 2.94927374301676,
      "grad_norm": 0.057860467582941055,
      "learning_rate": 5e-05,
      "loss": 0.02,
      "step": 3300
    },
    {
      "epoch": 2.9582122905027934,
      "grad_norm": 0.07404229044914246,
      "learning_rate": 5e-05,
      "loss": 0.0196,
      "step": 3310
    },
    {
      "epoch": 2.967150837988827,
      "grad_norm": 0.07040568441152573,
      "learning_rate": 5e-05,
      "loss": 0.0201,
      "step": 3320
    },
    {
      "epoch": 2.9760893854748605,
      "grad_norm": 0.10220964252948761,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 3330
    },
    {
      "epoch": 2.985027932960894,
      "grad_norm": 0.09579332172870636,
      "learning_rate": 5e-05,
      "loss": 0.0198,
      "step": 3340
    },
    {
      "epoch": 2.9939664804469275,
      "grad_norm": 0.06849901378154755,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3350
    },
    {
      "epoch": 3.0,
      "eval_AnatEM": 0.5869565216884451,
      "eval_FabNER": 0.022847100140015168,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.4999999999494382,
      "eval_MultiNERD": 0.8646748681395041,
      "eval_Ontonotes": 0.8824082784070433,
      "eval_TweetNER7": 0.6071428570927678,
      "eval_WikiANN-en": 0.8058076224542463,
      "eval_WikiNeural": 0.8910614524637239,
      "eval_average": 0.7158004717827573,
      "eval_bc2gm": 0.6470588234801188,
      "eval_bc4chemd": 0.6507936507436648,
      "eval_bc5cdr": 0.8140703517086703,
      "eval_broad_twitter_corpus": 0.6465364120281163,
      "eval_conll2003": 0.9423815620496314,
      "eval_conllpp": 0.9423815620496314,
      "eval_mit-movie": 0.7086383601254933,
      "eval_mit-restaurant": 0.7780269057794423,
      "eval_ncbi": 0.8725212464085196,
      "eval_runtime": 413.9533,
      "eval_samples_per_second": 8.697,
      "eval_steps_per_second": 0.273,
      "step": 3357
    },
    {
      "epoch": 3.00268156424581,
      "grad_norm": 0.08008407056331635,
      "learning_rate": 5e-05,
      "loss": 0.0195,
      "step": 3360
    },
    {
      "epoch": 3.0116201117318435,
      "grad_norm": 0.09626758098602295,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3370
    },
    {
      "epoch": 3.020558659217877,
      "grad_norm": 0.08318818360567093,
      "learning_rate": 5e-05,
      "loss": 0.0186,
      "step": 3380
    },
    {
      "epoch": 3.0294972067039105,
      "grad_norm": 0.10009198635816574,
      "learning_rate": 5e-05,
      "loss": 0.0193,
      "step": 3390
    },
    {
      "epoch": 3.038435754189944,
      "grad_norm": 0.06235237419605255,
      "learning_rate": 5e-05,
      "loss": 0.0193,
      "step": 3400
    },
    {
      "epoch": 3.0473743016759776,
      "grad_norm": 0.06793558597564697,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 3410
    },
    {
      "epoch": 3.056312849162011,
      "grad_norm": 0.06527332216501236,
      "learning_rate": 5e-05,
      "loss": 0.0199,
      "step": 3420
    },
    {
      "epoch": 3.0652513966480446,
      "grad_norm": 0.07867272198200226,
      "learning_rate": 5e-05,
      "loss": 0.0177,
      "step": 3430
    },
    {
      "epoch": 3.074189944134078,
      "grad_norm": 0.07065330445766449,
      "learning_rate": 5e-05,
      "loss": 0.0189,
      "step": 3440
    },
    {
      "epoch": 3.0831284916201116,
      "grad_norm": 0.056074339896440506,
      "learning_rate": 5e-05,
      "loss": 0.0179,
      "step": 3450
    },
    {
      "epoch": 3.092067039106145,
      "grad_norm": 0.06052088364958763,
      "learning_rate": 5e-05,
      "loss": 0.0188,
      "step": 3460
    },
    {
      "epoch": 3.1010055865921786,
      "grad_norm": 0.0993955135345459,
      "learning_rate": 5e-05,
      "loss": 0.0196,
      "step": 3470
    },
    {
      "epoch": 3.109944134078212,
      "grad_norm": 0.07718206197023392,
      "learning_rate": 5e-05,
      "loss": 0.0177,
      "step": 3480
    },
    {
      "epoch": 3.1188826815642456,
      "grad_norm": 0.05006742477416992,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 3490
    },
    {
      "epoch": 3.127821229050279,
      "grad_norm": 0.060653068125247955,
      "learning_rate": 5e-05,
      "loss": 0.0182,
      "step": 3500
    },
    {
      "epoch": 3.1367597765363127,
      "grad_norm": 0.06863798201084137,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3510
    },
    {
      "epoch": 3.145698324022346,
      "grad_norm": 0.0676480382680893,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 3520
    },
    {
      "epoch": 3.1546368715083797,
      "grad_norm": 0.07655251026153564,
      "learning_rate": 5e-05,
      "loss": 0.0198,
      "step": 3530
    },
    {
      "epoch": 3.163575418994413,
      "grad_norm": 0.06238405033946037,
      "learning_rate": 5e-05,
      "loss": 0.0182,
      "step": 3540
    },
    {
      "epoch": 3.172513966480447,
      "grad_norm": 0.07862807810306549,
      "learning_rate": 5e-05,
      "loss": 0.0174,
      "step": 3550
    },
    {
      "epoch": 3.18145251396648,
      "grad_norm": 0.05245174095034599,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 3560
    },
    {
      "epoch": 3.190391061452514,
      "grad_norm": 0.07806263864040375,
      "learning_rate": 5e-05,
      "loss": 0.0192,
      "step": 3570
    },
    {
      "epoch": 3.1993296089385477,
      "grad_norm": 0.15419740974903107,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 3580
    },
    {
      "epoch": 3.208268156424581,
      "grad_norm": 0.04951081424951553,
      "learning_rate": 5e-05,
      "loss": 0.0188,
      "step": 3590
    },
    {
      "epoch": 3.2172067039106147,
      "grad_norm": 0.06233980134129524,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 3600
    },
    {
      "epoch": 3.226145251396648,
      "grad_norm": 0.04424412548542023,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 3610
    },
    {
      "epoch": 3.2350837988826817,
      "grad_norm": 0.06799854338169098,
      "learning_rate": 5e-05,
      "loss": 0.0185,
      "step": 3620
    },
    {
      "epoch": 3.2440223463687152,
      "grad_norm": 0.05765991657972336,
      "learning_rate": 5e-05,
      "loss": 0.0178,
      "step": 3630
    },
    {
      "epoch": 3.2529608938547487,
      "grad_norm": 0.0871673971414566,
      "learning_rate": 5e-05,
      "loss": 0.0185,
      "step": 3640
    },
    {
      "epoch": 3.2618994413407822,
      "grad_norm": 0.09332705289125443,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 3650
    },
    {
      "epoch": 3.2708379888268158,
      "grad_norm": 0.07065564393997192,
      "learning_rate": 5e-05,
      "loss": 0.0186,
      "step": 3660
    },
    {
      "epoch": 3.2797765363128493,
      "grad_norm": 0.05186448618769646,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 3670
    },
    {
      "epoch": 3.2887150837988828,
      "grad_norm": 0.062186725437641144,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 3680
    },
    {
      "epoch": 3.2976536312849163,
      "grad_norm": 0.05951305106282234,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 3690
    },
    {
      "epoch": 3.30659217877095,
      "grad_norm": 0.05356950685381889,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 3700
    },
    {
      "epoch": 3.3155307262569833,
      "grad_norm": 0.0594225712120533,
      "learning_rate": 5e-05,
      "loss": 0.0191,
      "step": 3710
    },
    {
      "epoch": 3.324469273743017,
      "grad_norm": 0.08661416918039322,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 3720
    },
    {
      "epoch": 3.3334078212290503,
      "grad_norm": 0.06787597388029099,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 3730
    },
    {
      "epoch": 3.342346368715084,
      "grad_norm": 0.07032711803913116,
      "learning_rate": 5e-05,
      "loss": 0.0178,
      "step": 3740
    },
    {
      "epoch": 3.3512849162011173,
      "grad_norm": 0.07225609570741653,
      "learning_rate": 5e-05,
      "loss": 0.0179,
      "step": 3750
    },
    {
      "epoch": 3.360223463687151,
      "grad_norm": 0.04880257323384285,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3760
    },
    {
      "epoch": 3.3691620111731844,
      "grad_norm": 0.07507622987031937,
      "learning_rate": 5e-05,
      "loss": 0.0187,
      "step": 3770
    },
    {
      "epoch": 3.378100558659218,
      "grad_norm": 0.09596063196659088,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 3780
    },
    {
      "epoch": 3.3870391061452514,
      "grad_norm": 0.060180384665727615,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 3790
    },
    {
      "epoch": 3.395977653631285,
      "grad_norm": 0.07634594291448593,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3800
    },
    {
      "epoch": 3.4049162011173184,
      "grad_norm": 0.07325893640518188,
      "learning_rate": 5e-05,
      "loss": 0.0179,
      "step": 3810
    },
    {
      "epoch": 3.413854748603352,
      "grad_norm": 0.09582708775997162,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 3820
    },
    {
      "epoch": 3.4227932960893854,
      "grad_norm": 0.08824034780263901,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3830
    },
    {
      "epoch": 3.431731843575419,
      "grad_norm": 0.0622943677008152,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 3840
    },
    {
      "epoch": 3.4406703910614524,
      "grad_norm": 0.06072480231523514,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 3850
    },
    {
      "epoch": 3.449608938547486,
      "grad_norm": 0.047146957367658615,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 3860
    },
    {
      "epoch": 3.4585474860335195,
      "grad_norm": 0.08018393069505692,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3870
    },
    {
      "epoch": 3.467486033519553,
      "grad_norm": 0.06273078918457031,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 3880
    },
    {
      "epoch": 3.4764245810055865,
      "grad_norm": 0.07045785337686539,
      "learning_rate": 5e-05,
      "loss": 0.0176,
      "step": 3890
    },
    {
      "epoch": 3.48536312849162,
      "grad_norm": 0.07526876032352448,
      "learning_rate": 5e-05,
      "loss": 0.0172,
      "step": 3900
    },
    {
      "epoch": 3.4943016759776535,
      "grad_norm": 0.054696228355169296,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 3910
    },
    {
      "epoch": 3.503240223463687,
      "grad_norm": 0.05539889633655548,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 3920
    },
    {
      "epoch": 3.5121787709497205,
      "grad_norm": 0.06722493469715118,
      "learning_rate": 5e-05,
      "loss": 0.0179,
      "step": 3930
    },
    {
      "epoch": 3.521117318435754,
      "grad_norm": 0.17513029277324677,
      "learning_rate": 5e-05,
      "loss": 0.0174,
      "step": 3940
    },
    {
      "epoch": 3.5300558659217876,
      "grad_norm": 0.05724392458796501,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 3950
    },
    {
      "epoch": 3.5389944134078215,
      "grad_norm": 0.11253676563501358,
      "learning_rate": 5e-05,
      "loss": 0.0175,
      "step": 3960
    },
    {
      "epoch": 3.5479329608938546,
      "grad_norm": 0.04731522127985954,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 3970
    },
    {
      "epoch": 3.5568715083798885,
      "grad_norm": 0.06756937503814697,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 3980
    },
    {
      "epoch": 3.5658100558659216,
      "grad_norm": 0.09835589677095413,
      "learning_rate": 5e-05,
      "loss": 0.02,
      "step": 3990
    },
    {
      "epoch": 3.5747486033519555,
      "grad_norm": 0.06316357105970383,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 4000
    },
    {
      "epoch": 3.5836871508379886,
      "grad_norm": 0.05765169858932495,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 4010
    },
    {
      "epoch": 3.5926256983240226,
      "grad_norm": 0.06437837332487106,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 4020
    },
    {
      "epoch": 3.6015642458100556,
      "grad_norm": 0.059858810156583786,
      "learning_rate": 5e-05,
      "loss": 0.0178,
      "step": 4030
    },
    {
      "epoch": 3.6105027932960896,
      "grad_norm": 0.06543754041194916,
      "learning_rate": 5e-05,
      "loss": 0.0186,
      "step": 4040
    },
    {
      "epoch": 3.6194413407821227,
      "grad_norm": 0.06826002150774002,
      "learning_rate": 5e-05,
      "loss": 0.0176,
      "step": 4050
    },
    {
      "epoch": 3.6283798882681566,
      "grad_norm": 0.08246096223592758,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 4060
    },
    {
      "epoch": 3.6373184357541897,
      "grad_norm": 0.07008208334445953,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 4070
    },
    {
      "epoch": 3.6462569832402236,
      "grad_norm": 0.061419229954481125,
      "learning_rate": 5e-05,
      "loss": 0.0179,
      "step": 4080
    },
    {
      "epoch": 3.655195530726257,
      "grad_norm": 0.07486468553543091,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 4090
    },
    {
      "epoch": 3.6641340782122906,
      "grad_norm": 0.07389748096466064,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 4100
    },
    {
      "epoch": 3.673072625698324,
      "grad_norm": 0.05643019452691078,
      "learning_rate": 5e-05,
      "loss": 0.0181,
      "step": 4110
    },
    {
      "epoch": 3.6820111731843577,
      "grad_norm": 0.07336767762899399,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 4120
    },
    {
      "epoch": 3.690949720670391,
      "grad_norm": 0.049813542515039444,
      "learning_rate": 5e-05,
      "loss": 0.0182,
      "step": 4130
    },
    {
      "epoch": 3.6998882681564247,
      "grad_norm": 0.07901697605848312,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 4140
    },
    {
      "epoch": 3.708826815642458,
      "grad_norm": 0.06297726184129715,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 4150
    },
    {
      "epoch": 3.7177653631284917,
      "grad_norm": 0.11307255178689957,
      "learning_rate": 5e-05,
      "loss": 0.0172,
      "step": 4160
    },
    {
      "epoch": 3.726703910614525,
      "grad_norm": 0.06102069094777107,
      "learning_rate": 5e-05,
      "loss": 0.0178,
      "step": 4170
    },
    {
      "epoch": 3.7356424581005587,
      "grad_norm": 0.07340805977582932,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4180
    },
    {
      "epoch": 3.7445810055865922,
      "grad_norm": 0.0528719499707222,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 4190
    },
    {
      "epoch": 3.7535195530726257,
      "grad_norm": 0.09258969128131866,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 4200
    },
    {
      "epoch": 3.7624581005586593,
      "grad_norm": 0.06171088665723801,
      "learning_rate": 5e-05,
      "loss": 0.0175,
      "step": 4210
    },
    {
      "epoch": 3.7713966480446928,
      "grad_norm": 0.060032207518815994,
      "learning_rate": 5e-05,
      "loss": 0.0167,
      "step": 4220
    },
    {
      "epoch": 3.7803351955307263,
      "grad_norm": 0.06321621686220169,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 4230
    },
    {
      "epoch": 3.78927374301676,
      "grad_norm": 0.05753963068127632,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4240
    },
    {
      "epoch": 3.7982122905027933,
      "grad_norm": 0.061236683279275894,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 4250
    },
    {
      "epoch": 3.807150837988827,
      "grad_norm": 0.05453597381711006,
      "learning_rate": 5e-05,
      "loss": 0.0184,
      "step": 4260
    },
    {
      "epoch": 3.8160893854748603,
      "grad_norm": 0.05736426264047623,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4270
    },
    {
      "epoch": 3.825027932960894,
      "grad_norm": 0.07220336049795151,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 4280
    },
    {
      "epoch": 3.8339664804469273,
      "grad_norm": 0.04926128312945366,
      "learning_rate": 5e-05,
      "loss": 0.0178,
      "step": 4290
    },
    {
      "epoch": 3.842905027932961,
      "grad_norm": 0.07826395332813263,
      "learning_rate": 5e-05,
      "loss": 0.019,
      "step": 4300
    },
    {
      "epoch": 3.8518435754189944,
      "grad_norm": 0.059167858213186264,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 4310
    },
    {
      "epoch": 3.860782122905028,
      "grad_norm": 0.05063674598932266,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 4320
    },
    {
      "epoch": 3.8697206703910614,
      "grad_norm": 0.057909220457077026,
      "learning_rate": 5e-05,
      "loss": 0.018,
      "step": 4330
    },
    {
      "epoch": 3.878659217877095,
      "grad_norm": 0.06644915044307709,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 4340
    },
    {
      "epoch": 3.8875977653631284,
      "grad_norm": 0.06108102202415466,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 4350
    },
    {
      "epoch": 3.896536312849162,
      "grad_norm": 0.04706362262368202,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4360
    },
    {
      "epoch": 3.9054748603351954,
      "grad_norm": 0.06298554688692093,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 4370
    },
    {
      "epoch": 3.914413407821229,
      "grad_norm": 0.07020290195941925,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 4380
    },
    {
      "epoch": 3.9233519553072624,
      "grad_norm": 0.10449738800525665,
      "learning_rate": 5e-05,
      "loss": 0.0177,
      "step": 4390
    },
    {
      "epoch": 3.932290502793296,
      "grad_norm": 0.06287740170955658,
      "learning_rate": 5e-05,
      "loss": 0.0183,
      "step": 4400
    },
    {
      "epoch": 3.9412290502793295,
      "grad_norm": 0.05861489847302437,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 4410
    },
    {
      "epoch": 3.950167597765363,
      "grad_norm": 0.05781719461083412,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4420
    },
    {
      "epoch": 3.959106145251397,
      "grad_norm": 0.07121217995882034,
      "learning_rate": 5e-05,
      "loss": 0.0189,
      "step": 4430
    },
    {
      "epoch": 3.96804469273743,
      "grad_norm": 0.07454195618629456,
      "learning_rate": 5e-05,
      "loss": 0.0165,
      "step": 4440
    },
    {
      "epoch": 3.976983240223464,
      "grad_norm": 0.048953428864479065,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4450
    },
    {
      "epoch": 3.985921787709497,
      "grad_norm": 0.07530011236667633,
      "learning_rate": 5e-05,
      "loss": 0.0177,
      "step": 4460
    },
    {
      "epoch": 3.994860335195531,
      "grad_norm": 0.07862987369298935,
      "learning_rate": 5e-05,
      "loss": 0.0177,
      "step": 4470
    },
    {
      "epoch": 4.0,
      "eval_AnatEM": 0.48351648346590986,
      "eval_FabNER": 0.01677539605540762,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.5276073619127705,
      "eval_MultiNERD": 0.8936170212262796,
      "eval_Ontonotes": 0.9075471697611582,
      "eval_TweetNER7": 0.6257575757075068,
      "eval_WikiANN-en": 0.7985480943235773,
      "eval_WikiNeural": 0.8942172072840215,
      "eval_average": 0.7249662361247633,
      "eval_bc2gm": 0.6734006733509961,
      "eval_bc4chemd": 0.7024128685828233,
      "eval_bc5cdr": 0.8381201043884238,
      "eval_broad_twitter_corpus": 0.6603773584411399,
      "eval_conll2003": 0.9398207425874042,
      "eval_conllpp": 0.9423815620496314,
      "eval_mit-movie": 0.7343283581587807,
      "eval_mit-restaurant": 0.7887005649215809,
      "eval_ncbi": 0.9011627906471658,
      "eval_runtime": 413.5911,
      "eval_samples_per_second": 8.704,
      "eval_steps_per_second": 0.273,
      "step": 4476
    },
    {
      "epoch": 4.003575418994413,
      "grad_norm": 0.17494869232177734,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 4480
    },
    {
      "epoch": 4.012513966480447,
      "grad_norm": 0.0713731124997139,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 4490
    },
    {
      "epoch": 4.02145251396648,
      "grad_norm": 0.04746704176068306,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4500
    },
    {
      "epoch": 4.030391061452514,
      "grad_norm": 0.05044730380177498,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4510
    },
    {
      "epoch": 4.039329608938547,
      "grad_norm": 0.06335592269897461,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 4520
    },
    {
      "epoch": 4.048268156424581,
      "grad_norm": 0.0508839376270771,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 4530
    },
    {
      "epoch": 4.057206703910614,
      "grad_norm": 0.04979725554585457,
      "learning_rate": 5e-05,
      "loss": 0.0163,
      "step": 4540
    },
    {
      "epoch": 4.066145251396648,
      "grad_norm": 0.0679984986782074,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 4550
    },
    {
      "epoch": 4.075083798882681,
      "grad_norm": 0.05319315567612648,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 4560
    },
    {
      "epoch": 4.084022346368715,
      "grad_norm": 0.07084185630083084,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 4570
    },
    {
      "epoch": 4.092960893854749,
      "grad_norm": 0.06786669045686722,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 4580
    },
    {
      "epoch": 4.101899441340782,
      "grad_norm": 0.0519118532538414,
      "learning_rate": 5e-05,
      "loss": 0.0167,
      "step": 4590
    },
    {
      "epoch": 4.110837988826816,
      "grad_norm": 0.07246249914169312,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 4600
    },
    {
      "epoch": 4.119776536312849,
      "grad_norm": 0.04590817540884018,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 4610
    },
    {
      "epoch": 4.128715083798883,
      "grad_norm": 0.05657728761434555,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 4620
    },
    {
      "epoch": 4.137653631284916,
      "grad_norm": 0.06923170387744904,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4630
    },
    {
      "epoch": 4.14659217877095,
      "grad_norm": 0.05992729961872101,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 4640
    },
    {
      "epoch": 4.155530726256983,
      "grad_norm": 0.07334999740123749,
      "learning_rate": 5e-05,
      "loss": 0.0162,
      "step": 4650
    },
    {
      "epoch": 4.164469273743017,
      "grad_norm": 0.07659419625997543,
      "learning_rate": 5e-05,
      "loss": 0.0169,
      "step": 4660
    },
    {
      "epoch": 4.17340782122905,
      "grad_norm": 0.054748132824897766,
      "learning_rate": 5e-05,
      "loss": 0.0166,
      "step": 4670
    },
    {
      "epoch": 4.182346368715084,
      "grad_norm": 0.06767779588699341,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 4680
    },
    {
      "epoch": 4.191284916201117,
      "grad_norm": 0.03593229502439499,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 4690
    },
    {
      "epoch": 4.200223463687151,
      "grad_norm": 0.044881585985422134,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 4700
    },
    {
      "epoch": 4.209162011173184,
      "grad_norm": 0.06191400811076164,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 4710
    },
    {
      "epoch": 4.218100558659218,
      "grad_norm": 0.07031531631946564,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 4720
    },
    {
      "epoch": 4.227039106145251,
      "grad_norm": 0.052775342017412186,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 4730
    },
    {
      "epoch": 4.235977653631285,
      "grad_norm": 0.061696432530879974,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 4740
    },
    {
      "epoch": 4.244916201117318,
      "grad_norm": 0.056942299008369446,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 4750
    },
    {
      "epoch": 4.253854748603352,
      "grad_norm": 0.04337730258703232,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 4760
    },
    {
      "epoch": 4.262793296089385,
      "grad_norm": 0.07176291197538376,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 4770
    },
    {
      "epoch": 4.271731843575419,
      "grad_norm": 0.056065499782562256,
      "learning_rate": 5e-05,
      "loss": 0.0163,
      "step": 4780
    },
    {
      "epoch": 4.280670391061452,
      "grad_norm": 0.053016118705272675,
      "learning_rate": 5e-05,
      "loss": 0.0171,
      "step": 4790
    },
    {
      "epoch": 4.289608938547486,
      "grad_norm": 0.0590113140642643,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 4800
    },
    {
      "epoch": 4.298547486033519,
      "grad_norm": 0.056279007345438004,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 4810
    },
    {
      "epoch": 4.307486033519553,
      "grad_norm": 0.14203014969825745,
      "learning_rate": 5e-05,
      "loss": 0.0168,
      "step": 4820
    },
    {
      "epoch": 4.316424581005586,
      "grad_norm": 0.044969212263822556,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 4830
    },
    {
      "epoch": 4.32536312849162,
      "grad_norm": 0.05260511860251427,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 4840
    },
    {
      "epoch": 4.334301675977653,
      "grad_norm": 0.0666169673204422,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 4850
    },
    {
      "epoch": 4.343240223463687,
      "grad_norm": 0.05110188573598862,
      "learning_rate": 5e-05,
      "loss": 0.017,
      "step": 4860
    },
    {
      "epoch": 4.35217877094972,
      "grad_norm": 0.0792146697640419,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 4870
    },
    {
      "epoch": 4.361117318435754,
      "grad_norm": 0.05939565598964691,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 4880
    },
    {
      "epoch": 4.370055865921787,
      "grad_norm": 0.05635537952184677,
      "learning_rate": 5e-05,
      "loss": 0.0158,
      "step": 4890
    },
    {
      "epoch": 4.378994413407821,
      "grad_norm": 0.07101117819547653,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 4900
    },
    {
      "epoch": 4.387932960893854,
      "grad_norm": 0.0772438570857048,
      "learning_rate": 5e-05,
      "loss": 0.0172,
      "step": 4910
    },
    {
      "epoch": 4.396871508379888,
      "grad_norm": 0.05731189250946045,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 4920
    },
    {
      "epoch": 4.4058100558659214,
      "grad_norm": 0.06410297006368637,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 4930
    },
    {
      "epoch": 4.414748603351955,
      "grad_norm": 0.05928853899240494,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4940
    },
    {
      "epoch": 4.4236871508379885,
      "grad_norm": 0.049595337361097336,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 4950
    },
    {
      "epoch": 4.432625698324022,
      "grad_norm": 0.049964211881160736,
      "learning_rate": 5e-05,
      "loss": 0.0157,
      "step": 4960
    },
    {
      "epoch": 4.4415642458100555,
      "grad_norm": 0.056165579706430435,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 4970
    },
    {
      "epoch": 4.450502793296089,
      "grad_norm": 0.07505769282579422,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 4980
    },
    {
      "epoch": 4.4594413407821225,
      "grad_norm": 0.11686425656080246,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 4990
    },
    {
      "epoch": 4.4683798882681565,
      "grad_norm": 0.041806597262620926,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 5000
    },
    {
      "epoch": 4.4773184357541895,
      "grad_norm": 0.058086782693862915,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 5010
    },
    {
      "epoch": 4.4862569832402235,
      "grad_norm": 0.06679004430770874,
      "learning_rate": 5e-05,
      "loss": 0.0161,
      "step": 5020
    },
    {
      "epoch": 4.4951955307262565,
      "grad_norm": 0.05813562497496605,
      "learning_rate": 5e-05,
      "loss": 0.0173,
      "step": 5030
    },
    {
      "epoch": 4.5041340782122905,
      "grad_norm": 0.05989662557840347,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 5040
    },
    {
      "epoch": 4.5130726256983245,
      "grad_norm": 0.05152296647429466,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5050
    },
    {
      "epoch": 4.5220111731843575,
      "grad_norm": 0.048263028264045715,
      "learning_rate": 5e-05,
      "loss": 0.0158,
      "step": 5060
    },
    {
      "epoch": 4.530949720670391,
      "grad_norm": 0.06493998318910599,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 5070
    },
    {
      "epoch": 4.5398882681564245,
      "grad_norm": 0.07779951393604279,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 5080
    },
    {
      "epoch": 4.5488268156424585,
      "grad_norm": 0.05777636170387268,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 5090
    },
    {
      "epoch": 4.557765363128492,
      "grad_norm": 0.046717025339603424,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 5100
    },
    {
      "epoch": 4.5667039106145255,
      "grad_norm": 0.05618543177843094,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5110
    },
    {
      "epoch": 4.575642458100559,
      "grad_norm": 0.04186323657631874,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5120
    },
    {
      "epoch": 4.5845810055865925,
      "grad_norm": 0.057042691856622696,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5130
    },
    {
      "epoch": 4.593519553072626,
      "grad_norm": 0.06019426882266998,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 5140
    },
    {
      "epoch": 4.60245810055866,
      "grad_norm": 0.07375122606754303,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 5150
    },
    {
      "epoch": 4.611396648044693,
      "grad_norm": 0.08956202864646912,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5160
    },
    {
      "epoch": 4.620335195530727,
      "grad_norm": 0.07787780463695526,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 5170
    },
    {
      "epoch": 4.62927374301676,
      "grad_norm": 0.056731872260570526,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 5180
    },
    {
      "epoch": 4.638212290502794,
      "grad_norm": 0.052484769374132156,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 5190
    },
    {
      "epoch": 4.647150837988827,
      "grad_norm": 0.05128709226846695,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 5200
    },
    {
      "epoch": 4.656089385474861,
      "grad_norm": 0.0629790872335434,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 5210
    },
    {
      "epoch": 4.665027932960894,
      "grad_norm": 0.05346520245075226,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5220
    },
    {
      "epoch": 4.673966480446928,
      "grad_norm": 0.05462246760725975,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5230
    },
    {
      "epoch": 4.682905027932961,
      "grad_norm": 0.09992624074220657,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5240
    },
    {
      "epoch": 4.691843575418995,
      "grad_norm": 0.1031874492764473,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 5250
    },
    {
      "epoch": 4.700782122905028,
      "grad_norm": 0.06219610571861267,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5260
    },
    {
      "epoch": 4.709720670391062,
      "grad_norm": 0.08152445405721664,
      "learning_rate": 5e-05,
      "loss": 0.0162,
      "step": 5270
    },
    {
      "epoch": 4.718659217877095,
      "grad_norm": 0.05292317643761635,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 5280
    },
    {
      "epoch": 4.727597765363129,
      "grad_norm": 0.0895262062549591,
      "learning_rate": 5e-05,
      "loss": 0.0157,
      "step": 5290
    },
    {
      "epoch": 4.736536312849162,
      "grad_norm": 0.051820773631334305,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5300
    },
    {
      "epoch": 4.745474860335196,
      "grad_norm": 0.07286673039197922,
      "learning_rate": 5e-05,
      "loss": 0.0162,
      "step": 5310
    },
    {
      "epoch": 4.754413407821229,
      "grad_norm": 0.06186940148472786,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 5320
    },
    {
      "epoch": 4.763351955307263,
      "grad_norm": 0.0655544102191925,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 5330
    },
    {
      "epoch": 4.772290502793296,
      "grad_norm": 0.059750597923994064,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 5340
    },
    {
      "epoch": 4.78122905027933,
      "grad_norm": 0.0924263671040535,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5350
    },
    {
      "epoch": 4.790167597765363,
      "grad_norm": 0.05401109531521797,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 5360
    },
    {
      "epoch": 4.799106145251397,
      "grad_norm": 0.06316637992858887,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 5370
    },
    {
      "epoch": 4.80804469273743,
      "grad_norm": 0.06350414454936981,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5380
    },
    {
      "epoch": 4.816983240223464,
      "grad_norm": 0.10951284319162369,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 5390
    },
    {
      "epoch": 4.825921787709497,
      "grad_norm": 0.05594818666577339,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 5400
    },
    {
      "epoch": 4.834860335195531,
      "grad_norm": 0.05739104747772217,
      "learning_rate": 5e-05,
      "loss": 0.0157,
      "step": 5410
    },
    {
      "epoch": 4.843798882681564,
      "grad_norm": 0.07118017226457596,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5420
    },
    {
      "epoch": 4.852737430167598,
      "grad_norm": 0.07980290055274963,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5430
    },
    {
      "epoch": 4.861675977653631,
      "grad_norm": 0.05949757248163223,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5440
    },
    {
      "epoch": 4.870614525139665,
      "grad_norm": 0.06974615156650543,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5450
    },
    {
      "epoch": 4.879553072625699,
      "grad_norm": 0.05215122550725937,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 5460
    },
    {
      "epoch": 4.888491620111732,
      "grad_norm": 0.06634943187236786,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5470
    },
    {
      "epoch": 4.897430167597765,
      "grad_norm": 0.06404969096183777,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 5480
    },
    {
      "epoch": 4.906368715083799,
      "grad_norm": 0.06260303407907486,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 5490
    },
    {
      "epoch": 4.915307262569833,
      "grad_norm": 0.046542439609766006,
      "learning_rate": 5e-05,
      "loss": 0.0147,
      "step": 5500
    },
    {
      "epoch": 4.924245810055866,
      "grad_norm": 0.0705830529332161,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5510
    },
    {
      "epoch": 4.933184357541899,
      "grad_norm": 0.0964333564043045,
      "learning_rate": 5e-05,
      "loss": 0.0159,
      "step": 5520
    },
    {
      "epoch": 4.942122905027933,
      "grad_norm": 0.04614430293440819,
      "learning_rate": 5e-05,
      "loss": 0.0164,
      "step": 5530
    },
    {
      "epoch": 4.951061452513967,
      "grad_norm": 0.061710529029369354,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5540
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.06951291114091873,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5550
    },
    {
      "epoch": 4.968938547486033,
      "grad_norm": 0.07227683067321777,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 5560
    },
    {
      "epoch": 4.977877094972067,
      "grad_norm": 0.08918789774179459,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 5570
    },
    {
      "epoch": 4.986815642458101,
      "grad_norm": 0.05329503118991852,
      "learning_rate": 5e-05,
      "loss": 0.016,
      "step": 5580
    },
    {
      "epoch": 4.995754189944134,
      "grad_norm": 0.08424188941717148,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 5590
    },
    {
      "epoch": 5.0,
      "eval_AnatEM": 0.495238095189551,
      "eval_FabNER": 0.019097222185512363,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.5862068965010503,
      "eval_MultiNERD": 0.9045936395256521,
      "eval_Ontonotes": 0.904627006559846,
      "eval_TweetNER7": 0.6177777777276862,
      "eval_WikiANN-en": 0.8065099457001644,
      "eval_WikiNeural": 0.8988764044441311,
      "eval_average": 0.7332611186161143,
      "eval_bc2gm": 0.694158075551465,
      "eval_bc4chemd": 0.6531645569117667,
      "eval_bc5cdr": 0.8505747125934904,
      "eval_broad_twitter_corpus": 0.7060931899141615,
      "eval_conll2003": 0.9589743589241135,
      "eval_conllpp": 0.965250965200717,
      "eval_mit-movie": 0.7302533531539953,
      "eval_mit-restaurant": 0.789651293538138,
      "eval_ncbi": 0.896551724087457,
      "eval_runtime": 412.6328,
      "eval_samples_per_second": 8.724,
      "eval_steps_per_second": 0.274,
      "step": 5595
    },
    {
      "epoch": 5.004469273743017,
      "grad_norm": 0.06495565921068192,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5600
    },
    {
      "epoch": 5.01340782122905,
      "grad_norm": 0.060542263090610504,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5610
    },
    {
      "epoch": 5.022346368715084,
      "grad_norm": 0.05932730436325073,
      "learning_rate": 5e-05,
      "loss": 0.0158,
      "step": 5620
    },
    {
      "epoch": 5.031284916201117,
      "grad_norm": 0.05619826540350914,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 5630
    },
    {
      "epoch": 5.040223463687151,
      "grad_norm": 0.11465010792016983,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 5640
    },
    {
      "epoch": 5.049162011173184,
      "grad_norm": 0.04114681854844093,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 5650
    },
    {
      "epoch": 5.058100558659218,
      "grad_norm": 0.057844050228595734,
      "learning_rate": 5e-05,
      "loss": 0.0144,
      "step": 5660
    },
    {
      "epoch": 5.067039106145251,
      "grad_norm": 0.04515876621007919,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 5670
    },
    {
      "epoch": 5.075977653631285,
      "grad_norm": 0.055829934775829315,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 5680
    },
    {
      "epoch": 5.084916201117318,
      "grad_norm": 0.07684481143951416,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 5690
    },
    {
      "epoch": 5.093854748603352,
      "grad_norm": 0.06129546836018562,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 5700
    },
    {
      "epoch": 5.102793296089385,
      "grad_norm": 0.04674190282821655,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 5710
    },
    {
      "epoch": 5.111731843575419,
      "grad_norm": 0.0505216009914875,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5720
    },
    {
      "epoch": 5.120670391061452,
      "grad_norm": 0.06996286660432816,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5730
    },
    {
      "epoch": 5.129608938547486,
      "grad_norm": 0.13403286039829254,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 5740
    },
    {
      "epoch": 5.138547486033519,
      "grad_norm": 0.0486881360411644,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 5750
    },
    {
      "epoch": 5.147486033519553,
      "grad_norm": 0.04681948944926262,
      "learning_rate": 5e-05,
      "loss": 0.0157,
      "step": 5760
    },
    {
      "epoch": 5.156424581005586,
      "grad_norm": 0.06808116286993027,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 5770
    },
    {
      "epoch": 5.16536312849162,
      "grad_norm": 0.06035122275352478,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 5780
    },
    {
      "epoch": 5.174301675977653,
      "grad_norm": 0.07020304352045059,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 5790
    },
    {
      "epoch": 5.183240223463687,
      "grad_norm": 0.04970591515302658,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 5800
    },
    {
      "epoch": 5.19217877094972,
      "grad_norm": 0.04617176577448845,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 5810
    },
    {
      "epoch": 5.201117318435754,
      "grad_norm": 0.05641733855009079,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 5820
    },
    {
      "epoch": 5.210055865921788,
      "grad_norm": 0.05349750071763992,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5830
    },
    {
      "epoch": 5.218994413407821,
      "grad_norm": 0.06226544454693794,
      "learning_rate": 5e-05,
      "loss": 0.0144,
      "step": 5840
    },
    {
      "epoch": 5.227932960893855,
      "grad_norm": 0.05463336035609245,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 5850
    },
    {
      "epoch": 5.236871508379888,
      "grad_norm": 0.050956323742866516,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 5860
    },
    {
      "epoch": 5.245810055865922,
      "grad_norm": 0.06115911900997162,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 5870
    },
    {
      "epoch": 5.254748603351955,
      "grad_norm": 0.05929257348179817,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5880
    },
    {
      "epoch": 5.263687150837989,
      "grad_norm": 0.11525014787912369,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 5890
    },
    {
      "epoch": 5.272625698324022,
      "grad_norm": 0.04855990782380104,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 5900
    },
    {
      "epoch": 5.281564245810056,
      "grad_norm": 0.04680242761969566,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 5910
    },
    {
      "epoch": 5.290502793296089,
      "grad_norm": 0.06271890550851822,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 5920
    },
    {
      "epoch": 5.299441340782123,
      "grad_norm": 0.04422951117157936,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 5930
    },
    {
      "epoch": 5.308379888268156,
      "grad_norm": 0.13217978179454803,
      "learning_rate": 5e-05,
      "loss": 0.0156,
      "step": 5940
    },
    {
      "epoch": 5.31731843575419,
      "grad_norm": 0.05326513573527336,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 5950
    },
    {
      "epoch": 5.326256983240223,
      "grad_norm": 0.041988685727119446,
      "learning_rate": 5e-05,
      "loss": 0.0144,
      "step": 5960
    },
    {
      "epoch": 5.335195530726257,
      "grad_norm": 0.056090980768203735,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 5970
    },
    {
      "epoch": 5.34413407821229,
      "grad_norm": 0.048942986875772476,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 5980
    },
    {
      "epoch": 5.353072625698324,
      "grad_norm": 0.06008031964302063,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 5990
    },
    {
      "epoch": 5.362011173184357,
      "grad_norm": 0.05921143293380737,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6000
    },
    {
      "epoch": 5.370949720670391,
      "grad_norm": 0.06867013871669769,
      "learning_rate": 5e-05,
      "loss": 0.0147,
      "step": 6010
    },
    {
      "epoch": 5.379888268156424,
      "grad_norm": 0.061137475073337555,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 6020
    },
    {
      "epoch": 5.388826815642458,
      "grad_norm": 0.06351587176322937,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 6030
    },
    {
      "epoch": 5.397765363128491,
      "grad_norm": 0.053006429225206375,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 6040
    },
    {
      "epoch": 5.406703910614525,
      "grad_norm": 0.05078910291194916,
      "learning_rate": 5e-05,
      "loss": 0.0155,
      "step": 6050
    },
    {
      "epoch": 5.415642458100558,
      "grad_norm": 0.06527196615934372,
      "learning_rate": 5e-05,
      "loss": 0.0154,
      "step": 6060
    },
    {
      "epoch": 5.424581005586592,
      "grad_norm": 0.067191943526268,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6070
    },
    {
      "epoch": 5.4335195530726255,
      "grad_norm": 0.05320098251104355,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 6080
    },
    {
      "epoch": 5.442458100558659,
      "grad_norm": 0.07152381539344788,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 6090
    },
    {
      "epoch": 5.4513966480446925,
      "grad_norm": 0.05642586573958397,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6100
    },
    {
      "epoch": 5.460335195530726,
      "grad_norm": 0.07099372893571854,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6110
    },
    {
      "epoch": 5.4692737430167595,
      "grad_norm": 0.064338319003582,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 6120
    },
    {
      "epoch": 5.4782122905027935,
      "grad_norm": 0.06006501615047455,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 6130
    },
    {
      "epoch": 5.4871508379888265,
      "grad_norm": 0.05377480015158653,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6140
    },
    {
      "epoch": 5.4960893854748605,
      "grad_norm": 0.050505898892879486,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 6150
    },
    {
      "epoch": 5.5050279329608935,
      "grad_norm": 0.10207324475049973,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 6160
    },
    {
      "epoch": 5.5139664804469275,
      "grad_norm": 0.05361558869481087,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 6170
    },
    {
      "epoch": 5.522905027932961,
      "grad_norm": 0.08535362035036087,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6180
    },
    {
      "epoch": 5.5318435754189945,
      "grad_norm": 0.04632649943232536,
      "learning_rate": 5e-05,
      "loss": 0.0147,
      "step": 6190
    },
    {
      "epoch": 5.540782122905028,
      "grad_norm": 0.09786040335893631,
      "learning_rate": 5e-05,
      "loss": 0.0135,
      "step": 6200
    },
    {
      "epoch": 5.5497206703910615,
      "grad_norm": 0.056132785975933075,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6210
    },
    {
      "epoch": 5.558659217877095,
      "grad_norm": 0.06718161702156067,
      "learning_rate": 5e-05,
      "loss": 0.0152,
      "step": 6220
    },
    {
      "epoch": 5.567597765363129,
      "grad_norm": 0.05938387289643288,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6230
    },
    {
      "epoch": 5.576536312849162,
      "grad_norm": 0.10272454470396042,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 6240
    },
    {
      "epoch": 5.585474860335196,
      "grad_norm": 0.10649467259645462,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 6250
    },
    {
      "epoch": 5.594413407821229,
      "grad_norm": 0.060060687363147736,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 6260
    },
    {
      "epoch": 5.603351955307263,
      "grad_norm": 0.07652319222688675,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 6270
    },
    {
      "epoch": 5.6122905027932966,
      "grad_norm": 0.06992480158805847,
      "learning_rate": 5e-05,
      "loss": 0.0158,
      "step": 6280
    },
    {
      "epoch": 5.62122905027933,
      "grad_norm": 0.07976130396127701,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 6290
    },
    {
      "epoch": 5.630167597765363,
      "grad_norm": 0.0759866014122963,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6300
    },
    {
      "epoch": 5.639106145251397,
      "grad_norm": 0.07904518395662308,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 6310
    },
    {
      "epoch": 5.648044692737431,
      "grad_norm": 0.052858106791973114,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 6320
    },
    {
      "epoch": 5.656983240223464,
      "grad_norm": 0.07951066642999649,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 6330
    },
    {
      "epoch": 5.665921787709497,
      "grad_norm": 0.0503229983150959,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 6340
    },
    {
      "epoch": 5.674860335195531,
      "grad_norm": 0.04900679737329483,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 6350
    },
    {
      "epoch": 5.683798882681565,
      "grad_norm": 0.04648089408874512,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 6360
    },
    {
      "epoch": 5.692737430167598,
      "grad_norm": 0.05180809274315834,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6370
    },
    {
      "epoch": 5.701675977653632,
      "grad_norm": 0.04682798311114311,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6380
    },
    {
      "epoch": 5.710614525139665,
      "grad_norm": 0.040507376194000244,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6390
    },
    {
      "epoch": 5.719553072625699,
      "grad_norm": 0.05329282581806183,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 6400
    },
    {
      "epoch": 5.728491620111732,
      "grad_norm": 0.0754038468003273,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 6410
    },
    {
      "epoch": 5.737430167597766,
      "grad_norm": 0.041678089648485184,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 6420
    },
    {
      "epoch": 5.746368715083799,
      "grad_norm": 0.10341214388608932,
      "learning_rate": 5e-05,
      "loss": 0.015,
      "step": 6430
    },
    {
      "epoch": 5.755307262569833,
      "grad_norm": 0.08065998554229736,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6440
    },
    {
      "epoch": 5.764245810055866,
      "grad_norm": 0.049597498029470444,
      "learning_rate": 5e-05,
      "loss": 0.0149,
      "step": 6450
    },
    {
      "epoch": 5.7731843575419,
      "grad_norm": 0.05292259156703949,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 6460
    },
    {
      "epoch": 5.782122905027933,
      "grad_norm": 0.07510729879140854,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6470
    },
    {
      "epoch": 5.791061452513967,
      "grad_norm": 0.06309475004673004,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 6480
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.09058559685945511,
      "learning_rate": 5e-05,
      "loss": 0.0147,
      "step": 6490
    },
    {
      "epoch": 5.808938547486034,
      "grad_norm": 0.1058143898844719,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6500
    },
    {
      "epoch": 5.817877094972067,
      "grad_norm": 0.0771198496222496,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6510
    },
    {
      "epoch": 5.826815642458101,
      "grad_norm": 0.08364948630332947,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 6520
    },
    {
      "epoch": 5.835754189944134,
      "grad_norm": 0.04818226769566536,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 6530
    },
    {
      "epoch": 5.844692737430168,
      "grad_norm": 0.05314639210700989,
      "learning_rate": 5e-05,
      "loss": 0.0148,
      "step": 6540
    },
    {
      "epoch": 5.853631284916201,
      "grad_norm": 0.04522636905312538,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 6550
    },
    {
      "epoch": 5.862569832402235,
      "grad_norm": 0.06499237567186356,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6560
    },
    {
      "epoch": 5.871508379888268,
      "grad_norm": 0.04954953491687775,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 6570
    },
    {
      "epoch": 5.880446927374302,
      "grad_norm": 0.08075820654630661,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 6580
    },
    {
      "epoch": 5.889385474860335,
      "grad_norm": 0.044531356543302536,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6590
    },
    {
      "epoch": 5.898324022346369,
      "grad_norm": 0.04869179427623749,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 6600
    },
    {
      "epoch": 5.907262569832402,
      "grad_norm": 0.06986898183822632,
      "learning_rate": 5e-05,
      "loss": 0.0146,
      "step": 6610
    },
    {
      "epoch": 5.916201117318436,
      "grad_norm": 0.04823469743132591,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 6620
    },
    {
      "epoch": 5.925139664804469,
      "grad_norm": 0.04982394352555275,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 6630
    },
    {
      "epoch": 5.934078212290503,
      "grad_norm": 0.05927162617444992,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 6640
    },
    {
      "epoch": 5.943016759776536,
      "grad_norm": 0.06863203644752502,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6650
    },
    {
      "epoch": 5.95195530726257,
      "grad_norm": 0.04899239540100098,
      "learning_rate": 5e-05,
      "loss": 0.0135,
      "step": 6660
    },
    {
      "epoch": 5.960893854748603,
      "grad_norm": 0.06128808483481407,
      "learning_rate": 5e-05,
      "loss": 0.0144,
      "step": 6670
    },
    {
      "epoch": 5.969832402234637,
      "grad_norm": 0.0979231521487236,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6680
    },
    {
      "epoch": 5.97877094972067,
      "grad_norm": 0.06690818071365356,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 6690
    },
    {
      "epoch": 5.987709497206704,
      "grad_norm": 0.07186027616262436,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6700
    },
    {
      "epoch": 5.996648044692737,
      "grad_norm": 0.11855007708072662,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6710
    },
    {
      "epoch": 6.0,
      "eval_AnatEM": 0.6373626373117256,
      "eval_FabNER": 0.024432809736825413,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.644444444393784,
      "eval_MultiNERD": 0.8958333332830373,
      "eval_Ontonotes": 0.909433962213988,
      "eval_TweetNER7": 0.6210995541846779,
      "eval_WikiANN-en": 0.8014571948495262,
      "eval_WikiNeural": 0.9057665259694366,
      "eval_average": 0.7439565674003673,
      "eval_bc2gm": 0.7117437721917658,
      "eval_bc4chemd": 0.6940874035487606,
      "eval_bc5cdr": 0.8431618569134877,
      "eval_broad_twitter_corpus": 0.6739130434283639,
      "eval_conll2003": 0.9615384614882153,
      "eval_conllpp": 0.9627727855723459,
      "eval_mit-movie": 0.7291666666164855,
      "eval_mit-restaurant": 0.79596412551038,
      "eval_ncbi": 0.8579387186126459,
      "eval_runtime": 417.4955,
      "eval_samples_per_second": 8.623,
      "eval_steps_per_second": 0.271,
      "step": 6714
    },
    {
      "epoch": 6.00536312849162,
      "grad_norm": 0.053826954215765,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 6720
    },
    {
      "epoch": 6.014301675977654,
      "grad_norm": 0.045169148594141006,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 6730
    },
    {
      "epoch": 6.023240223463687,
      "grad_norm": 0.12730932235717773,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 6740
    },
    {
      "epoch": 6.032178770949721,
      "grad_norm": 0.07216012477874756,
      "learning_rate": 5e-05,
      "loss": 0.0142,
      "step": 6750
    },
    {
      "epoch": 6.041117318435754,
      "grad_norm": 0.053454503417015076,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 6760
    },
    {
      "epoch": 6.050055865921788,
      "grad_norm": 0.06314307451248169,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 6770
    },
    {
      "epoch": 6.058994413407821,
      "grad_norm": 0.09096784144639969,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 6780
    },
    {
      "epoch": 6.067932960893855,
      "grad_norm": 0.04754931107163429,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 6790
    },
    {
      "epoch": 6.076871508379888,
      "grad_norm": 0.0491979643702507,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 6800
    },
    {
      "epoch": 6.085810055865922,
      "grad_norm": 0.051739953458309174,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 6810
    },
    {
      "epoch": 6.094748603351955,
      "grad_norm": 0.07115407288074493,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 6820
    },
    {
      "epoch": 6.103687150837989,
      "grad_norm": 0.054596882313489914,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 6830
    },
    {
      "epoch": 6.112625698324022,
      "grad_norm": 0.057797349989414215,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6840
    },
    {
      "epoch": 6.121564245810056,
      "grad_norm": 0.11564368009567261,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6850
    },
    {
      "epoch": 6.130502793296089,
      "grad_norm": 0.05243533104658127,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 6860
    },
    {
      "epoch": 6.139441340782123,
      "grad_norm": 0.0550055205821991,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 6870
    },
    {
      "epoch": 6.148379888268156,
      "grad_norm": 0.05049854516983032,
      "learning_rate": 5e-05,
      "loss": 0.0135,
      "step": 6880
    },
    {
      "epoch": 6.15731843575419,
      "grad_norm": 0.06307535618543625,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 6890
    },
    {
      "epoch": 6.166256983240223,
      "grad_norm": 0.06628455966711044,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 6900
    },
    {
      "epoch": 6.175195530726257,
      "grad_norm": 0.06608272343873978,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 6910
    },
    {
      "epoch": 6.18413407821229,
      "grad_norm": 0.06589428335428238,
      "learning_rate": 5e-05,
      "loss": 0.0145,
      "step": 6920
    },
    {
      "epoch": 6.193072625698324,
      "grad_norm": 0.07328581064939499,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 6930
    },
    {
      "epoch": 6.202011173184357,
      "grad_norm": 0.04507888853549957,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 6940
    },
    {
      "epoch": 6.210949720670391,
      "grad_norm": 0.04735711216926575,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 6950
    },
    {
      "epoch": 6.219888268156424,
      "grad_norm": 0.06076807901263237,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 6960
    },
    {
      "epoch": 6.228826815642458,
      "grad_norm": 0.05383080989122391,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 6970
    },
    {
      "epoch": 6.237765363128491,
      "grad_norm": 0.03860091418027878,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 6980
    },
    {
      "epoch": 6.246703910614525,
      "grad_norm": 0.05477886274456978,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 6990
    },
    {
      "epoch": 6.255642458100558,
      "grad_norm": 0.06558316946029663,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 7000
    },
    {
      "epoch": 6.264581005586592,
      "grad_norm": 0.04795249551534653,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 7010
    },
    {
      "epoch": 6.273519553072625,
      "grad_norm": 0.08383374661207199,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7020
    },
    {
      "epoch": 6.282458100558659,
      "grad_norm": 0.06356168538331985,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7030
    },
    {
      "epoch": 6.291396648044692,
      "grad_norm": 0.05114109069108963,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 7040
    },
    {
      "epoch": 6.300335195530726,
      "grad_norm": 0.05215233936905861,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 7050
    },
    {
      "epoch": 6.309273743016759,
      "grad_norm": 0.08460520207881927,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 7060
    },
    {
      "epoch": 6.318212290502793,
      "grad_norm": 0.05039995163679123,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 7070
    },
    {
      "epoch": 6.327150837988826,
      "grad_norm": 0.06894014030694962,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 7080
    },
    {
      "epoch": 6.33608938547486,
      "grad_norm": 0.05307549238204956,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 7090
    },
    {
      "epoch": 6.345027932960894,
      "grad_norm": 0.06600427627563477,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 7100
    },
    {
      "epoch": 6.353966480446927,
      "grad_norm": 0.06507733464241028,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 7110
    },
    {
      "epoch": 6.36290502793296,
      "grad_norm": 0.03873063996434212,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7120
    },
    {
      "epoch": 6.371843575418994,
      "grad_norm": 0.04512146860361099,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 7130
    },
    {
      "epoch": 6.380782122905028,
      "grad_norm": 0.049132268875837326,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 7140
    },
    {
      "epoch": 6.389720670391061,
      "grad_norm": 0.05476585030555725,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 7150
    },
    {
      "epoch": 6.398659217877095,
      "grad_norm": 0.04400996118783951,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 7160
    },
    {
      "epoch": 6.407597765363128,
      "grad_norm": 0.09005684405565262,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 7170
    },
    {
      "epoch": 6.416536312849162,
      "grad_norm": 0.06949735432863235,
      "learning_rate": 5e-05,
      "loss": 0.0135,
      "step": 7180
    },
    {
      "epoch": 6.425474860335195,
      "grad_norm": 0.05231619253754616,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 7190
    },
    {
      "epoch": 6.434413407821229,
      "grad_norm": 0.059064578264951706,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 7200
    },
    {
      "epoch": 6.4433519553072625,
      "grad_norm": 0.057739194482564926,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 7210
    },
    {
      "epoch": 6.452290502793296,
      "grad_norm": 0.05012756586074829,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 7220
    },
    {
      "epoch": 6.4612290502793295,
      "grad_norm": 0.06277350336313248,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7230
    },
    {
      "epoch": 6.470167597765363,
      "grad_norm": 0.12440746277570724,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7240
    },
    {
      "epoch": 6.4791061452513965,
      "grad_norm": 0.048576731234788895,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 7250
    },
    {
      "epoch": 6.4880446927374305,
      "grad_norm": 0.07756438851356506,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 7260
    },
    {
      "epoch": 6.4969832402234635,
      "grad_norm": 0.09138619899749756,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 7270
    },
    {
      "epoch": 6.5059217877094975,
      "grad_norm": 0.053920723497867584,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 7280
    },
    {
      "epoch": 6.5148603351955305,
      "grad_norm": 0.04642247408628464,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 7290
    },
    {
      "epoch": 6.5237988826815645,
      "grad_norm": 0.08366930484771729,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 7300
    },
    {
      "epoch": 6.532737430167598,
      "grad_norm": 0.10611703991889954,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 7310
    },
    {
      "epoch": 6.5416759776536315,
      "grad_norm": 0.061463844031095505,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 7320
    },
    {
      "epoch": 6.550614525139665,
      "grad_norm": 0.053980790078639984,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 7330
    },
    {
      "epoch": 6.5595530726256985,
      "grad_norm": 0.048147231340408325,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 7340
    },
    {
      "epoch": 6.568491620111732,
      "grad_norm": 0.04429398477077484,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 7350
    },
    {
      "epoch": 6.5774301675977656,
      "grad_norm": 0.08326467871665955,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 7360
    },
    {
      "epoch": 6.586368715083799,
      "grad_norm": 0.05710627883672714,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7370
    },
    {
      "epoch": 6.595307262569833,
      "grad_norm": 0.0432669073343277,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 7380
    },
    {
      "epoch": 6.604245810055866,
      "grad_norm": 0.1025797426700592,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 7390
    },
    {
      "epoch": 6.6131843575419,
      "grad_norm": 0.04250428080558777,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7400
    },
    {
      "epoch": 6.622122905027933,
      "grad_norm": 0.09912525862455368,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7410
    },
    {
      "epoch": 6.631061452513967,
      "grad_norm": 0.12958674132823944,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 7420
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.0520484484732151,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 7430
    },
    {
      "epoch": 6.648938547486034,
      "grad_norm": 0.03762315213680267,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 7440
    },
    {
      "epoch": 6.657877094972067,
      "grad_norm": 0.03857412934303284,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 7450
    },
    {
      "epoch": 6.666815642458101,
      "grad_norm": 0.05614443123340607,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 7460
    },
    {
      "epoch": 6.675754189944134,
      "grad_norm": 0.04923378676176071,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 7470
    },
    {
      "epoch": 6.684692737430168,
      "grad_norm": 0.06610307842493057,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7480
    },
    {
      "epoch": 6.693631284916201,
      "grad_norm": 0.04326480254530907,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7490
    },
    {
      "epoch": 6.702569832402235,
      "grad_norm": 0.07104115933179855,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7500
    },
    {
      "epoch": 6.711508379888269,
      "grad_norm": 0.12280949205160141,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 7510
    },
    {
      "epoch": 6.720446927374302,
      "grad_norm": 0.047347426414489746,
      "learning_rate": 5e-05,
      "loss": 0.0153,
      "step": 7520
    },
    {
      "epoch": 6.729385474860335,
      "grad_norm": 0.08965204656124115,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 7530
    },
    {
      "epoch": 6.738324022346369,
      "grad_norm": 0.041965022683143616,
      "learning_rate": 5e-05,
      "loss": 0.0143,
      "step": 7540
    },
    {
      "epoch": 6.747262569832403,
      "grad_norm": 0.06718753278255463,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 7550
    },
    {
      "epoch": 6.756201117318436,
      "grad_norm": 0.05305391177535057,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7560
    },
    {
      "epoch": 6.765139664804469,
      "grad_norm": 0.03952297195792198,
      "learning_rate": 5e-05,
      "loss": 0.0138,
      "step": 7570
    },
    {
      "epoch": 6.774078212290503,
      "grad_norm": 0.05765474587678909,
      "learning_rate": 5e-05,
      "loss": 0.014,
      "step": 7580
    },
    {
      "epoch": 6.783016759776537,
      "grad_norm": 0.04436797648668289,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 7590
    },
    {
      "epoch": 6.79195530726257,
      "grad_norm": 0.07675573974847794,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7600
    },
    {
      "epoch": 6.800893854748603,
      "grad_norm": 0.04611947387456894,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 7610
    },
    {
      "epoch": 6.809832402234637,
      "grad_norm": 0.042060110718011856,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 7620
    },
    {
      "epoch": 6.818770949720671,
      "grad_norm": 0.0594911202788353,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 7630
    },
    {
      "epoch": 6.827709497206704,
      "grad_norm": 0.04003332182765007,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7640
    },
    {
      "epoch": 6.836648044692738,
      "grad_norm": 0.05259152129292488,
      "learning_rate": 5e-05,
      "loss": 0.0137,
      "step": 7650
    },
    {
      "epoch": 6.845586592178771,
      "grad_norm": 0.04721738398075104,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 7660
    },
    {
      "epoch": 6.854525139664805,
      "grad_norm": 0.040166739374399185,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7670
    },
    {
      "epoch": 6.863463687150838,
      "grad_norm": 0.04406600818037987,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7680
    },
    {
      "epoch": 6.872402234636872,
      "grad_norm": 0.06469191610813141,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 7690
    },
    {
      "epoch": 6.881340782122905,
      "grad_norm": 0.054716773331165314,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 7700
    },
    {
      "epoch": 6.890279329608939,
      "grad_norm": 0.039335697889328,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 7710
    },
    {
      "epoch": 6.899217877094972,
      "grad_norm": 0.0561070442199707,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 7720
    },
    {
      "epoch": 6.908156424581006,
      "grad_norm": 0.04602069407701492,
      "learning_rate": 5e-05,
      "loss": 0.0151,
      "step": 7730
    },
    {
      "epoch": 6.917094972067039,
      "grad_norm": 0.05043197050690651,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 7740
    },
    {
      "epoch": 6.926033519553073,
      "grad_norm": 0.043026749044656754,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 7750
    },
    {
      "epoch": 6.934972067039106,
      "grad_norm": 0.047134269028902054,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7760
    },
    {
      "epoch": 6.94391061452514,
      "grad_norm": 0.0467156283557415,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 7770
    },
    {
      "epoch": 6.952849162011173,
      "grad_norm": 0.0625816136598587,
      "learning_rate": 5e-05,
      "loss": 0.0141,
      "step": 7780
    },
    {
      "epoch": 6.961787709497207,
      "grad_norm": 0.054084669798612595,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 7790
    },
    {
      "epoch": 6.97072625698324,
      "grad_norm": 0.0468403659760952,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 7800
    },
    {
      "epoch": 6.979664804469274,
      "grad_norm": 0.05662177503108978,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 7810
    },
    {
      "epoch": 6.988603351955307,
      "grad_norm": 0.06686495989561081,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7820
    },
    {
      "epoch": 6.997541899441341,
      "grad_norm": 0.05185149237513542,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 7830
    },
    {
      "epoch": 7.0,
      "eval_AnatEM": 0.6304347825579159,
      "eval_FabNER": 0.015761821330008342,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.6256983239716862,
      "eval_MultiNERD": 0.8978873238933465,
      "eval_Ontonotes": 0.9128787878286193,
      "eval_TweetNER7": 0.6398176291292674,
      "eval_WikiANN-en": 0.8160291438476992,
      "eval_WikiNeural": 0.9093484418760884,
      "eval_average": 0.751945921717772,
      "eval_bc2gm": 0.7446808510136059,
      "eval_bc4chemd": 0.7296587926008253,
      "eval_bc5cdr": 0.8355091383309833,
      "eval_broad_twitter_corpus": 0.6614481408513861,
      "eval_conll2003": 0.9730423619523177,
      "eval_conllpp": 0.9730423619523177,
      "eval_mit-movie": 0.7398496240099985,
      "eval_mit-restaurant": 0.7950450449948785,
      "eval_ncbi": 0.9137931033977921,
      "eval_runtime": 414.2881,
      "eval_samples_per_second": 8.69,
      "eval_steps_per_second": 0.273,
      "step": 7833
    },
    {
      "epoch": 7.006256983240223,
      "grad_norm": 0.047635097056627274,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 7840
    },
    {
      "epoch": 7.015195530726257,
      "grad_norm": 0.050842396914958954,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 7850
    },
    {
      "epoch": 7.02413407821229,
      "grad_norm": 0.0416795015335083,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7860
    },
    {
      "epoch": 7.033072625698324,
      "grad_norm": 0.07627113163471222,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 7870
    },
    {
      "epoch": 7.042011173184358,
      "grad_norm": 0.05475622043013573,
      "learning_rate": 5e-05,
      "loss": 0.0139,
      "step": 7880
    },
    {
      "epoch": 7.050949720670391,
      "grad_norm": 0.05688629671931267,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 7890
    },
    {
      "epoch": 7.059888268156425,
      "grad_norm": 0.05510402470827103,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 7900
    },
    {
      "epoch": 7.068826815642458,
      "grad_norm": 0.0371435321867466,
      "learning_rate": 5e-05,
      "loss": 0.0136,
      "step": 7910
    },
    {
      "epoch": 7.077765363128492,
      "grad_norm": 0.056912537664175034,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 7920
    },
    {
      "epoch": 7.086703910614525,
      "grad_norm": 0.04865361377596855,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7930
    },
    {
      "epoch": 7.095642458100559,
      "grad_norm": 0.04857444018125534,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 7940
    },
    {
      "epoch": 7.104581005586592,
      "grad_norm": 0.06291568279266357,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 7950
    },
    {
      "epoch": 7.113519553072626,
      "grad_norm": 0.0475720576941967,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 7960
    },
    {
      "epoch": 7.122458100558659,
      "grad_norm": 0.06256259977817535,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 7970
    },
    {
      "epoch": 7.131396648044693,
      "grad_norm": 0.062218666076660156,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 7980
    },
    {
      "epoch": 7.140335195530726,
      "grad_norm": 0.04308442771434784,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 7990
    },
    {
      "epoch": 7.14927374301676,
      "grad_norm": 0.06309285014867783,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 8000
    },
    {
      "epoch": 7.158212290502793,
      "grad_norm": 0.09882111847400665,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8010
    },
    {
      "epoch": 7.167150837988827,
      "grad_norm": 0.04331418126821518,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 8020
    },
    {
      "epoch": 7.17608938547486,
      "grad_norm": 0.04430776834487915,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 8030
    },
    {
      "epoch": 7.185027932960894,
      "grad_norm": 0.08989149332046509,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8040
    },
    {
      "epoch": 7.193966480446927,
      "grad_norm": 0.06906475126743317,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 8050
    },
    {
      "epoch": 7.202905027932961,
      "grad_norm": 0.07158166170120239,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 8060
    },
    {
      "epoch": 7.211843575418994,
      "grad_norm": 0.04997288063168526,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 8070
    },
    {
      "epoch": 7.220782122905028,
      "grad_norm": 0.040854062885046005,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 8080
    },
    {
      "epoch": 7.229720670391061,
      "grad_norm": 0.04698793590068817,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8090
    },
    {
      "epoch": 7.238659217877095,
      "grad_norm": 0.05198858305811882,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 8100
    },
    {
      "epoch": 7.247597765363128,
      "grad_norm": 0.04848574846982956,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 8110
    },
    {
      "epoch": 7.256536312849162,
      "grad_norm": 0.06316719204187393,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 8120
    },
    {
      "epoch": 7.265474860335195,
      "grad_norm": 0.05002933740615845,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 8130
    },
    {
      "epoch": 7.274413407821229,
      "grad_norm": 0.1311350166797638,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 8140
    },
    {
      "epoch": 7.283351955307262,
      "grad_norm": 0.05794018507003784,
      "learning_rate": 5e-05,
      "loss": 0.0135,
      "step": 8150
    },
    {
      "epoch": 7.292290502793296,
      "grad_norm": 0.060770876705646515,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 8160
    },
    {
      "epoch": 7.301229050279329,
      "grad_norm": 0.08373558521270752,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 8170
    },
    {
      "epoch": 7.310167597765363,
      "grad_norm": 0.051591940224170685,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 8180
    },
    {
      "epoch": 7.319106145251396,
      "grad_norm": 0.04777263104915619,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 8190
    },
    {
      "epoch": 7.32804469273743,
      "grad_norm": 0.062057577073574066,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8200
    },
    {
      "epoch": 7.336983240223463,
      "grad_norm": 0.06614775955677032,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 8210
    },
    {
      "epoch": 7.345921787709497,
      "grad_norm": 0.044900164008140564,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 8220
    },
    {
      "epoch": 7.35486033519553,
      "grad_norm": 0.04421132057905197,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 8230
    },
    {
      "epoch": 7.363798882681564,
      "grad_norm": 0.0499531589448452,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 8240
    },
    {
      "epoch": 7.372737430167597,
      "grad_norm": 0.043988343328237534,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8250
    },
    {
      "epoch": 7.381675977653631,
      "grad_norm": 0.06523681432008743,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 8260
    },
    {
      "epoch": 7.390614525139664,
      "grad_norm": 0.06447627395391464,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 8270
    },
    {
      "epoch": 7.399553072625698,
      "grad_norm": 0.04586800932884216,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 8280
    },
    {
      "epoch": 7.4084916201117315,
      "grad_norm": 0.042171087116003036,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 8290
    },
    {
      "epoch": 7.417430167597765,
      "grad_norm": 0.06860937178134918,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 8300
    },
    {
      "epoch": 7.4263687150837985,
      "grad_norm": 0.05998002365231514,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8310
    },
    {
      "epoch": 7.435307262569832,
      "grad_norm": 0.061544761061668396,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 8320
    },
    {
      "epoch": 7.444245810055866,
      "grad_norm": 0.040425729006528854,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 8330
    },
    {
      "epoch": 7.4531843575418995,
      "grad_norm": 0.04951008781790733,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8340
    },
    {
      "epoch": 7.4621229050279325,
      "grad_norm": 0.04805770516395569,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8350
    },
    {
      "epoch": 7.4710614525139665,
      "grad_norm": 0.053085807710886,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8360
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.05567792430520058,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 8370
    },
    {
      "epoch": 7.4889385474860335,
      "grad_norm": 0.09692912548780441,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 8380
    },
    {
      "epoch": 7.4978770949720674,
      "grad_norm": 0.05534759536385536,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 8390
    },
    {
      "epoch": 7.5068156424581005,
      "grad_norm": 0.06236272305250168,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 8400
    },
    {
      "epoch": 7.5157541899441345,
      "grad_norm": 0.045854322612285614,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8410
    },
    {
      "epoch": 7.5246927374301675,
      "grad_norm": 0.04586983472108841,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 8420
    },
    {
      "epoch": 7.5336312849162015,
      "grad_norm": 0.04333603382110596,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 8430
    },
    {
      "epoch": 7.5425698324022346,
      "grad_norm": 0.08484555035829544,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8440
    },
    {
      "epoch": 7.5515083798882685,
      "grad_norm": 0.04845423623919487,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8450
    },
    {
      "epoch": 7.560446927374302,
      "grad_norm": 0.05157897248864174,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8460
    },
    {
      "epoch": 7.5693854748603355,
      "grad_norm": 0.1007465049624443,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 8470
    },
    {
      "epoch": 7.578324022346369,
      "grad_norm": 0.0406373105943203,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8480
    },
    {
      "epoch": 7.5872625698324025,
      "grad_norm": 0.044166576117277145,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 8490
    },
    {
      "epoch": 7.596201117318436,
      "grad_norm": 0.053141575306653976,
      "learning_rate": 5e-05,
      "loss": 0.0119,
      "step": 8500
    },
    {
      "epoch": 7.60513966480447,
      "grad_norm": 0.047667715698480606,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 8510
    },
    {
      "epoch": 7.614078212290503,
      "grad_norm": 0.07467934489250183,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8520
    },
    {
      "epoch": 7.623016759776537,
      "grad_norm": 0.047545790672302246,
      "learning_rate": 5e-05,
      "loss": 0.013,
      "step": 8530
    },
    {
      "epoch": 7.63195530726257,
      "grad_norm": 0.04951932281255722,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8540
    },
    {
      "epoch": 7.640893854748604,
      "grad_norm": 0.04784753918647766,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8550
    },
    {
      "epoch": 7.649832402234637,
      "grad_norm": 0.06985203921794891,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 8560
    },
    {
      "epoch": 7.658770949720671,
      "grad_norm": 0.04569092392921448,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 8570
    },
    {
      "epoch": 7.667709497206704,
      "grad_norm": 0.05140741169452667,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8580
    },
    {
      "epoch": 7.676648044692738,
      "grad_norm": 0.046633124351501465,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 8590
    },
    {
      "epoch": 7.685586592178771,
      "grad_norm": 0.0394902266561985,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8600
    },
    {
      "epoch": 7.694525139664805,
      "grad_norm": 0.05019938200712204,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 8610
    },
    {
      "epoch": 7.703463687150838,
      "grad_norm": 0.0558980256319046,
      "learning_rate": 5e-05,
      "loss": 0.0132,
      "step": 8620
    },
    {
      "epoch": 7.712402234636872,
      "grad_norm": 0.042042069137096405,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 8630
    },
    {
      "epoch": 7.721340782122905,
      "grad_norm": 0.10070327669382095,
      "learning_rate": 5e-05,
      "loss": 0.0131,
      "step": 8640
    },
    {
      "epoch": 7.730279329608939,
      "grad_norm": 0.05857079103589058,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 8650
    },
    {
      "epoch": 7.739217877094972,
      "grad_norm": 0.05998248979449272,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 8660
    },
    {
      "epoch": 7.748156424581006,
      "grad_norm": 0.039589982479810715,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8670
    },
    {
      "epoch": 7.757094972067039,
      "grad_norm": 0.041707612574100494,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 8680
    },
    {
      "epoch": 7.766033519553073,
      "grad_norm": 0.04978235438466072,
      "learning_rate": 5e-05,
      "loss": 0.0133,
      "step": 8690
    },
    {
      "epoch": 7.774972067039106,
      "grad_norm": 0.06172997131943703,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 8700
    },
    {
      "epoch": 7.78391061452514,
      "grad_norm": 0.057844530791044235,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 8710
    },
    {
      "epoch": 7.792849162011173,
      "grad_norm": 0.04215551167726517,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 8720
    },
    {
      "epoch": 7.801787709497207,
      "grad_norm": 0.07681286334991455,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 8730
    },
    {
      "epoch": 7.81072625698324,
      "grad_norm": 0.1360873430967331,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 8740
    },
    {
      "epoch": 7.819664804469274,
      "grad_norm": 0.06638185679912567,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 8750
    },
    {
      "epoch": 7.828603351955307,
      "grad_norm": 0.05664951726794243,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 8760
    },
    {
      "epoch": 7.837541899441341,
      "grad_norm": 0.048195332288742065,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 8770
    },
    {
      "epoch": 7.846480446927375,
      "grad_norm": 0.05579643324017525,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8780
    },
    {
      "epoch": 7.855418994413408,
      "grad_norm": 0.04901966080069542,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 8790
    },
    {
      "epoch": 7.864357541899441,
      "grad_norm": 0.095386803150177,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 8800
    },
    {
      "epoch": 7.873296089385475,
      "grad_norm": 0.0458810068666935,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 8810
    },
    {
      "epoch": 7.882234636871509,
      "grad_norm": 0.07677838206291199,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8820
    },
    {
      "epoch": 7.891173184357542,
      "grad_norm": 0.04173871502280235,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 8830
    },
    {
      "epoch": 7.900111731843575,
      "grad_norm": 0.04038482531905174,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 8840
    },
    {
      "epoch": 7.909050279329609,
      "grad_norm": 0.08390922099351883,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 8850
    },
    {
      "epoch": 7.917988826815643,
      "grad_norm": 0.06370270997285843,
      "learning_rate": 5e-05,
      "loss": 0.0128,
      "step": 8860
    },
    {
      "epoch": 7.926927374301676,
      "grad_norm": 0.08166873455047607,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 8870
    },
    {
      "epoch": 7.935865921787709,
      "grad_norm": 0.18762104213237762,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 8880
    },
    {
      "epoch": 7.944804469273743,
      "grad_norm": 0.07160831987857819,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 8890
    },
    {
      "epoch": 7.953743016759777,
      "grad_norm": 0.04349366948008537,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 8900
    },
    {
      "epoch": 7.96268156424581,
      "grad_norm": 0.23762290179729462,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 8910
    },
    {
      "epoch": 7.971620111731844,
      "grad_norm": 0.10508806258440018,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 8920
    },
    {
      "epoch": 7.980558659217877,
      "grad_norm": 0.046914227306842804,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 8930
    },
    {
      "epoch": 7.989497206703911,
      "grad_norm": 0.050590209662914276,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8940
    },
    {
      "epoch": 7.998435754189944,
      "grad_norm": 0.05594617873430252,
      "learning_rate": 5e-05,
      "loss": 0.0134,
      "step": 8950
    },
    {
      "epoch": 8.0,
      "eval_AnatEM": 0.5918367346440024,
      "eval_FabNER": 0.010762331804576322,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.6702702702197225,
      "eval_MultiNERD": 0.9033391915138316,
      "eval_Ontonotes": 0.9113207546668178,
      "eval_TweetNER7": 0.618526622852912,
      "eval_WikiANN-en": 0.8260869564714426,
      "eval_WikiNeural": 0.9083215796394477,
      "eval_average": 0.7529238250561032,
      "eval_bc2gm": 0.7428571428068673,
      "eval_bc4chemd": 0.7499999999498387,
      "eval_bc5cdr": 0.8426527957885345,
      "eval_broad_twitter_corpus": 0.6642335765925096,
      "eval_conll2003": 0.9743589743087249,
      "eval_conllpp": 0.9743589743087249,
      "eval_mit-movie": 0.7369985140657119,
      "eval_mit-restaurant": 0.7891770010772284,
      "eval_ncbi": 0.9164265129178052,
      "eval_runtime": 410.3363,
      "eval_samples_per_second": 8.773,
      "eval_steps_per_second": 0.275,
      "step": 8952
    },
    {
      "epoch": 8.007150837988826,
      "grad_norm": 0.06557919830083847,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 8960
    },
    {
      "epoch": 8.01608938547486,
      "grad_norm": 0.04149052873253822,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 8970
    },
    {
      "epoch": 8.025027932960894,
      "grad_norm": 0.04587254300713539,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 8980
    },
    {
      "epoch": 8.033966480446928,
      "grad_norm": 0.049320414662361145,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 8990
    },
    {
      "epoch": 8.04290502793296,
      "grad_norm": 0.06360862404108047,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 9000
    },
    {
      "epoch": 8.051843575418994,
      "grad_norm": 0.06800028681755066,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 9010
    },
    {
      "epoch": 8.060782122905028,
      "grad_norm": 0.05220181122422218,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 9020
    },
    {
      "epoch": 8.069720670391062,
      "grad_norm": 0.04126646742224693,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 9030
    },
    {
      "epoch": 8.078659217877094,
      "grad_norm": 0.07885920256376266,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9040
    },
    {
      "epoch": 8.087597765363128,
      "grad_norm": 0.05119401589035988,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9050
    },
    {
      "epoch": 8.096536312849162,
      "grad_norm": 0.05566345900297165,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9060
    },
    {
      "epoch": 8.105474860335196,
      "grad_norm": 0.039457887411117554,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9070
    },
    {
      "epoch": 8.114413407821228,
      "grad_norm": 0.06282570958137512,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 9080
    },
    {
      "epoch": 8.123351955307262,
      "grad_norm": 0.05369266867637634,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9090
    },
    {
      "epoch": 8.132290502793296,
      "grad_norm": 0.046366170048713684,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 9100
    },
    {
      "epoch": 8.14122905027933,
      "grad_norm": 0.05360233783721924,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 9110
    },
    {
      "epoch": 8.150167597765362,
      "grad_norm": 0.05887829512357712,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 9120
    },
    {
      "epoch": 8.159106145251396,
      "grad_norm": 0.08651210367679596,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 9130
    },
    {
      "epoch": 8.16804469273743,
      "grad_norm": 0.052127763628959656,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 9140
    },
    {
      "epoch": 8.176983240223464,
      "grad_norm": 0.05299254134297371,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 9150
    },
    {
      "epoch": 8.185921787709498,
      "grad_norm": 0.03459654375910759,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9160
    },
    {
      "epoch": 8.19486033519553,
      "grad_norm": 0.04694093391299248,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9170
    },
    {
      "epoch": 8.203798882681564,
      "grad_norm": 0.04086066409945488,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9180
    },
    {
      "epoch": 8.212737430167598,
      "grad_norm": 0.05105803906917572,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9190
    },
    {
      "epoch": 8.221675977653632,
      "grad_norm": 0.03519224375486374,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 9200
    },
    {
      "epoch": 8.230614525139664,
      "grad_norm": 0.045906927436590195,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 9210
    },
    {
      "epoch": 8.239553072625698,
      "grad_norm": 0.05398601293563843,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 9220
    },
    {
      "epoch": 8.248491620111732,
      "grad_norm": 0.04690544307231903,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 9230
    },
    {
      "epoch": 8.257430167597766,
      "grad_norm": 0.07355736196041107,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9240
    },
    {
      "epoch": 8.266368715083798,
      "grad_norm": 0.07872141152620316,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9250
    },
    {
      "epoch": 8.275307262569832,
      "grad_norm": 0.058966439217329025,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9260
    },
    {
      "epoch": 8.284245810055866,
      "grad_norm": 0.051345035433769226,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 9270
    },
    {
      "epoch": 8.2931843575419,
      "grad_norm": 0.0718325674533844,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9280
    },
    {
      "epoch": 8.302122905027932,
      "grad_norm": 0.09305808693170547,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 9290
    },
    {
      "epoch": 8.311061452513966,
      "grad_norm": 0.04877157881855965,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 9300
    },
    {
      "epoch": 8.32,
      "grad_norm": 0.05643497779965401,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 9310
    },
    {
      "epoch": 8.328938547486034,
      "grad_norm": 0.06130095571279526,
      "learning_rate": 5e-05,
      "loss": 0.0126,
      "step": 9320
    },
    {
      "epoch": 8.337877094972066,
      "grad_norm": 0.04476438835263252,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 9330
    },
    {
      "epoch": 8.3468156424581,
      "grad_norm": 0.053418129682540894,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 9340
    },
    {
      "epoch": 8.355754189944134,
      "grad_norm": 0.04605856537818909,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9350
    },
    {
      "epoch": 8.364692737430168,
      "grad_norm": 0.05601014196872711,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9360
    },
    {
      "epoch": 8.3736312849162,
      "grad_norm": 0.04252716526389122,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9370
    },
    {
      "epoch": 8.382569832402234,
      "grad_norm": 0.05986237898468971,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9380
    },
    {
      "epoch": 8.391508379888268,
      "grad_norm": 0.04209395498037338,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9390
    },
    {
      "epoch": 8.400446927374302,
      "grad_norm": 0.04672703519463539,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 9400
    },
    {
      "epoch": 8.409385474860334,
      "grad_norm": 0.037030644714832306,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9410
    },
    {
      "epoch": 8.418324022346368,
      "grad_norm": 0.040957558900117874,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9420
    },
    {
      "epoch": 8.427262569832402,
      "grad_norm": 0.03994130715727806,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 9430
    },
    {
      "epoch": 8.436201117318436,
      "grad_norm": 0.03985917195677757,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9440
    },
    {
      "epoch": 8.445139664804469,
      "grad_norm": 0.08277518302202225,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 9450
    },
    {
      "epoch": 8.454078212290502,
      "grad_norm": 0.05523339658975601,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 9460
    },
    {
      "epoch": 8.463016759776536,
      "grad_norm": 0.052594222128391266,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 9470
    },
    {
      "epoch": 8.47195530726257,
      "grad_norm": 0.038790263235569,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9480
    },
    {
      "epoch": 8.480893854748603,
      "grad_norm": 0.08516677469015121,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 9490
    },
    {
      "epoch": 8.489832402234637,
      "grad_norm": 0.04821412265300751,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9500
    },
    {
      "epoch": 8.49877094972067,
      "grad_norm": 0.05911944434046745,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9510
    },
    {
      "epoch": 8.507709497206704,
      "grad_norm": 0.05033231899142265,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 9520
    },
    {
      "epoch": 8.516648044692737,
      "grad_norm": 0.04533014073967934,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 9530
    },
    {
      "epoch": 8.52558659217877,
      "grad_norm": 0.041091274470090866,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 9540
    },
    {
      "epoch": 8.534525139664805,
      "grad_norm": 0.040027085691690445,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9550
    },
    {
      "epoch": 8.543463687150838,
      "grad_norm": 0.05214015394449234,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 9560
    },
    {
      "epoch": 8.552402234636872,
      "grad_norm": 0.057501230388879776,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9570
    },
    {
      "epoch": 8.561340782122905,
      "grad_norm": 0.03705643489956856,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9580
    },
    {
      "epoch": 8.570279329608939,
      "grad_norm": 0.14089708030223846,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9590
    },
    {
      "epoch": 8.579217877094973,
      "grad_norm": 0.05704084038734436,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9600
    },
    {
      "epoch": 8.588156424581005,
      "grad_norm": 0.060727182775735855,
      "learning_rate": 5e-05,
      "loss": 0.0127,
      "step": 9610
    },
    {
      "epoch": 8.597094972067039,
      "grad_norm": 0.05009506642818451,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9620
    },
    {
      "epoch": 8.606033519553073,
      "grad_norm": 0.0460076667368412,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 9630
    },
    {
      "epoch": 8.614972067039107,
      "grad_norm": 0.04100862890481949,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 9640
    },
    {
      "epoch": 8.62391061452514,
      "grad_norm": 0.049474142491817474,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 9650
    },
    {
      "epoch": 8.632849162011173,
      "grad_norm": 0.10945496708154678,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9660
    },
    {
      "epoch": 8.641787709497207,
      "grad_norm": 0.04904274642467499,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 9670
    },
    {
      "epoch": 8.65072625698324,
      "grad_norm": 0.0433683842420578,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 9680
    },
    {
      "epoch": 8.659664804469275,
      "grad_norm": 0.05375516787171364,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9690
    },
    {
      "epoch": 8.668603351955307,
      "grad_norm": 0.07228795439004898,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 9700
    },
    {
      "epoch": 8.67754189944134,
      "grad_norm": 0.04214736074209213,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9710
    },
    {
      "epoch": 8.686480446927375,
      "grad_norm": 0.04424595460295677,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9720
    },
    {
      "epoch": 8.695418994413409,
      "grad_norm": 0.05675951763987541,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 9730
    },
    {
      "epoch": 8.70435754189944,
      "grad_norm": 0.079963319003582,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 9740
    },
    {
      "epoch": 8.713296089385475,
      "grad_norm": 0.03955523297190666,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 9750
    },
    {
      "epoch": 8.722234636871509,
      "grad_norm": 0.043013863265514374,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 9760
    },
    {
      "epoch": 8.731173184357543,
      "grad_norm": 0.1067243441939354,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 9770
    },
    {
      "epoch": 8.740111731843575,
      "grad_norm": 0.07121215760707855,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 9780
    },
    {
      "epoch": 8.749050279329609,
      "grad_norm": 0.041247088462114334,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 9790
    },
    {
      "epoch": 8.757988826815643,
      "grad_norm": 0.041169580072164536,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 9800
    },
    {
      "epoch": 8.766927374301677,
      "grad_norm": 0.046717915683984756,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9810
    },
    {
      "epoch": 8.775865921787709,
      "grad_norm": 0.04155109450221062,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9820
    },
    {
      "epoch": 8.784804469273743,
      "grad_norm": 0.04422983154654503,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 9830
    },
    {
      "epoch": 8.793743016759777,
      "grad_norm": 0.053954802453517914,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 9840
    },
    {
      "epoch": 8.80268156424581,
      "grad_norm": 0.05944836512207985,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 9850
    },
    {
      "epoch": 8.811620111731843,
      "grad_norm": 0.05891576409339905,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 9860
    },
    {
      "epoch": 8.820558659217877,
      "grad_norm": 0.07334785908460617,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 9870
    },
    {
      "epoch": 8.82949720670391,
      "grad_norm": 0.048434965312480927,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 9880
    },
    {
      "epoch": 8.838435754189945,
      "grad_norm": 0.04059312865138054,
      "learning_rate": 5e-05,
      "loss": 0.0125,
      "step": 9890
    },
    {
      "epoch": 8.847374301675977,
      "grad_norm": 0.04512565955519676,
      "learning_rate": 5e-05,
      "loss": 0.0116,
      "step": 9900
    },
    {
      "epoch": 8.856312849162011,
      "grad_norm": 0.04065469652414322,
      "learning_rate": 5e-05,
      "loss": 0.0129,
      "step": 9910
    },
    {
      "epoch": 8.865251396648045,
      "grad_norm": 0.04184337705373764,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 9920
    },
    {
      "epoch": 8.874189944134079,
      "grad_norm": 0.039622869342565536,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 9930
    },
    {
      "epoch": 8.883128491620111,
      "grad_norm": 0.05130494758486748,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9940
    },
    {
      "epoch": 8.892067039106145,
      "grad_norm": 0.046695902943611145,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 9950
    },
    {
      "epoch": 8.901005586592179,
      "grad_norm": 0.048910994082689285,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 9960
    },
    {
      "epoch": 8.909944134078213,
      "grad_norm": 0.03377477824687958,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 9970
    },
    {
      "epoch": 8.918882681564245,
      "grad_norm": 0.0640205442905426,
      "learning_rate": 5e-05,
      "loss": 0.0122,
      "step": 9980
    },
    {
      "epoch": 8.927821229050279,
      "grad_norm": 0.04765256121754646,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 9990
    },
    {
      "epoch": 8.936759776536313,
      "grad_norm": 0.03601935878396034,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10000
    },
    {
      "epoch": 8.945698324022347,
      "grad_norm": 0.038708582520484924,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 10010
    },
    {
      "epoch": 8.954636871508379,
      "grad_norm": 0.05394058674573898,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 10020
    },
    {
      "epoch": 8.963575418994413,
      "grad_norm": 0.04389715567231178,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10030
    },
    {
      "epoch": 8.972513966480447,
      "grad_norm": 0.08747280389070511,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 10040
    },
    {
      "epoch": 8.981452513966481,
      "grad_norm": 0.04840885102748871,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 10050
    },
    {
      "epoch": 8.990391061452513,
      "grad_norm": 0.06658001244068146,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 10060
    },
    {
      "epoch": 8.999329608938547,
      "grad_norm": 0.03614313527941704,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 10070
    },
    {
      "epoch": 9.0,
      "eval_AnatEM": 0.595744680800611,
      "eval_FabNER": 0.014222222187454893,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.644444444393784,
      "eval_MultiNERD": 0.8978873238933465,
      "eval_Ontonotes": 0.9073724007059785,
      "eval_TweetNER7": 0.6375092660729597,
      "eval_WikiANN-en": 0.8211678831613791,
      "eval_WikiNeural": 0.9167842030527035,
      "eval_average": 0.752701596749419,
      "eval_bc2gm": 0.7509293679792376,
      "eval_bc4chemd": 0.7300771207723925,
      "eval_bc5cdr": 0.84318766061818,
      "eval_broad_twitter_corpus": 0.6925925925429129,
      "eval_conll2003": 0.9717948717446231,
      "eval_conllpp": 0.9717948717446231,
      "eval_mit-movie": 0.7376311843576345,
      "eval_mit-restaurant": 0.8067796609667746,
      "eval_ncbi": 0.8876080691137872,
      "eval_runtime": 403.1477,
      "eval_samples_per_second": 8.93,
      "eval_steps_per_second": 0.28,
      "step": 10071
    },
    {
      "epoch": 9.00804469273743,
      "grad_norm": 0.06702913343906403,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 10080
    },
    {
      "epoch": 9.016983240223464,
      "grad_norm": 0.08469841629266739,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10090
    },
    {
      "epoch": 9.025921787709498,
      "grad_norm": 0.037987563759088516,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10100
    },
    {
      "epoch": 9.03486033519553,
      "grad_norm": 0.042985036969184875,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 10110
    },
    {
      "epoch": 9.043798882681564,
      "grad_norm": 0.050838664174079895,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10120
    },
    {
      "epoch": 9.052737430167598,
      "grad_norm": 0.08108794689178467,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10130
    },
    {
      "epoch": 9.061675977653632,
      "grad_norm": 0.04376551881432533,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 10140
    },
    {
      "epoch": 9.070614525139664,
      "grad_norm": 0.046986762434244156,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10150
    },
    {
      "epoch": 9.079553072625698,
      "grad_norm": 0.07935816049575806,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10160
    },
    {
      "epoch": 9.088491620111732,
      "grad_norm": 0.03827700763940811,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 10170
    },
    {
      "epoch": 9.097430167597766,
      "grad_norm": 0.05473005026578903,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 10180
    },
    {
      "epoch": 9.106368715083798,
      "grad_norm": 0.06936666369438171,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 10190
    },
    {
      "epoch": 9.115307262569832,
      "grad_norm": 0.07988383620977402,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 10200
    },
    {
      "epoch": 9.124245810055866,
      "grad_norm": 0.0487217903137207,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 10210
    },
    {
      "epoch": 9.1331843575419,
      "grad_norm": 0.05484503135085106,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10220
    },
    {
      "epoch": 9.142122905027932,
      "grad_norm": 0.0504276417195797,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 10230
    },
    {
      "epoch": 9.151061452513966,
      "grad_norm": 0.0532933846116066,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 10240
    },
    {
      "epoch": 9.16,
      "grad_norm": 0.038646161556243896,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 10250
    },
    {
      "epoch": 9.168938547486034,
      "grad_norm": 0.04433022812008858,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10260
    },
    {
      "epoch": 9.177877094972066,
      "grad_norm": 0.05060300976037979,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10270
    },
    {
      "epoch": 9.1868156424581,
      "grad_norm": 0.053601134568452835,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 10280
    },
    {
      "epoch": 9.195754189944134,
      "grad_norm": 0.04087357223033905,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10290
    },
    {
      "epoch": 9.204692737430168,
      "grad_norm": 0.05386947840452194,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 10300
    },
    {
      "epoch": 9.2136312849162,
      "grad_norm": 0.05865461006760597,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 10310
    },
    {
      "epoch": 9.222569832402234,
      "grad_norm": 0.04221941903233528,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 10320
    },
    {
      "epoch": 9.231508379888268,
      "grad_norm": 0.05096808075904846,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10330
    },
    {
      "epoch": 9.240446927374302,
      "grad_norm": 0.09066841006278992,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 10340
    },
    {
      "epoch": 9.249385474860334,
      "grad_norm": 0.03934495523571968,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10350
    },
    {
      "epoch": 9.258324022346368,
      "grad_norm": 0.04593970999121666,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 10360
    },
    {
      "epoch": 9.267262569832402,
      "grad_norm": 0.06287027150392532,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10370
    },
    {
      "epoch": 9.276201117318436,
      "grad_norm": 0.055771201848983765,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10380
    },
    {
      "epoch": 9.285139664804468,
      "grad_norm": 0.06064755842089653,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 10390
    },
    {
      "epoch": 9.294078212290502,
      "grad_norm": 0.04019109532237053,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 10400
    },
    {
      "epoch": 9.303016759776536,
      "grad_norm": 0.03964633867144585,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10410
    },
    {
      "epoch": 9.31195530726257,
      "grad_norm": 0.03668239712715149,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 10420
    },
    {
      "epoch": 9.320893854748604,
      "grad_norm": 0.058774229139089584,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 10430
    },
    {
      "epoch": 9.329832402234636,
      "grad_norm": 0.04209594801068306,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10440
    },
    {
      "epoch": 9.33877094972067,
      "grad_norm": 0.055268775671720505,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10450
    },
    {
      "epoch": 9.347709497206704,
      "grad_norm": 0.054526906460523605,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 10460
    },
    {
      "epoch": 9.356648044692738,
      "grad_norm": 0.04721551015973091,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10470
    },
    {
      "epoch": 9.36558659217877,
      "grad_norm": 0.05886709690093994,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10480
    },
    {
      "epoch": 9.374525139664804,
      "grad_norm": 0.04398653283715248,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10490
    },
    {
      "epoch": 9.383463687150838,
      "grad_norm": 0.05848555266857147,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10500
    },
    {
      "epoch": 9.392402234636872,
      "grad_norm": 0.06654107570648193,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10510
    },
    {
      "epoch": 9.401340782122904,
      "grad_norm": 0.04284747689962387,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 10520
    },
    {
      "epoch": 9.410279329608938,
      "grad_norm": 0.06064795330166817,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10530
    },
    {
      "epoch": 9.419217877094972,
      "grad_norm": 0.08671627193689346,
      "learning_rate": 5e-05,
      "loss": 0.0123,
      "step": 10540
    },
    {
      "epoch": 9.428156424581006,
      "grad_norm": 0.0519912950694561,
      "learning_rate": 5e-05,
      "loss": 0.0111,
      "step": 10550
    },
    {
      "epoch": 9.437094972067039,
      "grad_norm": 0.04941064864397049,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 10560
    },
    {
      "epoch": 9.446033519553072,
      "grad_norm": 0.1006961539387703,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10570
    },
    {
      "epoch": 9.454972067039106,
      "grad_norm": 0.043583985418081284,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10580
    },
    {
      "epoch": 9.46391061452514,
      "grad_norm": 0.03357970714569092,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10590
    },
    {
      "epoch": 9.472849162011173,
      "grad_norm": 0.03693069890141487,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10600
    },
    {
      "epoch": 9.481787709497207,
      "grad_norm": 0.08230528980493546,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 10610
    },
    {
      "epoch": 9.49072625698324,
      "grad_norm": 0.0798557698726654,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10620
    },
    {
      "epoch": 9.499664804469274,
      "grad_norm": 0.09431149065494537,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 10630
    },
    {
      "epoch": 9.508603351955307,
      "grad_norm": 0.04162086173892021,
      "learning_rate": 5e-05,
      "loss": 0.0121,
      "step": 10640
    },
    {
      "epoch": 9.51754189944134,
      "grad_norm": 0.044231314212083817,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10650
    },
    {
      "epoch": 9.526480446927375,
      "grad_norm": 0.04454148933291435,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10660
    },
    {
      "epoch": 9.535418994413408,
      "grad_norm": 0.05263981223106384,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10670
    },
    {
      "epoch": 9.54435754189944,
      "grad_norm": 0.07716790586709976,
      "learning_rate": 5e-05,
      "loss": 0.011,
      "step": 10680
    },
    {
      "epoch": 9.553296089385475,
      "grad_norm": 0.05199461802840233,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 10690
    },
    {
      "epoch": 9.562234636871509,
      "grad_norm": 0.04567494988441467,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 10700
    },
    {
      "epoch": 9.571173184357542,
      "grad_norm": 0.08195937424898148,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 10710
    },
    {
      "epoch": 9.580111731843575,
      "grad_norm": 0.03776835277676582,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 10720
    },
    {
      "epoch": 9.589050279329609,
      "grad_norm": 0.04513868689537048,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10730
    },
    {
      "epoch": 9.597988826815643,
      "grad_norm": 0.08283619582653046,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10740
    },
    {
      "epoch": 9.606927374301677,
      "grad_norm": 0.10684533417224884,
      "learning_rate": 5e-05,
      "loss": 0.0112,
      "step": 10750
    },
    {
      "epoch": 9.615865921787709,
      "grad_norm": 0.0610501654446125,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10760
    },
    {
      "epoch": 9.624804469273743,
      "grad_norm": 0.050246287137269974,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 10770
    },
    {
      "epoch": 9.633743016759777,
      "grad_norm": 0.050434838980436325,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10780
    },
    {
      "epoch": 9.64268156424581,
      "grad_norm": 0.03407534956932068,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 10790
    },
    {
      "epoch": 9.651620111731843,
      "grad_norm": 0.04387139901518822,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 10800
    },
    {
      "epoch": 9.660558659217877,
      "grad_norm": 0.03855683282017708,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 10810
    },
    {
      "epoch": 9.66949720670391,
      "grad_norm": 0.04510730504989624,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 10820
    },
    {
      "epoch": 9.678435754189945,
      "grad_norm": 0.0452432855963707,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10830
    },
    {
      "epoch": 9.687374301675977,
      "grad_norm": 0.04992888122797012,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 10840
    },
    {
      "epoch": 9.69631284916201,
      "grad_norm": 0.06625550240278244,
      "learning_rate": 5e-05,
      "loss": 0.0124,
      "step": 10850
    },
    {
      "epoch": 9.705251396648045,
      "grad_norm": 0.06859876215457916,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 10860
    },
    {
      "epoch": 9.714189944134079,
      "grad_norm": 0.039321351796388626,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 10870
    },
    {
      "epoch": 9.723128491620113,
      "grad_norm": 0.05081142485141754,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 10880
    },
    {
      "epoch": 9.732067039106145,
      "grad_norm": 0.06057777628302574,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 10890
    },
    {
      "epoch": 9.741005586592179,
      "grad_norm": 0.04855276271700859,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10900
    },
    {
      "epoch": 9.749944134078213,
      "grad_norm": 0.03928617015480995,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 10910
    },
    {
      "epoch": 9.758882681564245,
      "grad_norm": 0.032803017646074295,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10920
    },
    {
      "epoch": 9.767821229050279,
      "grad_norm": 0.04958372190594673,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 10930
    },
    {
      "epoch": 9.776759776536313,
      "grad_norm": 0.04681883752346039,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 10940
    },
    {
      "epoch": 9.785698324022347,
      "grad_norm": 0.0715731680393219,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 10950
    },
    {
      "epoch": 9.79463687150838,
      "grad_norm": 0.0600854717195034,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 10960
    },
    {
      "epoch": 9.803575418994413,
      "grad_norm": 0.07406876236200333,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 10970
    },
    {
      "epoch": 9.812513966480447,
      "grad_norm": 0.03446158766746521,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 10980
    },
    {
      "epoch": 9.82145251396648,
      "grad_norm": 0.05425722524523735,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 10990
    },
    {
      "epoch": 9.830391061452515,
      "grad_norm": 0.05491270497441292,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 11000
    },
    {
      "epoch": 9.839329608938547,
      "grad_norm": 0.07070639729499817,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 11010
    },
    {
      "epoch": 9.84826815642458,
      "grad_norm": 0.038097720593214035,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11020
    },
    {
      "epoch": 9.857206703910615,
      "grad_norm": 0.04694293811917305,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11030
    },
    {
      "epoch": 9.866145251396649,
      "grad_norm": 0.04057200625538826,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11040
    },
    {
      "epoch": 9.875083798882681,
      "grad_norm": 0.06172199174761772,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 11050
    },
    {
      "epoch": 9.884022346368715,
      "grad_norm": 0.06551259756088257,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11060
    },
    {
      "epoch": 9.892960893854749,
      "grad_norm": 0.046209558844566345,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11070
    },
    {
      "epoch": 9.901899441340783,
      "grad_norm": 0.06441362202167511,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 11080
    },
    {
      "epoch": 9.910837988826815,
      "grad_norm": 0.07334759831428528,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 11090
    },
    {
      "epoch": 9.919776536312849,
      "grad_norm": 0.041211605072021484,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11100
    },
    {
      "epoch": 9.928715083798883,
      "grad_norm": 0.035195693373680115,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 11110
    },
    {
      "epoch": 9.937653631284917,
      "grad_norm": 0.03822663798928261,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 11120
    },
    {
      "epoch": 9.946592178770949,
      "grad_norm": 0.04396253079175949,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 11130
    },
    {
      "epoch": 9.955530726256983,
      "grad_norm": 0.05159008130431175,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 11140
    },
    {
      "epoch": 9.964469273743017,
      "grad_norm": 0.06655295938253403,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 11150
    },
    {
      "epoch": 9.973407821229051,
      "grad_norm": 0.09984661638736725,
      "learning_rate": 5e-05,
      "loss": 0.0115,
      "step": 11160
    },
    {
      "epoch": 9.982346368715083,
      "grad_norm": 0.055314257740974426,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 11170
    },
    {
      "epoch": 9.991284916201117,
      "grad_norm": 0.04075523465871811,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11180
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.05104866251349449,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11190
    },
    {
      "epoch": 10.0,
      "eval_AnatEM": 0.6590909090396436,
      "eval_FabNER": 0.013973799090410632,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.6285714285207119,
      "eval_MultiNERD": 0.9090909090405968,
      "eval_Ontonotes": 0.9054820415377368,
      "eval_TweetNER7": 0.643887623336418,
      "eval_WikiANN-en": 0.8088235293614702,
      "eval_WikiNeural": 0.9209039547519998,
      "eval_average": 0.7572571488326677,
      "eval_bc2gm": 0.7545787545283152,
      "eval_bc4chemd": 0.7268041236611262,
      "eval_bc5cdr": 0.8437499999497816,
      "eval_broad_twitter_corpus": 0.6601562499511527,
      "eval_conll2003": 0.9820051413379224,
      "eval_conllpp": 0.9845758354253253,
      "eval_mit-movie": 0.7425149700097121,
      "eval_mit-restaurant": 0.8008998874638947,
      "eval_ncbi": 0.9244186046006405,
      "eval_runtime": 403.7201,
      "eval_samples_per_second": 8.917,
      "eval_steps_per_second": 0.28,
      "step": 11190
    },
    {
      "epoch": 10.008938547486034,
      "grad_norm": 0.04965933412313461,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11200
    },
    {
      "epoch": 10.017877094972068,
      "grad_norm": 0.05327834188938141,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 11210
    },
    {
      "epoch": 10.0268156424581,
      "grad_norm": 0.06675481051206589,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 11220
    },
    {
      "epoch": 10.035754189944134,
      "grad_norm": 0.048461299389600754,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11230
    },
    {
      "epoch": 10.044692737430168,
      "grad_norm": 0.040169138461351395,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 11240
    },
    {
      "epoch": 10.053631284916202,
      "grad_norm": 0.04373715817928314,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 11250
    },
    {
      "epoch": 10.062569832402234,
      "grad_norm": 0.04390673711895943,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11260
    },
    {
      "epoch": 10.071508379888268,
      "grad_norm": 0.05264197662472725,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 11270
    },
    {
      "epoch": 10.080446927374302,
      "grad_norm": 0.05714159458875656,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 11280
    },
    {
      "epoch": 10.089385474860336,
      "grad_norm": 0.04270476847887039,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 11290
    },
    {
      "epoch": 10.098324022346368,
      "grad_norm": 0.027349766343832016,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 11300
    },
    {
      "epoch": 10.107262569832402,
      "grad_norm": 0.04616083577275276,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11310
    },
    {
      "epoch": 10.116201117318436,
      "grad_norm": 0.04194048047065735,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 11320
    },
    {
      "epoch": 10.12513966480447,
      "grad_norm": 0.0584256574511528,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11330
    },
    {
      "epoch": 10.134078212290502,
      "grad_norm": 0.04313267394900322,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 11340
    },
    {
      "epoch": 10.143016759776536,
      "grad_norm": 0.038753654807806015,
      "learning_rate": 5e-05,
      "loss": 0.0113,
      "step": 11350
    },
    {
      "epoch": 10.15195530726257,
      "grad_norm": 0.03731808438897133,
      "learning_rate": 5e-05,
      "loss": 0.0114,
      "step": 11360
    },
    {
      "epoch": 10.160893854748604,
      "grad_norm": 0.03173785284161568,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 11370
    },
    {
      "epoch": 10.169832402234636,
      "grad_norm": 0.0460832305252552,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11380
    },
    {
      "epoch": 10.17877094972067,
      "grad_norm": 0.058802228420972824,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 11390
    },
    {
      "epoch": 10.187709497206704,
      "grad_norm": 0.03733490779995918,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 11400
    },
    {
      "epoch": 10.196648044692738,
      "grad_norm": 0.03919476643204689,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 11410
    },
    {
      "epoch": 10.20558659217877,
      "grad_norm": 0.04945804551243782,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11420
    },
    {
      "epoch": 10.214525139664804,
      "grad_norm": 0.05354943126440048,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11430
    },
    {
      "epoch": 10.223463687150838,
      "grad_norm": 0.04736382141709328,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11440
    },
    {
      "epoch": 10.232402234636872,
      "grad_norm": 0.030929120257496834,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11450
    },
    {
      "epoch": 10.241340782122904,
      "grad_norm": 0.04524258151650429,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 11460
    },
    {
      "epoch": 10.250279329608938,
      "grad_norm": 0.03427139297127724,
      "learning_rate": 5e-05,
      "loss": 0.0118,
      "step": 11470
    },
    {
      "epoch": 10.259217877094972,
      "grad_norm": 0.049467507749795914,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11480
    },
    {
      "epoch": 10.268156424581006,
      "grad_norm": 0.10961565375328064,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 11490
    },
    {
      "epoch": 10.277094972067038,
      "grad_norm": 0.0769292488694191,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 11500
    },
    {
      "epoch": 10.286033519553072,
      "grad_norm": 0.0402216836810112,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11510
    },
    {
      "epoch": 10.294972067039106,
      "grad_norm": 0.05623471364378929,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11520
    },
    {
      "epoch": 10.30391061452514,
      "grad_norm": 0.04526073858141899,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11530
    },
    {
      "epoch": 10.312849162011172,
      "grad_norm": 0.05643528327345848,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11540
    },
    {
      "epoch": 10.321787709497206,
      "grad_norm": 0.03985654562711716,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 11550
    },
    {
      "epoch": 10.33072625698324,
      "grad_norm": 0.07055838406085968,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11560
    },
    {
      "epoch": 10.339664804469274,
      "grad_norm": 0.05358139052987099,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 11570
    },
    {
      "epoch": 10.348603351955306,
      "grad_norm": 0.0939294621348381,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 11580
    },
    {
      "epoch": 10.35754189944134,
      "grad_norm": 0.04080308973789215,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 11590
    },
    {
      "epoch": 10.366480446927374,
      "grad_norm": 0.0495779849588871,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 11600
    },
    {
      "epoch": 10.375418994413408,
      "grad_norm": 0.05621039867401123,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11610
    },
    {
      "epoch": 10.38435754189944,
      "grad_norm": 0.04392796382308006,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 11620
    },
    {
      "epoch": 10.393296089385474,
      "grad_norm": 0.16467958688735962,
      "learning_rate": 5e-05,
      "loss": 0.0087,
      "step": 11630
    },
    {
      "epoch": 10.402234636871508,
      "grad_norm": 0.057046789675951004,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11640
    },
    {
      "epoch": 10.411173184357542,
      "grad_norm": 0.03983137756586075,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 11650
    },
    {
      "epoch": 10.420111731843576,
      "grad_norm": 0.04460163041949272,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 11660
    },
    {
      "epoch": 10.429050279329608,
      "grad_norm": 0.04637780413031578,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 11670
    },
    {
      "epoch": 10.437988826815642,
      "grad_norm": 0.03191172704100609,
      "learning_rate": 5e-05,
      "loss": 0.0117,
      "step": 11680
    },
    {
      "epoch": 10.446927374301676,
      "grad_norm": 0.040805965662002563,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11690
    },
    {
      "epoch": 10.45586592178771,
      "grad_norm": 0.046934954822063446,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 11700
    },
    {
      "epoch": 10.464804469273743,
      "grad_norm": 0.061480190604925156,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 11710
    },
    {
      "epoch": 10.473743016759776,
      "grad_norm": 0.054110847413539886,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11720
    },
    {
      "epoch": 10.48268156424581,
      "grad_norm": 0.04441452771425247,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11730
    },
    {
      "epoch": 10.491620111731844,
      "grad_norm": 0.06126147508621216,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 11740
    },
    {
      "epoch": 10.500558659217877,
      "grad_norm": 0.037403274327516556,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 11750
    },
    {
      "epoch": 10.50949720670391,
      "grad_norm": 0.06256019324064255,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11760
    },
    {
      "epoch": 10.518435754189944,
      "grad_norm": 0.036675792187452316,
      "learning_rate": 5e-05,
      "loss": 0.0109,
      "step": 11770
    },
    {
      "epoch": 10.527374301675978,
      "grad_norm": 0.060831956565380096,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11780
    },
    {
      "epoch": 10.53631284916201,
      "grad_norm": 0.037633806467056274,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 11790
    },
    {
      "epoch": 10.545251396648045,
      "grad_norm": 0.03839433938264847,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 11800
    },
    {
      "epoch": 10.554189944134079,
      "grad_norm": 0.05714126303792,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 11810
    },
    {
      "epoch": 10.563128491620112,
      "grad_norm": 0.051119811832904816,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 11820
    },
    {
      "epoch": 10.572067039106145,
      "grad_norm": 0.05964180827140808,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 11830
    },
    {
      "epoch": 10.581005586592179,
      "grad_norm": 0.06525088846683502,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 11840
    },
    {
      "epoch": 10.589944134078213,
      "grad_norm": 0.04657416045665741,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 11850
    },
    {
      "epoch": 10.598882681564247,
      "grad_norm": 0.03752671554684639,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 11860
    },
    {
      "epoch": 10.607821229050279,
      "grad_norm": 0.03553701564669609,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 11870
    },
    {
      "epoch": 10.616759776536313,
      "grad_norm": 0.03338171914219856,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 11880
    },
    {
      "epoch": 10.625698324022347,
      "grad_norm": 0.03945200890302658,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 11890
    },
    {
      "epoch": 10.63463687150838,
      "grad_norm": 0.045081451535224915,
      "learning_rate": 5e-05,
      "loss": 0.0108,
      "step": 11900
    },
    {
      "epoch": 10.643575418994413,
      "grad_norm": 0.043782301247119904,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 11910
    },
    {
      "epoch": 10.652513966480447,
      "grad_norm": 0.0515778511762619,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 11920
    },
    {
      "epoch": 10.66145251396648,
      "grad_norm": 0.03786514326930046,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 11930
    },
    {
      "epoch": 10.670391061452515,
      "grad_norm": 0.13108758628368378,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 11940
    },
    {
      "epoch": 10.679329608938547,
      "grad_norm": 0.07706952840089798,
      "learning_rate": 5e-05,
      "loss": 0.012,
      "step": 11950
    },
    {
      "epoch": 10.68826815642458,
      "grad_norm": 0.0644155740737915,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11960
    },
    {
      "epoch": 10.697206703910615,
      "grad_norm": 0.04778064042329788,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 11970
    },
    {
      "epoch": 10.706145251396649,
      "grad_norm": 0.04902468994259834,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 11980
    },
    {
      "epoch": 10.71508379888268,
      "grad_norm": 0.05353515222668648,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 11990
    },
    {
      "epoch": 10.724022346368715,
      "grad_norm": 0.054199326783418655,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 12000
    },
    {
      "epoch": 10.732960893854749,
      "grad_norm": 0.04237053170800209,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12010
    },
    {
      "epoch": 10.741899441340783,
      "grad_norm": 0.05430413410067558,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12020
    },
    {
      "epoch": 10.750837988826815,
      "grad_norm": 0.068397156894207,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12030
    },
    {
      "epoch": 10.759776536312849,
      "grad_norm": 0.08185914903879166,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 12040
    },
    {
      "epoch": 10.768715083798883,
      "grad_norm": 0.05240537226200104,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 12050
    },
    {
      "epoch": 10.777653631284917,
      "grad_norm": 0.051665209233760834,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12060
    },
    {
      "epoch": 10.786592178770949,
      "grad_norm": 0.03453824296593666,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 12070
    },
    {
      "epoch": 10.795530726256983,
      "grad_norm": 0.07053407281637192,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 12080
    },
    {
      "epoch": 10.804469273743017,
      "grad_norm": 0.04642994701862335,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12090
    },
    {
      "epoch": 10.81340782122905,
      "grad_norm": 0.10116076469421387,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12100
    },
    {
      "epoch": 10.822346368715085,
      "grad_norm": 0.04869517683982849,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 12110
    },
    {
      "epoch": 10.831284916201117,
      "grad_norm": 0.05368679389357567,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 12120
    },
    {
      "epoch": 10.84022346368715,
      "grad_norm": 0.03819344937801361,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12130
    },
    {
      "epoch": 10.849162011173185,
      "grad_norm": 0.058271341025829315,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 12140
    },
    {
      "epoch": 10.858100558659217,
      "grad_norm": 0.03995318338274956,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 12150
    },
    {
      "epoch": 10.867039106145251,
      "grad_norm": 0.04113134369254112,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 12160
    },
    {
      "epoch": 10.875977653631285,
      "grad_norm": 0.055806517601013184,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12170
    },
    {
      "epoch": 10.884916201117319,
      "grad_norm": 0.04162692278623581,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 12180
    },
    {
      "epoch": 10.893854748603353,
      "grad_norm": 0.07464383542537689,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12190
    },
    {
      "epoch": 10.902793296089385,
      "grad_norm": 0.04758956655859947,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12200
    },
    {
      "epoch": 10.911731843575419,
      "grad_norm": 0.03914184868335724,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12210
    },
    {
      "epoch": 10.920670391061453,
      "grad_norm": 0.04406445100903511,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12220
    },
    {
      "epoch": 10.929608938547487,
      "grad_norm": 0.03454060107469559,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12230
    },
    {
      "epoch": 10.938547486033519,
      "grad_norm": 0.04515894874930382,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12240
    },
    {
      "epoch": 10.947486033519553,
      "grad_norm": 0.046451639384031296,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12250
    },
    {
      "epoch": 10.956424581005587,
      "grad_norm": 0.05430113524198532,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12260
    },
    {
      "epoch": 10.96536312849162,
      "grad_norm": 0.07103299349546432,
      "learning_rate": 5e-05,
      "loss": 0.0078,
      "step": 12270
    },
    {
      "epoch": 10.974301675977653,
      "grad_norm": 0.044980816543102264,
      "learning_rate": 5e-05,
      "loss": 0.0103,
      "step": 12280
    },
    {
      "epoch": 10.983240223463687,
      "grad_norm": 0.05137035250663757,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12290
    },
    {
      "epoch": 10.992178770949721,
      "grad_norm": 0.04876958206295967,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 12300
    },
    {
      "epoch": 11.0,
      "eval_AnatEM": 0.6304347825579159,
      "eval_FabNER": 0.015517241342067629,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.6666666666159815,
      "eval_MultiNERD": 0.9014084506539085,
      "eval_Ontonotes": 0.902738432433312,
      "eval_TweetNER7": 0.6234522941960994,
      "eval_WikiANN-en": 0.8175182481248842,
      "eval_WikiNeural": 0.9196050775237887,
      "eval_average": 0.7613216436784657,
      "eval_bc2gm": 0.7456445992530588,
      "eval_bc4chemd": 0.7692307691807442,
      "eval_bc5cdr": 0.8515624999497796,
      "eval_broad_twitter_corpus": 0.6972477063722354,
      "eval_conll2003": 0.9794871794369288,
      "eval_conllpp": 0.9794871794369288,
      "eval_mit-movie": 0.7511045654873472,
      "eval_mit-restaurant": 0.8044692736928646,
      "eval_ncbi": 0.9271137025733768,
      "eval_runtime": 402.5961,
      "eval_samples_per_second": 8.942,
      "eval_steps_per_second": 0.281,
      "step": 12309
    },
    {
      "epoch": 11.000893854748604,
      "grad_norm": 0.044493455439805984,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12310
    },
    {
      "epoch": 11.009832402234636,
      "grad_norm": 0.041503600776195526,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 12320
    },
    {
      "epoch": 11.01877094972067,
      "grad_norm": 0.04494136944413185,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12330
    },
    {
      "epoch": 11.027709497206704,
      "grad_norm": 0.0537383034825325,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12340
    },
    {
      "epoch": 11.036648044692738,
      "grad_norm": 0.04908817633986473,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 12350
    },
    {
      "epoch": 11.04558659217877,
      "grad_norm": 0.05949468910694122,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12360
    },
    {
      "epoch": 11.054525139664804,
      "grad_norm": 0.06869401782751083,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 12370
    },
    {
      "epoch": 11.063463687150838,
      "grad_norm": 0.04127134755253792,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12380
    },
    {
      "epoch": 11.072402234636872,
      "grad_norm": 0.09178542345762253,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12390
    },
    {
      "epoch": 11.081340782122904,
      "grad_norm": 0.050045955926179886,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12400
    },
    {
      "epoch": 11.090279329608938,
      "grad_norm": 0.03602694347500801,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 12410
    },
    {
      "epoch": 11.099217877094972,
      "grad_norm": 0.03149737790226936,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12420
    },
    {
      "epoch": 11.108156424581006,
      "grad_norm": 0.03631337732076645,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12430
    },
    {
      "epoch": 11.117094972067038,
      "grad_norm": 0.07286515086889267,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12440
    },
    {
      "epoch": 11.126033519553072,
      "grad_norm": 0.060304079204797745,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 12450
    },
    {
      "epoch": 11.134972067039106,
      "grad_norm": 0.04957444965839386,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12460
    },
    {
      "epoch": 11.14391061452514,
      "grad_norm": 0.12534011900424957,
      "learning_rate": 5e-05,
      "loss": 0.0091,
      "step": 12470
    },
    {
      "epoch": 11.152849162011174,
      "grad_norm": 0.05810300260782242,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 12480
    },
    {
      "epoch": 11.161787709497206,
      "grad_norm": 0.04308425262570381,
      "learning_rate": 5e-05,
      "loss": 0.0082,
      "step": 12490
    },
    {
      "epoch": 11.17072625698324,
      "grad_norm": 0.04043905809521675,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12500
    },
    {
      "epoch": 11.179664804469274,
      "grad_norm": 0.09977506846189499,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12510
    },
    {
      "epoch": 11.188603351955308,
      "grad_norm": 0.0468580387532711,
      "learning_rate": 5e-05,
      "loss": 0.0087,
      "step": 12520
    },
    {
      "epoch": 11.19754189944134,
      "grad_norm": 0.04023228585720062,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12530
    },
    {
      "epoch": 11.206480446927374,
      "grad_norm": 0.06095028668642044,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 12540
    },
    {
      "epoch": 11.215418994413408,
      "grad_norm": 0.07082413882017136,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12550
    },
    {
      "epoch": 11.224357541899442,
      "grad_norm": 0.044295307248830795,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12560
    },
    {
      "epoch": 11.233296089385474,
      "grad_norm": 0.050178322941064835,
      "learning_rate": 5e-05,
      "loss": 0.0079,
      "step": 12570
    },
    {
      "epoch": 11.242234636871508,
      "grad_norm": 0.04261380806565285,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12580
    },
    {
      "epoch": 11.251173184357542,
      "grad_norm": 0.05253119394183159,
      "learning_rate": 5e-05,
      "loss": 0.0085,
      "step": 12590
    },
    {
      "epoch": 11.260111731843576,
      "grad_norm": 0.054809216409921646,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12600
    },
    {
      "epoch": 11.269050279329608,
      "grad_norm": 0.05545812472701073,
      "learning_rate": 5e-05,
      "loss": 0.0091,
      "step": 12610
    },
    {
      "epoch": 11.277988826815642,
      "grad_norm": 0.03819510340690613,
      "learning_rate": 5e-05,
      "loss": 0.0087,
      "step": 12620
    },
    {
      "epoch": 11.286927374301676,
      "grad_norm": 0.039072804152965546,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12630
    },
    {
      "epoch": 11.29586592178771,
      "grad_norm": 0.04285901039838791,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12640
    },
    {
      "epoch": 11.304804469273742,
      "grad_norm": 0.0378011129796505,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12650
    },
    {
      "epoch": 11.313743016759776,
      "grad_norm": 0.08588173985481262,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12660
    },
    {
      "epoch": 11.32268156424581,
      "grad_norm": 0.04237968474626541,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12670
    },
    {
      "epoch": 11.331620111731844,
      "grad_norm": 0.04021594673395157,
      "learning_rate": 5e-05,
      "loss": 0.0085,
      "step": 12680
    },
    {
      "epoch": 11.340558659217876,
      "grad_norm": 0.04576186090707779,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12690
    },
    {
      "epoch": 11.34949720670391,
      "grad_norm": 0.049619462341070175,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 12700
    },
    {
      "epoch": 11.358435754189944,
      "grad_norm": 0.05232465639710426,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12710
    },
    {
      "epoch": 11.367374301675978,
      "grad_norm": 0.08222241699695587,
      "learning_rate": 5e-05,
      "loss": 0.0091,
      "step": 12720
    },
    {
      "epoch": 11.37631284916201,
      "grad_norm": 0.07301127165555954,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12730
    },
    {
      "epoch": 11.385251396648044,
      "grad_norm": 0.05036957189440727,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 12740
    },
    {
      "epoch": 11.394189944134078,
      "grad_norm": 0.03727877140045166,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12750
    },
    {
      "epoch": 11.403128491620112,
      "grad_norm": 0.06350552290678024,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 12760
    },
    {
      "epoch": 11.412067039106145,
      "grad_norm": 0.03260790556669235,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12770
    },
    {
      "epoch": 11.421005586592178,
      "grad_norm": 0.055711548775434494,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12780
    },
    {
      "epoch": 11.429944134078212,
      "grad_norm": 0.04231850802898407,
      "learning_rate": 5e-05,
      "loss": 0.0096,
      "step": 12790
    },
    {
      "epoch": 11.438882681564246,
      "grad_norm": 0.052106406539678574,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12800
    },
    {
      "epoch": 11.447821229050279,
      "grad_norm": 0.042752545326948166,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12810
    },
    {
      "epoch": 11.456759776536313,
      "grad_norm": 0.038819894194602966,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 12820
    },
    {
      "epoch": 11.465698324022346,
      "grad_norm": 0.05250398814678192,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 12830
    },
    {
      "epoch": 11.47463687150838,
      "grad_norm": 0.0376921147108078,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12840
    },
    {
      "epoch": 11.483575418994413,
      "grad_norm": 0.0347149632871151,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 12850
    },
    {
      "epoch": 11.492513966480447,
      "grad_norm": 0.03476336970925331,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 12860
    },
    {
      "epoch": 11.50145251396648,
      "grad_norm": 0.052027780562639236,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12870
    },
    {
      "epoch": 11.510391061452514,
      "grad_norm": 0.04211127385497093,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12880
    },
    {
      "epoch": 11.519329608938548,
      "grad_norm": 0.0920143648982048,
      "learning_rate": 5e-05,
      "loss": 0.0102,
      "step": 12890
    },
    {
      "epoch": 11.52826815642458,
      "grad_norm": 0.10431535542011261,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 12900
    },
    {
      "epoch": 11.537206703910615,
      "grad_norm": 0.05049949511885643,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 12910
    },
    {
      "epoch": 11.546145251396648,
      "grad_norm": 0.031338389962911606,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 12920
    },
    {
      "epoch": 11.55508379888268,
      "grad_norm": 0.059908606112003326,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 12930
    },
    {
      "epoch": 11.564022346368715,
      "grad_norm": 0.03841109573841095,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 12940
    },
    {
      "epoch": 11.572960893854749,
      "grad_norm": 0.043276745826005936,
      "learning_rate": 5e-05,
      "loss": 0.0086,
      "step": 12950
    },
    {
      "epoch": 11.581899441340783,
      "grad_norm": 0.043219421058893204,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 12960
    },
    {
      "epoch": 11.590837988826816,
      "grad_norm": 0.024913569912314415,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 12970
    },
    {
      "epoch": 11.599776536312849,
      "grad_norm": 0.04583018273115158,
      "learning_rate": 5e-05,
      "loss": 0.01,
      "step": 12980
    },
    {
      "epoch": 11.608715083798883,
      "grad_norm": 0.12507128715515137,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 12990
    },
    {
      "epoch": 11.617653631284917,
      "grad_norm": 0.042381636798381805,
      "learning_rate": 5e-05,
      "loss": 0.0086,
      "step": 13000
    },
    {
      "epoch": 11.62659217877095,
      "grad_norm": 0.06315802037715912,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 13010
    },
    {
      "epoch": 11.635530726256983,
      "grad_norm": 0.05911143869161606,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13020
    },
    {
      "epoch": 11.644469273743017,
      "grad_norm": 0.04620740935206413,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 13030
    },
    {
      "epoch": 11.65340782122905,
      "grad_norm": 0.05104711279273033,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 13040
    },
    {
      "epoch": 11.662346368715085,
      "grad_norm": 0.060264069586992264,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 13050
    },
    {
      "epoch": 11.671284916201117,
      "grad_norm": 0.04830444976687431,
      "learning_rate": 5e-05,
      "loss": 0.0106,
      "step": 13060
    },
    {
      "epoch": 11.68022346368715,
      "grad_norm": 0.03533978760242462,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 13070
    },
    {
      "epoch": 11.689162011173185,
      "grad_norm": 0.03576960042119026,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13080
    },
    {
      "epoch": 11.698100558659219,
      "grad_norm": 0.03774126619100571,
      "learning_rate": 5e-05,
      "loss": 0.0087,
      "step": 13090
    },
    {
      "epoch": 11.70703910614525,
      "grad_norm": 0.04041971638798714,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13100
    },
    {
      "epoch": 11.715977653631285,
      "grad_norm": 0.04619597643613815,
      "learning_rate": 5e-05,
      "loss": 0.0085,
      "step": 13110
    },
    {
      "epoch": 11.724916201117319,
      "grad_norm": 0.05256134271621704,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13120
    },
    {
      "epoch": 11.733854748603353,
      "grad_norm": 0.04013059660792351,
      "learning_rate": 5e-05,
      "loss": 0.0097,
      "step": 13130
    },
    {
      "epoch": 11.742793296089385,
      "grad_norm": 0.0426117442548275,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 13140
    },
    {
      "epoch": 11.751731843575419,
      "grad_norm": 0.04619374871253967,
      "learning_rate": 5e-05,
      "loss": 0.0088,
      "step": 13150
    },
    {
      "epoch": 11.760670391061453,
      "grad_norm": 0.04006064310669899,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 13160
    },
    {
      "epoch": 11.769608938547487,
      "grad_norm": 0.03824731707572937,
      "learning_rate": 5e-05,
      "loss": 0.0093,
      "step": 13170
    },
    {
      "epoch": 11.778547486033519,
      "grad_norm": 0.05571911111474037,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 13180
    },
    {
      "epoch": 11.787486033519553,
      "grad_norm": 0.04199110344052315,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 13190
    },
    {
      "epoch": 11.796424581005587,
      "grad_norm": 0.03718411922454834,
      "learning_rate": 5e-05,
      "loss": 0.0091,
      "step": 13200
    },
    {
      "epoch": 11.80536312849162,
      "grad_norm": 0.05602681264281273,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 13210
    },
    {
      "epoch": 11.814301675977653,
      "grad_norm": 0.03473935276269913,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 13220
    },
    {
      "epoch": 11.823240223463687,
      "grad_norm": 0.03732480853796005,
      "learning_rate": 5e-05,
      "loss": 0.0087,
      "step": 13230
    },
    {
      "epoch": 11.83217877094972,
      "grad_norm": 0.0587119497358799,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 13240
    },
    {
      "epoch": 11.841117318435755,
      "grad_norm": 0.0541885681450367,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 13250
    },
    {
      "epoch": 11.850055865921787,
      "grad_norm": 0.07128863036632538,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13260
    },
    {
      "epoch": 11.858994413407821,
      "grad_norm": 0.051500268280506134,
      "learning_rate": 5e-05,
      "loss": 0.0098,
      "step": 13270
    },
    {
      "epoch": 11.867932960893855,
      "grad_norm": 0.061990104615688324,
      "learning_rate": 5e-05,
      "loss": 0.0089,
      "step": 13280
    },
    {
      "epoch": 11.876871508379889,
      "grad_norm": 0.047407060861587524,
      "learning_rate": 5e-05,
      "loss": 0.0083,
      "step": 13290
    },
    {
      "epoch": 11.885810055865921,
      "grad_norm": 0.04138926416635513,
      "learning_rate": 5e-05,
      "loss": 0.0107,
      "step": 13300
    },
    {
      "epoch": 11.894748603351955,
      "grad_norm": 0.056880321353673935,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 13310
    },
    {
      "epoch": 11.903687150837989,
      "grad_norm": 0.04596791788935661,
      "learning_rate": 5e-05,
      "loss": 0.0101,
      "step": 13320
    },
    {
      "epoch": 11.912625698324023,
      "grad_norm": 0.03800718113780022,
      "learning_rate": 5e-05,
      "loss": 0.0092,
      "step": 13330
    },
    {
      "epoch": 11.921564245810057,
      "grad_norm": 0.04111894965171814,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 13340
    },
    {
      "epoch": 11.930502793296089,
      "grad_norm": 0.04247434809803963,
      "learning_rate": 5e-05,
      "loss": 0.0094,
      "step": 13350
    },
    {
      "epoch": 11.939441340782123,
      "grad_norm": 0.041438303887844086,
      "learning_rate": 5e-05,
      "loss": 0.0099,
      "step": 13360
    },
    {
      "epoch": 11.948379888268157,
      "grad_norm": 0.04770268127322197,
      "learning_rate": 5e-05,
      "loss": 0.0095,
      "step": 13370
    },
    {
      "epoch": 11.957318435754189,
      "grad_norm": 0.039019111543893814,
      "learning_rate": 5e-05,
      "loss": 0.0104,
      "step": 13380
    },
    {
      "epoch": 11.966256983240223,
      "grad_norm": 0.0414949506521225,
      "learning_rate": 5e-05,
      "loss": 0.0091,
      "step": 13390
    },
    {
      "epoch": 11.975195530726257,
      "grad_norm": 0.039161935448646545,
      "learning_rate": 5e-05,
      "loss": 0.0105,
      "step": 13400
    },
    {
      "epoch": 11.984134078212291,
      "grad_norm": 0.04769616201519966,
      "learning_rate": 5e-05,
      "loss": 0.009,
      "step": 13410
    },
    {
      "epoch": 11.989497206703911,
      "eval_AnatEM": 0.6823529411249273,
      "eval_FabNER": 0.013998250182636136,
      "eval_FindVehicle": 0.7211009173811594,
      "eval_HarveyNER": 0.6629834253637068,
      "eval_MultiNERD": 0.8896672503875206,
      "eval_Ontonotes": 0.904627006559846,
      "eval_TweetNER7": 0.6579352850039079,
      "eval_WikiANN-en": 0.8263254112842502,
      "eval_WikiNeural": 0.9167842030527035,
      "eval_average": 0.7604250948006603,
      "eval_bc2gm": 0.7490636703614585,
      "eval_bc4chemd": 0.7636363635861779,
      "eval_bc5cdr": 0.844155844105628,
      "eval_broad_twitter_corpus": 0.652751423100565,
      "eval_conll2003": 0.9679075737623318,
      "eval_conllpp": 0.9679075737623318,
      "eval_mit-movie": 0.7481371087426636,
      "eval_mit-restaurant": 0.8081725311643486,
      "eval_ncbi": 0.910144927485725,
      "eval_runtime": 403.4746,
      "eval_samples_per_second": 8.922,
      "eval_steps_per_second": 0.28,
      "step": 13416
    },
    {
      "epoch": 11.989497206703911,
      "step": 13416,
      "total_flos": 6.270481843208847e+17,
      "train_loss": 0.018949945644988107,
      "train_runtime": 21452.7807,
      "train_samples_per_second": 80.09,
      "train_steps_per_second": 0.625
    }
  ],
  "logging_steps": 10,
  "max_steps": 13416,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.270481843208847e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
