+ CUDA_VISIBLE_DEVICES=4,5,6,7++ shuf -i25000-30000 -n1+ MASTER_PORT=26649++ hostname+ RUN_NAME=SFT-yuyang-BL-dgx-a100+ DATA_DIR=data+ OUTPUT_DIR=output-lfs+ DATA_CONFIG_DIR=configs/dataset/SFT+ INSTRUCTION_FILE=configs/instruction/GNER-paper.json+ DEEPSPEED_CONFIG=configs/deepspeed/ds2_t5.json+ MODEL_NAME_OR_PATH=google/flan-t5-base+ deepspeed --include=localhost:4,5,6,7 --master_port 26649 gner/run.py --do_train --do_predict --predict_with_generate --run_name SFT-yuyang-BL-dgx-a100 --data_dir data --output_dir output-lfs/SFT-yuyang-BL-dgx-a100 --data_config_dir configs/dataset/SFT --instruction_file configs/instruction/GNER-paper.json --deepspeed configs/deepspeed/ds2_t5.json --model_name_or_path google/flan-t5-base --metric_for_best_model eval_average --load_best_model_at_end True --greater_is_better True --gradient_accumulation_steps 4 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --preprocessing_num_workers 4 --overwrite_output_dir --overwrite_cache --num_train_epochs 12 --max_source_length 640 --max_target_length 640 --generation_max_length 640 --lr_scheduler_type constant --learning_rate 5e-05 --warmup_steps 0 --logging_steps 10 --logging_strategy steps --eval_strategy epoch --save_strategy epoch --seed 1234[2025-02-24 02:40:29,676] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:35,557] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.[2025-02-24 02:40:35,557] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python3.12 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=26649 --enable_each_rank_log=None gner/run.py --do_train --do_predict --predict_with_generate --run_name SFT-yuyang-BL-dgx-a100 --data_dir data --output_dir output-lfs/SFT-yuyang-BL-dgx-a100 --data_config_dir configs/dataset/SFT --instruction_file configs/instruction/GNER-paper.json --deepspeed configs/deepspeed/ds2_t5.json --model_name_or_path google/flan-t5-base --metric_for_best_model eval_average --load_best_model_at_end True --greater_is_better True --gradient_accumulation_steps 4 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --preprocessing_num_workers 4 --overwrite_output_dir --overwrite_cache --num_train_epochs 12 --max_source_length 640 --max_target_length 640 --generation_max_length 640 --lr_scheduler_type constant --learning_rate 5e-05 --warmup_steps 0 --logging_steps 10 --logging_strategy steps --eval_strategy epoch --save_strategy epoch --seed 1234[2025-02-24 02:40:37,931] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:43,600] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [4, 5, 6, 7]}[2025-02-24 02:40:43,601] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0[2025-02-24 02:40:43,601] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})[2025-02-24 02:40:43,601] [INFO] [launch.py:164:main] dist_world_size=4[2025-02-24 02:40:43,601] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=4,5,6,7[2025-02-24 02:40:43,602] [INFO] [launch.py:256:main] process 1242727 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=0', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'SFT-yuyang-BL-dgx-a100', '--data_dir', 'data', '--output_dir', 'output-lfs/SFT-yuyang-BL-dgx-a100', '--data_config_dir', 'configs/dataset/SFT', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234'][2025-02-24 02:40:43,602] [INFO] [launch.py:256:main] process 1242728 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=1', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'SFT-yuyang-BL-dgx-a100', '--data_dir', 'data', '--output_dir', 'output-lfs/SFT-yuyang-BL-dgx-a100', '--data_config_dir', 'configs/dataset/SFT', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234'][2025-02-24 02:40:43,603] [INFO] [launch.py:256:main] process 1242729 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=2', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'SFT-yuyang-BL-dgx-a100', '--data_dir', 'data', '--output_dir', 'output-lfs/SFT-yuyang-BL-dgx-a100', '--data_config_dir', 'configs/dataset/SFT', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234'][2025-02-24 02:40:43,603] [INFO] [launch.py:256:main] process 1242730 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.12', '-u', 'gner/run.py', '--local_rank=3', '--do_train', '--do_predict', '--predict_with_generate', '--run_name', 'SFT-yuyang-BL-dgx-a100', '--data_dir', 'data', '--output_dir', 'output-lfs/SFT-yuyang-BL-dgx-a100', '--data_config_dir', 'configs/dataset/SFT', '--instruction_file', 'configs/instruction/GNER-paper.json', '--deepspeed', 'configs/deepspeed/ds2_t5.json', '--model_name_or_path', 'google/flan-t5-base', '--metric_for_best_model', 'eval_average', '--load_best_model_at_end', 'True', '--greater_is_better', 'True', '--gradient_accumulation_steps', '4', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--preprocessing_num_workers', '4', '--overwrite_output_dir', '--overwrite_cache', '--num_train_epochs', '12', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '640', '--lr_scheduler_type', 'constant', '--learning_rate', '5e-05', '--warmup_steps', '0', '--logging_steps', '10', '--logging_strategy', 'steps', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--seed', '1234'][2025-02-24 02:40:53,687] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:53,692] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:53,694] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:53,699] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)[2025-02-24 02:40:54,325] [INFO] [comm.py:652:init_distributed] cdb=None[2025-02-24 02:40:54,325] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl[2025-02-24 02:40:54,328] [INFO] [comm.py:652:init_distributed] cdb=None[2025-02-24 02:40:54,334] [INFO] [comm.py:652:init_distributed] cdb=None[2025-02-24 02:40:54,353] [INFO] [comm.py:652:init_distributed] cdb=NoneUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).02/24/2025 02:40:54 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: FalseUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).02/24/2025 02:40:54 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: FalseGenerating train split: 0 examples [00:00, ? examples/s]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).02/24/2025 02:40:54 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: FalseUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).02/24/2025 02:40:54 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False02/24/2025 02:40:54 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(_n_gpu=1,accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,average_tokens_across_devices=False,batch_eval_metrics=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=0,dataloader_persistent_workers=False,dataloader_pin_memory=True,dataloader_prefetch_factor=None,ddp_backend=None,ddp_broadcast_buffers=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=configs/deepspeed/ds2_t5.json,disable_tqdm=False,dispatch_batches=None,do_eval=True,do_predict=True,do_train=True,eval_accumulation_steps=None,eval_delay=0,eval_do_concat_batches=True,eval_on_start=False,eval_steps=None,eval_strategy=IntervalStrategy.EPOCH,eval_use_gather_object=False,evaluation_strategy=None,fp16=False,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,generation_config=None,generation_max_length=640,generation_num_beams=None,gradient_accumulation_steps=4,gradient_checkpointing=False,gradient_checkpointing_kwargs=None,greater_is_better=True,group_by_length=False,half_precision_backend=auto,hub_always_push=False,hub_model_id=None,hub_private_repo=None,hub_strategy=HubStrategy.EVERY_SAVE,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_for_metrics=[],include_inputs_for_metrics=False,include_num_input_tokens_seen=False,include_tokens_per_second=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=5e-05,length_column_name=length,load_best_model_at_end=True,local_rank=0,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=output-lfs/SFT-yuyang-BL-dgx-a100/runs/Feb24_02-40-50_dgx-a100,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=IntervalStrategy.STEPS,lr_scheduler_kwargs={},lr_scheduler_type=SchedulerType.CONSTANT,max_grad_norm=1.0,max_steps=-1,metric_for_best_model=eval_average,mp_parameters=,neftune_noise_alpha=None,no_cuda=False,num_train_epochs=12.0,optim=OptimizerNames.ADAMW_TORCH,optim_args=None,optim_target_modules=None,output_dir=output-lfs/SFT-yuyang-BL-dgx-a100,overwrite_output_dir=True,past_index=-1,per_device_eval_batch_size=8,per_device_train_batch_size=8,predict_with_generate=True,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=True,report_to=['tensorboard'],restore_callback_states_from_checkpoint=False,resume_from_checkpoint=None,run_name=SFT-yuyang-BL-dgx-a100,save_on_each_node=False,save_only_model=False,save_safetensors=True,save_steps=500,save_strategy=SaveStrategy.EPOCH,save_total_limit=None,seed=1234,skip_memory_metrics=True,sortish_sampler=False,split_batches=None,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torch_empty_cache_steps=None,torchdynamo=None,tp_size=0,tpu_metrics_debug=False,tpu_num_cores=None,use_cpu=False,use_ipex=False,use_legacy_prediction_loop=False,use_liger_kernel=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=0,weight_decay=0.0,)Using custom data configuration default-1b35419a31a11c1302/24/2025 02:40:54 - INFO - datasets.builder - Using custom data configuration default-1b35419a31a11c13Loading Dataset Infos from /raid/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac802/24/2025 02:40:54 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8Generating train split: 143180 examples [00:17, 8372.18 examples/s]Generating validation split: 3600 examples [00:02, 1378.07 examples/s]Generating test split: 124835 examples [00:10, 12375.70 examples/s]Found cached dataset gner_dataset (/raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8)02/24/2025 02:41:24 - INFO - datasets.builder - Found cached dataset gner_dataset (/raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8)Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac802/24/2025 02:41:24 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac802/24/2025 02:41:24 - INFO - datasets.arrow_dataset - Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac802/24/2025 02:41:24 - INFO - datasets.arrow_dataset - Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac802/24/2025 02:41:24 - INFO - datasets.arrow_dataset - Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8[INFO|configuration_utils.py:699] 2025-02-24 02:41:24,616 >> loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/config.json[INFO|configuration_utils.py:771] 2025-02-24 02:41:24,620 >> Model config T5Config {  "_name_or_path": "google/flan-t5-base",  "architectures": [    "T5ForConditionalGeneration"  ],  "classifier_dropout": 0.0,  "d_ff": 2048,  "d_kv": 64,  "d_model": 768,  "decoder_start_token_id": 0,  "dense_act_fn": "gelu_new",  "dropout_rate": 0.1,  "eos_token_id": 1,  "feed_forward_proj": "gated-gelu",  "initializer_factor": 1.0,  "is_encoder_decoder": true,  "is_gated_act": true,  "layer_norm_epsilon": 1e-06,  "model_type": "t5",  "n_positions": 512,  "num_decoder_layers": 12,  "num_heads": 12,  "num_layers": 12,  "output_past": true,  "pad_token_id": 0,  "relative_attention_max_distance": 128,  "relative_attention_num_buckets": 32,  "task_specific_params": {    "summarization": {      "early_stopping": true,      "length_penalty": 2.0,      "max_length": 200,      "min_length": 30,      "no_repeat_ngram_size": 3,      "num_beams": 4,      "prefix": "summarize: "    },    "translation_en_to_de": {      "early_stopping": true,      "max_length": 300,      "num_beams": 4,      "prefix": "translate English to German: "    },    "translation_en_to_fr": {      "early_stopping": true,      "max_length": 300,      "num_beams": 4,      "prefix": "translate English to French: "    },    "translation_en_to_ro": {      "early_stopping": true,      "max_length": 300,      "num_beams": 4,      "prefix": "translate English to Romanian: "    }  },  "tie_word_embeddings": false,  "transformers_version": "4.50.0.dev0",  "use_cache": true,  "vocab_size": 32128}[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file spiece.model from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/spiece.model[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer.json[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file added_tokens.json from cache at None[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/special_tokens_map.json[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/tokenizer_config.json[INFO|tokenization_utils_base.py:2050] 2025-02-24 02:41:24,823 >> loading file chat_template.jinja from cache at None[INFO|modeling_utils.py:4024] 2025-02-24 02:41:24,905 >> loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/model.safetensors[INFO|configuration_utils.py:1140] 2025-02-24 02:41:24,915 >> Generate config GenerationConfig {  "decoder_start_token_id": 0,  "eos_token_id": 1,  "pad_token_id": 0}[INFO|modeling_utils.py:5017] 2025-02-24 02:41:25,279 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.[INFO|modeling_utils.py:5025] 2025-02-24 02:41:25,279 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.[INFO|configuration_utils.py:1095] 2025-02-24 02:41:25,492 >> loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/7bcac572ce56db69c1ea7c8af255c5d7c9672fc2/generation_config.json[INFO|configuration_utils.py:1140] 2025-02-24 02:41:25,492 >> Generate config GenerationConfig {  "decoder_start_token_id": 0,  "eos_token_id": 1,  "pad_token_id": 0}02/24/2025 02:41:25 - INFO - __main__ - len(dataset) = 143180Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00000_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00000_of_00004.arrowProcess #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00001_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00001_of_00004.arrowProcess #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00002_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00002_of_00004.arrowProcess #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00003_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00003_of_00004.arrowSpawning 4 processes02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Spawning 4 processesRunning tokenizer on train dataset (num_proc=4):   0%|          | 0/143180 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00000_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00000_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00001_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00001_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00002_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00002_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00003_of_00004.arrow02/24/2025 02:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-b6ba1c2d343cef04_00003_of_00004.arrowRunning tokenizer on train dataset (num_proc=4):   3%|▎         | 3710/143180 [00:00<00:22, 6269.57 examples/s][rank2]:[W224 02:41:26.083186759 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.Running tokenizer on train dataset (num_proc=4):   3%|▎         | 4411/143180 [00:00<00:26, 5219.95 examples/s][rank1]:[W224 02:41:26.092534623 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[rank3]:[W224 02:41:26.114409903 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.Running tokenizer on train dataset (num_proc=4): 100%|██████████| 143180/143180 [00:28<00:00, 5081.88 examples/s]Concatenating 4 shards02/24/2025 02:41:53 - INFO - datasets.arrow_dataset - Concatenating 4 shards[rank0]:[W224 02:41:56.048436912 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00000_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00000_of_00004.arrowProcess #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00001_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00001_of_00004.arrowProcess #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00002_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00002_of_00004.arrowProcess #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00003_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00003_of_00004.arrowSpawning 4 processes02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Spawning 4 processesRunning tokenizer on train dataset (num_proc=4):   0%|          | 0/143180 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00000_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00000_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00001_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00001_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00003_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00002_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00003_of_00004.arrow02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-66c82e2263327966_00002_of_00004.arrowRunning tokenizer on validation dataset (num_proc=4): 100%|██████████| 3600/3600 [00:00<00:00, 5319.93 examples/s]Running tokenizer on train dataset (num_proc=4):   3%|▎         | 3651/143180 [00:00<00:23, 6002.49 examples/s]Concatenating 4 shards02/24/2025 02:42:00 - INFO - datasets.arrow_dataset - Concatenating 4 shardsRunning tokenizer on train dataset (num_proc=4): 100%|██████████| 143180/143180 [00:28<00:00, 5040.61 examples/s]Running tokenizer on train dataset (num_proc=4): 100%|██████████| 143180/143180 [00:28<00:00, 5025.99 examples/s]Running tokenizer on train dataset (num_proc=4): 100%|██████████| 143180/143180 [00:29<00:00, 4933.51 examples/s]Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00000_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00000_of_00004.arrowProcess #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00001_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00001_of_00004.arrowProcess #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00002_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00002_of_00004.arrowProcess #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00003_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00003_of_00004.arrowSpawning 4 processes02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Spawning 4 processesRunning tokenizer on validation dataset (num_proc=4):   0%|          | 0/3600 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00000_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00000_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00002_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00003_of_00004.arrowCaching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00001_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00002_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00003_of_00004.arrow02/24/2025 02:42:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-1b35419a31a11c13/0.0.0/9109c58ada9591a04abc1d5c98f192ca2eec81ea7fa627a55c09354537b66ac8/cache-d2f865ea226ebf98_00001_of_00004.arrowRunning tokenizer on validation dataset (num_proc=4): 100%|██████████| 3600/3600 [00:00<00:00, 5501.91 examples/s]s]Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 3600/3600 [00:00<00:00, 5411.63 examples/s]Running tokenizer on validation dataset (num_proc=4): 100%|██████████| 3600/3600 [00:00<00:00, 5445.06 examples/s]Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 124835/124835 [00:18<00:00, 6738.01 examples/s]Concatenating 4 shards02/24/2025 02:42:48 - INFO - datasets.arrow_dataset - Concatenating 4 shardsRunning tokenizer on prediction dataset (num_proc=4):   0%|          | 91/124835 [00:00<03:15, 638.14 examples/s][2025-02-24 02:42:48,293] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown[2025-02-24 02:42:48,294] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 124835/124835 [00:18<00:00, 6626.56 examples/s]Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 124835/124835 [00:18<00:00, 6592.04 examples/s]Running tokenizer on prediction dataset (num_proc=4): 100%|██████████| 124835/124835 [00:19<00:00, 6567.71 examples/s][2025-02-24 02:43:09,028] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False[2025-02-24 02:43:09,032] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer[2025-02-24 02:43:09,032] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer[2025-02-24 02:43:09,041] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam[2025-02-24 02:43:09,041] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>[2025-02-24 02:43:09,041] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer[2025-02-24 02:43:09,041] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000[2025-02-24 02:43:09,041] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000[2025-02-24 02:43:09,041] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False[2025-02-24 02:43:09,041] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False[2025-02-24 02:43:16,434] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started[2025-02-24 02:43:17,905] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started[2025-02-24 02:43:17,905] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started[2025-02-24 02:43:18,465] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states[2025-02-24 02:43:18,466] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.15 GB         CA 1.16 GB         Max_CA 1 GB[2025-02-24 02:43:18,466] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.2 GB, percent = 7.4%[2025-02-24 02:43:19,110] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states[2025-02-24 02:43:19,111] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.38 GB         CA 1.39 GB         Max_CA 1 GB[2025-02-24 02:43:19,111] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.22 GB, percent = 7.4%[2025-02-24 02:43:19,111] [INFO] [stage_1_and_2.py:545:__init__] optimizer state initialized[WARNING|logging.py:329] 2025-02-24 02:43:19,201 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.[WARNING|logging.py:329] 2025-02-24 02:43:19,233 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.[WARNING|logging.py:329] 2025-02-24 02:43:19,246 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.[2025-02-24 02:43:19,287] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer[2025-02-24 02:43:19,288] [INFO] [utils.py:782:see_memory_usage] MA 1.15 GB         Max_MA 1.15 GB         CA 1.39 GB         Max_CA 1 GB[2025-02-24 02:43:19,288] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.25 GB, percent = 7.4%[2025-02-24 02:43:19,290] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer[2025-02-24 02:43:19,290] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started[2025-02-24 02:43:19,290] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR[2025-02-24 02:43:19,290] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f6342ab14f0>[2025-02-24 02:43:19,290] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]][2025-02-24 02:43:19,291] [INFO] [config.py:999:print] DeepSpeedEngine configuration:[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   activation_checkpointing_config  {    "partition_activations": false,    "contiguous_memory_optimization": false,    "cpu_checkpointing": false,    "number_checkpoints": null,    "synchronize_checkpoint_boundary": false,    "profile": false}[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   amp_enabled .................. False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   amp_params ................... False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   autotuning_config ............ {    "enabled": false,    "start_step": null,    "end_step": null,    "metric_path": null,    "arg_mappings": null,    "metric": "throughput",    "model_info": null,    "results_dir": "autotuning_results",    "exps_dir": "autotuning_exps",    "overwrite": true,    "fast": true,    "start_profile_step": 3,    "end_profile_step": 5,    "tuner_type": "gridsearch",    "tuner_early_stopping": 5,    "tuner_num_trials": 50,    "model_info_path": null,    "mp_size": 1,    "max_train_batch_size": null,    "min_train_batch_size": 1,    "max_train_micro_batch_size_per_gpu": 1.024000e+03,    "min_train_micro_batch_size_per_gpu": 1,    "num_tuning_micro_batch_sizes": 3}[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6302ce0470>[2025-02-24 02:43:19,292] [INFO] [config.py:1003:print]   communication_data_type ...... None[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   disable_allgather ............ False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   dump_state ................... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   elasticity_enabled ........... False[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   flops_profiler_config ........ {    "enabled": false,    "recompute_fwd_factor": 0.0,    "profile_step": 1,    "module_depth": -1,    "top_modules": 1,    "detailed": true,    "output_file": null}[2025-02-24 02:43:19,293] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   fp16_enabled ................. False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   global_rank .................. 0[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   graph_harvesting ............. False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   loss_scale ................... 0[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   memory_breakdown ............. False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   mics_shard_size .............. -1[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   nebula_config ................ {    "enabled": false,    "persistent_storage_path": null,    "persistent_time_interval": 100,    "num_of_version_in_retention": 2,    "enable_nebula_load": true,    "load_path": null}[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False[2025-02-24 02:43:19,294] [INFO] [config.py:1003:print]   optimizer_name ............... adamw[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   pld_enabled .................. False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   pld_params ................... False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   prescale_gradients ........... False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   scheduler_name ............... WarmupLR[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   sparse_attention ............. None[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   steps_per_print .............. inf[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   train_batch_size ............. 128[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   use_node_local_storage ....... False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   weight_quantization_config ... None[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   world_size ................... 4[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False[2025-02-24 02:43:19,295] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True[2025-02-24 02:43:19,296] [INFO] [config.py:1003:print]   zero_enabled ................. True[2025-02-24 02:43:19,296] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True[2025-02-24 02:43:19,296] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2[2025-02-24 02:43:19,296] [INFO] [config.py:989:print_user_config]   json = {    "fp16": {        "enabled": false    },    "bf16": {        "enabled": false    },    "optimizer": {        "type": "AdamW",        "params": {            "lr": 5e-05,            "betas": [0.9, 0.999],            "eps": 1e-08,            "weight_decay": 0.0        }    },    "scheduler": {        "type": "WarmupLR",        "params": {            "warmup_min_lr": 0,            "warmup_max_lr": 5e-05,            "warmup_num_steps": 0        }    },    "zero_optimization": {        "stage": 2,        "allgather_partitions": true,        "allgather_bucket_size": 2.000000e+08,        "overlap_comm": true,        "reduce_scatter": true,        "reduce_bucket_size": 2.000000e+08,        "contiguous_gradients": true    },    "gradient_accumulation_steps": 4,    "gradient_clipping": 1.0,    "train_batch_size": 128,    "train_micro_batch_size_per_gpu": 8,    "steps_per_print": inf}[INFO|trainer.py:2407] 2025-02-24 02:43:19,297 >> ***** Running training *****[INFO|trainer.py:2408] 2025-02-24 02:43:19,297 >>   Num examples = 143,180[INFO|trainer.py:2409] 2025-02-24 02:43:19,297 >>   Num Epochs = 12[INFO|trainer.py:2410] 2025-02-24 02:43:19,297 >>   Instantaneous batch size per device = 8[INFO|trainer.py:2413] 2025-02-24 02:43:19,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 128[INFO|trainer.py:2414] 2025-02-24 02:43:19,297 >>   Gradient Accumulation steps = 4[INFO|trainer.py:2415] 2025-02-24 02:43:19,297 >>   Total optimization steps = 13,416[INFO|trainer.py:2416] 2025-02-24 02:43:19,298 >>   Number of trainable parameters = 247,577,856  0%|          | 0/13416 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-02-24 02:43:19,635 >> Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.{'loss': 1.0643, 'grad_norm': 1.3608158826828003, 'learning_rate': 5e-05, 'epoch': 0.01}{'loss': 0.3152, 'grad_norm': 0.45210137963294983, 'learning_rate': 5e-05, 'epoch': 0.02}{'loss': 0.2056, 'grad_norm': 0.34399768710136414, 'learning_rate': 5e-05, 'epoch': 0.03}{'loss': 0.1636, 'grad_norm': 0.32977738976478577, 'learning_rate': 5e-05, 'epoch': 0.04}{'loss': 0.1391, 'grad_norm': 0.23794935643672943, 'learning_rate': 5e-05, 'epoch': 0.04}{'loss': 0.1296, 'grad_norm': 0.21703729033470154, 'learning_rate': 5e-05, 'epoch': 0.05}{'loss': 0.1149, 'grad_norm': 0.21630258858203888, 'learning_rate': 5e-05, 'epoch': 0.06}{'loss': 0.1107, 'grad_norm': 0.23872493207454681, 'learning_rate': 5e-05, 'epoch': 0.07}{'loss': 0.0983, 'grad_norm': 0.1507975161075592, 'learning_rate': 5e-05, 'epoch': 0.08}{'loss': 0.1008, 'grad_norm': 0.19742536544799805, 'learning_rate': 5e-05, 'epoch': 0.09}{'loss': 0.1, 'grad_norm': 0.20530596375465393, 'learning_rate': 5e-05, 'epoch': 0.1}{'loss': 0.0881, 'grad_norm': 0.20546752214431763, 'learning_rate': 5e-05, 'epoch': 0.11}{'loss': 0.087, 'grad_norm': 0.28300124406814575, 'learning_rate': 5e-05, 'epoch': 0.12}{'loss': 0.0835, 'grad_norm': 0.1980736255645752, 'learning_rate': 5e-05, 'epoch': 0.13}{'loss': 0.0788, 'grad_norm': 0.1548335999250412, 'learning_rate': 5e-05, 'epoch': 0.13}{'loss': 0.0806, 'grad_norm': 0.16939006745815277, 'learning_rate': 5e-05, 'epoch': 0.14}{'loss': 0.0762, 'grad_norm': 0.17983846366405487, 'learning_rate': 5e-05, 'epoch': 0.15}{'loss': 0.0733, 'grad_norm': 0.17687354981899261, 'learning_rate': 5e-05, 'epoch': 0.16}{'loss': 0.0731, 'grad_norm': 0.33722972869873047, 'learning_rate': 5e-05, 'epoch': 0.17}{'loss': 0.0705, 'grad_norm': 0.1837114840745926, 'learning_rate': 5e-05, 'epoch': 0.18}{'loss': 0.0719, 'grad_norm': 0.14230914413928986, 'learning_rate': 5e-05, 'epoch': 0.19}{'loss': 0.0669, 'grad_norm': 0.1280413269996643, 'learning_rate': 5e-05, 'epoch': 0.2}{'loss': 0.0644, 'grad_norm': 0.19885091483592987, 'learning_rate': 5e-05, 'epoch': 0.21}{'loss': 0.0626, 'grad_norm': 0.1613454520702362, 'learning_rate': 5e-05, 'epoch': 0.21}{'loss': 0.0672, 'grad_norm': 0.14053606986999512, 'learning_rate': 5e-05, 'epoch': 0.22}{'loss': 0.0636, 'grad_norm': 0.1670684814453125, 'learning_rate': 5e-05, 'epoch': 0.23}{'loss': 0.0626, 'grad_norm': 0.10619278997182846, 'learning_rate': 5e-05, 'epoch': 0.24}{'loss': 0.0598, 'grad_norm': 0.23772716522216797, 'learning_rate': 5e-05, 'epoch': 0.25}{'loss': 0.0637, 'grad_norm': 0.1508154571056366, 'learning_rate': 5e-05, 'epoch': 0.26}{'loss': 0.0587, 'grad_norm': 0.14329862594604492, 'learning_rate': 5e-05, 'epoch': 0.27}{'loss': 0.0598, 'grad_norm': 0.15412208437919617, 'learning_rate': 5e-05, 'epoch': 0.28}{'loss': 0.0587, 'grad_norm': 0.14805443584918976, 'learning_rate': 5e-05, 'epoch': 0.29}{'loss': 0.056, 'grad_norm': 0.11470546573400497, 'learning_rate': 5e-05, 'epoch': 0.29}{'loss': 0.0569, 'grad_norm': 0.12347155064344406, 'learning_rate': 5e-05, 'epoch': 0.3}{'loss': 0.0552, 'grad_norm': 0.12979483604431152, 'learning_rate': 5e-05, 'epoch': 0.31}{'loss': 0.054, 'grad_norm': 0.14177380502223969, 'learning_rate': 5e-05, 'epoch': 0.32}{'loss': 0.0554, 'grad_norm': 0.13859522342681885, 'learning_rate': 5e-05, 'epoch': 0.33}{'loss': 0.0519, 'grad_norm': 0.13452379405498505, 'learning_rate': 5e-05, 'epoch': 0.34}{'loss': 0.0536, 'grad_norm': 0.1670239120721817, 'learning_rate': 5e-05, 'epoch': 0.35}{'loss': 0.0547, 'grad_norm': 0.13676439225673676, 'learning_rate': 5e-05, 'epoch': 0.36}{'loss': 0.0524, 'grad_norm': 0.16163888573646545, 'learning_rate': 5e-05, 'epoch': 0.37}{'loss': 0.0507, 'grad_norm': 0.11922567337751389, 'learning_rate': 5e-05, 'epoch': 0.38}{'loss': 0.0522, 'grad_norm': 0.13087250292301178, 'learning_rate': 5e-05, 'epoch': 0.38}{'loss': 0.0499, 'grad_norm': 0.10397513210773468, 'learning_rate': 5e-05, 'epoch': 0.39}{'loss': 0.0498, 'grad_norm': 0.11999259889125824, 'learning_rate': 5e-05, 'epoch': 0.4}{'loss': 0.0496, 'grad_norm': 0.13263726234436035, 'learning_rate': 5e-05, 'epoch': 0.41}{'loss': 0.0491, 'grad_norm': 0.11318276822566986, 'learning_rate': 5e-05, 'epoch': 0.42}{'loss': 0.0494, 'grad_norm': 0.1488083004951477, 'learning_rate': 5e-05, 'epoch': 0.43}{'loss': 0.0476, 'grad_norm': 0.12324687093496323, 'learning_rate': 5e-05, 'epoch': 0.44}{'loss': 0.0491, 'grad_norm': 0.18245026469230652, 'learning_rate': 5e-05, 'epoch': 0.45}{'loss': 0.047, 'grad_norm': 0.11413589119911194, 'learning_rate': 5e-05, 'epoch': 0.46}{'loss': 0.0473, 'grad_norm': 0.11385564506053925, 'learning_rate': 5e-05, 'epoch': 0.46}{'loss': 0.0471, 'grad_norm': 0.13580097258090973, 'learning_rate': 5e-05, 'epoch': 0.47}{'loss': 0.0426, 'grad_norm': 0.11891704052686691, 'learning_rate': 5e-05, 'epoch': 0.48}{'loss': 0.0458, 'grad_norm': 0.14956477284431458, 'learning_rate': 5e-05, 'epoch': 0.49}{'loss': 0.0447, 'grad_norm': 0.09692291170358658, 'learning_rate': 5e-05, 'epoch': 0.5}{'loss': 0.0445, 'grad_norm': 0.12099242955446243, 'learning_rate': 5e-05, 'epoch': 0.51}{'loss': 0.0427, 'grad_norm': 0.11015671491622925, 'learning_rate': 5e-05, 'epoch': 0.52}{'loss': 0.0424, 'grad_norm': 0.1056525707244873, 'learning_rate': 5e-05, 'epoch': 0.53}{'loss': 0.0437, 'grad_norm': 0.09845397621393204, 'learning_rate': 5e-05, 'epoch': 0.54}{'loss': 0.0444, 'grad_norm': 0.14905968308448792, 'learning_rate': 5e-05, 'epoch': 0.55}{'loss': 0.0437, 'grad_norm': 0.12235887348651886, 'learning_rate': 5e-05, 'epoch': 0.55}{'loss': 0.0433, 'grad_norm': 0.10665979236364365, 'learning_rate': 5e-05, 'epoch': 0.56}{'loss': 0.043, 'grad_norm': 0.11393610388040543, 'learning_rate': 5e-05, 'epoch': 0.57}{'loss': 0.0411, 'grad_norm': 0.1350659728050232, 'learning_rate': 5e-05, 'epoch': 0.58}{'loss': 0.0384, 'grad_norm': 0.13946954905986786, 'learning_rate': 5e-05, 'epoch': 0.59}{'loss': 0.0421, 'grad_norm': 0.10625141859054565, 'learning_rate': 5e-05, 'epoch': 0.6}{'loss': 0.0419, 'grad_norm': 0.12473705410957336, 'learning_rate': 5e-05, 'epoch': 0.61}{'loss': 0.0403, 'grad_norm': 0.10658219456672668, 'learning_rate': 5e-05, 'epoch': 0.62}{'loss': 0.0396, 'grad_norm': 0.10108156502246857, 'learning_rate': 5e-05, 'epoch': 0.63}{'loss': 0.0388, 'grad_norm': 0.10756638646125793, 'learning_rate': 5e-05, 'epoch': 0.63}{'loss': 0.0412, 'grad_norm': 0.09791532903909683, 'learning_rate': 5e-05, 'epoch': 0.64}{'loss': 0.0372, 'grad_norm': 0.0995614305138588, 'learning_rate': 5e-05, 'epoch': 0.65}{'loss': 0.04, 'grad_norm': 0.15138044953346252, 'learning_rate': 5e-05, 'epoch': 0.66}{'loss': 0.0381, 'grad_norm': 0.09500790387392044, 'learning_rate': 5e-05, 'epoch': 0.67}{'loss': 0.039, 'grad_norm': 0.12435303628444672, 'learning_rate': 5e-05, 'epoch': 0.68}{'loss': 0.0375, 'grad_norm': 0.12718519568443298, 'learning_rate': 5e-05, 'epoch': 0.69}{'loss': 0.0405, 'grad_norm': 0.1373368501663208, 'learning_rate': 5e-05, 'epoch': 0.7}{'loss': 0.039, 'grad_norm': 0.08546178042888641, 'learning_rate': 5e-05, 'epoch': 0.71}{'loss': 0.0353, 'grad_norm': 0.11154385656118393, 'learning_rate': 5e-05, 'epoch': 0.72}{'loss': 0.0385, 'grad_norm': 0.11031264811754227, 'learning_rate': 5e-05, 'epoch': 0.72}{'loss': 0.0369, 'grad_norm': 0.10928153991699219, 'learning_rate': 5e-05, 'epoch': 0.73}{'loss': 0.0357, 'grad_norm': 0.10808121412992477, 'learning_rate': 5e-05, 'epoch': 0.74}{'loss': 0.036, 'grad_norm': 0.1271839141845703, 'learning_rate': 5e-05, 'epoch': 0.75}{'loss': 0.0357, 'grad_norm': 0.10396832972764969, 'learning_rate': 5e-05, 'epoch': 0.76}{'loss': 0.0375, 'grad_norm': 0.12680551409721375, 'learning_rate': 5e-05, 'epoch': 0.77}{'loss': 0.0384, 'grad_norm': 0.20622310042381287, 'learning_rate': 5e-05, 'epoch': 0.78}{'loss': 0.0375, 'grad_norm': 0.11346101760864258, 'learning_rate': 5e-05, 'epoch': 0.79}{'loss': 0.0373, 'grad_norm': 0.1240425780415535, 'learning_rate': 5e-05, 'epoch': 0.8}{'loss': 0.036, 'grad_norm': 0.11950046569108963, 'learning_rate': 5e-05, 'epoch': 0.8}{'loss': 0.0335, 'grad_norm': 0.08711783587932587, 'learning_rate': 5e-05, 'epoch': 0.81}{'loss': 0.0359, 'grad_norm': 0.17376133799552917, 'learning_rate': 5e-05, 'epoch': 0.82}{'loss': 0.0349, 'grad_norm': 0.1176934540271759, 'learning_rate': 5e-05, 'epoch': 0.83}{'loss': 0.0348, 'grad_norm': 0.10209446400403976, 'learning_rate': 5e-05, 'epoch': 0.84}{'loss': 0.0341, 'grad_norm': 0.0930309146642685, 'learning_rate': 5e-05, 'epoch': 0.85}{'loss': 0.0364, 'grad_norm': 0.13660287857055664, 'learning_rate': 5e-05, 'epoch': 0.86}{'loss': 0.0364, 'grad_norm': 0.09471837431192398, 'learning_rate': 5e-05, 'epoch': 0.87}{'loss': 0.0352, 'grad_norm': 0.12352073192596436, 'learning_rate': 5e-05, 'epoch': 0.88}{'loss': 0.0349, 'grad_norm': 0.10920679569244385, 'learning_rate': 5e-05, 'epoch': 0.88}{'loss': 0.0343, 'grad_norm': 0.09083276987075806, 'learning_rate': 5e-05, 'epoch': 0.89}{'loss': 0.0346, 'grad_norm': 0.13556431233882904, 'learning_rate': 5e-05, 'epoch': 0.9}{'loss': 0.0313, 'grad_norm': 0.15446995198726654, 'learning_rate': 5e-05, 'epoch': 0.91}{'loss': 0.0335, 'grad_norm': 0.10300192981958389, 'learning_rate': 5e-05, 'epoch': 0.92}{'loss': 0.033, 'grad_norm': 0.0983043983578682, 'learning_rate': 5e-05, 'epoch': 0.93}{'loss': 0.0341, 'grad_norm': 0.09812165051698685, 'learning_rate': 5e-05, 'epoch': 0.94}{'loss': 0.0339, 'grad_norm': 0.09566853195428848, 'learning_rate': 5e-05, 'epoch': 0.95}{'loss': 0.0328, 'grad_norm': 0.0970584824681282, 'learning_rate': 5e-05, 'epoch': 0.96}{'loss': 0.0301, 'grad_norm': 0.07697637379169464, 'learning_rate': 5e-05, 'epoch': 0.97}{'loss': 0.0313, 'grad_norm': 0.08765705674886703, 'learning_rate': 5e-05, 'epoch': 0.97}{'loss': 0.0331, 'grad_norm': 0.11163213849067688, 'learning_rate': 5e-05, 'epoch': 0.98}{'loss': 0.0338, 'grad_norm': 0.0979473739862442, 'learning_rate': 5e-05, 'epoch': 0.99}{'eval_AnatEM': 0.291262135873843, 'eval_bc2gm': 0.5070422534711269, 'eval_bc4chemd': 0.3942652329356508, 'eval_bc5cdr': 0.7386091126600072, 'eval_broad_twitter_corpus': 0.6113207546675736, 'eval_conllpp': 0.8934531450075371, 'eval_conll2003': 0.8908857509125442, 'eval_FabNER': 0.005769230742271265, 'eval_FindVehicle': 0.7201834861884989, 'eval_HarveyNER': 0.3048780487303019, 'eval_mit-movie': 0.7031019201861475, 'eval_mit-restaurant': 0.7191011235453602, 'eval_MultiNERD': 0.8306010928459203, 'eval_ncbi': 0.7420289854568368, 'eval_Ontonotes': 0.8303655107277456, 'eval_TweetNER7': 0.5903151421482892, 'eval_WikiANN-en': 0.7536231883555266, 'eval_WikiNeural': 0.8273894436016949, 'eval_average': 0.6307886421142708, 'eval_runtime': 406.4851, 'eval_samples_per_second': 8.856, 'eval_steps_per_second': 0.278, 'epoch': 1.0}  8%|▊         | 1119/13416 [30:00<3:37:43,  1.06[INFO|trainer.py:3948] 2025-02-24 03:13:20,414 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119[INFO|configuration_utils.py:423] 2025-02-24 03:13:20,416 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/config.json[INFO|configuration_utils.py:909] 2025-02-24 03:13:20,417 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 03:13:21,551 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 03:13:21,553 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 03:13:21,553 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 03:13:21,554 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/spiece.model[2025-02-24 03:13:21,571] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1118 is about to be saved![2025-02-24 03:13:21,607] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/mp_rank_00_model_states.pt[2025-02-24 03:13:21,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/mp_rank_00_model_states.pt...[2025-02-24 03:13:22,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/mp_rank_00_model_states.pt.[2025-02-24 03:13:22,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 03:13:23,883] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 03:13:23,884] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-1119/global_step1118/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 03:13:23,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1118 is ready now!{'loss': 0.0318, 'grad_norm': 0.1508300006389618, 'learning_rate': 5e-05, 'epoch': 1.0}{'loss': 0.0323, 'grad_norm': 0.08522915095090866, 'learning_rate': 5e-05, 'epoch': 1.01}{'loss': 0.031, 'grad_norm': 0.09854909777641296, 'learning_rate': 5e-05, 'epoch': 1.02}{'loss': 0.032, 'grad_norm': 0.09986922889947891, 'learning_rate': 5e-05, 'epoch': 1.03}{'loss': 0.0318, 'grad_norm': 0.12007535994052887, 'learning_rate': 5e-05, 'epoch': 1.04}{'loss': 0.0313, 'grad_norm': 0.09712943434715271, 'learning_rate': 5e-05, 'epoch': 1.05}{'loss': 0.0311, 'grad_norm': 0.10448198765516281, 'learning_rate': 5e-05, 'epoch': 1.05}{'loss': 0.0312, 'grad_norm': 0.09175737202167511, 'learning_rate': 5e-05, 'epoch': 1.06}{'loss': 0.0314, 'grad_norm': 0.1079220175743103, 'learning_rate': 5e-05, 'epoch': 1.07}{'loss': 0.0301, 'grad_norm': 0.09090516716241837, 'learning_rate': 5e-05, 'epoch': 1.08}{'loss': 0.0316, 'grad_norm': 0.08954821527004242, 'learning_rate': 5e-05, 'epoch': 1.09}{'loss': 0.0301, 'grad_norm': 0.11446047574281693, 'learning_rate': 5e-05, 'epoch': 1.1}{'loss': 0.0303, 'grad_norm': 0.09049084037542343, 'learning_rate': 5e-05, 'epoch': 1.11}{'loss': 0.0297, 'grad_norm': 0.1349681168794632, 'learning_rate': 5e-05, 'epoch': 1.12}{'loss': 0.0287, 'grad_norm': 0.09404056519269943, 'learning_rate': 5e-05, 'epoch': 1.13}{'loss': 0.0299, 'grad_norm': 0.10767237097024918, 'learning_rate': 5e-05, 'epoch': 1.13}{'loss': 0.0304, 'grad_norm': 0.1136915311217308, 'learning_rate': 5e-05, 'epoch': 1.14}{'loss': 0.0291, 'grad_norm': 0.13066191971302032, 'learning_rate': 5e-05, 'epoch': 1.15}{'loss': 0.0294, 'grad_norm': 0.09551674127578735, 'learning_rate': 5e-05, 'epoch': 1.16}{'loss': 0.03, 'grad_norm': 0.11234297603368759, 'learning_rate': 5e-05, 'epoch': 1.17}{'loss': 0.0307, 'grad_norm': 0.11956655979156494, 'learning_rate': 5e-05, 'epoch': 1.18}{'loss': 0.0318, 'grad_norm': 0.14179959893226624, 'learning_rate': 5e-05, 'epoch': 1.19}{'loss': 0.0302, 'grad_norm': 0.09096759557723999, 'learning_rate': 5e-05, 'epoch': 1.2}{'loss': 0.0293, 'grad_norm': 0.13477696478366852, 'learning_rate': 5e-05, 'epoch': 1.21}{'loss': 0.0307, 'grad_norm': 0.09984178841114044, 'learning_rate': 5e-05, 'epoch': 1.22}{'loss': 0.0308, 'grad_norm': 0.10375247150659561, 'learning_rate': 5e-05, 'epoch': 1.22}{'loss': 0.0293, 'grad_norm': 0.09686172008514404, 'learning_rate': 5e-05, 'epoch': 1.23}{'loss': 0.0295, 'grad_norm': 0.0706484392285347, 'learning_rate': 5e-05, 'epoch': 1.24}{'loss': 0.0291, 'grad_norm': 0.1369088590145111, 'learning_rate': 5e-05, 'epoch': 1.25}{'loss': 0.029, 'grad_norm': 0.15020212531089783, 'learning_rate': 5e-05, 'epoch': 1.26}{'loss': 0.0298, 'grad_norm': 0.1457827240228653, 'learning_rate': 5e-05, 'epoch': 1.27}{'loss': 0.0299, 'grad_norm': 0.11641058325767517, 'learning_rate': 5e-05, 'epoch': 1.28}{'loss': 0.0302, 'grad_norm': 0.10550510138273239, 'learning_rate': 5e-05, 'epoch': 1.29}{'loss': 0.0282, 'grad_norm': 0.13687492907047272, 'learning_rate': 5e-05, 'epoch': 1.3}{'loss': 0.0273, 'grad_norm': 0.11215363442897797, 'learning_rate': 5e-05, 'epoch': 1.3}{'loss': 0.031, 'grad_norm': 0.09749653190374374, 'learning_rate': 5e-05, 'epoch': 1.31}{'loss': 0.0292, 'grad_norm': 0.07411917299032211, 'learning_rate': 5e-05, 'epoch': 1.32}{'loss': 0.0263, 'grad_norm': 0.10616487264633179, 'learning_rate': 5e-05, 'epoch': 1.33}{'loss': 0.0294, 'grad_norm': 0.08675713837146759, 'learning_rate': 5e-05, 'epoch': 1.34}{'loss': 0.0284, 'grad_norm': 0.11582214385271072, 'learning_rate': 5e-05, 'epoch': 1.35}{'loss': 0.0282, 'grad_norm': 0.08086715638637543, 'learning_rate': 5e-05, 'epoch': 1.36}{'loss': 0.0279, 'grad_norm': 0.08152089267969131, 'learning_rate': 5e-05, 'epoch': 1.37}{'loss': 0.0276, 'grad_norm': 0.08737868815660477, 'learning_rate': 5e-05, 'epoch': 1.38}{'loss': 0.0293, 'grad_norm': 0.09282663464546204, 'learning_rate': 5e-05, 'epoch': 1.39}{'loss': 0.0279, 'grad_norm': 0.08959618210792542, 'learning_rate': 5e-05, 'epoch': 1.39}{'loss': 0.029, 'grad_norm': 0.12422776967287064, 'learning_rate': 5e-05, 'epoch': 1.4}{'loss': 0.028, 'grad_norm': 0.06420591473579407, 'learning_rate': 5e-05, 'epoch': 1.41}{'loss': 0.0289, 'grad_norm': 0.0820733904838562, 'learning_rate': 5e-05, 'epoch': 1.42}{'loss': 0.0258, 'grad_norm': 0.08058261126279831, 'learning_rate': 5e-05, 'epoch': 1.43}{'loss': 0.0276, 'grad_norm': 0.0982515960931778, 'learning_rate': 5e-05, 'epoch': 1.44}{'loss': 0.0279, 'grad_norm': 0.09529227763414383, 'learning_rate': 5e-05, 'epoch': 1.45}{'loss': 0.0265, 'grad_norm': 0.08704535663127899, 'learning_rate': 5e-05, 'epoch': 1.46}{'loss': 0.0258, 'grad_norm': 0.11137789487838745, 'learning_rate': 5e-05, 'epoch': 1.47}{'loss': 0.0275, 'grad_norm': 0.0889832004904747, 'learning_rate': 5e-05, 'epoch': 1.47}{'loss': 0.0265, 'grad_norm': 0.1158461943268776, 'learning_rate': 5e-05, 'epoch': 1.48}{'loss': 0.0251, 'grad_norm': 0.06763870269060135, 'learning_rate': 5e-05, 'epoch': 1.49}{'loss': 0.0277, 'grad_norm': 0.16856549680233002, 'learning_rate': 5e-05, 'epoch': 1.5}{'loss': 0.0255, 'grad_norm': 0.0754920169711113, 'learning_rate': 5e-05, 'epoch': 1.51}{'loss': 0.0258, 'grad_norm': 0.12911169230937958, 'learning_rate': 5e-05, 'epoch': 1.52}{'loss': 0.0272, 'grad_norm': 0.06482373923063278, 'learning_rate': 5e-05, 'epoch': 1.53}{'loss': 0.0276, 'grad_norm': 0.08769845217466354, 'learning_rate': 5e-05, 'epoch': 1.54}{'loss': 0.0279, 'grad_norm': 0.06961068511009216, 'learning_rate': 5e-05, 'epoch': 1.55}{'loss': 0.0245, 'grad_norm': 0.1080135628581047, 'learning_rate': 5e-05, 'epoch': 1.56}{'loss': 0.024, 'grad_norm': 0.08524374663829803, 'learning_rate': 5e-05, 'epoch': 1.56}{'loss': 0.028, 'grad_norm': 0.0834922045469284, 'learning_rate': 5e-05, 'epoch': 1.57}{'loss': 0.0285, 'grad_norm': 0.11286009103059769, 'learning_rate': 5e-05, 'epoch': 1.58}{'loss': 0.026, 'grad_norm': 0.08881642669439316, 'learning_rate': 5e-05, 'epoch': 1.59}{'loss': 0.0244, 'grad_norm': 0.0863368958234787, 'learning_rate': 5e-05, 'epoch': 1.6}{'loss': 0.0261, 'grad_norm': 0.07552596181631088, 'learning_rate': 5e-05, 'epoch': 1.61}{'loss': 0.0261, 'grad_norm': 0.07613509148359299, 'learning_rate': 5e-05, 'epoch': 1.62}{'loss': 0.0247, 'grad_norm': 0.06286927312612534, 'learning_rate': 5e-05, 'epoch': 1.63}{'loss': 0.0254, 'grad_norm': 0.07220866531133652, 'learning_rate': 5e-05, 'epoch': 1.64}{'loss': 0.0258, 'grad_norm': 0.06455791741609573, 'learning_rate': 5e-05, 'epoch': 1.64}{'loss': 0.0259, 'grad_norm': 0.07678110152482986, 'learning_rate': 5e-05, 'epoch': 1.65}{'loss': 0.0251, 'grad_norm': 0.08650211244821548, 'learning_rate': 5e-05, 'epoch': 1.66}{'loss': 0.0252, 'grad_norm': 0.10006625205278397, 'learning_rate': 5e-05, 'epoch': 1.67}{'loss': 0.0241, 'grad_norm': 0.09318188577890396, 'learning_rate': 5e-05, 'epoch': 1.68}{'loss': 0.0231, 'grad_norm': 0.08340974152088165, 'learning_rate': 5e-05, 'epoch': 1.69}{'loss': 0.0235, 'grad_norm': 0.0885632336139679, 'learning_rate': 5e-05, 'epoch': 1.7}{'loss': 0.025, 'grad_norm': 0.07894979417324066, 'learning_rate': 5e-05, 'epoch': 1.71}{'loss': 0.0249, 'grad_norm': 0.06937915831804276, 'learning_rate': 5e-05, 'epoch': 1.72}{'loss': 0.0246, 'grad_norm': 0.07017017155885696, 'learning_rate': 5e-05, 'epoch': 1.72}{'loss': 0.0239, 'grad_norm': 0.07401300966739655, 'learning_rate': 5e-05, 'epoch': 1.73}{'loss': 0.0231, 'grad_norm': 0.06589407473802567, 'learning_rate': 5e-05, 'epoch': 1.74}{'loss': 0.0233, 'grad_norm': 0.10907035320997238, 'learning_rate': 5e-05, 'epoch': 1.75}{'loss': 0.0239, 'grad_norm': 0.0623776838183403, 'learning_rate': 5e-05, 'epoch': 1.76}{'loss': 0.0231, 'grad_norm': 0.10180862993001938, 'learning_rate': 5e-05, 'epoch': 1.77}{'loss': 0.0247, 'grad_norm': 0.08448014408349991, 'learning_rate': 5e-05, 'epoch': 1.78}{'loss': 0.0264, 'grad_norm': 0.07123516499996185, 'learning_rate': 5e-05, 'epoch': 1.79}{'loss': 0.0239, 'grad_norm': 0.0730782002210617, 'learning_rate': 5e-05, 'epoch': 1.8}{'loss': 0.0248, 'grad_norm': 0.08645619451999664, 'learning_rate': 5e-05, 'epoch': 1.81}{'loss': 0.0235, 'grad_norm': 0.09724660962820053, 'learning_rate': 5e-05, 'epoch': 1.81}{'loss': 0.0255, 'grad_norm': 0.069844089448452, 'learning_rate': 5e-05, 'epoch': 1.82}{'loss': 0.0228, 'grad_norm': 0.07002456486225128, 'learning_rate': 5e-05, 'epoch': 1.83}{'loss': 0.0247, 'grad_norm': 0.08539856970310211, 'learning_rate': 5e-05, 'epoch': 1.84}{'loss': 0.0242, 'grad_norm': 0.0809856429696083, 'learning_rate': 5e-05, 'epoch': 1.85}{'loss': 0.0246, 'grad_norm': 0.0948903039097786, 'learning_rate': 5e-05, 'epoch': 1.86}{'loss': 0.0251, 'grad_norm': 0.10183800756931305, 'learning_rate': 5e-05, 'epoch': 1.87}{'loss': 0.0239, 'grad_norm': 0.06816703081130981, 'learning_rate': 5e-05, 'epoch': 1.88}{'loss': 0.0232, 'grad_norm': 0.07817968726158142, 'learning_rate': 5e-05, 'epoch': 1.89}{'loss': 0.0218, 'grad_norm': 0.1420930027961731, 'learning_rate': 5e-05, 'epoch': 1.89}{'loss': 0.022, 'grad_norm': 0.09106304496526718, 'learning_rate': 5e-05, 'epoch': 1.9}{'loss': 0.0249, 'grad_norm': 0.12210578471422195, 'learning_rate': 5e-05, 'epoch': 1.91}{'loss': 0.0237, 'grad_norm': 0.07335058599710464, 'learning_rate': 5e-05, 'epoch': 1.92}{'loss': 0.0239, 'grad_norm': 0.09029153734445572, 'learning_rate': 5e-05, 'epoch': 1.93}{'loss': 0.0232, 'grad_norm': 0.06431115418672562, 'learning_rate': 5e-05, 'epoch': 1.94}{'loss': 0.0239, 'grad_norm': 0.08800571411848068, 'learning_rate': 5e-05, 'epoch': 1.95}{'loss': 0.0235, 'grad_norm': 0.1121697723865509, 'learning_rate': 5e-05, 'epoch': 1.96}{'loss': 0.0236, 'grad_norm': 0.10417275130748749, 'learning_rate': 5e-05, 'epoch': 1.97}{'loss': 0.0228, 'grad_norm': 0.09014527499675751, 'learning_rate': 5e-05, 'epoch': 1.98}{'loss': 0.0249, 'grad_norm': 0.09343826770782471, 'learning_rate': 5e-05, 'epoch': 1.98}{'loss': 0.0224, 'grad_norm': 0.1620931625366211, 'learning_rate': 5e-05, 'epoch': 1.99}{'eval_AnatEM': 0.4848484847989797, 'eval_bc2gm': 0.6149068322496084, 'eval_bc4chemd': 0.5570291776689205, 'eval_bc5cdr': 0.761104441726871, 'eval_broad_twitter_corpus': 0.6923076922574608, 'eval_conllpp': 0.9242618741474522, 'eval_conll2003': 0.9230769230266868, 'eval_FabNER': 0.015693112430939585, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.5149700598297107, 'eval_mit-movie': 0.7286135692713347, 'eval_mit-restaurant': 0.7723669308671576, 'eval_MultiNERD': 0.8621908126705434, 'eval_ncbi': 0.8044077134484166, 'eval_Ontonotes': 0.8682464454474692, 'eval_TweetNER7': 0.6020710058670707, 'eval_WikiANN-en': 0.7700729926504489, 'eval_WikiNeural': 0.8707865168036896, 'eval_average': 0.69378086125244, 'eval_runtime': 404.2341, 'eval_samples_per_second': 8.906, 'eval_steps_per_second': 0.28, 'epoch': 2.0} 17%|█▋        | 2238/13416 [59:42<3:26:49,  1.11[INFO|trainer.py:3948] 2025-02-24 03:43:02,408 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238[INFO|configuration_utils.py:423] 2025-02-24 03:43:02,410 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/config.json[INFO|configuration_utils.py:909] 2025-02-24 03:43:02,410 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 03:43:03,608 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 03:43:03,610 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 03:43:03,610 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 03:43:03,611 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/spiece.model[2025-02-24 03:43:03,637] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2237 is about to be saved![2025-02-24 03:43:03,669] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/mp_rank_00_model_states.pt[2025-02-24 03:43:03,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/mp_rank_00_model_states.pt...[2025-02-24 03:43:04,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/mp_rank_00_model_states.pt.[2025-02-24 03:43:04,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 03:43:06,082] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 03:43:06,082] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-2238/global_step2237/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 03:43:06,082] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2237 is ready now!{'loss': 0.0222, 'grad_norm': 0.07295242697000504, 'learning_rate': 5e-05, 'epoch': 2.0}{'loss': 0.0235, 'grad_norm': 0.08282249420881271, 'learning_rate': 5e-05, 'epoch': 2.01}{'loss': 0.0224, 'grad_norm': 0.10827043652534485, 'learning_rate': 5e-05, 'epoch': 2.02}{'loss': 0.0215, 'grad_norm': 0.06632424145936966, 'learning_rate': 5e-05, 'epoch': 2.03}{'loss': 0.0236, 'grad_norm': 0.05865640938282013, 'learning_rate': 5e-05, 'epoch': 2.04}{'loss': 0.0223, 'grad_norm': 0.06918152421712875, 'learning_rate': 5e-05, 'epoch': 2.05}{'loss': 0.0221, 'grad_norm': 0.07738669961690903, 'learning_rate': 5e-05, 'epoch': 2.06}{'loss': 0.0241, 'grad_norm': 0.06028103828430176, 'learning_rate': 5e-05, 'epoch': 2.06}{'loss': 0.0217, 'grad_norm': 0.09276604652404785, 'learning_rate': 5e-05, 'epoch': 2.07}{'loss': 0.0222, 'grad_norm': 0.09783940017223358, 'learning_rate': 5e-05, 'epoch': 2.08}{'loss': 0.023, 'grad_norm': 0.07749822735786438, 'learning_rate': 5e-05, 'epoch': 2.09}{'loss': 0.0223, 'grad_norm': 0.08176068216562271, 'learning_rate': 5e-05, 'epoch': 2.1}{'loss': 0.0218, 'grad_norm': 0.1319935917854309, 'learning_rate': 5e-05, 'epoch': 2.11}{'loss': 0.0223, 'grad_norm': 0.06932628899812698, 'learning_rate': 5e-05, 'epoch': 2.12}{'loss': 0.023, 'grad_norm': 0.09528433531522751, 'learning_rate': 5e-05, 'epoch': 2.13}{'loss': 0.0221, 'grad_norm': 0.0997343510389328, 'learning_rate': 5e-05, 'epoch': 2.14}{'loss': 0.022, 'grad_norm': 0.10258489102125168, 'learning_rate': 5e-05, 'epoch': 2.14}{'loss': 0.0223, 'grad_norm': 0.0885210931301117, 'learning_rate': 5e-05, 'epoch': 2.15}{'loss': 0.0211, 'grad_norm': 0.059652842581272125, 'learning_rate': 5e-05, 'epoch': 2.16}{'loss': 0.023, 'grad_norm': 0.06880370527505875, 'learning_rate': 5e-05, 'epoch': 2.17}{'loss': 0.0222, 'grad_norm': 0.06287036836147308, 'learning_rate': 5e-05, 'epoch': 2.18}{'loss': 0.0229, 'grad_norm': 0.10189153254032135, 'learning_rate': 5e-05, 'epoch': 2.19}{'loss': 0.0211, 'grad_norm': 0.07999865710735321, 'learning_rate': 5e-05, 'epoch': 2.2}{'loss': 0.0212, 'grad_norm': 0.08220622688531876, 'learning_rate': 5e-05, 'epoch': 2.21}{'loss': 0.0221, 'grad_norm': 0.11143281310796738, 'learning_rate': 5e-05, 'epoch': 2.22}{'loss': 0.0219, 'grad_norm': 0.06572776287794113, 'learning_rate': 5e-05, 'epoch': 2.23}{'loss': 0.0202, 'grad_norm': 0.0528181828558445, 'learning_rate': 5e-05, 'epoch': 2.23}{'loss': 0.0211, 'grad_norm': 0.0935136005282402, 'learning_rate': 5e-05, 'epoch': 2.24}{'loss': 0.0218, 'grad_norm': 0.06667246669530869, 'learning_rate': 5e-05, 'epoch': 2.25}{'loss': 0.0214, 'grad_norm': 0.08003605157136917, 'learning_rate': 5e-05, 'epoch': 2.26}{'loss': 0.0191, 'grad_norm': 0.11532309651374817, 'learning_rate': 5e-05, 'epoch': 2.27}{'loss': 0.0206, 'grad_norm': 0.09388979524374008, 'learning_rate': 5e-05, 'epoch': 2.28}{'loss': 0.0209, 'grad_norm': 0.11965835094451904, 'learning_rate': 5e-05, 'epoch': 2.29}{'loss': 0.0223, 'grad_norm': 0.12147701531648636, 'learning_rate': 5e-05, 'epoch': 2.3}{'loss': 0.0216, 'grad_norm': 0.08655991405248642, 'learning_rate': 5e-05, 'epoch': 2.31}{'loss': 0.0235, 'grad_norm': 0.05370333790779114, 'learning_rate': 5e-05, 'epoch': 2.31}{'loss': 0.0204, 'grad_norm': 0.061878908425569534, 'learning_rate': 5e-05, 'epoch': 2.32}{'loss': 0.0205, 'grad_norm': 0.08124025911092758, 'learning_rate': 5e-05, 'epoch': 2.33}{'loss': 0.021, 'grad_norm': 0.05364851653575897, 'learning_rate': 5e-05, 'epoch': 2.34}{'loss': 0.0205, 'grad_norm': 0.09796231240034103, 'learning_rate': 5e-05, 'epoch': 2.35}{'loss': 0.0208, 'grad_norm': 0.08311126381158829, 'learning_rate': 5e-05, 'epoch': 2.36}{'loss': 0.0219, 'grad_norm': 0.08263669908046722, 'learning_rate': 5e-05, 'epoch': 2.37}{'loss': 0.0217, 'grad_norm': 0.06732891499996185, 'learning_rate': 5e-05, 'epoch': 2.38}{'loss': 0.0225, 'grad_norm': 0.06959524750709534, 'learning_rate': 5e-05, 'epoch': 2.39}{'loss': 0.0213, 'grad_norm': 0.06103057414293289, 'learning_rate': 5e-05, 'epoch': 2.4}{'loss': 0.0216, 'grad_norm': 0.08510878682136536, 'learning_rate': 5e-05, 'epoch': 2.4}{'loss': 0.0212, 'grad_norm': 0.09547621011734009, 'learning_rate': 5e-05, 'epoch': 2.41}{'loss': 0.0187, 'grad_norm': 0.06075263395905495, 'learning_rate': 5e-05, 'epoch': 2.42}{'loss': 0.023, 'grad_norm': 0.06113536283373833, 'learning_rate': 5e-05, 'epoch': 2.43}{'loss': 0.0207, 'grad_norm': 0.07309572398662567, 'learning_rate': 5e-05, 'epoch': 2.44}{'loss': 0.0211, 'grad_norm': 0.09811059385538101, 'learning_rate': 5e-05, 'epoch': 2.45}{'loss': 0.0212, 'grad_norm': 0.08523035794496536, 'learning_rate': 5e-05, 'epoch': 2.46}{'loss': 0.0205, 'grad_norm': 0.06966949999332428, 'learning_rate': 5e-05, 'epoch': 2.47}{'loss': 0.0215, 'grad_norm': 0.07057671993970871, 'learning_rate': 5e-05, 'epoch': 2.48}{'loss': 0.0209, 'grad_norm': 0.06904683262109756, 'learning_rate': 5e-05, 'epoch': 2.48}{'loss': 0.0206, 'grad_norm': 0.07364901155233383, 'learning_rate': 5e-05, 'epoch': 2.49}{'loss': 0.0205, 'grad_norm': 0.09494862705469131, 'learning_rate': 5e-05, 'epoch': 2.5}{'loss': 0.0204, 'grad_norm': 0.06009683385491371, 'learning_rate': 5e-05, 'epoch': 2.51}{'loss': 0.0208, 'grad_norm': 0.07366452366113663, 'learning_rate': 5e-05, 'epoch': 2.52}{'loss': 0.0208, 'grad_norm': 0.19381015002727509, 'learning_rate': 5e-05, 'epoch': 2.53}{'loss': 0.0218, 'grad_norm': 0.06649117916822433, 'learning_rate': 5e-05, 'epoch': 2.54}{'loss': 0.0207, 'grad_norm': 0.08848918229341507, 'learning_rate': 5e-05, 'epoch': 2.55}{'loss': 0.0207, 'grad_norm': 0.08407234400510788, 'learning_rate': 5e-05, 'epoch': 2.56}{'loss': 0.0213, 'grad_norm': 0.07764944434165955, 'learning_rate': 5e-05, 'epoch': 2.56}{'loss': 0.0194, 'grad_norm': 0.060752023011446, 'learning_rate': 5e-05, 'epoch': 2.57}{'loss': 0.0197, 'grad_norm': 0.05982639268040657, 'learning_rate': 5e-05, 'epoch': 2.58}{'loss': 0.0205, 'grad_norm': 0.08323317766189575, 'learning_rate': 5e-05, 'epoch': 2.59}{'loss': 0.0212, 'grad_norm': 0.08636189252138138, 'learning_rate': 5e-05, 'epoch': 2.6}{'loss': 0.0216, 'grad_norm': 0.0952286422252655, 'learning_rate': 5e-05, 'epoch': 2.61}{'loss': 0.0218, 'grad_norm': 0.06431462615728378, 'learning_rate': 5e-05, 'epoch': 2.62}{'loss': 0.0193, 'grad_norm': 0.08118824660778046, 'learning_rate': 5e-05, 'epoch': 2.63}{'loss': 0.0207, 'grad_norm': 0.07015486806631088, 'learning_rate': 5e-05, 'epoch': 2.64}{'loss': 0.0218, 'grad_norm': 0.0705484002828598, 'learning_rate': 5e-05, 'epoch': 2.65}{'loss': 0.0203, 'grad_norm': 0.06199301406741142, 'learning_rate': 5e-05, 'epoch': 2.65}{'loss': 0.0202, 'grad_norm': 0.09719175845384598, 'learning_rate': 5e-05, 'epoch': 2.66}{'loss': 0.0185, 'grad_norm': 0.06595801562070847, 'learning_rate': 5e-05, 'epoch': 2.67}{'loss': 0.0216, 'grad_norm': 0.0761902779340744, 'learning_rate': 5e-05, 'epoch': 2.68}{'loss': 0.0208, 'grad_norm': 0.06012892350554466, 'learning_rate': 5e-05, 'epoch': 2.69}{'loss': 0.0205, 'grad_norm': 0.06983626633882523, 'learning_rate': 5e-05, 'epoch': 2.7}{'loss': 0.0208, 'grad_norm': 0.07424576580524445, 'learning_rate': 5e-05, 'epoch': 2.71}{'loss': 0.0202, 'grad_norm': 0.07890335470438004, 'learning_rate': 5e-05, 'epoch': 2.72}{'loss': 0.0202, 'grad_norm': 0.12625639140605927, 'learning_rate': 5e-05, 'epoch': 2.73}{'loss': 0.0187, 'grad_norm': 0.06045696511864662, 'learning_rate': 5e-05, 'epoch': 2.73}{'loss': 0.0183, 'grad_norm': 0.06754139065742493, 'learning_rate': 5e-05, 'epoch': 2.74}{'loss': 0.0193, 'grad_norm': 0.06542805582284927, 'learning_rate': 5e-05, 'epoch': 2.75}{'loss': 0.0209, 'grad_norm': 0.10007724910974503, 'learning_rate': 5e-05, 'epoch': 2.76}{'loss': 0.0183, 'grad_norm': 0.048854146152734756, 'learning_rate': 5e-05, 'epoch': 2.77}{'loss': 0.019, 'grad_norm': 0.06700440496206284, 'learning_rate': 5e-05, 'epoch': 2.78}{'loss': 0.0192, 'grad_norm': 0.08089668303728104, 'learning_rate': 5e-05, 'epoch': 2.79}{'loss': 0.0202, 'grad_norm': 0.078087218105793, 'learning_rate': 5e-05, 'epoch': 2.8}{'loss': 0.0191, 'grad_norm': 0.08642126619815826, 'learning_rate': 5e-05, 'epoch': 2.81}{'loss': 0.0206, 'grad_norm': 0.06957536935806274, 'learning_rate': 5e-05, 'epoch': 2.82}{'loss': 0.0209, 'grad_norm': 0.052545685321092606, 'learning_rate': 5e-05, 'epoch': 2.82}{'loss': 0.0207, 'grad_norm': 0.08133012801408768, 'learning_rate': 5e-05, 'epoch': 2.83}{'loss': 0.0191, 'grad_norm': 0.2003796547651291, 'learning_rate': 5e-05, 'epoch': 2.84}{'loss': 0.0201, 'grad_norm': 0.0922231674194336, 'learning_rate': 5e-05, 'epoch': 2.85}{'loss': 0.0192, 'grad_norm': 0.07898340374231339, 'learning_rate': 5e-05, 'epoch': 2.86}{'loss': 0.019, 'grad_norm': 0.07189009338617325, 'learning_rate': 5e-05, 'epoch': 2.87}{'loss': 0.0189, 'grad_norm': 0.057267043739557266, 'learning_rate': 5e-05, 'epoch': 2.88}{'loss': 0.0215, 'grad_norm': 0.08078800141811371, 'learning_rate': 5e-05, 'epoch': 2.89}{'loss': 0.0193, 'grad_norm': 0.05960184708237648, 'learning_rate': 5e-05, 'epoch': 2.9}{'loss': 0.0206, 'grad_norm': 0.08033890277147293, 'learning_rate': 5e-05, 'epoch': 2.9}{'loss': 0.0175, 'grad_norm': 0.0758681669831276, 'learning_rate': 5e-05, 'epoch': 2.91}{'loss': 0.0212, 'grad_norm': 0.071729376912117, 'learning_rate': 5e-05, 'epoch': 2.92}{'loss': 0.02, 'grad_norm': 0.060297079384326935, 'learning_rate': 5e-05, 'epoch': 2.93}{'loss': 0.0192, 'grad_norm': 0.07588402181863785, 'learning_rate': 5e-05, 'epoch': 2.94}{'loss': 0.02, 'grad_norm': 0.057860467582941055, 'learning_rate': 5e-05, 'epoch': 2.95}{'loss': 0.0196, 'grad_norm': 0.07404229044914246, 'learning_rate': 5e-05, 'epoch': 2.96}{'loss': 0.0201, 'grad_norm': 0.07040568441152573, 'learning_rate': 5e-05, 'epoch': 2.97}{'loss': 0.0183, 'grad_norm': 0.10220964252948761, 'learning_rate': 5e-05, 'epoch': 2.98}{'loss': 0.0198, 'grad_norm': 0.09579332172870636, 'learning_rate': 5e-05, 'epoch': 2.99}{'loss': 0.0192, 'grad_norm': 0.06849901378154755, 'learning_rate': 5e-05, 'epoch': 2.99}{'eval_AnatEM': 0.5869565216884451, 'eval_bc2gm': 0.6470588234801188, 'eval_bc4chemd': 0.6507936507436648, 'eval_bc5cdr': 0.8140703517086703, 'eval_broad_twitter_corpus': 0.6465364120281163, 'eval_conllpp': 0.9423815620496314, 'eval_conll2003': 0.9423815620496314, 'eval_FabNER': 0.022847100140015168, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.4999999999494382, 'eval_mit-movie': 0.7086383601254933, 'eval_mit-restaurant': 0.7780269057794423, 'eval_MultiNERD': 0.8646748681395041, 'eval_ncbi': 0.8725212464085196, 'eval_Ontonotes': 0.8824082784070433, 'eval_TweetNER7': 0.6071428570927678, 'eval_WikiANN-en': 0.8058076224542463, 'eval_WikiNeural': 0.8910614524637239, 'eval_average': 0.7158004717827573, 'eval_runtime': 413.9533, 'eval_samples_per_second': 8.697, 'eval_steps_per_second': 0.273, 'epoch': 3.0} 25%|██▌       | 3357/13416 [1:29:35<3:07:01,  1.[INFO|trainer.py:3948] 2025-02-24 04:12:55,818 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357[INFO|configuration_utils.py:423] 2025-02-24 04:12:55,820 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/config.json[INFO|configuration_utils.py:909] 2025-02-24 04:12:55,821 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 04:12:56,924 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 04:12:56,926 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 04:12:56,926 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 04:12:56,927 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/spiece.model[2025-02-24 04:12:56,950] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3356 is about to be saved![2025-02-24 04:12:56,957] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/mp_rank_00_model_states.pt[2025-02-24 04:12:56,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/mp_rank_00_model_states.pt...[2025-02-24 04:12:58,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/mp_rank_00_model_states.pt.[2025-02-24 04:12:58,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 04:12:59,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 04:12:59,290] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-3357/global_step3356/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 04:12:59,290] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3356 is ready now!{'loss': 0.0195, 'grad_norm': 0.08008407056331635, 'learning_rate': 5e-05, 'epoch': 3.0}{'loss': 0.0181, 'grad_norm': 0.09626758098602295, 'learning_rate': 5e-05, 'epoch': 3.01}{'loss': 0.0186, 'grad_norm': 0.08318818360567093, 'learning_rate': 5e-05, 'epoch': 3.02}{'loss': 0.0193, 'grad_norm': 0.10009198635816574, 'learning_rate': 5e-05, 'epoch': 3.03}{'loss': 0.0193, 'grad_norm': 0.06235237419605255, 'learning_rate': 5e-05, 'epoch': 3.04}{'loss': 0.019, 'grad_norm': 0.06793558597564697, 'learning_rate': 5e-05, 'epoch': 3.05}{'loss': 0.0199, 'grad_norm': 0.06527332216501236, 'learning_rate': 5e-05, 'epoch': 3.06}{'loss': 0.0177, 'grad_norm': 0.07867272198200226, 'learning_rate': 5e-05, 'epoch': 3.07}{'loss': 0.0189, 'grad_norm': 0.07065330445766449, 'learning_rate': 5e-05, 'epoch': 3.07}{'loss': 0.0179, 'grad_norm': 0.056074339896440506, 'learning_rate': 5e-05, 'epoch': 3.08}{'loss': 0.0188, 'grad_norm': 0.06052088364958763, 'learning_rate': 5e-05, 'epoch': 3.09}{'loss': 0.0196, 'grad_norm': 0.0993955135345459, 'learning_rate': 5e-05, 'epoch': 3.1}{'loss': 0.0177, 'grad_norm': 0.07718206197023392, 'learning_rate': 5e-05, 'epoch': 3.11}{'loss': 0.017, 'grad_norm': 0.05006742477416992, 'learning_rate': 5e-05, 'epoch': 3.12}{'loss': 0.0182, 'grad_norm': 0.060653068125247955, 'learning_rate': 5e-05, 'epoch': 3.13}{'loss': 0.0192, 'grad_norm': 0.06863798201084137, 'learning_rate': 5e-05, 'epoch': 3.14}{'loss': 0.0184, 'grad_norm': 0.0676480382680893, 'learning_rate': 5e-05, 'epoch': 3.15}{'loss': 0.0198, 'grad_norm': 0.07655251026153564, 'learning_rate': 5e-05, 'epoch': 3.15}{'loss': 0.0182, 'grad_norm': 0.06238405033946037, 'learning_rate': 5e-05, 'epoch': 3.16}{'loss': 0.0174, 'grad_norm': 0.07862807810306549, 'learning_rate': 5e-05, 'epoch': 3.17}{'loss': 0.018, 'grad_norm': 0.05245174095034599, 'learning_rate': 5e-05, 'epoch': 3.18}{'loss': 0.0192, 'grad_norm': 0.07806263864040375, 'learning_rate': 5e-05, 'epoch': 3.19}{'loss': 0.0171, 'grad_norm': 0.15419740974903107, 'learning_rate': 5e-05, 'epoch': 3.2}{'loss': 0.0188, 'grad_norm': 0.04951081424951553, 'learning_rate': 5e-05, 'epoch': 3.21}{'loss': 0.017, 'grad_norm': 0.06233980134129524, 'learning_rate': 5e-05, 'epoch': 3.22}{'loss': 0.018, 'grad_norm': 0.04424412548542023, 'learning_rate': 5e-05, 'epoch': 3.23}{'loss': 0.0185, 'grad_norm': 0.06799854338169098, 'learning_rate': 5e-05, 'epoch': 3.24}{'loss': 0.0178, 'grad_norm': 0.05765991657972336, 'learning_rate': 5e-05, 'epoch': 3.24}{'loss': 0.0185, 'grad_norm': 0.0871673971414566, 'learning_rate': 5e-05, 'epoch': 3.25}{'loss': 0.0184, 'grad_norm': 0.09332705289125443, 'learning_rate': 5e-05, 'epoch': 3.26}{'loss': 0.0186, 'grad_norm': 0.07065564393997192, 'learning_rate': 5e-05, 'epoch': 3.27}{'loss': 0.0184, 'grad_norm': 0.05186448618769646, 'learning_rate': 5e-05, 'epoch': 3.28}{'loss': 0.0171, 'grad_norm': 0.062186725437641144, 'learning_rate': 5e-05, 'epoch': 3.29}{'loss': 0.019, 'grad_norm': 0.05951305106282234, 'learning_rate': 5e-05, 'epoch': 3.3}{'loss': 0.0173, 'grad_norm': 0.05356950685381889, 'learning_rate': 5e-05, 'epoch': 3.31}{'loss': 0.0191, 'grad_norm': 0.0594225712120533, 'learning_rate': 5e-05, 'epoch': 3.32}{'loss': 0.0164, 'grad_norm': 0.08661416918039322, 'learning_rate': 5e-05, 'epoch': 3.32}{'loss': 0.018, 'grad_norm': 0.06787597388029099, 'learning_rate': 5e-05, 'epoch': 3.33}{'loss': 0.0178, 'grad_norm': 0.07032711803913116, 'learning_rate': 5e-05, 'epoch': 3.34}{'loss': 0.0179, 'grad_norm': 0.07225609570741653, 'learning_rate': 5e-05, 'epoch': 3.35}{'loss': 0.0181, 'grad_norm': 0.04880257323384285, 'learning_rate': 5e-05, 'epoch': 3.36}{'loss': 0.0187, 'grad_norm': 0.07507622987031937, 'learning_rate': 5e-05, 'epoch': 3.37}{'loss': 0.016, 'grad_norm': 0.09596063196659088, 'learning_rate': 5e-05, 'epoch': 3.38}{'loss': 0.0168, 'grad_norm': 0.060180384665727615, 'learning_rate': 5e-05, 'epoch': 3.39}{'loss': 0.0181, 'grad_norm': 0.07634594291448593, 'learning_rate': 5e-05, 'epoch': 3.4}{'loss': 0.0179, 'grad_norm': 0.07325893640518188, 'learning_rate': 5e-05, 'epoch': 3.4}{'loss': 0.0183, 'grad_norm': 0.09582708775997162, 'learning_rate': 5e-05, 'epoch': 3.41}{'loss': 0.0181, 'grad_norm': 0.08824034780263901, 'learning_rate': 5e-05, 'epoch': 3.42}{'loss': 0.0166, 'grad_norm': 0.0622943677008152, 'learning_rate': 5e-05, 'epoch': 3.43}{'loss': 0.0169, 'grad_norm': 0.06072480231523514, 'learning_rate': 5e-05, 'epoch': 3.44}{'loss': 0.016, 'grad_norm': 0.047146957367658615, 'learning_rate': 5e-05, 'epoch': 3.45}{'loss': 0.0181, 'grad_norm': 0.08018393069505692, 'learning_rate': 5e-05, 'epoch': 3.46}{'loss': 0.0181, 'grad_norm': 0.06273078918457031, 'learning_rate': 5e-05, 'epoch': 3.47}{'loss': 0.0176, 'grad_norm': 0.07045785337686539, 'learning_rate': 5e-05, 'epoch': 3.48}{'loss': 0.0172, 'grad_norm': 0.07526876032352448, 'learning_rate': 5e-05, 'epoch': 3.49}{'loss': 0.0173, 'grad_norm': 0.054696228355169296, 'learning_rate': 5e-05, 'epoch': 3.49}{'loss': 0.0183, 'grad_norm': 0.05539889633655548, 'learning_rate': 5e-05, 'epoch': 3.5}{'loss': 0.0179, 'grad_norm': 0.06722493469715118, 'learning_rate': 5e-05, 'epoch': 3.51}{'loss': 0.0174, 'grad_norm': 0.17513029277324677, 'learning_rate': 5e-05, 'epoch': 3.52}{'loss': 0.0168, 'grad_norm': 0.05724392458796501, 'learning_rate': 5e-05, 'epoch': 3.53}{'loss': 0.0175, 'grad_norm': 0.11253676563501358, 'learning_rate': 5e-05, 'epoch': 3.54}{'loss': 0.0171, 'grad_norm': 0.04731522127985954, 'learning_rate': 5e-05, 'epoch': 3.55}{'loss': 0.0171, 'grad_norm': 0.06756937503814697, 'learning_rate': 5e-05, 'epoch': 3.56}{'loss': 0.02, 'grad_norm': 0.09835589677095413, 'learning_rate': 5e-05, 'epoch': 3.57}{'loss': 0.0161, 'grad_norm': 0.06316357105970383, 'learning_rate': 5e-05, 'epoch': 3.57}{'loss': 0.0184, 'grad_norm': 0.05765169858932495, 'learning_rate': 5e-05, 'epoch': 3.58}{'loss': 0.0181, 'grad_norm': 0.06437837332487106, 'learning_rate': 5e-05, 'epoch': 3.59}{'loss': 0.0178, 'grad_norm': 0.059858810156583786, 'learning_rate': 5e-05, 'epoch': 3.6}{'loss': 0.0186, 'grad_norm': 0.06543754041194916, 'learning_rate': 5e-05, 'epoch': 3.61}{'loss': 0.0176, 'grad_norm': 0.06826002150774002, 'learning_rate': 5e-05, 'epoch': 3.62}{'loss': 0.019, 'grad_norm': 0.08246096223592758, 'learning_rate': 5e-05, 'epoch': 3.63}{'loss': 0.018, 'grad_norm': 0.07008208334445953, 'learning_rate': 5e-05, 'epoch': 3.64}{'loss': 0.0179, 'grad_norm': 0.061419229954481125, 'learning_rate': 5e-05, 'epoch': 3.65}{'loss': 0.0173, 'grad_norm': 0.07486468553543091, 'learning_rate': 5e-05, 'epoch': 3.66}{'loss': 0.018, 'grad_norm': 0.07389748096466064, 'learning_rate': 5e-05, 'epoch': 3.66}{'loss': 0.0181, 'grad_norm': 0.05643019452691078, 'learning_rate': 5e-05, 'epoch': 3.67}{'loss': 0.0184, 'grad_norm': 0.07336767762899399, 'learning_rate': 5e-05, 'epoch': 3.68}{'loss': 0.0182, 'grad_norm': 0.049813542515039444, 'learning_rate': 5e-05, 'epoch': 3.69}{'loss': 0.0173, 'grad_norm': 0.07901697605848312, 'learning_rate': 5e-05, 'epoch': 3.7}{'loss': 0.0166, 'grad_norm': 0.06297726184129715, 'learning_rate': 5e-05, 'epoch': 3.71}{'loss': 0.0172, 'grad_norm': 0.11307255178689957, 'learning_rate': 5e-05, 'epoch': 3.72}{'loss': 0.0178, 'grad_norm': 0.06102069094777107, 'learning_rate': 5e-05, 'epoch': 3.73}{'loss': 0.0169, 'grad_norm': 0.07340805977582932, 'learning_rate': 5e-05, 'epoch': 3.74}{'loss': 0.018, 'grad_norm': 0.0528719499707222, 'learning_rate': 5e-05, 'epoch': 3.74}{'loss': 0.018, 'grad_norm': 0.09258969128131866, 'learning_rate': 5e-05, 'epoch': 3.75}{'loss': 0.0175, 'grad_norm': 0.06171088665723801, 'learning_rate': 5e-05, 'epoch': 3.76}{'loss': 0.0167, 'grad_norm': 0.060032207518815994, 'learning_rate': 5e-05, 'epoch': 3.77}{'loss': 0.0164, 'grad_norm': 0.06321621686220169, 'learning_rate': 5e-05, 'epoch': 3.78}{'loss': 0.0169, 'grad_norm': 0.05753963068127632, 'learning_rate': 5e-05, 'epoch': 3.79}{'loss': 0.0171, 'grad_norm': 0.061236683279275894, 'learning_rate': 5e-05, 'epoch': 3.8}{'loss': 0.0184, 'grad_norm': 0.05453597381711006, 'learning_rate': 5e-05, 'epoch': 3.81}{'loss': 0.0169, 'grad_norm': 0.05736426264047623, 'learning_rate': 5e-05, 'epoch': 3.82}{'loss': 0.0164, 'grad_norm': 0.07220336049795151, 'learning_rate': 5e-05, 'epoch': 3.83}{'loss': 0.0178, 'grad_norm': 0.04926128312945366, 'learning_rate': 5e-05, 'epoch': 3.83}{'loss': 0.019, 'grad_norm': 0.07826395332813263, 'learning_rate': 5e-05, 'epoch': 3.84}{'loss': 0.0166, 'grad_norm': 0.059167858213186264, 'learning_rate': 5e-05, 'epoch': 3.85}{'loss': 0.0164, 'grad_norm': 0.05063674598932266, 'learning_rate': 5e-05, 'epoch': 3.86}{'loss': 0.018, 'grad_norm': 0.057909220457077026, 'learning_rate': 5e-05, 'epoch': 3.87}{'loss': 0.0171, 'grad_norm': 0.06644915044307709, 'learning_rate': 5e-05, 'epoch': 3.88}{'loss': 0.0166, 'grad_norm': 0.06108102202415466, 'learning_rate': 5e-05, 'epoch': 3.89}{'loss': 0.016, 'grad_norm': 0.04706362262368202, 'learning_rate': 5e-05, 'epoch': 3.9}{'loss': 0.0173, 'grad_norm': 0.06298554688692093, 'learning_rate': 5e-05, 'epoch': 3.91}{'loss': 0.017, 'grad_norm': 0.07020290195941925, 'learning_rate': 5e-05, 'epoch': 3.91}{'loss': 0.0177, 'grad_norm': 0.10449738800525665, 'learning_rate': 5e-05, 'epoch': 3.92}{'loss': 0.0183, 'grad_norm': 0.06287740170955658, 'learning_rate': 5e-05, 'epoch': 3.93}{'loss': 0.0171, 'grad_norm': 0.05861489847302437, 'learning_rate': 5e-05, 'epoch': 3.94}{'loss': 0.016, 'grad_norm': 0.05781719461083412, 'learning_rate': 5e-05, 'epoch': 3.95}{'loss': 0.0189, 'grad_norm': 0.07121217995882034, 'learning_rate': 5e-05, 'epoch': 3.96}{'loss': 0.0165, 'grad_norm': 0.07454195618629456, 'learning_rate': 5e-05, 'epoch': 3.97}{'loss': 0.0169, 'grad_norm': 0.048953428864479065, 'learning_rate': 5e-05, 'epoch': 3.98}{'loss': 0.0177, 'grad_norm': 0.07530011236667633, 'learning_rate': 5e-05, 'epoch': 3.99}{'loss': 0.0177, 'grad_norm': 0.07862987369298935, 'learning_rate': 5e-05, 'epoch': 3.99}{'eval_AnatEM': 0.48351648346590986, 'eval_bc2gm': 0.6734006733509961, 'eval_bc4chemd': 0.7024128685828233, 'eval_bc5cdr': 0.8381201043884238, 'eval_broad_twitter_corpus': 0.6603773584411399, 'eval_conllpp': 0.9423815620496314, 'eval_conll2003': 0.9398207425874042, 'eval_FabNER': 0.01677539605540762, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.5276073619127705, 'eval_mit-movie': 0.7343283581587807, 'eval_mit-restaurant': 0.7887005649215809, 'eval_MultiNERD': 0.8936170212262796, 'eval_ncbi': 0.9011627906471658, 'eval_Ontonotes': 0.9075471697611582, 'eval_TweetNER7': 0.6257575757075068, 'eval_WikiANN-en': 0.7985480943235773, 'eval_WikiNeural': 0.8942172072840215, 'eval_average': 0.7249662361247633, 'eval_runtime': 413.5911, 'eval_samples_per_second': 8.704, 'eval_steps_per_second': 0.273, 'epoch': 4.0} 33%|███▎      | 4476/13416 [1:59:26<2:56:43,  1.[INFO|trainer.py:3948] 2025-02-24 04:42:46,532 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476[INFO|configuration_utils.py:423] 2025-02-24 04:42:46,535 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/config.json[INFO|configuration_utils.py:909] 2025-02-24 04:42:46,535 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 04:42:47,841 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 04:42:47,842 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 04:42:47,843 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 04:42:47,843 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/spiece.model[2025-02-24 04:42:47,882] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4475 is about to be saved![2025-02-24 04:42:47,889] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/mp_rank_00_model_states.pt[2025-02-24 04:42:47,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/mp_rank_00_model_states.pt...[2025-02-24 04:42:49,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/mp_rank_00_model_states.pt.[2025-02-24 04:42:49,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 04:42:50,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 04:42:50,279] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-4476/global_step4475/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 04:42:50,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4475 is ready now!{'loss': 0.0156, 'grad_norm': 0.17494869232177734, 'learning_rate': 5e-05, 'epoch': 4.0}{'loss': 0.0156, 'grad_norm': 0.0713731124997139, 'learning_rate': 5e-05, 'epoch': 4.01}{'loss': 0.016, 'grad_norm': 0.04746704176068306, 'learning_rate': 5e-05, 'epoch': 4.02}{'loss': 0.0169, 'grad_norm': 0.05044730380177498, 'learning_rate': 5e-05, 'epoch': 4.03}{'loss': 0.0164, 'grad_norm': 0.06335592269897461, 'learning_rate': 5e-05, 'epoch': 4.04}{'loss': 0.0166, 'grad_norm': 0.0508839376270771, 'learning_rate': 5e-05, 'epoch': 4.05}{'loss': 0.0163, 'grad_norm': 0.04979725554585457, 'learning_rate': 5e-05, 'epoch': 4.06}{'loss': 0.0154, 'grad_norm': 0.0679984986782074, 'learning_rate': 5e-05, 'epoch': 4.07}{'loss': 0.017, 'grad_norm': 0.05319315567612648, 'learning_rate': 5e-05, 'epoch': 4.08}{'loss': 0.0164, 'grad_norm': 0.07084185630083084, 'learning_rate': 5e-05, 'epoch': 4.08}{'loss': 0.0154, 'grad_norm': 0.06786669045686722, 'learning_rate': 5e-05, 'epoch': 4.09}{'loss': 0.0167, 'grad_norm': 0.0519118532538414, 'learning_rate': 5e-05, 'epoch': 4.1}{'loss': 0.0159, 'grad_norm': 0.07246249914169312, 'learning_rate': 5e-05, 'epoch': 4.11}{'loss': 0.0168, 'grad_norm': 0.04590817540884018, 'learning_rate': 5e-05, 'epoch': 4.12}{'loss': 0.0146, 'grad_norm': 0.05657728761434555, 'learning_rate': 5e-05, 'epoch': 4.13}{'loss': 0.016, 'grad_norm': 0.06923170387744904, 'learning_rate': 5e-05, 'epoch': 4.14}{'loss': 0.0161, 'grad_norm': 0.05992729961872101, 'learning_rate': 5e-05, 'epoch': 4.15}{'loss': 0.0162, 'grad_norm': 0.07334999740123749, 'learning_rate': 5e-05, 'epoch': 4.16}{'loss': 0.0169, 'grad_norm': 0.07659419625997543, 'learning_rate': 5e-05, 'epoch': 4.16}{'loss': 0.0166, 'grad_norm': 0.054748132824897766, 'learning_rate': 5e-05, 'epoch': 4.17}{'loss': 0.0161, 'grad_norm': 0.06767779588699341, 'learning_rate': 5e-05, 'epoch': 4.18}{'loss': 0.0151, 'grad_norm': 0.03593229502439499, 'learning_rate': 5e-05, 'epoch': 4.19}{'loss': 0.0161, 'grad_norm': 0.044881585985422134, 'learning_rate': 5e-05, 'epoch': 4.2}{'loss': 0.0151, 'grad_norm': 0.06191400811076164, 'learning_rate': 5e-05, 'epoch': 4.21}{'loss': 0.0152, 'grad_norm': 0.07031531631946564, 'learning_rate': 5e-05, 'epoch': 4.22}{'loss': 0.0168, 'grad_norm': 0.052775342017412186, 'learning_rate': 5e-05, 'epoch': 4.23}{'loss': 0.017, 'grad_norm': 0.061696432530879974, 'learning_rate': 5e-05, 'epoch': 4.24}{'loss': 0.0168, 'grad_norm': 0.056942299008369446, 'learning_rate': 5e-05, 'epoch': 4.24}{'loss': 0.017, 'grad_norm': 0.04337730258703232, 'learning_rate': 5e-05, 'epoch': 4.25}{'loss': 0.0159, 'grad_norm': 0.07176291197538376, 'learning_rate': 5e-05, 'epoch': 4.26}{'loss': 0.0163, 'grad_norm': 0.056065499782562256, 'learning_rate': 5e-05, 'epoch': 4.27}{'loss': 0.0171, 'grad_norm': 0.053016118705272675, 'learning_rate': 5e-05, 'epoch': 4.28}{'loss': 0.0156, 'grad_norm': 0.0590113140642643, 'learning_rate': 5e-05, 'epoch': 4.29}{'loss': 0.0159, 'grad_norm': 0.056279007345438004, 'learning_rate': 5e-05, 'epoch': 4.3}{'loss': 0.0168, 'grad_norm': 0.14203014969825745, 'learning_rate': 5e-05, 'epoch': 4.31}{'loss': 0.0146, 'grad_norm': 0.044969212263822556, 'learning_rate': 5e-05, 'epoch': 4.32}{'loss': 0.0153, 'grad_norm': 0.05260511860251427, 'learning_rate': 5e-05, 'epoch': 4.33}{'loss': 0.0155, 'grad_norm': 0.0666169673204422, 'learning_rate': 5e-05, 'epoch': 4.33}{'loss': 0.017, 'grad_norm': 0.05110188573598862, 'learning_rate': 5e-05, 'epoch': 4.34}{'loss': 0.0154, 'grad_norm': 0.0792146697640419, 'learning_rate': 5e-05, 'epoch': 4.35}{'loss': 0.0152, 'grad_norm': 0.05939565598964691, 'learning_rate': 5e-05, 'epoch': 4.36}{'loss': 0.0158, 'grad_norm': 0.05635537952184677, 'learning_rate': 5e-05, 'epoch': 4.37}{'loss': 0.0154, 'grad_norm': 0.07101117819547653, 'learning_rate': 5e-05, 'epoch': 4.38}{'loss': 0.0172, 'grad_norm': 0.0772438570857048, 'learning_rate': 5e-05, 'epoch': 4.39}{'loss': 0.0152, 'grad_norm': 0.05731189250946045, 'learning_rate': 5e-05, 'epoch': 4.4}{'loss': 0.0155, 'grad_norm': 0.06410297006368637, 'learning_rate': 5e-05, 'epoch': 4.41}{'loss': 0.016, 'grad_norm': 0.05928853899240494, 'learning_rate': 5e-05, 'epoch': 4.41}{'loss': 0.0151, 'grad_norm': 0.049595337361097336, 'learning_rate': 5e-05, 'epoch': 4.42}{'loss': 0.0157, 'grad_norm': 0.049964211881160736, 'learning_rate': 5e-05, 'epoch': 4.43}{'loss': 0.0153, 'grad_norm': 0.056165579706430435, 'learning_rate': 5e-05, 'epoch': 4.44}{'loss': 0.016, 'grad_norm': 0.07505769282579422, 'learning_rate': 5e-05, 'epoch': 4.45}{'loss': 0.0161, 'grad_norm': 0.11686425656080246, 'learning_rate': 5e-05, 'epoch': 4.46}{'loss': 0.016, 'grad_norm': 0.041806597262620926, 'learning_rate': 5e-05, 'epoch': 4.47}{'loss': 0.016, 'grad_norm': 0.058086782693862915, 'learning_rate': 5e-05, 'epoch': 4.48}{'loss': 0.0161, 'grad_norm': 0.06679004430770874, 'learning_rate': 5e-05, 'epoch': 4.49}{'loss': 0.0173, 'grad_norm': 0.05813562497496605, 'learning_rate': 5e-05, 'epoch': 4.5}{'loss': 0.0153, 'grad_norm': 0.05989662557840347, 'learning_rate': 5e-05, 'epoch': 4.5}{'loss': 0.0152, 'grad_norm': 0.05152296647429466, 'learning_rate': 5e-05, 'epoch': 4.51}{'loss': 0.0158, 'grad_norm': 0.048263028264045715, 'learning_rate': 5e-05, 'epoch': 4.52}{'loss': 0.0149, 'grad_norm': 0.06493998318910599, 'learning_rate': 5e-05, 'epoch': 4.53}{'loss': 0.0155, 'grad_norm': 0.07779951393604279, 'learning_rate': 5e-05, 'epoch': 4.54}{'loss': 0.0145, 'grad_norm': 0.05777636170387268, 'learning_rate': 5e-05, 'epoch': 4.55}{'loss': 0.0159, 'grad_norm': 0.046717025339603424, 'learning_rate': 5e-05, 'epoch': 4.56}{'loss': 0.0151, 'grad_norm': 0.05618543177843094, 'learning_rate': 5e-05, 'epoch': 4.57}{'loss': 0.0156, 'grad_norm': 0.04186323657631874, 'learning_rate': 5e-05, 'epoch': 4.58}{'loss': 0.0156, 'grad_norm': 0.057042691856622696, 'learning_rate': 5e-05, 'epoch': 4.58}{'loss': 0.0154, 'grad_norm': 0.06019426882266998, 'learning_rate': 5e-05, 'epoch': 4.59}{'loss': 0.0164, 'grad_norm': 0.07375122606754303, 'learning_rate': 5e-05, 'epoch': 4.6}{'loss': 0.0152, 'grad_norm': 0.08956202864646912, 'learning_rate': 5e-05, 'epoch': 4.61}{'loss': 0.0134, 'grad_norm': 0.07787780463695526, 'learning_rate': 5e-05, 'epoch': 4.62}{'loss': 0.016, 'grad_norm': 0.056731872260570526, 'learning_rate': 5e-05, 'epoch': 4.63}{'loss': 0.0142, 'grad_norm': 0.052484769374132156, 'learning_rate': 5e-05, 'epoch': 4.64}{'loss': 0.0154, 'grad_norm': 0.05128709226846695, 'learning_rate': 5e-05, 'epoch': 4.65}{'loss': 0.0159, 'grad_norm': 0.0629790872335434, 'learning_rate': 5e-05, 'epoch': 4.66}{'loss': 0.0151, 'grad_norm': 0.05346520245075226, 'learning_rate': 5e-05, 'epoch': 4.67}{'loss': 0.0152, 'grad_norm': 0.05462246760725975, 'learning_rate': 5e-05, 'epoch': 4.67}{'loss': 0.0151, 'grad_norm': 0.09992624074220657, 'learning_rate': 5e-05, 'epoch': 4.68}{'loss': 0.0145, 'grad_norm': 0.1031874492764473, 'learning_rate': 5e-05, 'epoch': 4.69}{'loss': 0.0151, 'grad_norm': 0.06219610571861267, 'learning_rate': 5e-05, 'epoch': 4.7}{'loss': 0.0162, 'grad_norm': 0.08152445405721664, 'learning_rate': 5e-05, 'epoch': 4.71}{'loss': 0.0164, 'grad_norm': 0.05292317643761635, 'learning_rate': 5e-05, 'epoch': 4.72}{'loss': 0.0157, 'grad_norm': 0.0895262062549591, 'learning_rate': 5e-05, 'epoch': 4.73}{'loss': 0.015, 'grad_norm': 0.051820773631334305, 'learning_rate': 5e-05, 'epoch': 4.74}{'loss': 0.0162, 'grad_norm': 0.07286673039197922, 'learning_rate': 5e-05, 'epoch': 4.75}{'loss': 0.0153, 'grad_norm': 0.06186940148472786, 'learning_rate': 5e-05, 'epoch': 4.75}{'loss': 0.0136, 'grad_norm': 0.0655544102191925, 'learning_rate': 5e-05, 'epoch': 4.76}{'loss': 0.016, 'grad_norm': 0.059750597923994064, 'learning_rate': 5e-05, 'epoch': 4.77}{'loss': 0.0156, 'grad_norm': 0.0924263671040535, 'learning_rate': 5e-05, 'epoch': 4.78}{'loss': 0.0142, 'grad_norm': 0.05401109531521797, 'learning_rate': 5e-05, 'epoch': 4.79}{'loss': 0.0149, 'grad_norm': 0.06316637992858887, 'learning_rate': 5e-05, 'epoch': 4.8}{'loss': 0.015, 'grad_norm': 0.06350414454936981, 'learning_rate': 5e-05, 'epoch': 4.81}{'loss': 0.0148, 'grad_norm': 0.10951284319162369, 'learning_rate': 5e-05, 'epoch': 4.82}{'loss': 0.0159, 'grad_norm': 0.05594818666577339, 'learning_rate': 5e-05, 'epoch': 4.83}{'loss': 0.0157, 'grad_norm': 0.05739104747772217, 'learning_rate': 5e-05, 'epoch': 4.83}{'loss': 0.0156, 'grad_norm': 0.07118017226457596, 'learning_rate': 5e-05, 'epoch': 4.84}{'loss': 0.0156, 'grad_norm': 0.07980290055274963, 'learning_rate': 5e-05, 'epoch': 4.85}{'loss': 0.0151, 'grad_norm': 0.05949757248163223, 'learning_rate': 5e-05, 'epoch': 4.86}{'loss': 0.0151, 'grad_norm': 0.06974615156650543, 'learning_rate': 5e-05, 'epoch': 4.87}{'loss': 0.0148, 'grad_norm': 0.05215122550725937, 'learning_rate': 5e-05, 'epoch': 4.88}{'loss': 0.0151, 'grad_norm': 0.06634943187236786, 'learning_rate': 5e-05, 'epoch': 4.89}{'loss': 0.0159, 'grad_norm': 0.06404969096183777, 'learning_rate': 5e-05, 'epoch': 4.9}{'loss': 0.0155, 'grad_norm': 0.06260303407907486, 'learning_rate': 5e-05, 'epoch': 4.91}{'loss': 0.0147, 'grad_norm': 0.046542439609766006, 'learning_rate': 5e-05, 'epoch': 4.92}{'loss': 0.0152, 'grad_norm': 0.0705830529332161, 'learning_rate': 5e-05, 'epoch': 4.92}{'loss': 0.0159, 'grad_norm': 0.0964333564043045, 'learning_rate': 5e-05, 'epoch': 4.93}{'loss': 0.0164, 'grad_norm': 0.04614430293440819, 'learning_rate': 5e-05, 'epoch': 4.94}{'loss': 0.015, 'grad_norm': 0.061710529029369354, 'learning_rate': 5e-05, 'epoch': 4.95}{'loss': 0.0151, 'grad_norm': 0.06951291114091873, 'learning_rate': 5e-05, 'epoch': 4.96}{'loss': 0.0155, 'grad_norm': 0.07227683067321777, 'learning_rate': 5e-05, 'epoch': 4.97}{'loss': 0.0155, 'grad_norm': 0.08918789774179459, 'learning_rate': 5e-05, 'epoch': 4.98}{'loss': 0.016, 'grad_norm': 0.05329503118991852, 'learning_rate': 5e-05, 'epoch': 4.99}{'loss': 0.0155, 'grad_norm': 0.08424188941717148, 'learning_rate': 5e-05, 'epoch': 5.0}{'eval_AnatEM': 0.495238095189551, 'eval_bc2gm': 0.694158075551465, 'eval_bc4chemd': 0.6531645569117667, 'eval_bc5cdr': 0.8505747125934904, 'eval_broad_twitter_corpus': 0.7060931899141615, 'eval_conllpp': 0.965250965200717, 'eval_conll2003': 0.9589743589241135, 'eval_FabNER': 0.019097222185512363, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.5862068965010503, 'eval_mit-movie': 0.7302533531539953, 'eval_mit-restaurant': 0.789651293538138, 'eval_MultiNERD': 0.9045936395256521, 'eval_ncbi': 0.896551724087457, 'eval_Ontonotes': 0.904627006559846, 'eval_TweetNER7': 0.6177777777276862, 'eval_WikiANN-en': 0.8065099457001644, 'eval_WikiNeural': 0.8988764044441311, 'eval_average': 0.7332611186161143, 'eval_runtime': 412.6328, 'eval_samples_per_second': 8.724, 'eval_steps_per_second': 0.274, 'epoch': 5.0} 42%|████▏     | 5595/13416 [2:29:18<2:35:22,  1.[INFO|trainer.py:3948] 2025-02-24 05:12:38,469 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595[INFO|configuration_utils.py:423] 2025-02-24 05:12:38,471 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/config.json[INFO|configuration_utils.py:909] 2025-02-24 05:12:38,471 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 05:12:39,568 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 05:12:39,570 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 05:12:39,570 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 05:12:39,570 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/spiece.model[2025-02-24 05:12:39,590] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5593 is about to be saved![2025-02-24 05:12:39,597] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/mp_rank_00_model_states.pt[2025-02-24 05:12:39,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/mp_rank_00_model_states.pt...[2025-02-24 05:12:40,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/mp_rank_00_model_states.pt.[2025-02-24 05:12:40,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 05:12:41,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 05:12:41,898] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-5595/global_step5593/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 05:12:41,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5593 is ready now!{'loss': 0.0151, 'grad_norm': 0.06495565921068192, 'learning_rate': 5e-05, 'epoch': 5.0}{'loss': 0.015, 'grad_norm': 0.060542263090610504, 'learning_rate': 5e-05, 'epoch': 5.01}{'loss': 0.0158, 'grad_norm': 0.05932730436325073, 'learning_rate': 5e-05, 'epoch': 5.02}{'loss': 0.0153, 'grad_norm': 0.05619826540350914, 'learning_rate': 5e-05, 'epoch': 5.03}{'loss': 0.013, 'grad_norm': 0.11465010792016983, 'learning_rate': 5e-05, 'epoch': 5.04}{'loss': 0.0154, 'grad_norm': 0.04114681854844093, 'learning_rate': 5e-05, 'epoch': 5.05}{'loss': 0.0144, 'grad_norm': 0.057844050228595734, 'learning_rate': 5e-05, 'epoch': 5.06}{'loss': 0.0134, 'grad_norm': 0.04515876621007919, 'learning_rate': 5e-05, 'epoch': 5.07}{'loss': 0.0146, 'grad_norm': 0.055829934775829315, 'learning_rate': 5e-05, 'epoch': 5.08}{'loss': 0.0141, 'grad_norm': 0.07684481143951416, 'learning_rate': 5e-05, 'epoch': 5.08}{'loss': 0.0146, 'grad_norm': 0.06129546836018562, 'learning_rate': 5e-05, 'epoch': 5.09}{'loss': 0.0151, 'grad_norm': 0.04674190282821655, 'learning_rate': 5e-05, 'epoch': 5.1}{'loss': 0.0152, 'grad_norm': 0.0505216009914875, 'learning_rate': 5e-05, 'epoch': 5.11}{'loss': 0.015, 'grad_norm': 0.06996286660432816, 'learning_rate': 5e-05, 'epoch': 5.12}{'loss': 0.0149, 'grad_norm': 0.13403286039829254, 'learning_rate': 5e-05, 'epoch': 5.13}{'loss': 0.0136, 'grad_norm': 0.0486881360411644, 'learning_rate': 5e-05, 'epoch': 5.14}{'loss': 0.0157, 'grad_norm': 0.04681948944926262, 'learning_rate': 5e-05, 'epoch': 5.15}{'loss': 0.0154, 'grad_norm': 0.06808116286993027, 'learning_rate': 5e-05, 'epoch': 5.16}{'loss': 0.0133, 'grad_norm': 0.06035122275352478, 'learning_rate': 5e-05, 'epoch': 5.17}{'loss': 0.0148, 'grad_norm': 0.07020304352045059, 'learning_rate': 5e-05, 'epoch': 5.17}{'loss': 0.0146, 'grad_norm': 0.04970591515302658, 'learning_rate': 5e-05, 'epoch': 5.18}{'loss': 0.0137, 'grad_norm': 0.04617176577448845, 'learning_rate': 5e-05, 'epoch': 5.19}{'loss': 0.0142, 'grad_norm': 0.05641733855009079, 'learning_rate': 5e-05, 'epoch': 5.2}{'loss': 0.015, 'grad_norm': 0.05349750071763992, 'learning_rate': 5e-05, 'epoch': 5.21}{'loss': 0.0144, 'grad_norm': 0.06226544454693794, 'learning_rate': 5e-05, 'epoch': 5.22}{'loss': 0.0134, 'grad_norm': 0.05463336035609245, 'learning_rate': 5e-05, 'epoch': 5.23}{'loss': 0.0133, 'grad_norm': 0.050956323742866516, 'learning_rate': 5e-05, 'epoch': 5.24}{'loss': 0.013, 'grad_norm': 0.06115911900997162, 'learning_rate': 5e-05, 'epoch': 5.25}{'loss': 0.015, 'grad_norm': 0.05929257348179817, 'learning_rate': 5e-05, 'epoch': 5.25}{'loss': 0.0149, 'grad_norm': 0.11525014787912369, 'learning_rate': 5e-05, 'epoch': 5.26}{'loss': 0.0143, 'grad_norm': 0.04855990782380104, 'learning_rate': 5e-05, 'epoch': 5.27}{'loss': 0.015, 'grad_norm': 0.04680242761969566, 'learning_rate': 5e-05, 'epoch': 5.28}{'loss': 0.0149, 'grad_norm': 0.06271890550851822, 'learning_rate': 5e-05, 'epoch': 5.29}{'loss': 0.0152, 'grad_norm': 0.04422951117157936, 'learning_rate': 5e-05, 'epoch': 5.3}{'loss': 0.0156, 'grad_norm': 0.13217978179454803, 'learning_rate': 5e-05, 'epoch': 5.31}{'loss': 0.0141, 'grad_norm': 0.05326513573527336, 'learning_rate': 5e-05, 'epoch': 5.32}{'loss': 0.0144, 'grad_norm': 0.041988685727119446, 'learning_rate': 5e-05, 'epoch': 5.33}{'loss': 0.0126, 'grad_norm': 0.056090980768203735, 'learning_rate': 5e-05, 'epoch': 5.34}{'loss': 0.0142, 'grad_norm': 0.048942986875772476, 'learning_rate': 5e-05, 'epoch': 5.34}{'loss': 0.0133, 'grad_norm': 0.06008031964302063, 'learning_rate': 5e-05, 'epoch': 5.35}{'loss': 0.0127, 'grad_norm': 0.05921143293380737, 'learning_rate': 5e-05, 'epoch': 5.36}{'loss': 0.0147, 'grad_norm': 0.06867013871669769, 'learning_rate': 5e-05, 'epoch': 5.37}{'loss': 0.0148, 'grad_norm': 0.061137475073337555, 'learning_rate': 5e-05, 'epoch': 5.38}{'loss': 0.0131, 'grad_norm': 0.06351587176322937, 'learning_rate': 5e-05, 'epoch': 5.39}{'loss': 0.0154, 'grad_norm': 0.053006429225206375, 'learning_rate': 5e-05, 'epoch': 5.4}{'loss': 0.0155, 'grad_norm': 0.05078910291194916, 'learning_rate': 5e-05, 'epoch': 5.41}{'loss': 0.0154, 'grad_norm': 0.06527196615934372, 'learning_rate': 5e-05, 'epoch': 5.42}{'loss': 0.0141, 'grad_norm': 0.067191943526268, 'learning_rate': 5e-05, 'epoch': 5.42}{'loss': 0.0145, 'grad_norm': 0.05320098251104355, 'learning_rate': 5e-05, 'epoch': 5.43}{'loss': 0.0128, 'grad_norm': 0.07152381539344788, 'learning_rate': 5e-05, 'epoch': 5.44}{'loss': 0.0141, 'grad_norm': 0.05642586573958397, 'learning_rate': 5e-05, 'epoch': 5.45}{'loss': 0.0136, 'grad_norm': 0.07099372893571854, 'learning_rate': 5e-05, 'epoch': 5.46}{'loss': 0.0142, 'grad_norm': 0.064338319003582, 'learning_rate': 5e-05, 'epoch': 5.47}{'loss': 0.015, 'grad_norm': 0.06006501615047455, 'learning_rate': 5e-05, 'epoch': 5.48}{'loss': 0.0143, 'grad_norm': 0.05377480015158653, 'learning_rate': 5e-05, 'epoch': 5.49}{'loss': 0.0139, 'grad_norm': 0.050505898892879486, 'learning_rate': 5e-05, 'epoch': 5.5}{'loss': 0.0133, 'grad_norm': 0.10207324475049973, 'learning_rate': 5e-05, 'epoch': 5.51}{'loss': 0.0151, 'grad_norm': 0.05361558869481087, 'learning_rate': 5e-05, 'epoch': 5.51}{'loss': 0.0141, 'grad_norm': 0.08535362035036087, 'learning_rate': 5e-05, 'epoch': 5.52}{'loss': 0.0147, 'grad_norm': 0.04632649943232536, 'learning_rate': 5e-05, 'epoch': 5.53}{'loss': 0.0135, 'grad_norm': 0.09786040335893631, 'learning_rate': 5e-05, 'epoch': 5.54}{'loss': 0.0136, 'grad_norm': 0.056132785975933075, 'learning_rate': 5e-05, 'epoch': 5.55}{'loss': 0.0152, 'grad_norm': 0.06718161702156067, 'learning_rate': 5e-05, 'epoch': 5.56}{'loss': 0.0141, 'grad_norm': 0.05938387289643288, 'learning_rate': 5e-05, 'epoch': 5.57}{'loss': 0.0138, 'grad_norm': 0.10272454470396042, 'learning_rate': 5e-05, 'epoch': 5.58}{'loss': 0.014, 'grad_norm': 0.10649467259645462, 'learning_rate': 5e-05, 'epoch': 5.59}{'loss': 0.0148, 'grad_norm': 0.060060687363147736, 'learning_rate': 5e-05, 'epoch': 5.59}{'loss': 0.0146, 'grad_norm': 0.07652319222688675, 'learning_rate': 5e-05, 'epoch': 5.6}{'loss': 0.0158, 'grad_norm': 0.06992480158805847, 'learning_rate': 5e-05, 'epoch': 5.61}{'loss': 0.0146, 'grad_norm': 0.07976130396127701, 'learning_rate': 5e-05, 'epoch': 5.62}{'loss': 0.0136, 'grad_norm': 0.0759866014122963, 'learning_rate': 5e-05, 'epoch': 5.63}{'loss': 0.0151, 'grad_norm': 0.07904518395662308, 'learning_rate': 5e-05, 'epoch': 5.64}{'loss': 0.0151, 'grad_norm': 0.052858106791973114, 'learning_rate': 5e-05, 'epoch': 5.65}{'loss': 0.0145, 'grad_norm': 0.07951066642999649, 'learning_rate': 5e-05, 'epoch': 5.66}{'loss': 0.0145, 'grad_norm': 0.0503229983150959, 'learning_rate': 5e-05, 'epoch': 5.67}{'loss': 0.0146, 'grad_norm': 0.04900679737329483, 'learning_rate': 5e-05, 'epoch': 5.67}{'loss': 0.0138, 'grad_norm': 0.04648089408874512, 'learning_rate': 5e-05, 'epoch': 5.68}{'loss': 0.0143, 'grad_norm': 0.05180809274315834, 'learning_rate': 5e-05, 'epoch': 5.69}{'loss': 0.0143, 'grad_norm': 0.04682798311114311, 'learning_rate': 5e-05, 'epoch': 5.7}{'loss': 0.0143, 'grad_norm': 0.040507376194000244, 'learning_rate': 5e-05, 'epoch': 5.71}{'loss': 0.015, 'grad_norm': 0.05329282581806183, 'learning_rate': 5e-05, 'epoch': 5.72}{'loss': 0.0139, 'grad_norm': 0.0754038468003273, 'learning_rate': 5e-05, 'epoch': 5.73}{'loss': 0.0137, 'grad_norm': 0.041678089648485184, 'learning_rate': 5e-05, 'epoch': 5.74}{'loss': 0.015, 'grad_norm': 0.10341214388608932, 'learning_rate': 5e-05, 'epoch': 5.75}{'loss': 0.0136, 'grad_norm': 0.08065998554229736, 'learning_rate': 5e-05, 'epoch': 5.76}{'loss': 0.0149, 'grad_norm': 0.049597498029470444, 'learning_rate': 5e-05, 'epoch': 5.76}{'loss': 0.0139, 'grad_norm': 0.05292259156703949, 'learning_rate': 5e-05, 'epoch': 5.77}{'loss': 0.0143, 'grad_norm': 0.07510729879140854, 'learning_rate': 5e-05, 'epoch': 5.78}{'loss': 0.0148, 'grad_norm': 0.06309475004673004, 'learning_rate': 5e-05, 'epoch': 5.79}{'loss': 0.0147, 'grad_norm': 0.09058559685945511, 'learning_rate': 5e-05, 'epoch': 5.8}{'loss': 0.0127, 'grad_norm': 0.1058143898844719, 'learning_rate': 5e-05, 'epoch': 5.81}{'loss': 0.0141, 'grad_norm': 0.0771198496222496, 'learning_rate': 5e-05, 'epoch': 5.82}{'loss': 0.0137, 'grad_norm': 0.08364948630332947, 'learning_rate': 5e-05, 'epoch': 5.83}{'loss': 0.0138, 'grad_norm': 0.04818226769566536, 'learning_rate': 5e-05, 'epoch': 5.84}{'loss': 0.0148, 'grad_norm': 0.05314639210700989, 'learning_rate': 5e-05, 'epoch': 5.84}{'loss': 0.0142, 'grad_norm': 0.04522636905312538, 'learning_rate': 5e-05, 'epoch': 5.85}{'loss': 0.0143, 'grad_norm': 0.06499237567186356, 'learning_rate': 5e-05, 'epoch': 5.86}{'loss': 0.014, 'grad_norm': 0.04954953491687775, 'learning_rate': 5e-05, 'epoch': 5.87}{'loss': 0.0129, 'grad_norm': 0.08075820654630661, 'learning_rate': 5e-05, 'epoch': 5.88}{'loss': 0.0127, 'grad_norm': 0.044531356543302536, 'learning_rate': 5e-05, 'epoch': 5.89}{'loss': 0.014, 'grad_norm': 0.04869179427623749, 'learning_rate': 5e-05, 'epoch': 5.9}{'loss': 0.0146, 'grad_norm': 0.06986898183822632, 'learning_rate': 5e-05, 'epoch': 5.91}{'loss': 0.0137, 'grad_norm': 0.04823469743132591, 'learning_rate': 5e-05, 'epoch': 5.92}{'loss': 0.0145, 'grad_norm': 0.04982394352555275, 'learning_rate': 5e-05, 'epoch': 5.93}{'loss': 0.0133, 'grad_norm': 0.05927162617444992, 'learning_rate': 5e-05, 'epoch': 5.93}{'loss': 0.0127, 'grad_norm': 0.06863203644752502, 'learning_rate': 5e-05, 'epoch': 5.94}{'loss': 0.0135, 'grad_norm': 0.04899239540100098, 'learning_rate': 5e-05, 'epoch': 5.95}{'loss': 0.0144, 'grad_norm': 0.06128808483481407, 'learning_rate': 5e-05, 'epoch': 5.96}{'loss': 0.0143, 'grad_norm': 0.0979231521487236, 'learning_rate': 5e-05, 'epoch': 5.97}{'loss': 0.013, 'grad_norm': 0.06690818071365356, 'learning_rate': 5e-05, 'epoch': 5.98}{'loss': 0.0143, 'grad_norm': 0.07186027616262436, 'learning_rate': 5e-05, 'epoch': 5.99}{'loss': 0.0136, 'grad_norm': 0.11855007708072662, 'learning_rate': 5e-05, 'epoch': 6.0}{'eval_AnatEM': 0.6373626373117256, 'eval_bc2gm': 0.7117437721917658, 'eval_bc4chemd': 0.6940874035487606, 'eval_bc5cdr': 0.8431618569134877, 'eval_broad_twitter_corpus': 0.6739130434283639, 'eval_conllpp': 0.9627727855723459, 'eval_conll2003': 0.9615384614882153, 'eval_FabNER': 0.024432809736825413, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.644444444393784, 'eval_mit-movie': 0.7291666666164855, 'eval_mit-restaurant': 0.79596412551038, 'eval_MultiNERD': 0.8958333332830373, 'eval_ncbi': 0.8579387186126459, 'eval_Ontonotes': 0.909433962213988, 'eval_TweetNER7': 0.6210995541846779, 'eval_WikiANN-en': 0.8014571948495262, 'eval_WikiNeural': 0.9057665259694366, 'eval_average': 0.7439565674003673, 'eval_runtime': 417.4955, 'eval_samples_per_second': 8.623, 'eval_steps_per_second': 0.271, 'epoch': 6.0} 50%|█████     | 6714/13416 [2:59:13<2:09:48,  1.[INFO|trainer.py:3948] 2025-02-24 05:42:33,332 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714[INFO|configuration_utils.py:423] 2025-02-24 05:42:33,335 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/config.json[INFO|configuration_utils.py:909] 2025-02-24 05:42:33,335 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 05:42:34,416 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 05:42:34,418 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 05:42:34,418 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 05:42:34,419 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/spiece.model[2025-02-24 05:42:34,448] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6712 is about to be saved![2025-02-24 05:42:34,466] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/mp_rank_00_model_states.pt[2025-02-24 05:42:34,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/mp_rank_00_model_states.pt...[2025-02-24 05:42:35,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/mp_rank_00_model_states.pt.[2025-02-24 05:42:35,710] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 05:42:36,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 05:42:36,812] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-6714/global_step6712/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 05:42:36,812] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6712 is ready now!{'loss': 0.014, 'grad_norm': 0.053826954215765, 'learning_rate': 5e-05, 'epoch': 6.01}{'loss': 0.0129, 'grad_norm': 0.045169148594141006, 'learning_rate': 5e-05, 'epoch': 6.01}{'loss': 0.0128, 'grad_norm': 0.12730932235717773, 'learning_rate': 5e-05, 'epoch': 6.02}{'loss': 0.0142, 'grad_norm': 0.07216012477874756, 'learning_rate': 5e-05, 'epoch': 6.03}{'loss': 0.0129, 'grad_norm': 0.053454503417015076, 'learning_rate': 5e-05, 'epoch': 6.04}{'loss': 0.0141, 'grad_norm': 0.06314307451248169, 'learning_rate': 5e-05, 'epoch': 6.05}{'loss': 0.013, 'grad_norm': 0.09096784144639969, 'learning_rate': 5e-05, 'epoch': 6.06}{'loss': 0.0132, 'grad_norm': 0.04754931107163429, 'learning_rate': 5e-05, 'epoch': 6.07}{'loss': 0.0143, 'grad_norm': 0.0491979643702507, 'learning_rate': 5e-05, 'epoch': 6.08}{'loss': 0.0121, 'grad_norm': 0.051739953458309174, 'learning_rate': 5e-05, 'epoch': 6.09}{'loss': 0.0134, 'grad_norm': 0.07115407288074493, 'learning_rate': 5e-05, 'epoch': 6.09}{'loss': 0.013, 'grad_norm': 0.054596882313489914, 'learning_rate': 5e-05, 'epoch': 6.1}{'loss': 0.0136, 'grad_norm': 0.057797349989414215, 'learning_rate': 5e-05, 'epoch': 6.11}{'loss': 0.0127, 'grad_norm': 0.11564368009567261, 'learning_rate': 5e-05, 'epoch': 6.12}{'loss': 0.0137, 'grad_norm': 0.05243533104658127, 'learning_rate': 5e-05, 'epoch': 6.13}{'loss': 0.0133, 'grad_norm': 0.0550055205821991, 'learning_rate': 5e-05, 'epoch': 6.14}{'loss': 0.0135, 'grad_norm': 0.05049854516983032, 'learning_rate': 5e-05, 'epoch': 6.15}{'loss': 0.0136, 'grad_norm': 0.06307535618543625, 'learning_rate': 5e-05, 'epoch': 6.16}{'loss': 0.0121, 'grad_norm': 0.06628455966711044, 'learning_rate': 5e-05, 'epoch': 6.17}{'loss': 0.0124, 'grad_norm': 0.06608272343873978, 'learning_rate': 5e-05, 'epoch': 6.18}{'loss': 0.0145, 'grad_norm': 0.06589428335428238, 'learning_rate': 5e-05, 'epoch': 6.18}{'loss': 0.0129, 'grad_norm': 0.07328581064939499, 'learning_rate': 5e-05, 'epoch': 6.19}{'loss': 0.0124, 'grad_norm': 0.04507888853549957, 'learning_rate': 5e-05, 'epoch': 6.2}{'loss': 0.0138, 'grad_norm': 0.04735711216926575, 'learning_rate': 5e-05, 'epoch': 6.21}{'loss': 0.0127, 'grad_norm': 0.06076807901263237, 'learning_rate': 5e-05, 'epoch': 6.22}{'loss': 0.0122, 'grad_norm': 0.05383080989122391, 'learning_rate': 5e-05, 'epoch': 6.23}{'loss': 0.013, 'grad_norm': 0.03860091418027878, 'learning_rate': 5e-05, 'epoch': 6.24}{'loss': 0.0132, 'grad_norm': 0.05477886274456978, 'learning_rate': 5e-05, 'epoch': 6.25}{'loss': 0.0124, 'grad_norm': 0.06558316946029663, 'learning_rate': 5e-05, 'epoch': 6.26}{'loss': 0.014, 'grad_norm': 0.04795249551534653, 'learning_rate': 5e-05, 'epoch': 6.26}{'loss': 0.0139, 'grad_norm': 0.08383374661207199, 'learning_rate': 5e-05, 'epoch': 6.27}{'loss': 0.0126, 'grad_norm': 0.06356168538331985, 'learning_rate': 5e-05, 'epoch': 6.28}{'loss': 0.0133, 'grad_norm': 0.05114109069108963, 'learning_rate': 5e-05, 'epoch': 6.29}{'loss': 0.0131, 'grad_norm': 0.05215233936905861, 'learning_rate': 5e-05, 'epoch': 6.3}{'loss': 0.0134, 'grad_norm': 0.08460520207881927, 'learning_rate': 5e-05, 'epoch': 6.31}{'loss': 0.0124, 'grad_norm': 0.05039995163679123, 'learning_rate': 5e-05, 'epoch': 6.32}{'loss': 0.0133, 'grad_norm': 0.06894014030694962, 'learning_rate': 5e-05, 'epoch': 6.33}{'loss': 0.0138, 'grad_norm': 0.05307549238204956, 'learning_rate': 5e-05, 'epoch': 6.34}{'loss': 0.014, 'grad_norm': 0.06600427627563477, 'learning_rate': 5e-05, 'epoch': 6.35}{'loss': 0.0121, 'grad_norm': 0.06507733464241028, 'learning_rate': 5e-05, 'epoch': 6.35}{'loss': 0.0119, 'grad_norm': 0.03873063996434212, 'learning_rate': 5e-05, 'epoch': 6.36}{'loss': 0.0138, 'grad_norm': 0.04512146860361099, 'learning_rate': 5e-05, 'epoch': 6.37}{'loss': 0.0138, 'grad_norm': 0.049132268875837326, 'learning_rate': 5e-05, 'epoch': 6.38}{'loss': 0.0131, 'grad_norm': 0.05476585030555725, 'learning_rate': 5e-05, 'epoch': 6.39}{'loss': 0.0125, 'grad_norm': 0.04400996118783951, 'learning_rate': 5e-05, 'epoch': 6.4}{'loss': 0.0125, 'grad_norm': 0.09005684405565262, 'learning_rate': 5e-05, 'epoch': 6.41}{'loss': 0.0135, 'grad_norm': 0.06949735432863235, 'learning_rate': 5e-05, 'epoch': 6.42}{'loss': 0.0131, 'grad_norm': 0.05231619253754616, 'learning_rate': 5e-05, 'epoch': 6.43}{'loss': 0.013, 'grad_norm': 0.059064578264951706, 'learning_rate': 5e-05, 'epoch': 6.43}{'loss': 0.012, 'grad_norm': 0.057739194482564926, 'learning_rate': 5e-05, 'epoch': 6.44}{'loss': 0.0124, 'grad_norm': 0.05012756586074829, 'learning_rate': 5e-05, 'epoch': 6.45}{'loss': 0.0139, 'grad_norm': 0.06277350336313248, 'learning_rate': 5e-05, 'epoch': 6.46}{'loss': 0.0139, 'grad_norm': 0.12440746277570724, 'learning_rate': 5e-05, 'epoch': 6.47}{'loss': 0.0134, 'grad_norm': 0.048576731234788895, 'learning_rate': 5e-05, 'epoch': 6.48}{'loss': 0.0127, 'grad_norm': 0.07756438851356506, 'learning_rate': 5e-05, 'epoch': 6.49}{'loss': 0.0128, 'grad_norm': 0.09138619899749756, 'learning_rate': 5e-05, 'epoch': 6.5}{'loss': 0.013, 'grad_norm': 0.053920723497867584, 'learning_rate': 5e-05, 'epoch': 6.51}{'loss': 0.0121, 'grad_norm': 0.04642247408628464, 'learning_rate': 5e-05, 'epoch': 6.51}{'loss': 0.0127, 'grad_norm': 0.08366930484771729, 'learning_rate': 5e-05, 'epoch': 6.52}{'loss': 0.0128, 'grad_norm': 0.10611703991889954, 'learning_rate': 5e-05, 'epoch': 6.53}{'loss': 0.013, 'grad_norm': 0.061463844031095505, 'learning_rate': 5e-05, 'epoch': 6.54}{'loss': 0.0133, 'grad_norm': 0.053980790078639984, 'learning_rate': 5e-05, 'epoch': 6.55}{'loss': 0.0131, 'grad_norm': 0.048147231340408325, 'learning_rate': 5e-05, 'epoch': 6.56}{'loss': 0.0132, 'grad_norm': 0.04429398477077484, 'learning_rate': 5e-05, 'epoch': 6.57}{'loss': 0.0125, 'grad_norm': 0.08326467871665955, 'learning_rate': 5e-05, 'epoch': 6.58}{'loss': 0.0139, 'grad_norm': 0.05710627883672714, 'learning_rate': 5e-05, 'epoch': 6.59}{'loss': 0.013, 'grad_norm': 0.0432669073343277, 'learning_rate': 5e-05, 'epoch': 6.6}{'loss': 0.0124, 'grad_norm': 0.1025797426700592, 'learning_rate': 5e-05, 'epoch': 6.6}{'loss': 0.0139, 'grad_norm': 0.04250428080558777, 'learning_rate': 5e-05, 'epoch': 6.61}{'loss': 0.0139, 'grad_norm': 0.09912525862455368, 'learning_rate': 5e-05, 'epoch': 6.62}{'loss': 0.0143, 'grad_norm': 0.12958674132823944, 'learning_rate': 5e-05, 'epoch': 6.63}{'loss': 0.0133, 'grad_norm': 0.0520484484732151, 'learning_rate': 5e-05, 'epoch': 6.64}{'loss': 0.0132, 'grad_norm': 0.03762315213680267, 'learning_rate': 5e-05, 'epoch': 6.65}{'loss': 0.0125, 'grad_norm': 0.03857412934303284, 'learning_rate': 5e-05, 'epoch': 6.66}{'loss': 0.014, 'grad_norm': 0.05614443123340607, 'learning_rate': 5e-05, 'epoch': 6.67}{'loss': 0.0123, 'grad_norm': 0.04923378676176071, 'learning_rate': 5e-05, 'epoch': 6.68}{'loss': 0.0126, 'grad_norm': 0.06610307842493057, 'learning_rate': 5e-05, 'epoch': 6.68}{'loss': 0.0126, 'grad_norm': 0.04326480254530907, 'learning_rate': 5e-05, 'epoch': 6.69}{'loss': 0.0126, 'grad_norm': 0.07104115933179855, 'learning_rate': 5e-05, 'epoch': 6.7}{'loss': 0.0128, 'grad_norm': 0.12280949205160141, 'learning_rate': 5e-05, 'epoch': 6.71}{'loss': 0.0153, 'grad_norm': 0.047347426414489746, 'learning_rate': 5e-05, 'epoch': 6.72}{'loss': 0.0129, 'grad_norm': 0.08965204656124115, 'learning_rate': 5e-05, 'epoch': 6.73}{'loss': 0.0143, 'grad_norm': 0.041965022683143616, 'learning_rate': 5e-05, 'epoch': 6.74}{'loss': 0.0128, 'grad_norm': 0.06718753278255463, 'learning_rate': 5e-05, 'epoch': 6.75}{'loss': 0.0126, 'grad_norm': 0.05305391177535057, 'learning_rate': 5e-05, 'epoch': 6.76}{'loss': 0.0138, 'grad_norm': 0.03952297195792198, 'learning_rate': 5e-05, 'epoch': 6.77}{'loss': 0.014, 'grad_norm': 0.05765474587678909, 'learning_rate': 5e-05, 'epoch': 6.77}{'loss': 0.0121, 'grad_norm': 0.04436797648668289, 'learning_rate': 5e-05, 'epoch': 6.78}{'loss': 0.0139, 'grad_norm': 0.07675573974847794, 'learning_rate': 5e-05, 'epoch': 6.79}{'loss': 0.0121, 'grad_norm': 0.04611947387456894, 'learning_rate': 5e-05, 'epoch': 6.8}{'loss': 0.0136, 'grad_norm': 0.042060110718011856, 'learning_rate': 5e-05, 'epoch': 6.81}{'loss': 0.0132, 'grad_norm': 0.0594911202788353, 'learning_rate': 5e-05, 'epoch': 6.82}{'loss': 0.0119, 'grad_norm': 0.04003332182765007, 'learning_rate': 5e-05, 'epoch': 6.83}{'loss': 0.0137, 'grad_norm': 0.05259152129292488, 'learning_rate': 5e-05, 'epoch': 6.84}{'loss': 0.0123, 'grad_norm': 0.04721738398075104, 'learning_rate': 5e-05, 'epoch': 6.85}{'loss': 0.0119, 'grad_norm': 0.040166739374399185, 'learning_rate': 5e-05, 'epoch': 6.85}{'loss': 0.0119, 'grad_norm': 0.04406600818037987, 'learning_rate': 5e-05, 'epoch': 6.86}{'loss': 0.0129, 'grad_norm': 0.06469191610813141, 'learning_rate': 5e-05, 'epoch': 6.87}{'loss': 0.0118, 'grad_norm': 0.054716773331165314, 'learning_rate': 5e-05, 'epoch': 6.88}{'loss': 0.0117, 'grad_norm': 0.039335697889328, 'learning_rate': 5e-05, 'epoch': 6.89}{'loss': 0.0132, 'grad_norm': 0.0561070442199707, 'learning_rate': 5e-05, 'epoch': 6.9}{'loss': 0.0151, 'grad_norm': 0.04602069407701492, 'learning_rate': 5e-05, 'epoch': 6.91}{'loss': 0.0125, 'grad_norm': 0.05043197050690651, 'learning_rate': 5e-05, 'epoch': 6.92}{'loss': 0.013, 'grad_norm': 0.043026749044656754, 'learning_rate': 5e-05, 'epoch': 6.93}{'loss': 0.0126, 'grad_norm': 0.047134269028902054, 'learning_rate': 5e-05, 'epoch': 6.93}{'loss': 0.0123, 'grad_norm': 0.0467156283557415, 'learning_rate': 5e-05, 'epoch': 6.94}{'loss': 0.0141, 'grad_norm': 0.0625816136598587, 'learning_rate': 5e-05, 'epoch': 6.95}{'loss': 0.0133, 'grad_norm': 0.054084669798612595, 'learning_rate': 5e-05, 'epoch': 6.96}{'loss': 0.0124, 'grad_norm': 0.0468403659760952, 'learning_rate': 5e-05, 'epoch': 6.97}{'loss': 0.0123, 'grad_norm': 0.05662177503108978, 'learning_rate': 5e-05, 'epoch': 6.98}{'loss': 0.0126, 'grad_norm': 0.06686495989561081, 'learning_rate': 5e-05, 'epoch': 6.99}{'loss': 0.0129, 'grad_norm': 0.05185149237513542, 'learning_rate': 5e-05, 'epoch': 7.0}{'eval_AnatEM': 0.6304347825579159, 'eval_bc2gm': 0.7446808510136059, 'eval_bc4chemd': 0.7296587926008253, 'eval_bc5cdr': 0.8355091383309833, 'eval_broad_twitter_corpus': 0.6614481408513861, 'eval_conllpp': 0.9730423619523177, 'eval_conll2003': 0.9730423619523177, 'eval_FabNER': 0.015761821330008342, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.6256983239716862, 'eval_mit-movie': 0.7398496240099985, 'eval_mit-restaurant': 0.7950450449948785, 'eval_MultiNERD': 0.8978873238933465, 'eval_ncbi': 0.9137931033977921, 'eval_Ontonotes': 0.9128787878286193, 'eval_TweetNER7': 0.6398176291292674, 'eval_WikiANN-en': 0.8160291438476992, 'eval_WikiNeural': 0.9093484418760884, 'eval_average': 0.751945921717772, 'eval_runtime': 414.2881, 'eval_samples_per_second': 8.69, 'eval_steps_per_second': 0.273, 'epoch': 7.0} 58%|█████▊    | 7833/13416 [3:29:07<1:52:04,  1.[INFO|trainer.py:3948] 2025-02-24 06:12:26,942 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833[INFO|configuration_utils.py:423] 2025-02-24 06:12:26,944 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/config.json[INFO|configuration_utils.py:909] 2025-02-24 06:12:26,944 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 06:12:28,014 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 06:12:28,016 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 06:12:28,016 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 06:12:28,017 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/spiece.model[2025-02-24 06:12:28,035] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7831 is about to be saved![2025-02-24 06:12:28,042] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/mp_rank_00_model_states.pt[2025-02-24 06:12:28,043] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/mp_rank_00_model_states.pt...[2025-02-24 06:12:29,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/mp_rank_00_model_states.pt.[2025-02-24 06:12:29,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 06:12:30,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 06:12:30,388] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-7833/global_step7831/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 06:12:30,388] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7831 is ready now!{'loss': 0.0126, 'grad_norm': 0.047635097056627274, 'learning_rate': 5e-05, 'epoch': 7.01}{'loss': 0.0121, 'grad_norm': 0.050842396914958954, 'learning_rate': 5e-05, 'epoch': 7.02}{'loss': 0.0119, 'grad_norm': 0.0416795015335083, 'learning_rate': 5e-05, 'epoch': 7.02}{'loss': 0.012, 'grad_norm': 0.07627113163471222, 'learning_rate': 5e-05, 'epoch': 7.03}{'loss': 0.0139, 'grad_norm': 0.05475622043013573, 'learning_rate': 5e-05, 'epoch': 7.04}{'loss': 0.0134, 'grad_norm': 0.05688629671931267, 'learning_rate': 5e-05, 'epoch': 7.05}{'loss': 0.0122, 'grad_norm': 0.05510402470827103, 'learning_rate': 5e-05, 'epoch': 7.06}{'loss': 0.0136, 'grad_norm': 0.0371435321867466, 'learning_rate': 5e-05, 'epoch': 7.07}{'loss': 0.0116, 'grad_norm': 0.056912537664175034, 'learning_rate': 5e-05, 'epoch': 7.08}{'loss': 0.0119, 'grad_norm': 0.04865361377596855, 'learning_rate': 5e-05, 'epoch': 7.09}{'loss': 0.0128, 'grad_norm': 0.04857444018125534, 'learning_rate': 5e-05, 'epoch': 7.1}{'loss': 0.0119, 'grad_norm': 0.06291568279266357, 'learning_rate': 5e-05, 'epoch': 7.1}{'loss': 0.0123, 'grad_norm': 0.0475720576941967, 'learning_rate': 5e-05, 'epoch': 7.11}{'loss': 0.0117, 'grad_norm': 0.06256259977817535, 'learning_rate': 5e-05, 'epoch': 7.12}{'loss': 0.0129, 'grad_norm': 0.062218666076660156, 'learning_rate': 5e-05, 'epoch': 7.13}{'loss': 0.0117, 'grad_norm': 0.04308442771434784, 'learning_rate': 5e-05, 'epoch': 7.14}{'loss': 0.0122, 'grad_norm': 0.06309285014867783, 'learning_rate': 5e-05, 'epoch': 7.15}{'loss': 0.0115, 'grad_norm': 0.09882111847400665, 'learning_rate': 5e-05, 'epoch': 7.16}{'loss': 0.0126, 'grad_norm': 0.04331418126821518, 'learning_rate': 5e-05, 'epoch': 7.17}{'loss': 0.0128, 'grad_norm': 0.04430776834487915, 'learning_rate': 5e-05, 'epoch': 7.18}{'loss': 0.0117, 'grad_norm': 0.08989149332046509, 'learning_rate': 5e-05, 'epoch': 7.19}{'loss': 0.0124, 'grad_norm': 0.06906475126743317, 'learning_rate': 5e-05, 'epoch': 7.19}{'loss': 0.0127, 'grad_norm': 0.07158166170120239, 'learning_rate': 5e-05, 'epoch': 7.2}{'loss': 0.0134, 'grad_norm': 0.04997288063168526, 'learning_rate': 5e-05, 'epoch': 7.21}{'loss': 0.0112, 'grad_norm': 0.040854062885046005, 'learning_rate': 5e-05, 'epoch': 7.22}{'loss': 0.0115, 'grad_norm': 0.04698793590068817, 'learning_rate': 5e-05, 'epoch': 7.23}{'loss': 0.0126, 'grad_norm': 0.05198858305811882, 'learning_rate': 5e-05, 'epoch': 7.24}{'loss': 0.0121, 'grad_norm': 0.04848574846982956, 'learning_rate': 5e-05, 'epoch': 7.25}{'loss': 0.0111, 'grad_norm': 0.06316719204187393, 'learning_rate': 5e-05, 'epoch': 7.26}{'loss': 0.0128, 'grad_norm': 0.05002933740615845, 'learning_rate': 5e-05, 'epoch': 7.27}{'loss': 0.0119, 'grad_norm': 0.1311350166797638, 'learning_rate': 5e-05, 'epoch': 7.27}{'loss': 0.0135, 'grad_norm': 0.05794018507003784, 'learning_rate': 5e-05, 'epoch': 7.28}{'loss': 0.0119, 'grad_norm': 0.060770876705646515, 'learning_rate': 5e-05, 'epoch': 7.29}{'loss': 0.0108, 'grad_norm': 0.08373558521270752, 'learning_rate': 5e-05, 'epoch': 7.3}{'loss': 0.0129, 'grad_norm': 0.051591940224170685, 'learning_rate': 5e-05, 'epoch': 7.31}{'loss': 0.0124, 'grad_norm': 0.04777263104915619, 'learning_rate': 5e-05, 'epoch': 7.32}{'loss': 0.0116, 'grad_norm': 0.062057577073574066, 'learning_rate': 5e-05, 'epoch': 7.33}{'loss': 0.0125, 'grad_norm': 0.06614775955677032, 'learning_rate': 5e-05, 'epoch': 7.34}{'loss': 0.0127, 'grad_norm': 0.044900164008140564, 'learning_rate': 5e-05, 'epoch': 7.35}{'loss': 0.012, 'grad_norm': 0.04421132057905197, 'learning_rate': 5e-05, 'epoch': 7.35}{'loss': 0.013, 'grad_norm': 0.0499531589448452, 'learning_rate': 5e-05, 'epoch': 7.36}{'loss': 0.0117, 'grad_norm': 0.043988343328237534, 'learning_rate': 5e-05, 'epoch': 7.37}{'loss': 0.0132, 'grad_norm': 0.06523681432008743, 'learning_rate': 5e-05, 'epoch': 7.38}{'loss': 0.0119, 'grad_norm': 0.06447627395391464, 'learning_rate': 5e-05, 'epoch': 7.39}{'loss': 0.0129, 'grad_norm': 0.04586800932884216, 'learning_rate': 5e-05, 'epoch': 7.4}{'loss': 0.0119, 'grad_norm': 0.042171087116003036, 'learning_rate': 5e-05, 'epoch': 7.41}{'loss': 0.0113, 'grad_norm': 0.06860937178134918, 'learning_rate': 5e-05, 'epoch': 7.42}{'loss': 0.0116, 'grad_norm': 0.05998002365231514, 'learning_rate': 5e-05, 'epoch': 7.43}{'loss': 0.0126, 'grad_norm': 0.061544761061668396, 'learning_rate': 5e-05, 'epoch': 7.44}{'loss': 0.0114, 'grad_norm': 0.040425729006528854, 'learning_rate': 5e-05, 'epoch': 7.44}{'loss': 0.0115, 'grad_norm': 0.04951008781790733, 'learning_rate': 5e-05, 'epoch': 7.45}{'loss': 0.0118, 'grad_norm': 0.04805770516395569, 'learning_rate': 5e-05, 'epoch': 7.46}{'loss': 0.0115, 'grad_norm': 0.053085807710886, 'learning_rate': 5e-05, 'epoch': 7.47}{'loss': 0.0129, 'grad_norm': 0.05567792430520058, 'learning_rate': 5e-05, 'epoch': 7.48}{'loss': 0.0126, 'grad_norm': 0.09692912548780441, 'learning_rate': 5e-05, 'epoch': 7.49}{'loss': 0.0111, 'grad_norm': 0.05534759536385536, 'learning_rate': 5e-05, 'epoch': 7.5}{'loss': 0.0128, 'grad_norm': 0.06236272305250168, 'learning_rate': 5e-05, 'epoch': 7.51}{'loss': 0.0115, 'grad_norm': 0.045854322612285614, 'learning_rate': 5e-05, 'epoch': 7.52}{'loss': 0.0121, 'grad_norm': 0.04586983472108841, 'learning_rate': 5e-05, 'epoch': 7.52}{'loss': 0.0106, 'grad_norm': 0.04333603382110596, 'learning_rate': 5e-05, 'epoch': 7.53}{'loss': 0.0117, 'grad_norm': 0.08484555035829544, 'learning_rate': 5e-05, 'epoch': 7.54}{'loss': 0.0117, 'grad_norm': 0.04845423623919487, 'learning_rate': 5e-05, 'epoch': 7.55}{'loss': 0.0116, 'grad_norm': 0.05157897248864174, 'learning_rate': 5e-05, 'epoch': 7.56}{'loss': 0.0132, 'grad_norm': 0.1007465049624443, 'learning_rate': 5e-05, 'epoch': 7.57}{'loss': 0.0118, 'grad_norm': 0.0406373105943203, 'learning_rate': 5e-05, 'epoch': 7.58}{'loss': 0.0131, 'grad_norm': 0.044166576117277145, 'learning_rate': 5e-05, 'epoch': 7.59}{'loss': 0.0119, 'grad_norm': 0.053141575306653976, 'learning_rate': 5e-05, 'epoch': 7.6}{'loss': 0.0133, 'grad_norm': 0.047667715698480606, 'learning_rate': 5e-05, 'epoch': 7.61}{'loss': 0.0118, 'grad_norm': 0.07467934489250183, 'learning_rate': 5e-05, 'epoch': 7.61}{'loss': 0.013, 'grad_norm': 0.047545790672302246, 'learning_rate': 5e-05, 'epoch': 7.62}{'loss': 0.0118, 'grad_norm': 0.04951932281255722, 'learning_rate': 5e-05, 'epoch': 7.63}{'loss': 0.0117, 'grad_norm': 0.04784753918647766, 'learning_rate': 5e-05, 'epoch': 7.64}{'loss': 0.0131, 'grad_norm': 0.06985203921794891, 'learning_rate': 5e-05, 'epoch': 7.65}{'loss': 0.0112, 'grad_norm': 0.04569092392921448, 'learning_rate': 5e-05, 'epoch': 7.66}{'loss': 0.0115, 'grad_norm': 0.05140741169452667, 'learning_rate': 5e-05, 'epoch': 7.67}{'loss': 0.012, 'grad_norm': 0.046633124351501465, 'learning_rate': 5e-05, 'epoch': 7.68}{'loss': 0.0118, 'grad_norm': 0.0394902266561985, 'learning_rate': 5e-05, 'epoch': 7.69}{'loss': 0.0134, 'grad_norm': 0.05019938200712204, 'learning_rate': 5e-05, 'epoch': 7.69}{'loss': 0.0132, 'grad_norm': 0.0558980256319046, 'learning_rate': 5e-05, 'epoch': 7.7}{'loss': 0.0108, 'grad_norm': 0.042042069137096405, 'learning_rate': 5e-05, 'epoch': 7.71}{'loss': 0.0131, 'grad_norm': 0.10070327669382095, 'learning_rate': 5e-05, 'epoch': 7.72}{'loss': 0.0113, 'grad_norm': 0.05857079103589058, 'learning_rate': 5e-05, 'epoch': 7.73}{'loss': 0.0127, 'grad_norm': 0.05998248979449272, 'learning_rate': 5e-05, 'epoch': 7.74}{'loss': 0.0116, 'grad_norm': 0.039589982479810715, 'learning_rate': 5e-05, 'epoch': 7.75}{'loss': 0.0106, 'grad_norm': 0.041707612574100494, 'learning_rate': 5e-05, 'epoch': 7.76}{'loss': 0.0133, 'grad_norm': 0.04978235438466072, 'learning_rate': 5e-05, 'epoch': 7.77}{'loss': 0.012, 'grad_norm': 0.06172997131943703, 'learning_rate': 5e-05, 'epoch': 7.77}{'loss': 0.0126, 'grad_norm': 0.057844530791044235, 'learning_rate': 5e-05, 'epoch': 7.78}{'loss': 0.0114, 'grad_norm': 0.04215551167726517, 'learning_rate': 5e-05, 'epoch': 7.79}{'loss': 0.0127, 'grad_norm': 0.07681286334991455, 'learning_rate': 5e-05, 'epoch': 7.8}{'loss': 0.0112, 'grad_norm': 0.1360873430967331, 'learning_rate': 5e-05, 'epoch': 7.81}{'loss': 0.0134, 'grad_norm': 0.06638185679912567, 'learning_rate': 5e-05, 'epoch': 7.82}{'loss': 0.0121, 'grad_norm': 0.05664951726794243, 'learning_rate': 5e-05, 'epoch': 7.83}{'loss': 0.0123, 'grad_norm': 0.048195332288742065, 'learning_rate': 5e-05, 'epoch': 7.84}{'loss': 0.0116, 'grad_norm': 0.05579643324017525, 'learning_rate': 5e-05, 'epoch': 7.85}{'loss': 0.012, 'grad_norm': 0.04901966080069542, 'learning_rate': 5e-05, 'epoch': 7.86}{'loss': 0.0127, 'grad_norm': 0.095386803150177, 'learning_rate': 5e-05, 'epoch': 7.86}{'loss': 0.0124, 'grad_norm': 0.0458810068666935, 'learning_rate': 5e-05, 'epoch': 7.87}{'loss': 0.0118, 'grad_norm': 0.07677838206291199, 'learning_rate': 5e-05, 'epoch': 7.88}{'loss': 0.0115, 'grad_norm': 0.04173871502280235, 'learning_rate': 5e-05, 'epoch': 7.89}{'loss': 0.0111, 'grad_norm': 0.04038482531905174, 'learning_rate': 5e-05, 'epoch': 7.9}{'loss': 0.0112, 'grad_norm': 0.08390922099351883, 'learning_rate': 5e-05, 'epoch': 7.91}{'loss': 0.0128, 'grad_norm': 0.06370270997285843, 'learning_rate': 5e-05, 'epoch': 7.92}{'loss': 0.0113, 'grad_norm': 0.08166873455047607, 'learning_rate': 5e-05, 'epoch': 7.93}{'loss': 0.0107, 'grad_norm': 0.18762104213237762, 'learning_rate': 5e-05, 'epoch': 7.94}{'loss': 0.0111, 'grad_norm': 0.07160831987857819, 'learning_rate': 5e-05, 'epoch': 7.94}{'loss': 0.0105, 'grad_norm': 0.04349366948008537, 'learning_rate': 5e-05, 'epoch': 7.95}{'loss': 0.0118, 'grad_norm': 0.23762290179729462, 'learning_rate': 5e-05, 'epoch': 7.96}{'loss': 0.0125, 'grad_norm': 0.10508806258440018, 'learning_rate': 5e-05, 'epoch': 7.97}{'loss': 0.0123, 'grad_norm': 0.046914227306842804, 'learning_rate': 5e-05, 'epoch': 7.98}{'loss': 0.0117, 'grad_norm': 0.050590209662914276, 'learning_rate': 5e-05, 'epoch': 7.99}{'loss': 0.0134, 'grad_norm': 0.05594617873430252, 'learning_rate': 5e-05, 'epoch': 8.0}{'eval_AnatEM': 0.5918367346440024, 'eval_bc2gm': 0.7428571428068673, 'eval_bc4chemd': 0.7499999999498387, 'eval_bc5cdr': 0.8426527957885345, 'eval_broad_twitter_corpus': 0.6642335765925096, 'eval_conllpp': 0.9743589743087249, 'eval_conll2003': 0.9743589743087249, 'eval_FabNER': 0.010762331804576322, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.6702702702197225, 'eval_mit-movie': 0.7369985140657119, 'eval_mit-restaurant': 0.7891770010772284, 'eval_MultiNERD': 0.9033391915138316, 'eval_ncbi': 0.9164265129178052, 'eval_Ontonotes': 0.9113207546668178, 'eval_TweetNER7': 0.618526622852912, 'eval_WikiANN-en': 0.8260869564714426, 'eval_WikiNeural': 0.9083215796394477, 'eval_average': 0.7529238250561032, 'eval_runtime': 410.3363, 'eval_samples_per_second': 8.773, 'eval_steps_per_second': 0.275, 'epoch': 8.0} 67%|██████▋   | 8952/13416 [3:58:55<1:26:59,  1.[INFO|trainer.py:3948] 2025-02-24 06:42:15,525 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952[INFO|configuration_utils.py:423] 2025-02-24 06:42:15,527 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/config.json[INFO|configuration_utils.py:909] 2025-02-24 06:42:15,528 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 06:42:16,593 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 06:42:16,595 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 06:42:16,595 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 06:42:16,596 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/spiece.model[2025-02-24 06:42:16,616] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8950 is about to be saved![2025-02-24 06:42:16,627] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/mp_rank_00_model_states.pt[2025-02-24 06:42:16,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/mp_rank_00_model_states.pt...[2025-02-24 06:42:17,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/mp_rank_00_model_states.pt.[2025-02-24 06:42:17,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 06:42:18,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 06:42:18,943] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-8952/global_step8950/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 06:42:18,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8950 is ready now!{'loss': 0.012, 'grad_norm': 0.06557919830083847, 'learning_rate': 5e-05, 'epoch': 8.01}{'loss': 0.0117, 'grad_norm': 0.04149052873253822, 'learning_rate': 5e-05, 'epoch': 8.02}{'loss': 0.0108, 'grad_norm': 0.04587254300713539, 'learning_rate': 5e-05, 'epoch': 8.03}{'loss': 0.0116, 'grad_norm': 0.049320414662361145, 'learning_rate': 5e-05, 'epoch': 8.03}{'loss': 0.0105, 'grad_norm': 0.06360862404108047, 'learning_rate': 5e-05, 'epoch': 8.04}{'loss': 0.0109, 'grad_norm': 0.06800028681755066, 'learning_rate': 5e-05, 'epoch': 8.05}{'loss': 0.0106, 'grad_norm': 0.05220181122422218, 'learning_rate': 5e-05, 'epoch': 8.06}{'loss': 0.0114, 'grad_norm': 0.04126646742224693, 'learning_rate': 5e-05, 'epoch': 8.07}{'loss': 0.011, 'grad_norm': 0.07885920256376266, 'learning_rate': 5e-05, 'epoch': 8.08}{'loss': 0.0113, 'grad_norm': 0.05119401589035988, 'learning_rate': 5e-05, 'epoch': 8.09}{'loss': 0.011, 'grad_norm': 0.05566345900297165, 'learning_rate': 5e-05, 'epoch': 8.1}{'loss': 0.0111, 'grad_norm': 0.039457887411117554, 'learning_rate': 5e-05, 'epoch': 8.11}{'loss': 0.0122, 'grad_norm': 0.06282570958137512, 'learning_rate': 5e-05, 'epoch': 8.11}{'loss': 0.011, 'grad_norm': 0.05369266867637634, 'learning_rate': 5e-05, 'epoch': 8.12}{'loss': 0.0114, 'grad_norm': 0.046366170048713684, 'learning_rate': 5e-05, 'epoch': 8.13}{'loss': 0.0123, 'grad_norm': 0.05360233783721924, 'learning_rate': 5e-05, 'epoch': 8.14}{'loss': 0.0105, 'grad_norm': 0.05887829512357712, 'learning_rate': 5e-05, 'epoch': 8.15}{'loss': 0.0116, 'grad_norm': 0.08651210367679596, 'learning_rate': 5e-05, 'epoch': 8.16}{'loss': 0.0103, 'grad_norm': 0.052127763628959656, 'learning_rate': 5e-05, 'epoch': 8.17}{'loss': 0.0107, 'grad_norm': 0.05299254134297371, 'learning_rate': 5e-05, 'epoch': 8.18}{'loss': 0.0118, 'grad_norm': 0.03459654375910759, 'learning_rate': 5e-05, 'epoch': 8.19}{'loss': 0.0117, 'grad_norm': 0.04694093391299248, 'learning_rate': 5e-05, 'epoch': 8.19}{'loss': 0.011, 'grad_norm': 0.04086066409945488, 'learning_rate': 5e-05, 'epoch': 8.2}{'loss': 0.0117, 'grad_norm': 0.05105803906917572, 'learning_rate': 5e-05, 'epoch': 8.21}{'loss': 0.0112, 'grad_norm': 0.03519224375486374, 'learning_rate': 5e-05, 'epoch': 8.22}{'loss': 0.0103, 'grad_norm': 0.045906927436590195, 'learning_rate': 5e-05, 'epoch': 8.23}{'loss': 0.0104, 'grad_norm': 0.05398601293563843, 'learning_rate': 5e-05, 'epoch': 8.24}{'loss': 0.0107, 'grad_norm': 0.04690544307231903, 'learning_rate': 5e-05, 'epoch': 8.25}{'loss': 0.0118, 'grad_norm': 0.07355736196041107, 'learning_rate': 5e-05, 'epoch': 8.26}{'loss': 0.0111, 'grad_norm': 0.07872141152620316, 'learning_rate': 5e-05, 'epoch': 8.27}{'loss': 0.0117, 'grad_norm': 0.058966439217329025, 'learning_rate': 5e-05, 'epoch': 8.28}{'loss': 0.0097, 'grad_norm': 0.051345035433769226, 'learning_rate': 5e-05, 'epoch': 8.28}{'loss': 0.0113, 'grad_norm': 0.0718325674533844, 'learning_rate': 5e-05, 'epoch': 8.29}{'loss': 0.0112, 'grad_norm': 0.09305808693170547, 'learning_rate': 5e-05, 'epoch': 8.3}{'loss': 0.0109, 'grad_norm': 0.04877157881855965, 'learning_rate': 5e-05, 'epoch': 8.31}{'loss': 0.0115, 'grad_norm': 0.05643497779965401, 'learning_rate': 5e-05, 'epoch': 8.32}{'loss': 0.0126, 'grad_norm': 0.06130095571279526, 'learning_rate': 5e-05, 'epoch': 8.33}{'loss': 0.0114, 'grad_norm': 0.04476438835263252, 'learning_rate': 5e-05, 'epoch': 8.34}{'loss': 0.0115, 'grad_norm': 0.053418129682540894, 'learning_rate': 5e-05, 'epoch': 8.35}{'loss': 0.0118, 'grad_norm': 0.04605856537818909, 'learning_rate': 5e-05, 'epoch': 8.36}{'loss': 0.011, 'grad_norm': 0.05601014196872711, 'learning_rate': 5e-05, 'epoch': 8.36}{'loss': 0.011, 'grad_norm': 0.04252716526389122, 'learning_rate': 5e-05, 'epoch': 8.37}{'loss': 0.0113, 'grad_norm': 0.05986237898468971, 'learning_rate': 5e-05, 'epoch': 8.38}{'loss': 0.0118, 'grad_norm': 0.04209395498037338, 'learning_rate': 5e-05, 'epoch': 8.39}{'loss': 0.0115, 'grad_norm': 0.04672703519463539, 'learning_rate': 5e-05, 'epoch': 8.4}{'loss': 0.0113, 'grad_norm': 0.037030644714832306, 'learning_rate': 5e-05, 'epoch': 8.41}{'loss': 0.011, 'grad_norm': 0.040957558900117874, 'learning_rate': 5e-05, 'epoch': 8.42}{'loss': 0.012, 'grad_norm': 0.03994130715727806, 'learning_rate': 5e-05, 'epoch': 8.43}{'loss': 0.0113, 'grad_norm': 0.03985917195677757, 'learning_rate': 5e-05, 'epoch': 8.44}{'loss': 0.0124, 'grad_norm': 0.08277518302202225, 'learning_rate': 5e-05, 'epoch': 8.45}{'loss': 0.0112, 'grad_norm': 0.05523339658975601, 'learning_rate': 5e-05, 'epoch': 8.45}{'loss': 0.0122, 'grad_norm': 0.052594222128391266, 'learning_rate': 5e-05, 'epoch': 8.46}{'loss': 0.0113, 'grad_norm': 0.038790263235569, 'learning_rate': 5e-05, 'epoch': 8.47}{'loss': 0.0105, 'grad_norm': 0.08516677469015121, 'learning_rate': 5e-05, 'epoch': 8.48}{'loss': 0.0118, 'grad_norm': 0.04821412265300751, 'learning_rate': 5e-05, 'epoch': 8.49}{'loss': 0.0117, 'grad_norm': 0.05911944434046745, 'learning_rate': 5e-05, 'epoch': 8.5}{'loss': 0.0114, 'grad_norm': 0.05033231899142265, 'learning_rate': 5e-05, 'epoch': 8.51}{'loss': 0.0122, 'grad_norm': 0.04533014073967934, 'learning_rate': 5e-05, 'epoch': 8.52}{'loss': 0.012, 'grad_norm': 0.041091274470090866, 'learning_rate': 5e-05, 'epoch': 8.53}{'loss': 0.011, 'grad_norm': 0.040027085691690445, 'learning_rate': 5e-05, 'epoch': 8.53}{'loss': 0.0116, 'grad_norm': 0.05214015394449234, 'learning_rate': 5e-05, 'epoch': 8.54}{'loss': 0.0113, 'grad_norm': 0.057501230388879776, 'learning_rate': 5e-05, 'epoch': 8.55}{'loss': 0.011, 'grad_norm': 0.03705643489956856, 'learning_rate': 5e-05, 'epoch': 8.56}{'loss': 0.0117, 'grad_norm': 0.14089708030223846, 'learning_rate': 5e-05, 'epoch': 8.57}{'loss': 0.0111, 'grad_norm': 0.05704084038734436, 'learning_rate': 5e-05, 'epoch': 8.58}{'loss': 0.0127, 'grad_norm': 0.060727182775735855, 'learning_rate': 5e-05, 'epoch': 8.59}{'loss': 0.0118, 'grad_norm': 0.05009506642818451, 'learning_rate': 5e-05, 'epoch': 8.6}{'loss': 0.0109, 'grad_norm': 0.0460076667368412, 'learning_rate': 5e-05, 'epoch': 8.61}{'loss': 0.0107, 'grad_norm': 0.04100862890481949, 'learning_rate': 5e-05, 'epoch': 8.61}{'loss': 0.0121, 'grad_norm': 0.049474142491817474, 'learning_rate': 5e-05, 'epoch': 8.62}{'loss': 0.0117, 'grad_norm': 0.10945496708154678, 'learning_rate': 5e-05, 'epoch': 8.63}{'loss': 0.0104, 'grad_norm': 0.04904274642467499, 'learning_rate': 5e-05, 'epoch': 8.64}{'loss': 0.0102, 'grad_norm': 0.0433683842420578, 'learning_rate': 5e-05, 'epoch': 8.65}{'loss': 0.0111, 'grad_norm': 0.05375516787171364, 'learning_rate': 5e-05, 'epoch': 8.66}{'loss': 0.0121, 'grad_norm': 0.07228795439004898, 'learning_rate': 5e-05, 'epoch': 8.67}{'loss': 0.0111, 'grad_norm': 0.04214736074209213, 'learning_rate': 5e-05, 'epoch': 8.68}{'loss': 0.011, 'grad_norm': 0.04424595460295677, 'learning_rate': 5e-05, 'epoch': 8.69}{'loss': 0.0108, 'grad_norm': 0.05675951763987541, 'learning_rate': 5e-05, 'epoch': 8.7}{'loss': 0.0105, 'grad_norm': 0.079963319003582, 'learning_rate': 5e-05, 'epoch': 8.7}{'loss': 0.0116, 'grad_norm': 0.03955523297190666, 'learning_rate': 5e-05, 'epoch': 8.71}{'loss': 0.0109, 'grad_norm': 0.043013863265514374, 'learning_rate': 5e-05, 'epoch': 8.72}{'loss': 0.0124, 'grad_norm': 0.1067243441939354, 'learning_rate': 5e-05, 'epoch': 8.73}{'loss': 0.0108, 'grad_norm': 0.07121215760707855, 'learning_rate': 5e-05, 'epoch': 8.74}{'loss': 0.0104, 'grad_norm': 0.041247088462114334, 'learning_rate': 5e-05, 'epoch': 8.75}{'loss': 0.0112, 'grad_norm': 0.041169580072164536, 'learning_rate': 5e-05, 'epoch': 8.76}{'loss': 0.0111, 'grad_norm': 0.046717915683984756, 'learning_rate': 5e-05, 'epoch': 8.77}{'loss': 0.0118, 'grad_norm': 0.04155109450221062, 'learning_rate': 5e-05, 'epoch': 8.78}{'loss': 0.011, 'grad_norm': 0.04422983154654503, 'learning_rate': 5e-05, 'epoch': 8.78}{'loss': 0.0109, 'grad_norm': 0.053954802453517914, 'learning_rate': 5e-05, 'epoch': 8.79}{'loss': 0.0118, 'grad_norm': 0.05944836512207985, 'learning_rate': 5e-05, 'epoch': 8.8}{'loss': 0.0117, 'grad_norm': 0.05891576409339905, 'learning_rate': 5e-05, 'epoch': 8.81}{'loss': 0.0111, 'grad_norm': 0.07334785908460617, 'learning_rate': 5e-05, 'epoch': 8.82}{'loss': 0.012, 'grad_norm': 0.048434965312480927, 'learning_rate': 5e-05, 'epoch': 8.83}{'loss': 0.0125, 'grad_norm': 0.04059312865138054, 'learning_rate': 5e-05, 'epoch': 8.84}{'loss': 0.0116, 'grad_norm': 0.04512565955519676, 'learning_rate': 5e-05, 'epoch': 8.85}{'loss': 0.0129, 'grad_norm': 0.04065469652414322, 'learning_rate': 5e-05, 'epoch': 8.86}{'loss': 0.0101, 'grad_norm': 0.04184337705373764, 'learning_rate': 5e-05, 'epoch': 8.87}{'loss': 0.0104, 'grad_norm': 0.039622869342565536, 'learning_rate': 5e-05, 'epoch': 8.87}{'loss': 0.0113, 'grad_norm': 0.05130494758486748, 'learning_rate': 5e-05, 'epoch': 8.88}{'loss': 0.0114, 'grad_norm': 0.046695902943611145, 'learning_rate': 5e-05, 'epoch': 8.89}{'loss': 0.0113, 'grad_norm': 0.048910994082689285, 'learning_rate': 5e-05, 'epoch': 8.9}{'loss': 0.0105, 'grad_norm': 0.03377477824687958, 'learning_rate': 5e-05, 'epoch': 8.91}{'loss': 0.0122, 'grad_norm': 0.0640205442905426, 'learning_rate': 5e-05, 'epoch': 8.92}{'loss': 0.0115, 'grad_norm': 0.04765256121754646, 'learning_rate': 5e-05, 'epoch': 8.93}{'loss': 0.0108, 'grad_norm': 0.03601935878396034, 'learning_rate': 5e-05, 'epoch': 8.94}{'loss': 0.0113, 'grad_norm': 0.038708582520484924, 'learning_rate': 5e-05, 'epoch': 8.95}{'loss': 0.0093, 'grad_norm': 0.05394058674573898, 'learning_rate': 5e-05, 'epoch': 8.95}{'loss': 0.0112, 'grad_norm': 0.04389715567231178, 'learning_rate': 5e-05, 'epoch': 8.96}{'loss': 0.0111, 'grad_norm': 0.08747280389070511, 'learning_rate': 5e-05, 'epoch': 8.97}{'loss': 0.0109, 'grad_norm': 0.04840885102748871, 'learning_rate': 5e-05, 'epoch': 8.98}{'loss': 0.0114, 'grad_norm': 0.06658001244068146, 'learning_rate': 5e-05, 'epoch': 8.99}{'loss': 0.0095, 'grad_norm': 0.03614313527941704, 'learning_rate': 5e-05, 'epoch': 9.0}{'eval_AnatEM': 0.595744680800611, 'eval_bc2gm': 0.7509293679792376, 'eval_bc4chemd': 0.7300771207723925, 'eval_bc5cdr': 0.84318766061818, 'eval_broad_twitter_corpus': 0.6925925925429129, 'eval_conllpp': 0.9717948717446231, 'eval_conll2003': 0.9717948717446231, 'eval_FabNER': 0.014222222187454893, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.644444444393784, 'eval_mit-movie': 0.7376311843576345, 'eval_mit-restaurant': 0.8067796609667746, 'eval_MultiNERD': 0.8978873238933465, 'eval_ncbi': 0.8876080691137872, 'eval_Ontonotes': 0.9073724007059785, 'eval_TweetNER7': 0.6375092660729597, 'eval_WikiANN-en': 0.8211678831613791, 'eval_WikiNeural': 0.9167842030527035, 'eval_average': 0.752701596749419, 'eval_runtime': 403.1477, 'eval_samples_per_second': 8.93, 'eval_steps_per_second': 0.28, 'epoch': 9.0} 75%|███████▌  | 10071/13416 [4:28:37<1:11:00,  1[INFO|trainer.py:3948] 2025-02-24 07:11:57,656 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071[INFO|configuration_utils.py:423] 2025-02-24 07:11:57,658 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/config.json[INFO|configuration_utils.py:909] 2025-02-24 07:11:57,659 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 07:11:58,720 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 07:11:58,722 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 07:11:58,722 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 07:11:58,723 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/spiece.model[2025-02-24 07:11:58,742] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10068 is about to be saved![2025-02-24 07:11:58,749] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/mp_rank_00_model_states.pt[2025-02-24 07:11:58,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/mp_rank_00_model_states.pt...[2025-02-24 07:12:00,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/mp_rank_00_model_states.pt.[2025-02-24 07:12:00,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 07:12:01,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 07:12:01,094] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-10071/global_step10068/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 07:12:01,094] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10068 is ready now!{'loss': 0.01, 'grad_norm': 0.06702913343906403, 'learning_rate': 5e-05, 'epoch': 9.01}{'loss': 0.0105, 'grad_norm': 0.08469841629266739, 'learning_rate': 5e-05, 'epoch': 9.02}{'loss': 0.011, 'grad_norm': 0.037987563759088516, 'learning_rate': 5e-05, 'epoch': 9.03}{'loss': 0.0098, 'grad_norm': 0.042985036969184875, 'learning_rate': 5e-05, 'epoch': 9.03}{'loss': 0.0103, 'grad_norm': 0.050838664174079895, 'learning_rate': 5e-05, 'epoch': 9.04}{'loss': 0.0103, 'grad_norm': 0.08108794689178467, 'learning_rate': 5e-05, 'epoch': 9.05}{'loss': 0.0102, 'grad_norm': 0.04376551881432533, 'learning_rate': 5e-05, 'epoch': 9.06}{'loss': 0.011, 'grad_norm': 0.046986762434244156, 'learning_rate': 5e-05, 'epoch': 9.07}{'loss': 0.011, 'grad_norm': 0.07935816049575806, 'learning_rate': 5e-05, 'epoch': 9.08}{'loss': 0.0095, 'grad_norm': 0.03827700763940811, 'learning_rate': 5e-05, 'epoch': 9.09}{'loss': 0.0106, 'grad_norm': 0.05473005026578903, 'learning_rate': 5e-05, 'epoch': 9.1}{'loss': 0.0118, 'grad_norm': 0.06936666369438171, 'learning_rate': 5e-05, 'epoch': 9.11}{'loss': 0.0092, 'grad_norm': 0.07988383620977402, 'learning_rate': 5e-05, 'epoch': 9.12}{'loss': 0.0099, 'grad_norm': 0.0487217903137207, 'learning_rate': 5e-05, 'epoch': 9.12}{'loss': 0.011, 'grad_norm': 0.05484503135085106, 'learning_rate': 5e-05, 'epoch': 9.13}{'loss': 0.0118, 'grad_norm': 0.0504276417195797, 'learning_rate': 5e-05, 'epoch': 9.14}{'loss': 0.0099, 'grad_norm': 0.0532933846116066, 'learning_rate': 5e-05, 'epoch': 9.15}{'loss': 0.0089, 'grad_norm': 0.038646161556243896, 'learning_rate': 5e-05, 'epoch': 9.16}{'loss': 0.0103, 'grad_norm': 0.04433022812008858, 'learning_rate': 5e-05, 'epoch': 9.17}{'loss': 0.0101, 'grad_norm': 0.05060300976037979, 'learning_rate': 5e-05, 'epoch': 9.18}{'loss': 0.0113, 'grad_norm': 0.053601134568452835, 'learning_rate': 5e-05, 'epoch': 9.19}{'loss': 0.0105, 'grad_norm': 0.04087357223033905, 'learning_rate': 5e-05, 'epoch': 9.2}{'loss': 0.0102, 'grad_norm': 0.05386947840452194, 'learning_rate': 5e-05, 'epoch': 9.2}{'loss': 0.0114, 'grad_norm': 0.05865461006760597, 'learning_rate': 5e-05, 'epoch': 9.21}{'loss': 0.0109, 'grad_norm': 0.04221941903233528, 'learning_rate': 5e-05, 'epoch': 9.22}{'loss': 0.0105, 'grad_norm': 0.05096808075904846, 'learning_rate': 5e-05, 'epoch': 9.23}{'loss': 0.0109, 'grad_norm': 0.09066841006278992, 'learning_rate': 5e-05, 'epoch': 9.24}{'loss': 0.0112, 'grad_norm': 0.03934495523571968, 'learning_rate': 5e-05, 'epoch': 9.25}{'loss': 0.0099, 'grad_norm': 0.04593970999121666, 'learning_rate': 5e-05, 'epoch': 9.26}{'loss': 0.0112, 'grad_norm': 0.06287027150392532, 'learning_rate': 5e-05, 'epoch': 9.27}{'loss': 0.0112, 'grad_norm': 0.055771201848983765, 'learning_rate': 5e-05, 'epoch': 9.28}{'loss': 0.0115, 'grad_norm': 0.06064755842089653, 'learning_rate': 5e-05, 'epoch': 9.29}{'loss': 0.0117, 'grad_norm': 0.04019109532237053, 'learning_rate': 5e-05, 'epoch': 9.29}{'loss': 0.0105, 'grad_norm': 0.03964633867144585, 'learning_rate': 5e-05, 'epoch': 9.3}{'loss': 0.0094, 'grad_norm': 0.03668239712715149, 'learning_rate': 5e-05, 'epoch': 9.31}{'loss': 0.0106, 'grad_norm': 0.058774229139089584, 'learning_rate': 5e-05, 'epoch': 9.32}{'loss': 0.0108, 'grad_norm': 0.04209594801068306, 'learning_rate': 5e-05, 'epoch': 9.33}{'loss': 0.0108, 'grad_norm': 0.055268775671720505, 'learning_rate': 5e-05, 'epoch': 9.34}{'loss': 0.0096, 'grad_norm': 0.054526906460523605, 'learning_rate': 5e-05, 'epoch': 9.35}{'loss': 0.0108, 'grad_norm': 0.04721551015973091, 'learning_rate': 5e-05, 'epoch': 9.36}{'loss': 0.0112, 'grad_norm': 0.05886709690093994, 'learning_rate': 5e-05, 'epoch': 9.37}{'loss': 0.011, 'grad_norm': 0.04398653283715248, 'learning_rate': 5e-05, 'epoch': 9.37}{'loss': 0.0101, 'grad_norm': 0.05848555266857147, 'learning_rate': 5e-05, 'epoch': 9.38}{'loss': 0.0101, 'grad_norm': 0.06654107570648193, 'learning_rate': 5e-05, 'epoch': 9.39}{'loss': 0.0099, 'grad_norm': 0.04284747689962387, 'learning_rate': 5e-05, 'epoch': 9.4}{'loss': 0.0105, 'grad_norm': 0.06064795330166817, 'learning_rate': 5e-05, 'epoch': 9.41}{'loss': 0.0123, 'grad_norm': 0.08671627193689346, 'learning_rate': 5e-05, 'epoch': 9.42}{'loss': 0.0111, 'grad_norm': 0.0519912950694561, 'learning_rate': 5e-05, 'epoch': 9.43}{'loss': 0.0106, 'grad_norm': 0.04941064864397049, 'learning_rate': 5e-05, 'epoch': 9.44}{'loss': 0.0105, 'grad_norm': 0.1006961539387703, 'learning_rate': 5e-05, 'epoch': 9.45}{'loss': 0.0112, 'grad_norm': 0.043583985418081284, 'learning_rate': 5e-05, 'epoch': 9.45}{'loss': 0.0105, 'grad_norm': 0.03357970714569092, 'learning_rate': 5e-05, 'epoch': 9.46}{'loss': 0.0105, 'grad_norm': 0.03693069890141487, 'learning_rate': 5e-05, 'epoch': 9.47}{'loss': 0.0107, 'grad_norm': 0.08230528980493546, 'learning_rate': 5e-05, 'epoch': 9.48}{'loss': 0.0108, 'grad_norm': 0.0798557698726654, 'learning_rate': 5e-05, 'epoch': 9.49}{'loss': 0.0095, 'grad_norm': 0.09431149065494537, 'learning_rate': 5e-05, 'epoch': 9.5}{'loss': 0.0121, 'grad_norm': 0.04162086173892021, 'learning_rate': 5e-05, 'epoch': 9.51}{'loss': 0.011, 'grad_norm': 0.044231314212083817, 'learning_rate': 5e-05, 'epoch': 9.52}{'loss': 0.0103, 'grad_norm': 0.04454148933291435, 'learning_rate': 5e-05, 'epoch': 9.53}{'loss': 0.0105, 'grad_norm': 0.05263981223106384, 'learning_rate': 5e-05, 'epoch': 9.54}{'loss': 0.011, 'grad_norm': 0.07716790586709976, 'learning_rate': 5e-05, 'epoch': 9.54}{'loss': 0.0106, 'grad_norm': 0.05199461802840233, 'learning_rate': 5e-05, 'epoch': 9.55}{'loss': 0.0097, 'grad_norm': 0.04567494988441467, 'learning_rate': 5e-05, 'epoch': 9.56}{'loss': 0.0113, 'grad_norm': 0.08195937424898148, 'learning_rate': 5e-05, 'epoch': 9.57}{'loss': 0.0096, 'grad_norm': 0.03776835277676582, 'learning_rate': 5e-05, 'epoch': 9.58}{'loss': 0.0103, 'grad_norm': 0.04513868689537048, 'learning_rate': 5e-05, 'epoch': 9.59}{'loss': 0.0112, 'grad_norm': 0.08283619582653046, 'learning_rate': 5e-05, 'epoch': 9.6}{'loss': 0.0112, 'grad_norm': 0.10684533417224884, 'learning_rate': 5e-05, 'epoch': 9.61}{'loss': 0.0101, 'grad_norm': 0.0610501654446125, 'learning_rate': 5e-05, 'epoch': 9.62}{'loss': 0.0104, 'grad_norm': 0.050246287137269974, 'learning_rate': 5e-05, 'epoch': 9.62}{'loss': 0.0105, 'grad_norm': 0.050434838980436325, 'learning_rate': 5e-05, 'epoch': 9.63}{'loss': 0.0098, 'grad_norm': 0.03407534956932068, 'learning_rate': 5e-05, 'epoch': 9.64}{'loss': 0.0107, 'grad_norm': 0.04387139901518822, 'learning_rate': 5e-05, 'epoch': 9.65}{'loss': 0.0102, 'grad_norm': 0.03855683282017708, 'learning_rate': 5e-05, 'epoch': 9.66}{'loss': 0.0108, 'grad_norm': 0.04510730504989624, 'learning_rate': 5e-05, 'epoch': 9.67}{'loss': 0.0103, 'grad_norm': 0.0452432855963707, 'learning_rate': 5e-05, 'epoch': 9.68}{'loss': 0.0098, 'grad_norm': 0.04992888122797012, 'learning_rate': 5e-05, 'epoch': 9.69}{'loss': 0.0124, 'grad_norm': 0.06625550240278244, 'learning_rate': 5e-05, 'epoch': 9.7}{'loss': 0.0102, 'grad_norm': 0.06859876215457916, 'learning_rate': 5e-05, 'epoch': 9.71}{'loss': 0.0109, 'grad_norm': 0.039321351796388626, 'learning_rate': 5e-05, 'epoch': 9.71}{'loss': 0.01, 'grad_norm': 0.05081142485141754, 'learning_rate': 5e-05, 'epoch': 9.72}{'loss': 0.0109, 'grad_norm': 0.06057777628302574, 'learning_rate': 5e-05, 'epoch': 9.73}{'loss': 0.0101, 'grad_norm': 0.04855276271700859, 'learning_rate': 5e-05, 'epoch': 9.74}{'loss': 0.0101, 'grad_norm': 0.03928617015480995, 'learning_rate': 5e-05, 'epoch': 9.75}{'loss': 0.0105, 'grad_norm': 0.032803017646074295, 'learning_rate': 5e-05, 'epoch': 9.76}{'loss': 0.0105, 'grad_norm': 0.04958372190594673, 'learning_rate': 5e-05, 'epoch': 9.77}{'loss': 0.0114, 'grad_norm': 0.04681883752346039, 'learning_rate': 5e-05, 'epoch': 9.78}{'loss': 0.0104, 'grad_norm': 0.0715731680393219, 'learning_rate': 5e-05, 'epoch': 9.79}{'loss': 0.0103, 'grad_norm': 0.0600854717195034, 'learning_rate': 5e-05, 'epoch': 9.79}{'loss': 0.0107, 'grad_norm': 0.07406876236200333, 'learning_rate': 5e-05, 'epoch': 9.8}{'loss': 0.0104, 'grad_norm': 0.03446158766746521, 'learning_rate': 5e-05, 'epoch': 9.81}{'loss': 0.0106, 'grad_norm': 0.05425722524523735, 'learning_rate': 5e-05, 'epoch': 9.82}{'loss': 0.0102, 'grad_norm': 0.05491270497441292, 'learning_rate': 5e-05, 'epoch': 9.83}{'loss': 0.0118, 'grad_norm': 0.07070639729499817, 'learning_rate': 5e-05, 'epoch': 9.84}{'loss': 0.0107, 'grad_norm': 0.038097720593214035, 'learning_rate': 5e-05, 'epoch': 9.85}{'loss': 0.0098, 'grad_norm': 0.04694293811917305, 'learning_rate': 5e-05, 'epoch': 9.86}{'loss': 0.0101, 'grad_norm': 0.04057200625538826, 'learning_rate': 5e-05, 'epoch': 9.87}{'loss': 0.0105, 'grad_norm': 0.06172199174761772, 'learning_rate': 5e-05, 'epoch': 9.88}{'loss': 0.0107, 'grad_norm': 0.06551259756088257, 'learning_rate': 5e-05, 'epoch': 9.88}{'loss': 0.0101, 'grad_norm': 0.046209558844566345, 'learning_rate': 5e-05, 'epoch': 9.89}{'loss': 0.0095, 'grad_norm': 0.06441362202167511, 'learning_rate': 5e-05, 'epoch': 9.9}{'loss': 0.0106, 'grad_norm': 0.07334759831428528, 'learning_rate': 5e-05, 'epoch': 9.91}{'loss': 0.0107, 'grad_norm': 0.041211605072021484, 'learning_rate': 5e-05, 'epoch': 9.92}{'loss': 0.0109, 'grad_norm': 0.035195693373680115, 'learning_rate': 5e-05, 'epoch': 9.93}{'loss': 0.0094, 'grad_norm': 0.03822663798928261, 'learning_rate': 5e-05, 'epoch': 9.94}{'loss': 0.0105, 'grad_norm': 0.04396253079175949, 'learning_rate': 5e-05, 'epoch': 9.95}{'loss': 0.0096, 'grad_norm': 0.05159008130431175, 'learning_rate': 5e-05, 'epoch': 9.96}{'loss': 0.0113, 'grad_norm': 0.06655295938253403, 'learning_rate': 5e-05, 'epoch': 9.96}{'loss': 0.0115, 'grad_norm': 0.09984661638736725, 'learning_rate': 5e-05, 'epoch': 9.97}{'loss': 0.012, 'grad_norm': 0.055314257740974426, 'learning_rate': 5e-05, 'epoch': 9.98}{'loss': 0.0107, 'grad_norm': 0.04075523465871811, 'learning_rate': 5e-05, 'epoch': 9.99}{'loss': 0.0098, 'grad_norm': 0.05104866251349449, 'learning_rate': 5e-05, 'epoch': 10.0}{'eval_AnatEM': 0.6590909090396436, 'eval_bc2gm': 0.7545787545283152, 'eval_bc4chemd': 0.7268041236611262, 'eval_bc5cdr': 0.8437499999497816, 'eval_broad_twitter_corpus': 0.6601562499511527, 'eval_conllpp': 0.9845758354253253, 'eval_conll2003': 0.9820051413379224, 'eval_FabNER': 0.013973799090410632, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.6285714285207119, 'eval_mit-movie': 0.7425149700097121, 'eval_mit-restaurant': 0.8008998874638947, 'eval_MultiNERD': 0.9090909090405968, 'eval_ncbi': 0.9244186046006405, 'eval_Ontonotes': 0.9054820415377368, 'eval_TweetNER7': 0.643887623336418, 'eval_WikiANN-en': 0.8088235293614702, 'eval_WikiNeural': 0.9209039547519998, 'eval_average': 0.7572571488326677, 'eval_runtime': 403.7201, 'eval_samples_per_second': 8.917, 'eval_steps_per_second': 0.28, 'epoch': 10.0} 83%|████████▎ | 11190/13416 [4:58:20<41:34,  1.1[INFO|trainer.py:3948] 2025-02-24 07:41:39,817 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190[INFO|configuration_utils.py:423] 2025-02-24 07:41:39,819 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/config.json[INFO|configuration_utils.py:909] 2025-02-24 07:41:39,820 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 07:41:40,877 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 07:41:40,879 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 07:41:40,879 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 07:41:40,880 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/spiece.model[2025-02-24 07:41:40,901] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11187 is about to be saved![2025-02-24 07:41:40,908] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/mp_rank_00_model_states.pt[2025-02-24 07:41:40,908] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/mp_rank_00_model_states.pt...[2025-02-24 07:41:42,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/mp_rank_00_model_states.pt.[2025-02-24 07:41:42,159] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 07:41:43,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 07:41:43,258] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-11190/global_step11187/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 07:41:43,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11187 is ready now!{'loss': 0.01, 'grad_norm': 0.04965933412313461, 'learning_rate': 5e-05, 'epoch': 10.01}{'loss': 0.0103, 'grad_norm': 0.05327834188938141, 'learning_rate': 5e-05, 'epoch': 10.02}{'loss': 0.0099, 'grad_norm': 0.06675481051206589, 'learning_rate': 5e-05, 'epoch': 10.03}{'loss': 0.0098, 'grad_norm': 0.048461299389600754, 'learning_rate': 5e-05, 'epoch': 10.04}{'loss': 0.0099, 'grad_norm': 0.040169138461351395, 'learning_rate': 5e-05, 'epoch': 10.04}{'loss': 0.0096, 'grad_norm': 0.04373715817928314, 'learning_rate': 5e-05, 'epoch': 10.05}{'loss': 0.0098, 'grad_norm': 0.04390673711895943, 'learning_rate': 5e-05, 'epoch': 10.06}{'loss': 0.0114, 'grad_norm': 0.05264197662472725, 'learning_rate': 5e-05, 'epoch': 10.07}{'loss': 0.0108, 'grad_norm': 0.05714159458875656, 'learning_rate': 5e-05, 'epoch': 10.08}{'loss': 0.009, 'grad_norm': 0.04270476847887039, 'learning_rate': 5e-05, 'epoch': 10.09}{'loss': 0.0093, 'grad_norm': 0.027349766343832016, 'learning_rate': 5e-05, 'epoch': 10.1}{'loss': 0.0098, 'grad_norm': 0.04616083577275276, 'learning_rate': 5e-05, 'epoch': 10.11}{'loss': 0.0095, 'grad_norm': 0.04194048047065735, 'learning_rate': 5e-05, 'epoch': 10.12}{'loss': 0.0101, 'grad_norm': 0.0584256574511528, 'learning_rate': 5e-05, 'epoch': 10.13}{'loss': 0.0093, 'grad_norm': 0.04313267394900322, 'learning_rate': 5e-05, 'epoch': 10.13}{'loss': 0.0113, 'grad_norm': 0.038753654807806015, 'learning_rate': 5e-05, 'epoch': 10.14}{'loss': 0.0114, 'grad_norm': 0.03731808438897133, 'learning_rate': 5e-05, 'epoch': 10.15}{'loss': 0.0105, 'grad_norm': 0.03173785284161568, 'learning_rate': 5e-05, 'epoch': 10.16}{'loss': 0.0101, 'grad_norm': 0.0460832305252552, 'learning_rate': 5e-05, 'epoch': 10.17}{'loss': 0.0093, 'grad_norm': 0.058802228420972824, 'learning_rate': 5e-05, 'epoch': 10.18}{'loss': 0.0092, 'grad_norm': 0.03733490779995918, 'learning_rate': 5e-05, 'epoch': 10.19}{'loss': 0.0099, 'grad_norm': 0.03919476643204689, 'learning_rate': 5e-05, 'epoch': 10.2}{'loss': 0.0098, 'grad_norm': 0.04945804551243782, 'learning_rate': 5e-05, 'epoch': 10.21}{'loss': 0.0098, 'grad_norm': 0.05354943126440048, 'learning_rate': 5e-05, 'epoch': 10.21}{'loss': 0.0107, 'grad_norm': 0.04736382141709328, 'learning_rate': 5e-05, 'epoch': 10.22}{'loss': 0.01, 'grad_norm': 0.030929120257496834, 'learning_rate': 5e-05, 'epoch': 10.23}{'loss': 0.0102, 'grad_norm': 0.04524258151650429, 'learning_rate': 5e-05, 'epoch': 10.24}{'loss': 0.0118, 'grad_norm': 0.03427139297127724, 'learning_rate': 5e-05, 'epoch': 10.25}{'loss': 0.0107, 'grad_norm': 0.049467507749795914, 'learning_rate': 5e-05, 'epoch': 10.26}{'loss': 0.0103, 'grad_norm': 0.10961565375328064, 'learning_rate': 5e-05, 'epoch': 10.27}{'loss': 0.0099, 'grad_norm': 0.0769292488694191, 'learning_rate': 5e-05, 'epoch': 10.28}{'loss': 0.0101, 'grad_norm': 0.0402216836810112, 'learning_rate': 5e-05, 'epoch': 10.29}{'loss': 0.0107, 'grad_norm': 0.05623471364378929, 'learning_rate': 5e-05, 'epoch': 10.29}{'loss': 0.0107, 'grad_norm': 0.04526073858141899, 'learning_rate': 5e-05, 'epoch': 10.3}{'loss': 0.0101, 'grad_norm': 0.05643528327345848, 'learning_rate': 5e-05, 'epoch': 10.31}{'loss': 0.0103, 'grad_norm': 0.03985654562711716, 'learning_rate': 5e-05, 'epoch': 10.32}{'loss': 0.0107, 'grad_norm': 0.07055838406085968, 'learning_rate': 5e-05, 'epoch': 10.33}{'loss': 0.0107, 'grad_norm': 0.05358139052987099, 'learning_rate': 5e-05, 'epoch': 10.34}{'loss': 0.0106, 'grad_norm': 0.0939294621348381, 'learning_rate': 5e-05, 'epoch': 10.35}{'loss': 0.0108, 'grad_norm': 0.04080308973789215, 'learning_rate': 5e-05, 'epoch': 10.36}{'loss': 0.0102, 'grad_norm': 0.0495779849588871, 'learning_rate': 5e-05, 'epoch': 10.37}{'loss': 0.01, 'grad_norm': 0.05621039867401123, 'learning_rate': 5e-05, 'epoch': 10.38}{'loss': 0.0094, 'grad_norm': 0.04392796382308006, 'learning_rate': 5e-05, 'epoch': 10.38}{'loss': 0.0087, 'grad_norm': 0.16467958688735962, 'learning_rate': 5e-05, 'epoch': 10.39}{'loss': 0.0101, 'grad_norm': 0.057046789675951004, 'learning_rate': 5e-05, 'epoch': 10.4}{'loss': 0.0094, 'grad_norm': 0.03983137756586075, 'learning_rate': 5e-05, 'epoch': 10.41}{'loss': 0.0105, 'grad_norm': 0.04460163041949272, 'learning_rate': 5e-05, 'epoch': 10.42}{'loss': 0.0103, 'grad_norm': 0.04637780413031578, 'learning_rate': 5e-05, 'epoch': 10.43}{'loss': 0.0117, 'grad_norm': 0.03191172704100609, 'learning_rate': 5e-05, 'epoch': 10.44}{'loss': 0.01, 'grad_norm': 0.040805965662002563, 'learning_rate': 5e-05, 'epoch': 10.45}{'loss': 0.0095, 'grad_norm': 0.046934954822063446, 'learning_rate': 5e-05, 'epoch': 10.46}{'loss': 0.0097, 'grad_norm': 0.061480190604925156, 'learning_rate': 5e-05, 'epoch': 10.46}{'loss': 0.0098, 'grad_norm': 0.054110847413539886, 'learning_rate': 5e-05, 'epoch': 10.47}{'loss': 0.0101, 'grad_norm': 0.04441452771425247, 'learning_rate': 5e-05, 'epoch': 10.48}{'loss': 0.0108, 'grad_norm': 0.06126147508621216, 'learning_rate': 5e-05, 'epoch': 10.49}{'loss': 0.0096, 'grad_norm': 0.037403274327516556, 'learning_rate': 5e-05, 'epoch': 10.5}{'loss': 0.01, 'grad_norm': 0.06256019324064255, 'learning_rate': 5e-05, 'epoch': 10.51}{'loss': 0.0109, 'grad_norm': 0.036675792187452316, 'learning_rate': 5e-05, 'epoch': 10.52}{'loss': 0.0101, 'grad_norm': 0.060831956565380096, 'learning_rate': 5e-05, 'epoch': 10.53}{'loss': 0.0096, 'grad_norm': 0.037633806467056274, 'learning_rate': 5e-05, 'epoch': 10.54}{'loss': 0.0095, 'grad_norm': 0.03839433938264847, 'learning_rate': 5e-05, 'epoch': 10.55}{'loss': 0.0099, 'grad_norm': 0.05714126303792, 'learning_rate': 5e-05, 'epoch': 10.55}{'loss': 0.0094, 'grad_norm': 0.051119811832904816, 'learning_rate': 5e-05, 'epoch': 10.56}{'loss': 0.0097, 'grad_norm': 0.05964180827140808, 'learning_rate': 5e-05, 'epoch': 10.57}{'loss': 0.0092, 'grad_norm': 0.06525088846683502, 'learning_rate': 5e-05, 'epoch': 10.58}{'loss': 0.0098, 'grad_norm': 0.04657416045665741, 'learning_rate': 5e-05, 'epoch': 10.59}{'loss': 0.0092, 'grad_norm': 0.03752671554684639, 'learning_rate': 5e-05, 'epoch': 10.6}{'loss': 0.0103, 'grad_norm': 0.03553701564669609, 'learning_rate': 5e-05, 'epoch': 10.61}{'loss': 0.0101, 'grad_norm': 0.03338171914219856, 'learning_rate': 5e-05, 'epoch': 10.62}{'loss': 0.0092, 'grad_norm': 0.03945200890302658, 'learning_rate': 5e-05, 'epoch': 10.63}{'loss': 0.0108, 'grad_norm': 0.045081451535224915, 'learning_rate': 5e-05, 'epoch': 10.63}{'loss': 0.0105, 'grad_norm': 0.043782301247119904, 'learning_rate': 5e-05, 'epoch': 10.64}{'loss': 0.0095, 'grad_norm': 0.0515778511762619, 'learning_rate': 5e-05, 'epoch': 10.65}{'loss': 0.0092, 'grad_norm': 0.03786514326930046, 'learning_rate': 5e-05, 'epoch': 10.66}{'loss': 0.0097, 'grad_norm': 0.13108758628368378, 'learning_rate': 5e-05, 'epoch': 10.67}{'loss': 0.012, 'grad_norm': 0.07706952840089798, 'learning_rate': 5e-05, 'epoch': 10.68}{'loss': 0.01, 'grad_norm': 0.0644155740737915, 'learning_rate': 5e-05, 'epoch': 10.69}{'loss': 0.01, 'grad_norm': 0.04778064042329788, 'learning_rate': 5e-05, 'epoch': 10.7}{'loss': 0.009, 'grad_norm': 0.04902468994259834, 'learning_rate': 5e-05, 'epoch': 10.71}{'loss': 0.0097, 'grad_norm': 0.05353515222668648, 'learning_rate': 5e-05, 'epoch': 10.72}{'loss': 0.0102, 'grad_norm': 0.054199326783418655, 'learning_rate': 5e-05, 'epoch': 10.72}{'loss': 0.0093, 'grad_norm': 0.04237053170800209, 'learning_rate': 5e-05, 'epoch': 10.73}{'loss': 0.0098, 'grad_norm': 0.05430413410067558, 'learning_rate': 5e-05, 'epoch': 10.74}{'loss': 0.0098, 'grad_norm': 0.068397156894207, 'learning_rate': 5e-05, 'epoch': 10.75}{'loss': 0.0104, 'grad_norm': 0.08185914903879166, 'learning_rate': 5e-05, 'epoch': 10.76}{'loss': 0.0102, 'grad_norm': 0.05240537226200104, 'learning_rate': 5e-05, 'epoch': 10.77}{'loss': 0.0095, 'grad_norm': 0.051665209233760834, 'learning_rate': 5e-05, 'epoch': 10.78}{'loss': 0.0106, 'grad_norm': 0.03453824296593666, 'learning_rate': 5e-05, 'epoch': 10.79}{'loss': 0.0105, 'grad_norm': 0.07053407281637192, 'learning_rate': 5e-05, 'epoch': 10.8}{'loss': 0.0097, 'grad_norm': 0.04642994701862335, 'learning_rate': 5e-05, 'epoch': 10.8}{'loss': 0.0096, 'grad_norm': 0.10116076469421387, 'learning_rate': 5e-05, 'epoch': 10.81}{'loss': 0.0106, 'grad_norm': 0.04869517683982849, 'learning_rate': 5e-05, 'epoch': 10.82}{'loss': 0.0099, 'grad_norm': 0.05368679389357567, 'learning_rate': 5e-05, 'epoch': 10.83}{'loss': 0.01, 'grad_norm': 0.03819344937801361, 'learning_rate': 5e-05, 'epoch': 10.84}{'loss': 0.0103, 'grad_norm': 0.058271341025829315, 'learning_rate': 5e-05, 'epoch': 10.85}{'loss': 0.0102, 'grad_norm': 0.03995318338274956, 'learning_rate': 5e-05, 'epoch': 10.86}{'loss': 0.0099, 'grad_norm': 0.04113134369254112, 'learning_rate': 5e-05, 'epoch': 10.87}{'loss': 0.0098, 'grad_norm': 0.055806517601013184, 'learning_rate': 5e-05, 'epoch': 10.88}{'loss': 0.0106, 'grad_norm': 0.04162692278623581, 'learning_rate': 5e-05, 'epoch': 10.88}{'loss': 0.01, 'grad_norm': 0.07464383542537689, 'learning_rate': 5e-05, 'epoch': 10.89}{'loss': 0.0097, 'grad_norm': 0.04758956655859947, 'learning_rate': 5e-05, 'epoch': 10.9}{'loss': 0.0098, 'grad_norm': 0.03914184868335724, 'learning_rate': 5e-05, 'epoch': 10.91}{'loss': 0.0088, 'grad_norm': 0.04406445100903511, 'learning_rate': 5e-05, 'epoch': 10.92}{'loss': 0.0096, 'grad_norm': 0.03454060107469559, 'learning_rate': 5e-05, 'epoch': 10.93}{'loss': 0.0096, 'grad_norm': 0.04515894874930382, 'learning_rate': 5e-05, 'epoch': 10.94}{'loss': 0.01, 'grad_norm': 0.046451639384031296, 'learning_rate': 5e-05, 'epoch': 10.95}{'loss': 0.0092, 'grad_norm': 0.05430113524198532, 'learning_rate': 5e-05, 'epoch': 10.96}{'loss': 0.0078, 'grad_norm': 0.07103299349546432, 'learning_rate': 5e-05, 'epoch': 10.97}{'loss': 0.0103, 'grad_norm': 0.044980816543102264, 'learning_rate': 5e-05, 'epoch': 10.97}{'loss': 0.0094, 'grad_norm': 0.05137035250663757, 'learning_rate': 5e-05, 'epoch': 10.98}{'loss': 0.0089, 'grad_norm': 0.04876958206295967, 'learning_rate': 5e-05, 'epoch': 10.99}{'eval_AnatEM': 0.6304347825579159, 'eval_bc2gm': 0.7456445992530588, 'eval_bc4chemd': 0.7692307691807442, 'eval_bc5cdr': 0.8515624999497796, 'eval_broad_twitter_corpus': 0.6972477063722354, 'eval_conllpp': 0.9794871794369288, 'eval_conll2003': 0.9794871794369288, 'eval_FabNER': 0.015517241342067629, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.6666666666159815, 'eval_mit-movie': 0.7511045654873472, 'eval_mit-restaurant': 0.8044692736928646, 'eval_MultiNERD': 0.9014084506539085, 'eval_ncbi': 0.9271137025733768, 'eval_Ontonotes': 0.902738432433312, 'eval_TweetNER7': 0.6234522941960994, 'eval_WikiANN-en': 0.8175182481248842, 'eval_WikiNeural': 0.9196050775237887, 'eval_average': 0.7613216436784657, 'eval_runtime': 402.5961, 'eval_samples_per_second': 8.942, 'eval_steps_per_second': 0.281, 'epoch': 11.0} 92%|█████████▏| 12309/13416 [5:28:00<21:49,  1.1[INFO|trainer.py:3948] 2025-02-24 08:11:20,071 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309[INFO|configuration_utils.py:423] 2025-02-24 08:11:20,073 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/config.json[INFO|configuration_utils.py:909] 2025-02-24 08:11:20,074 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 08:11:21,132 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 08:11:21,134 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 08:11:21,134 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 08:11:21,135 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/spiece.model[2025-02-24 08:11:21,158] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12306 is about to be saved![2025-02-24 08:11:21,165] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt[2025-02-24 08:11:21,165] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt...[2025-02-24 08:11:22,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt.[2025-02-24 08:11:22,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 08:11:23,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 08:11:23,521] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 08:11:23,521] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12306 is ready now!{'loss': 0.0095, 'grad_norm': 0.044493455439805984, 'learning_rate': 5e-05, 'epoch': 11.0}{'loss': 0.0101, 'grad_norm': 0.041503600776195526, 'learning_rate': 5e-05, 'epoch': 11.01}{'loss': 0.01, 'grad_norm': 0.04494136944413185, 'learning_rate': 5e-05, 'epoch': 11.02}{'loss': 0.0096, 'grad_norm': 0.0537383034825325, 'learning_rate': 5e-05, 'epoch': 11.03}{'loss': 0.0106, 'grad_norm': 0.04908817633986473, 'learning_rate': 5e-05, 'epoch': 11.04}{'loss': 0.0098, 'grad_norm': 0.05949468910694122, 'learning_rate': 5e-05, 'epoch': 11.05}{'loss': 0.0089, 'grad_norm': 0.06869401782751083, 'learning_rate': 5e-05, 'epoch': 11.05}{'loss': 0.0096, 'grad_norm': 0.04127134755253792, 'learning_rate': 5e-05, 'epoch': 11.06}{'loss': 0.0092, 'grad_norm': 0.09178542345762253, 'learning_rate': 5e-05, 'epoch': 11.07}{'loss': 0.0095, 'grad_norm': 0.050045955926179886, 'learning_rate': 5e-05, 'epoch': 11.08}{'loss': 0.009, 'grad_norm': 0.03602694347500801, 'learning_rate': 5e-05, 'epoch': 11.09}{'loss': 0.0093, 'grad_norm': 0.03149737790226936, 'learning_rate': 5e-05, 'epoch': 11.1}{'loss': 0.0088, 'grad_norm': 0.03631337732076645, 'learning_rate': 5e-05, 'epoch': 11.11}{'loss': 0.0097, 'grad_norm': 0.07286515086889267, 'learning_rate': 5e-05, 'epoch': 11.12}{'loss': 0.0102, 'grad_norm': 0.060304079204797745, 'learning_rate': 5e-05, 'epoch': 11.13}{'loss': 0.0094, 'grad_norm': 0.04957444965839386, 'learning_rate': 5e-05, 'epoch': 11.13}{'loss': 0.0091, 'grad_norm': 0.12534011900424957, 'learning_rate': 5e-05, 'epoch': 11.14}{'loss': 0.0099, 'grad_norm': 0.05810300260782242, 'learning_rate': 5e-05, 'epoch': 11.15}{'loss': 0.0082, 'grad_norm': 0.04308425262570381, 'learning_rate': 5e-05, 'epoch': 11.16}{'loss': 0.0088, 'grad_norm': 0.04043905809521675, 'learning_rate': 5e-05, 'epoch': 11.17}{'loss': 0.0096, 'grad_norm': 0.09977506846189499, 'learning_rate': 5e-05, 'epoch': 11.18}{'loss': 0.0087, 'grad_norm': 0.0468580387532711, 'learning_rate': 5e-05, 'epoch': 11.19}{'loss': 0.0088, 'grad_norm': 0.04023228585720062, 'learning_rate': 5e-05, 'epoch': 11.2}{'loss': 0.0104, 'grad_norm': 0.06095028668642044, 'learning_rate': 5e-05, 'epoch': 11.21}{'loss': 0.01, 'grad_norm': 0.07082413882017136, 'learning_rate': 5e-05, 'epoch': 11.22}{'loss': 0.0092, 'grad_norm': 0.044295307248830795, 'learning_rate': 5e-05, 'epoch': 11.22}{'loss': 0.0079, 'grad_norm': 0.050178322941064835, 'learning_rate': 5e-05, 'epoch': 11.23}{'loss': 0.0094, 'grad_norm': 0.04261380806565285, 'learning_rate': 5e-05, 'epoch': 11.24}{'loss': 0.0085, 'grad_norm': 0.05253119394183159, 'learning_rate': 5e-05, 'epoch': 11.25}{'loss': 0.0097, 'grad_norm': 0.054809216409921646, 'learning_rate': 5e-05, 'epoch': 11.26}{'loss': 0.0091, 'grad_norm': 0.05545812472701073, 'learning_rate': 5e-05, 'epoch': 11.27}{'loss': 0.0087, 'grad_norm': 0.03819510340690613, 'learning_rate': 5e-05, 'epoch': 11.28}{'loss': 0.0096, 'grad_norm': 0.039072804152965546, 'learning_rate': 5e-05, 'epoch': 11.29}{'loss': 0.0093, 'grad_norm': 0.04285901039838791, 'learning_rate': 5e-05, 'epoch': 11.3}{'loss': 0.0093, 'grad_norm': 0.0378011129796505, 'learning_rate': 5e-05, 'epoch': 11.3}{'loss': 0.0096, 'grad_norm': 0.08588173985481262, 'learning_rate': 5e-05, 'epoch': 11.31}{'loss': 0.0095, 'grad_norm': 0.04237968474626541, 'learning_rate': 5e-05, 'epoch': 11.32}{'loss': 0.0085, 'grad_norm': 0.04021594673395157, 'learning_rate': 5e-05, 'epoch': 11.33}{'loss': 0.0094, 'grad_norm': 0.04576186090707779, 'learning_rate': 5e-05, 'epoch': 11.34}{'loss': 0.009, 'grad_norm': 0.049619462341070175, 'learning_rate': 5e-05, 'epoch': 11.35}{'loss': 0.0092, 'grad_norm': 0.05232465639710426, 'learning_rate': 5e-05, 'epoch': 11.36}{'loss': 0.0091, 'grad_norm': 0.08222241699695587, 'learning_rate': 5e-05, 'epoch': 11.37}{'loss': 0.01, 'grad_norm': 0.07301127165555954, 'learning_rate': 5e-05, 'epoch': 11.38}{'loss': 0.0098, 'grad_norm': 0.05036957189440727, 'learning_rate': 5e-05, 'epoch': 11.39}{'loss': 0.0088, 'grad_norm': 0.03727877140045166, 'learning_rate': 5e-05, 'epoch': 11.39}{'loss': 0.0089, 'grad_norm': 0.06350552290678024, 'learning_rate': 5e-05, 'epoch': 11.4}{'loss': 0.0097, 'grad_norm': 0.03260790556669235, 'learning_rate': 5e-05, 'epoch': 11.41}{'loss': 0.0096, 'grad_norm': 0.055711548775434494, 'learning_rate': 5e-05, 'epoch': 11.42}{'loss': 0.0096, 'grad_norm': 0.04231850802898407, 'learning_rate': 5e-05, 'epoch': 11.43}{'loss': 0.0095, 'grad_norm': 0.052106406539678574, 'learning_rate': 5e-05, 'epoch': 11.44}{'loss': 0.0094, 'grad_norm': 0.042752545326948166, 'learning_rate': 5e-05, 'epoch': 11.45}{'loss': 0.0101, 'grad_norm': 0.038819894194602966, 'learning_rate': 5e-05, 'epoch': 11.46}{'loss': 0.0097, 'grad_norm': 0.05250398814678192, 'learning_rate': 5e-05, 'epoch': 11.47}{'loss': 0.0092, 'grad_norm': 0.0376921147108078, 'learning_rate': 5e-05, 'epoch': 11.47}{'loss': 0.009, 'grad_norm': 0.0347149632871151, 'learning_rate': 5e-05, 'epoch': 11.48}{'loss': 0.0095, 'grad_norm': 0.03476336970925331, 'learning_rate': 5e-05, 'epoch': 11.49}{'loss': 0.0093, 'grad_norm': 0.052027780562639236, 'learning_rate': 5e-05, 'epoch': 11.5}{'loss': 0.0093, 'grad_norm': 0.04211127385497093, 'learning_rate': 5e-05, 'epoch': 11.51}{'loss': 0.0102, 'grad_norm': 0.0920143648982048, 'learning_rate': 5e-05, 'epoch': 11.52}{'loss': 0.0101, 'grad_norm': 0.10431535542011261, 'learning_rate': 5e-05, 'epoch': 11.53}{'loss': 0.0092, 'grad_norm': 0.05049949511885643, 'learning_rate': 5e-05, 'epoch': 11.54}{'loss': 0.009, 'grad_norm': 0.031338389962911606, 'learning_rate': 5e-05, 'epoch': 11.55}{'loss': 0.0088, 'grad_norm': 0.059908606112003326, 'learning_rate': 5e-05, 'epoch': 11.56}{'loss': 0.009, 'grad_norm': 0.03841109573841095, 'learning_rate': 5e-05, 'epoch': 11.56}{'loss': 0.0086, 'grad_norm': 0.043276745826005936, 'learning_rate': 5e-05, 'epoch': 11.57}{'loss': 0.0094, 'grad_norm': 0.043219421058893204, 'learning_rate': 5e-05, 'epoch': 11.58}{'loss': 0.0093, 'grad_norm': 0.024913569912314415, 'learning_rate': 5e-05, 'epoch': 11.59}{'loss': 0.01, 'grad_norm': 0.04583018273115158, 'learning_rate': 5e-05, 'epoch': 11.6}{'loss': 0.0104, 'grad_norm': 0.12507128715515137, 'learning_rate': 5e-05, 'epoch': 11.61}{'loss': 0.0086, 'grad_norm': 0.042381636798381805, 'learning_rate': 5e-05, 'epoch': 11.62}{'loss': 0.0098, 'grad_norm': 0.06315802037715912, 'learning_rate': 5e-05, 'epoch': 11.63}{'loss': 0.0099, 'grad_norm': 0.05911143869161606, 'learning_rate': 5e-05, 'epoch': 11.64}{'loss': 0.0094, 'grad_norm': 0.04620740935206413, 'learning_rate': 5e-05, 'epoch': 11.64}{'loss': 0.0089, 'grad_norm': 0.05104711279273033, 'learning_rate': 5e-05, 'epoch': 11.65}{'loss': 0.0093, 'grad_norm': 0.060264069586992264, 'learning_rate': 5e-05, 'epoch': 11.66}{'loss': 0.0106, 'grad_norm': 0.04830444976687431, 'learning_rate': 5e-05, 'epoch': 11.67}{'loss': 0.0093, 'grad_norm': 0.03533978760242462, 'learning_rate': 5e-05, 'epoch': 11.68}{'loss': 0.0099, 'grad_norm': 0.03576960042119026, 'learning_rate': 5e-05, 'epoch': 11.69}{'loss': 0.0087, 'grad_norm': 0.03774126619100571, 'learning_rate': 5e-05, 'epoch': 11.7}{'loss': 0.0099, 'grad_norm': 0.04041971638798714, 'learning_rate': 5e-05, 'epoch': 11.71}{'loss': 0.0085, 'grad_norm': 0.04619597643613815, 'learning_rate': 5e-05, 'epoch': 11.72}{'loss': 0.0099, 'grad_norm': 0.05256134271621704, 'learning_rate': 5e-05, 'epoch': 11.72}{'loss': 0.0097, 'grad_norm': 0.04013059660792351, 'learning_rate': 5e-05, 'epoch': 11.73}{'loss': 0.0092, 'grad_norm': 0.0426117442548275, 'learning_rate': 5e-05, 'epoch': 11.74}{'loss': 0.0088, 'grad_norm': 0.04619374871253967, 'learning_rate': 5e-05, 'epoch': 11.75}{'loss': 0.0095, 'grad_norm': 0.04006064310669899, 'learning_rate': 5e-05, 'epoch': 11.76}{'loss': 0.0093, 'grad_norm': 0.03824731707572937, 'learning_rate': 5e-05, 'epoch': 11.77}{'loss': 0.0107, 'grad_norm': 0.05571911111474037, 'learning_rate': 5e-05, 'epoch': 11.78}{'loss': 0.0095, 'grad_norm': 0.04199110344052315, 'learning_rate': 5e-05, 'epoch': 11.79}{'loss': 0.0091, 'grad_norm': 0.03718411922454834, 'learning_rate': 5e-05, 'epoch': 11.8}{'loss': 0.0095, 'grad_norm': 0.05602681264281273, 'learning_rate': 5e-05, 'epoch': 11.81}{'loss': 0.0089, 'grad_norm': 0.03473935276269913, 'learning_rate': 5e-05, 'epoch': 11.81}{'loss': 0.0087, 'grad_norm': 0.03732480853796005, 'learning_rate': 5e-05, 'epoch': 11.82}{'loss': 0.0092, 'grad_norm': 0.0587119497358799, 'learning_rate': 5e-05, 'epoch': 11.83}{'loss': 0.0094, 'grad_norm': 0.0541885681450367, 'learning_rate': 5e-05, 'epoch': 11.84}{'loss': 0.0099, 'grad_norm': 0.07128863036632538, 'learning_rate': 5e-05, 'epoch': 11.85}{'loss': 0.0098, 'grad_norm': 0.051500268280506134, 'learning_rate': 5e-05, 'epoch': 11.86}{'loss': 0.0089, 'grad_norm': 0.061990104615688324, 'learning_rate': 5e-05, 'epoch': 11.87}{'loss': 0.0083, 'grad_norm': 0.047407060861587524, 'learning_rate': 5e-05, 'epoch': 11.88}{'loss': 0.0107, 'grad_norm': 0.04138926416635513, 'learning_rate': 5e-05, 'epoch': 11.89}{'loss': 0.009, 'grad_norm': 0.056880321353673935, 'learning_rate': 5e-05, 'epoch': 11.89}{'loss': 0.0101, 'grad_norm': 0.04596791788935661, 'learning_rate': 5e-05, 'epoch': 11.9}{'loss': 0.0092, 'grad_norm': 0.03800718113780022, 'learning_rate': 5e-05, 'epoch': 11.91}{'loss': 0.0095, 'grad_norm': 0.04111894965171814, 'learning_rate': 5e-05, 'epoch': 11.92}{'loss': 0.0094, 'grad_norm': 0.04247434809803963, 'learning_rate': 5e-05, 'epoch': 11.93}{'loss': 0.0099, 'grad_norm': 0.041438303887844086, 'learning_rate': 5e-05, 'epoch': 11.94}{'loss': 0.0095, 'grad_norm': 0.04770268127322197, 'learning_rate': 5e-05, 'epoch': 11.95}{'loss': 0.0104, 'grad_norm': 0.039019111543893814, 'learning_rate': 5e-05, 'epoch': 11.96}{'loss': 0.0091, 'grad_norm': 0.0414949506521225, 'learning_rate': 5e-05, 'epoch': 11.97}{'loss': 0.0105, 'grad_norm': 0.039161935448646545, 'learning_rate': 5e-05, 'epoch': 11.98}{'loss': 0.009, 'grad_norm': 0.04769616201519966, 'learning_rate': 5e-05, 'epoch': 11.98}{'eval_AnatEM': 0.6823529411249273, 'eval_bc2gm': 0.7490636703614585, 'eval_bc4chemd': 0.7636363635861779, 'eval_bc5cdr': 0.844155844105628, 'eval_broad_twitter_corpus': 0.652751423100565, 'eval_conllpp': 0.9679075737623318, 'eval_conll2003': 0.9679075737623318, 'eval_FabNER': 0.013998250182636136, 'eval_FindVehicle': 0.7211009173811594, 'eval_HarveyNER': 0.6629834253637068, 'eval_mit-movie': 0.7481371087426636, 'eval_mit-restaurant': 0.8081725311643486, 'eval_MultiNERD': 0.8896672503875206, 'eval_ncbi': 0.910144927485725, 'eval_Ontonotes': 0.904627006559846, 'eval_TweetNER7': 0.6579352850039079, 'eval_WikiANN-en': 0.8263254112842502, 'eval_WikiNeural': 0.9167842030527035, 'eval_average': 0.7604250948006603, 'eval_runtime': 403.4746, 'eval_samples_per_second': 8.922, 'eval_steps_per_second': 0.28, 'epoch': 11.99}100%|██████████| 13416/13416 [5:57:27<00:00,  1.2[INFO|trainer.py:3948] 2025-02-24 08:40:47,072 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416[INFO|configuration_utils.py:423] 2025-02-24 08:40:47,074 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/config.json[INFO|configuration_utils.py:909] 2025-02-24 08:40:47,075 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 08:40:48,155 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 08:40:48,157 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 08:40:48,157 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 08:40:48,158 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/spiece.model[2025-02-24 08:40:48,195] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13413 is about to be saved![2025-02-24 08:40:48,207] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/mp_rank_00_model_states.pt[2025-02-24 08:40:48,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/mp_rank_00_model_states.pt...[2025-02-24 08:40:49,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/mp_rank_00_model_states.pt.[2025-02-24 08:40:49,451] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 08:40:50,508] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 08:40:50,509] [INFO] [engine.py:3572:_save_zero_checkpoint] zero checkpoint saved output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-13416/global_step13413/zero_pp_rank_0_mp_rank_00_optim_states.pt[2025-02-24 08:40:50,509] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13413 is ready now![INFO|trainer.py:2663] 2025-02-24 08:40:50,557 >>Training completed. Do not forget to share your model on huggingface.co/models =)[INFO|trainer.py:2901] 2025-02-24 08:40:50,558 >> Loading best model from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309 (score: 0.7613216436784657).[INFO|deepspeed.py:436] 2025-02-24 08:40:50,558 >> Attempting to resume from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309[2025-02-24 08:40:50,558] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt...[2025-02-24 08:40:50,973] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt.[2025-02-24 08:40:51,013] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt...[2025-02-24 08:40:51,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/mp_rank_00_model_states.pt.[2025-02-24 08:40:51,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/zero_pp_rank_0_mp_rank_00_optim_states.pt...[2025-02-24 08:40:51,983] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from output-lfs/SFT-yuyang-BL-dgx-a100/checkpoint-12309/global_step12306/zero_pp_rank_0_mp_rank_00_optim_states.pt.[2025-02-24 08:40:51,983] [INFO] [engine.py:3112:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0[2025-02-24 08:40:52,048] [INFO] [engine.py:3062:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0{'train_runtime': 21452.7807, 'train_samples_per_second': 80.09, 'train_steps_per_second': 0.625, 'train_loss': 0.018949945644988107, 'epoch': 11.99}100%|██████████| 13416/13416 [5:57:32<00:00,  1.60s/it][INFO|trainer.py:3948] 2025-02-24 08:40:52,521 >> Saving model checkpoint to output-lfs/SFT-yuyang-BL-dgx-a100[INFO|configuration_utils.py:423] 2025-02-24 08:40:52,523 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/config.json[INFO|configuration_utils.py:909] 2025-02-24 08:40:52,523 >> Configuration saved in output-lfs/SFT-yuyang-BL-dgx-a100/generation_config.json[INFO|modeling_utils.py:3077] 2025-02-24 08:40:53,609 >> Model weights saved in output-lfs/SFT-yuyang-BL-dgx-a100/model.safetensors[INFO|tokenization_utils_base.py:2500] 2025-02-24 08:40:53,610 >> tokenizer config file saved in output-lfs/SFT-yuyang-BL-dgx-a100/tokenizer_config.json[INFO|tokenization_utils_base.py:2509] 2025-02-24 08:40:53,610 >> Special tokens file saved in output-lfs/SFT-yuyang-BL-dgx-a100/special_tokens_map.json[INFO|tokenization_t5_fast.py:176] 2025-02-24 08:40:53,611 >> Copy vocab file to output-lfs/SFT-yuyang-BL-dgx-a100/spiece.model***** train metrics *****  epoch                    =     11.9895  total_flos               = 583984129GF  train_loss               =      0.0189  train_runtime            =  5:57:32.78  train_samples            =      143180  train_samples_per_second =       80.09  train_steps_per_second   =       0.62502/24/2025 08:40:53 - INFO - __main__ - Metrics {'train_runtime': 21452.7807, 'train_samples_per_second': 80.09, 'train_steps_per_second': 0.625, 'total_flos': 6.270481843208847e+17, 'train_loss': 0.018949945644988107, 'epoch': 11.989497206703911, 'train_samples': 143180}02/24/2025 08:40:53 - INFO - __main__ - *** Evaluate ***100%|██████████| 113/113 [06:35<00:00,  3.50s/it]***** eval metrics *****  epoch                     =    11.9895  eval_AnatEM               =     0.6304  eval_FabNER               =     0.0155  eval_FindVehicle          =     0.7211  eval_HarveyNER            =     0.6667  eval_MultiNERD            =     0.9014  eval_Ontonotes            =     0.9027  eval_TweetNER7            =     0.6235  eval_WikiANN-en           =     0.8175  eval_WikiNeural           =     0.9196  eval_average              =     0.7613  eval_bc2gm                =     0.7456  eval_bc4chemd             =     0.7692  eval_bc5cdr               =     0.8516  eval_broad_twitter_corpus =     0.6972  eval_conll2003            =     0.9795  eval_conllpp              =     0.9795  eval_mit-movie            =     0.7511  eval_mit-restaurant       =     0.8045  eval_ncbi                 =     0.9271  eval_runtime              = 0:06:41.27  eval_samples              =       3600  eval_samples_per_second   =      8.971  eval_steps_per_second     =      0.28202/24/2025 08:47:34 - INFO - __main__ - *** Predict ***100%|██████████| 3902/3902 [4:22:16<00:00,  4.03s/it]***** predict metrics *****  predict_AnatEM               =     0.7768  predict_FabNER               =     0.0189  predict_FindVehicle          =     0.5168  predict_HarveyNER            =     0.6573  predict_MultiNERD            =     0.9253  predict_Ontonotes            =     0.8629  predict_TweetNER7            =     0.6551  predict_WikiANN-en           =     0.8213  predict_WikiNeural           =     0.9018  predict_average              =     0.7637  predict_bc2gm                =     0.7799  predict_bc4chemd             =      0.748  predict_bc5cdr               =     0.8589  predict_broad_twitter_corpus =     0.8113  predict_conll2003            =       0.92  predict_conllpp              =      0.932  predict_mit-movie            =     0.8833  predict_mit-restaurant       =      0.824  predict_ncbi                 =     0.8526  predict_runtime              = 4:22:19.49  predict_samples              =     124835  predict_samples_per_second   =      7.931  predict_steps_per_second     =      0.248[rank0]:[W224 13:09:55.786354060 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())[2025-02-24 13:09:58,983] [INFO] [launch.py:351:main] Process 1242730 exits successfully.[2025-02-24 13:09:58,983] [INFO] [launch.py:351:main] Process 1242727 exits successfully.[2025-02-24 13:09:59,984] [INFO] [launch.py:351:main] Process 1242728 exits successfully.[2025-02-24 13:09:59,984] [INFO] [launch.py:351:main] Process 1242729 exits successfully.