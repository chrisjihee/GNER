{
  "best_metric": 0.4649075710565212,
  "best_model_checkpoint": "output-lfs/GNER-QE-HR-2/microsoft/deberta-v3-base-max_sampled=3/checkpoint-1575",
  "epoch": 8.105146124523507,
  "eval_steps": 315,
  "global_step": 25515,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0031766200762388818,
      "grad_norm": 46.5082893371582,
      "learning_rate": 2e-05,
      "loss": 8.6094,
      "step": 10
    },
    {
      "epoch": 0.0063532401524777635,
      "grad_norm": 19.684906005859375,
      "learning_rate": 2e-05,
      "loss": 3.3426,
      "step": 20
    },
    {
      "epoch": 0.009529860228716646,
      "grad_norm": 16.61274528503418,
      "learning_rate": 2e-05,
      "loss": 1.2804,
      "step": 30
    },
    {
      "epoch": 0.012706480304955527,
      "grad_norm": 2.1841068267822266,
      "learning_rate": 2e-05,
      "loss": 1.1724,
      "step": 40
    },
    {
      "epoch": 0.01588310038119441,
      "grad_norm": 7.889915943145752,
      "learning_rate": 2e-05,
      "loss": 1.1838,
      "step": 50
    },
    {
      "epoch": 0.01905972045743329,
      "grad_norm": 4.3719682693481445,
      "learning_rate": 2e-05,
      "loss": 1.2933,
      "step": 60
    },
    {
      "epoch": 0.022236340533672173,
      "grad_norm": 2.4145257472991943,
      "learning_rate": 2e-05,
      "loss": 1.1459,
      "step": 70
    },
    {
      "epoch": 0.025412960609911054,
      "grad_norm": 6.940952777862549,
      "learning_rate": 2e-05,
      "loss": 1.1781,
      "step": 80
    },
    {
      "epoch": 0.028589580686149935,
      "grad_norm": 3.1761162281036377,
      "learning_rate": 2e-05,
      "loss": 1.141,
      "step": 90
    },
    {
      "epoch": 0.03176620076238882,
      "grad_norm": 4.935880184173584,
      "learning_rate": 2e-05,
      "loss": 1.083,
      "step": 100
    },
    {
      "epoch": 0.0349428208386277,
      "grad_norm": 5.476981163024902,
      "learning_rate": 2e-05,
      "loss": 1.0397,
      "step": 110
    },
    {
      "epoch": 0.03811944091486658,
      "grad_norm": 7.33003568649292,
      "learning_rate": 2e-05,
      "loss": 1.1338,
      "step": 120
    },
    {
      "epoch": 0.041296060991105464,
      "grad_norm": 6.609833717346191,
      "learning_rate": 2e-05,
      "loss": 1.1562,
      "step": 130
    },
    {
      "epoch": 0.044472681067344345,
      "grad_norm": 15.473238945007324,
      "learning_rate": 2e-05,
      "loss": 1.2336,
      "step": 140
    },
    {
      "epoch": 0.04764930114358323,
      "grad_norm": 4.281121730804443,
      "learning_rate": 2e-05,
      "loss": 1.1589,
      "step": 150
    },
    {
      "epoch": 0.05082592121982211,
      "grad_norm": 8.602925300598145,
      "learning_rate": 2e-05,
      "loss": 1.1013,
      "step": 160
    },
    {
      "epoch": 0.05400254129606099,
      "grad_norm": 2.621365785598755,
      "learning_rate": 2e-05,
      "loss": 1.078,
      "step": 170
    },
    {
      "epoch": 0.05717916137229987,
      "grad_norm": 8.862702369689941,
      "learning_rate": 2e-05,
      "loss": 1.0882,
      "step": 180
    },
    {
      "epoch": 0.06035578144853875,
      "grad_norm": 3.413846731185913,
      "learning_rate": 2e-05,
      "loss": 1.0539,
      "step": 190
    },
    {
      "epoch": 0.06353240152477764,
      "grad_norm": 2.4821362495422363,
      "learning_rate": 2e-05,
      "loss": 1.1432,
      "step": 200
    },
    {
      "epoch": 0.06670902160101652,
      "grad_norm": 6.390665054321289,
      "learning_rate": 2e-05,
      "loss": 1.0915,
      "step": 210
    },
    {
      "epoch": 0.0698856416772554,
      "grad_norm": 6.224064350128174,
      "learning_rate": 2e-05,
      "loss": 1.0834,
      "step": 220
    },
    {
      "epoch": 0.07306226175349428,
      "grad_norm": 2.508686065673828,
      "learning_rate": 2e-05,
      "loss": 1.1207,
      "step": 230
    },
    {
      "epoch": 0.07623888182973317,
      "grad_norm": 4.937462329864502,
      "learning_rate": 2e-05,
      "loss": 1.0662,
      "step": 240
    },
    {
      "epoch": 0.07941550190597205,
      "grad_norm": 7.702390193939209,
      "learning_rate": 2e-05,
      "loss": 1.1221,
      "step": 250
    },
    {
      "epoch": 0.08259212198221093,
      "grad_norm": 5.293081283569336,
      "learning_rate": 2e-05,
      "loss": 1.1056,
      "step": 260
    },
    {
      "epoch": 0.08576874205844981,
      "grad_norm": 6.864859104156494,
      "learning_rate": 2e-05,
      "loss": 1.0685,
      "step": 270
    },
    {
      "epoch": 0.08894536213468869,
      "grad_norm": 9.072369575500488,
      "learning_rate": 2e-05,
      "loss": 1.0819,
      "step": 280
    },
    {
      "epoch": 0.09212198221092757,
      "grad_norm": 3.1707687377929688,
      "learning_rate": 2e-05,
      "loss": 1.1149,
      "step": 290
    },
    {
      "epoch": 0.09529860228716645,
      "grad_norm": 3.2364721298217773,
      "learning_rate": 2e-05,
      "loss": 1.0217,
      "step": 300
    },
    {
      "epoch": 0.09847522236340533,
      "grad_norm": 7.517877101898193,
      "learning_rate": 2e-05,
      "loss": 1.0223,
      "step": 310
    },
    {
      "epoch": 0.10006353240152478,
      "eval_loss": 1.8873565196990967,
      "eval_mse": 1.8865748475124309,
      "eval_pearson": 0.44619126177485136,
      "eval_runtime": 21.1539,
      "eval_samples_per_second": 1019.199,
      "eval_spearmanr": 0.43507776332736475,
      "eval_steps_per_second": 4.018,
      "step": 315
    },
    {
      "epoch": 0.10165184243964422,
      "grad_norm": 6.8385090827941895,
      "learning_rate": 2e-05,
      "loss": 1.0864,
      "step": 320
    },
    {
      "epoch": 0.1048284625158831,
      "grad_norm": 1.9456175565719604,
      "learning_rate": 2e-05,
      "loss": 1.0353,
      "step": 330
    },
    {
      "epoch": 0.10800508259212198,
      "grad_norm": 1.7117623090744019,
      "learning_rate": 2e-05,
      "loss": 1.0521,
      "step": 340
    },
    {
      "epoch": 0.11118170266836086,
      "grad_norm": 10.144000053405762,
      "learning_rate": 2e-05,
      "loss": 1.0449,
      "step": 350
    },
    {
      "epoch": 0.11435832274459974,
      "grad_norm": 4.676850318908691,
      "learning_rate": 2e-05,
      "loss": 1.0026,
      "step": 360
    },
    {
      "epoch": 0.11753494282083862,
      "grad_norm": 2.418290376663208,
      "learning_rate": 2e-05,
      "loss": 1.0243,
      "step": 370
    },
    {
      "epoch": 0.1207115628970775,
      "grad_norm": 1.6389840841293335,
      "learning_rate": 2e-05,
      "loss": 1.0487,
      "step": 380
    },
    {
      "epoch": 0.12388818297331639,
      "grad_norm": 2.011291027069092,
      "learning_rate": 2e-05,
      "loss": 1.034,
      "step": 390
    },
    {
      "epoch": 0.12706480304955528,
      "grad_norm": 3.783957004547119,
      "learning_rate": 2e-05,
      "loss": 0.9878,
      "step": 400
    },
    {
      "epoch": 0.13024142312579415,
      "grad_norm": 3.671513557434082,
      "learning_rate": 2e-05,
      "loss": 1.0559,
      "step": 410
    },
    {
      "epoch": 0.13341804320203304,
      "grad_norm": 2.633795738220215,
      "learning_rate": 2e-05,
      "loss": 1.0312,
      "step": 420
    },
    {
      "epoch": 0.1365946632782719,
      "grad_norm": 3.7324588298797607,
      "learning_rate": 2e-05,
      "loss": 0.9988,
      "step": 430
    },
    {
      "epoch": 0.1397712833545108,
      "grad_norm": 6.054218769073486,
      "learning_rate": 2e-05,
      "loss": 1.0544,
      "step": 440
    },
    {
      "epoch": 0.14294790343074967,
      "grad_norm": 2.57131028175354,
      "learning_rate": 2e-05,
      "loss": 1.0475,
      "step": 450
    },
    {
      "epoch": 0.14612452350698857,
      "grad_norm": 3.102874994277954,
      "learning_rate": 2e-05,
      "loss": 1.001,
      "step": 460
    },
    {
      "epoch": 0.14930114358322744,
      "grad_norm": 4.492082118988037,
      "learning_rate": 2e-05,
      "loss": 0.9863,
      "step": 470
    },
    {
      "epoch": 0.15247776365946633,
      "grad_norm": 4.700478553771973,
      "learning_rate": 2e-05,
      "loss": 0.9795,
      "step": 480
    },
    {
      "epoch": 0.1556543837357052,
      "grad_norm": 7.689640045166016,
      "learning_rate": 2e-05,
      "loss": 0.9971,
      "step": 490
    },
    {
      "epoch": 0.1588310038119441,
      "grad_norm": 5.525556564331055,
      "learning_rate": 2e-05,
      "loss": 1.0374,
      "step": 500
    },
    {
      "epoch": 0.16200762388818296,
      "grad_norm": 2.2272608280181885,
      "learning_rate": 2e-05,
      "loss": 0.9987,
      "step": 510
    },
    {
      "epoch": 0.16518424396442186,
      "grad_norm": 3.2903006076812744,
      "learning_rate": 2e-05,
      "loss": 0.9964,
      "step": 520
    },
    {
      "epoch": 0.16836086404066072,
      "grad_norm": 4.223043441772461,
      "learning_rate": 2e-05,
      "loss": 0.9663,
      "step": 530
    },
    {
      "epoch": 0.17153748411689962,
      "grad_norm": 4.460042953491211,
      "learning_rate": 2e-05,
      "loss": 0.9234,
      "step": 540
    },
    {
      "epoch": 0.17471410419313851,
      "grad_norm": 2.6424560546875,
      "learning_rate": 2e-05,
      "loss": 0.9863,
      "step": 550
    },
    {
      "epoch": 0.17789072426937738,
      "grad_norm": 2.2520203590393066,
      "learning_rate": 2e-05,
      "loss": 0.9957,
      "step": 560
    },
    {
      "epoch": 0.18106734434561628,
      "grad_norm": 5.336940765380859,
      "learning_rate": 2e-05,
      "loss": 0.9768,
      "step": 570
    },
    {
      "epoch": 0.18424396442185514,
      "grad_norm": 5.337527751922607,
      "learning_rate": 2e-05,
      "loss": 0.9996,
      "step": 580
    },
    {
      "epoch": 0.18742058449809404,
      "grad_norm": 2.4229648113250732,
      "learning_rate": 2e-05,
      "loss": 0.9754,
      "step": 590
    },
    {
      "epoch": 0.1905972045743329,
      "grad_norm": 2.3326592445373535,
      "learning_rate": 2e-05,
      "loss": 0.9809,
      "step": 600
    },
    {
      "epoch": 0.1937738246505718,
      "grad_norm": 2.657447338104248,
      "learning_rate": 2e-05,
      "loss": 0.9855,
      "step": 610
    },
    {
      "epoch": 0.19695044472681067,
      "grad_norm": 2.891988754272461,
      "learning_rate": 2e-05,
      "loss": 0.9703,
      "step": 620
    },
    {
      "epoch": 0.20012706480304956,
      "grad_norm": 5.88352108001709,
      "learning_rate": 2e-05,
      "loss": 0.9943,
      "step": 630
    },
    {
      "epoch": 0.20012706480304956,
      "eval_loss": 1.7503899335861206,
      "eval_mse": 1.749369034983895,
      "eval_pearson": 0.41663689023393613,
      "eval_runtime": 20.8507,
      "eval_samples_per_second": 1034.019,
      "eval_spearmanr": 0.40143199537108737,
      "eval_steps_per_second": 4.077,
      "step": 630
    },
    {
      "epoch": 0.20330368487928843,
      "grad_norm": 8.42436695098877,
      "learning_rate": 2e-05,
      "loss": 1.0469,
      "step": 640
    },
    {
      "epoch": 0.20648030495552733,
      "grad_norm": 6.064660549163818,
      "learning_rate": 2e-05,
      "loss": 0.9865,
      "step": 650
    },
    {
      "epoch": 0.2096569250317662,
      "grad_norm": 6.648838520050049,
      "learning_rate": 2e-05,
      "loss": 0.9766,
      "step": 660
    },
    {
      "epoch": 0.2128335451080051,
      "grad_norm": 6.4180588722229,
      "learning_rate": 2e-05,
      "loss": 0.9585,
      "step": 670
    },
    {
      "epoch": 0.21601016518424396,
      "grad_norm": 3.333021640777588,
      "learning_rate": 2e-05,
      "loss": 0.9504,
      "step": 680
    },
    {
      "epoch": 0.21918678526048285,
      "grad_norm": 5.298215389251709,
      "learning_rate": 2e-05,
      "loss": 0.9922,
      "step": 690
    },
    {
      "epoch": 0.22236340533672172,
      "grad_norm": 6.47930908203125,
      "learning_rate": 2e-05,
      "loss": 0.931,
      "step": 700
    },
    {
      "epoch": 0.22554002541296062,
      "grad_norm": 3.0726184844970703,
      "learning_rate": 2e-05,
      "loss": 0.9921,
      "step": 710
    },
    {
      "epoch": 0.22871664548919948,
      "grad_norm": 6.980097770690918,
      "learning_rate": 2e-05,
      "loss": 0.9352,
      "step": 720
    },
    {
      "epoch": 0.23189326556543838,
      "grad_norm": 5.223565578460693,
      "learning_rate": 2e-05,
      "loss": 0.9604,
      "step": 730
    },
    {
      "epoch": 0.23506988564167725,
      "grad_norm": 2.7939248085021973,
      "learning_rate": 2e-05,
      "loss": 0.9371,
      "step": 740
    },
    {
      "epoch": 0.23824650571791614,
      "grad_norm": 2.0318446159362793,
      "learning_rate": 2e-05,
      "loss": 0.9222,
      "step": 750
    },
    {
      "epoch": 0.241423125794155,
      "grad_norm": 4.2239990234375,
      "learning_rate": 2e-05,
      "loss": 0.9385,
      "step": 760
    },
    {
      "epoch": 0.2445997458703939,
      "grad_norm": 4.391247749328613,
      "learning_rate": 2e-05,
      "loss": 0.9086,
      "step": 770
    },
    {
      "epoch": 0.24777636594663277,
      "grad_norm": 2.7423431873321533,
      "learning_rate": 2e-05,
      "loss": 0.9059,
      "step": 780
    },
    {
      "epoch": 0.25095298602287164,
      "grad_norm": 6.397802829742432,
      "learning_rate": 2e-05,
      "loss": 0.8843,
      "step": 790
    },
    {
      "epoch": 0.25412960609911056,
      "grad_norm": 3.530900478363037,
      "learning_rate": 2e-05,
      "loss": 0.9752,
      "step": 800
    },
    {
      "epoch": 0.25730622617534943,
      "grad_norm": 2.4790117740631104,
      "learning_rate": 2e-05,
      "loss": 0.9437,
      "step": 810
    },
    {
      "epoch": 0.2604828462515883,
      "grad_norm": 3.9231042861938477,
      "learning_rate": 2e-05,
      "loss": 0.9291,
      "step": 820
    },
    {
      "epoch": 0.2636594663278272,
      "grad_norm": 2.5862510204315186,
      "learning_rate": 2e-05,
      "loss": 0.9368,
      "step": 830
    },
    {
      "epoch": 0.2668360864040661,
      "grad_norm": 2.528801202774048,
      "learning_rate": 2e-05,
      "loss": 0.9238,
      "step": 840
    },
    {
      "epoch": 0.27001270648030495,
      "grad_norm": 2.552480459213257,
      "learning_rate": 2e-05,
      "loss": 0.9733,
      "step": 850
    },
    {
      "epoch": 0.2731893265565438,
      "grad_norm": 2.8007185459136963,
      "learning_rate": 2e-05,
      "loss": 0.8904,
      "step": 860
    },
    {
      "epoch": 0.27636594663278274,
      "grad_norm": 3.794886350631714,
      "learning_rate": 2e-05,
      "loss": 0.9419,
      "step": 870
    },
    {
      "epoch": 0.2795425667090216,
      "grad_norm": 4.597224712371826,
      "learning_rate": 2e-05,
      "loss": 0.8786,
      "step": 880
    },
    {
      "epoch": 0.2827191867852605,
      "grad_norm": 2.932223081588745,
      "learning_rate": 2e-05,
      "loss": 0.872,
      "step": 890
    },
    {
      "epoch": 0.28589580686149935,
      "grad_norm": 2.752471923828125,
      "learning_rate": 2e-05,
      "loss": 0.9115,
      "step": 900
    },
    {
      "epoch": 0.28907242693773827,
      "grad_norm": 7.213926315307617,
      "learning_rate": 2e-05,
      "loss": 0.8754,
      "step": 910
    },
    {
      "epoch": 0.29224904701397714,
      "grad_norm": 4.786620616912842,
      "learning_rate": 2e-05,
      "loss": 0.8763,
      "step": 920
    },
    {
      "epoch": 0.295425667090216,
      "grad_norm": 4.232614517211914,
      "learning_rate": 2e-05,
      "loss": 0.8704,
      "step": 930
    },
    {
      "epoch": 0.29860228716645487,
      "grad_norm": 10.487123489379883,
      "learning_rate": 2e-05,
      "loss": 0.9208,
      "step": 940
    },
    {
      "epoch": 0.3001905972045743,
      "eval_loss": 1.7377268075942993,
      "eval_mse": 1.736972440350697,
      "eval_pearson": 0.455036899980226,
      "eval_runtime": 21.056,
      "eval_samples_per_second": 1023.936,
      "eval_spearmanr": 0.4423272241461804,
      "eval_steps_per_second": 4.037,
      "step": 945
    },
    {
      "epoch": 0.3017789072426938,
      "grad_norm": 3.067246437072754,
      "learning_rate": 2e-05,
      "loss": 0.8813,
      "step": 950
    },
    {
      "epoch": 0.30495552731893266,
      "grad_norm": 6.43064546585083,
      "learning_rate": 2e-05,
      "loss": 0.9088,
      "step": 960
    },
    {
      "epoch": 0.30813214739517153,
      "grad_norm": 3.518089532852173,
      "learning_rate": 2e-05,
      "loss": 0.929,
      "step": 970
    },
    {
      "epoch": 0.3113087674714104,
      "grad_norm": 3.2479660511016846,
      "learning_rate": 2e-05,
      "loss": 0.8626,
      "step": 980
    },
    {
      "epoch": 0.3144853875476493,
      "grad_norm": 4.311344146728516,
      "learning_rate": 2e-05,
      "loss": 0.89,
      "step": 990
    },
    {
      "epoch": 0.3176620076238882,
      "grad_norm": 5.751410484313965,
      "learning_rate": 2e-05,
      "loss": 0.8976,
      "step": 1000
    },
    {
      "epoch": 0.32083862770012705,
      "grad_norm": 4.809391975402832,
      "learning_rate": 2e-05,
      "loss": 0.888,
      "step": 1010
    },
    {
      "epoch": 0.3240152477763659,
      "grad_norm": 10.518319129943848,
      "learning_rate": 2e-05,
      "loss": 0.8992,
      "step": 1020
    },
    {
      "epoch": 0.32719186785260485,
      "grad_norm": 21.480812072753906,
      "learning_rate": 2e-05,
      "loss": 0.9287,
      "step": 1030
    },
    {
      "epoch": 0.3303684879288437,
      "grad_norm": 8.929781913757324,
      "learning_rate": 2e-05,
      "loss": 0.9476,
      "step": 1040
    },
    {
      "epoch": 0.3335451080050826,
      "grad_norm": 56.75931930541992,
      "learning_rate": 2e-05,
      "loss": 0.9363,
      "step": 1050
    },
    {
      "epoch": 0.33672172808132145,
      "grad_norm": 22.08205223083496,
      "learning_rate": 2e-05,
      "loss": 0.9729,
      "step": 1060
    },
    {
      "epoch": 0.33989834815756037,
      "grad_norm": 35.43511199951172,
      "learning_rate": 2e-05,
      "loss": 0.9595,
      "step": 1070
    },
    {
      "epoch": 0.34307496823379924,
      "grad_norm": 18.839292526245117,
      "learning_rate": 2e-05,
      "loss": 0.9972,
      "step": 1080
    },
    {
      "epoch": 0.3462515883100381,
      "grad_norm": 110.36648559570312,
      "learning_rate": 2e-05,
      "loss": 0.9085,
      "step": 1090
    },
    {
      "epoch": 0.34942820838627703,
      "grad_norm": 8.92534065246582,
      "learning_rate": 2e-05,
      "loss": 0.9488,
      "step": 1100
    },
    {
      "epoch": 0.3526048284625159,
      "grad_norm": 11.608102798461914,
      "learning_rate": 2e-05,
      "loss": 0.8808,
      "step": 1110
    },
    {
      "epoch": 0.35578144853875476,
      "grad_norm": 6.434874057769775,
      "learning_rate": 2e-05,
      "loss": 0.9642,
      "step": 1120
    },
    {
      "epoch": 0.35895806861499363,
      "grad_norm": 4.140525817871094,
      "learning_rate": 2e-05,
      "loss": 0.8911,
      "step": 1130
    },
    {
      "epoch": 0.36213468869123255,
      "grad_norm": 2.852341890335083,
      "learning_rate": 2e-05,
      "loss": 0.8957,
      "step": 1140
    },
    {
      "epoch": 0.3653113087674714,
      "grad_norm": 6.875419616699219,
      "learning_rate": 2e-05,
      "loss": 0.8298,
      "step": 1150
    },
    {
      "epoch": 0.3684879288437103,
      "grad_norm": 3.6962125301361084,
      "learning_rate": 2e-05,
      "loss": 0.8537,
      "step": 1160
    },
    {
      "epoch": 0.37166454891994916,
      "grad_norm": 5.2805562019348145,
      "learning_rate": 2e-05,
      "loss": 0.8749,
      "step": 1170
    },
    {
      "epoch": 0.3748411689961881,
      "grad_norm": 2.7559337615966797,
      "learning_rate": 2e-05,
      "loss": 0.8623,
      "step": 1180
    },
    {
      "epoch": 0.37801778907242695,
      "grad_norm": 2.494260549545288,
      "learning_rate": 2e-05,
      "loss": 0.8432,
      "step": 1190
    },
    {
      "epoch": 0.3811944091486658,
      "grad_norm": 11.510395050048828,
      "learning_rate": 2e-05,
      "loss": 0.8831,
      "step": 1200
    },
    {
      "epoch": 0.3843710292249047,
      "grad_norm": 4.399515628814697,
      "learning_rate": 2e-05,
      "loss": 0.8512,
      "step": 1210
    },
    {
      "epoch": 0.3875476493011436,
      "grad_norm": 2.9596993923187256,
      "learning_rate": 2e-05,
      "loss": 0.8968,
      "step": 1220
    },
    {
      "epoch": 0.39072426937738247,
      "grad_norm": 3.1170802116394043,
      "learning_rate": 2e-05,
      "loss": 0.8672,
      "step": 1230
    },
    {
      "epoch": 0.39390088945362134,
      "grad_norm": 2.704073905944824,
      "learning_rate": 2e-05,
      "loss": 0.8135,
      "step": 1240
    },
    {
      "epoch": 0.3970775095298602,
      "grad_norm": 4.941694736480713,
      "learning_rate": 2e-05,
      "loss": 0.8188,
      "step": 1250
    },
    {
      "epoch": 0.40025412960609913,
      "grad_norm": 4.272557258605957,
      "learning_rate": 2e-05,
      "loss": 0.8207,
      "step": 1260
    },
    {
      "epoch": 0.40025412960609913,
      "eval_loss": 2.0136754512786865,
      "eval_mse": 2.012776358693783,
      "eval_pearson": 0.4542090075262014,
      "eval_runtime": 20.9516,
      "eval_samples_per_second": 1029.04,
      "eval_spearmanr": 0.43948030151427836,
      "eval_steps_per_second": 4.057,
      "step": 1260
    },
    {
      "epoch": 0.403430749682338,
      "grad_norm": 5.39167594909668,
      "learning_rate": 2e-05,
      "loss": 0.8403,
      "step": 1270
    },
    {
      "epoch": 0.40660736975857686,
      "grad_norm": 14.270687103271484,
      "learning_rate": 2e-05,
      "loss": 0.8799,
      "step": 1280
    },
    {
      "epoch": 0.40978398983481573,
      "grad_norm": 4.476247310638428,
      "learning_rate": 2e-05,
      "loss": 0.8144,
      "step": 1290
    },
    {
      "epoch": 0.41296060991105465,
      "grad_norm": 4.225319862365723,
      "learning_rate": 2e-05,
      "loss": 0.8303,
      "step": 1300
    },
    {
      "epoch": 0.4161372299872935,
      "grad_norm": 4.936160087585449,
      "learning_rate": 2e-05,
      "loss": 0.8103,
      "step": 1310
    },
    {
      "epoch": 0.4193138500635324,
      "grad_norm": 3.121821403503418,
      "learning_rate": 2e-05,
      "loss": 0.8701,
      "step": 1320
    },
    {
      "epoch": 0.42249047013977126,
      "grad_norm": 2.6779963970184326,
      "learning_rate": 2e-05,
      "loss": 0.8404,
      "step": 1330
    },
    {
      "epoch": 0.4256670902160102,
      "grad_norm": 3.5336239337921143,
      "learning_rate": 2e-05,
      "loss": 0.8812,
      "step": 1340
    },
    {
      "epoch": 0.42884371029224905,
      "grad_norm": 7.130937576293945,
      "learning_rate": 2e-05,
      "loss": 0.821,
      "step": 1350
    },
    {
      "epoch": 0.4320203303684879,
      "grad_norm": 6.280232906341553,
      "learning_rate": 2e-05,
      "loss": 0.8494,
      "step": 1360
    },
    {
      "epoch": 0.43519695044472684,
      "grad_norm": 3.876124382019043,
      "learning_rate": 2e-05,
      "loss": 0.8142,
      "step": 1370
    },
    {
      "epoch": 0.4383735705209657,
      "grad_norm": 3.0751326084136963,
      "learning_rate": 2e-05,
      "loss": 0.8477,
      "step": 1380
    },
    {
      "epoch": 0.4415501905972046,
      "grad_norm": 3.45959734916687,
      "learning_rate": 2e-05,
      "loss": 0.8586,
      "step": 1390
    },
    {
      "epoch": 0.44472681067344344,
      "grad_norm": 3.730618476867676,
      "learning_rate": 2e-05,
      "loss": 0.8315,
      "step": 1400
    },
    {
      "epoch": 0.44790343074968236,
      "grad_norm": 9.414100646972656,
      "learning_rate": 2e-05,
      "loss": 0.784,
      "step": 1410
    },
    {
      "epoch": 0.45108005082592123,
      "grad_norm": 3.9235687255859375,
      "learning_rate": 2e-05,
      "loss": 0.7916,
      "step": 1420
    },
    {
      "epoch": 0.4542566709021601,
      "grad_norm": 3.7944583892822266,
      "learning_rate": 2e-05,
      "loss": 0.7894,
      "step": 1430
    },
    {
      "epoch": 0.45743329097839897,
      "grad_norm": 5.5693464279174805,
      "learning_rate": 2e-05,
      "loss": 0.8226,
      "step": 1440
    },
    {
      "epoch": 0.4606099110546379,
      "grad_norm": 3.8407175540924072,
      "learning_rate": 2e-05,
      "loss": 0.8118,
      "step": 1450
    },
    {
      "epoch": 0.46378653113087676,
      "grad_norm": 3.9785268306732178,
      "learning_rate": 2e-05,
      "loss": 0.7939,
      "step": 1460
    },
    {
      "epoch": 0.4669631512071156,
      "grad_norm": 4.848718643188477,
      "learning_rate": 2e-05,
      "loss": 0.7516,
      "step": 1470
    },
    {
      "epoch": 0.4701397712833545,
      "grad_norm": 3.309847354888916,
      "learning_rate": 2e-05,
      "loss": 0.8324,
      "step": 1480
    },
    {
      "epoch": 0.4733163913595934,
      "grad_norm": 10.45788288116455,
      "learning_rate": 2e-05,
      "loss": 0.7675,
      "step": 1490
    },
    {
      "epoch": 0.4764930114358323,
      "grad_norm": 5.306828498840332,
      "learning_rate": 2e-05,
      "loss": 0.8423,
      "step": 1500
    },
    {
      "epoch": 0.47966963151207115,
      "grad_norm": 5.112198829650879,
      "learning_rate": 2e-05,
      "loss": 0.8111,
      "step": 1510
    },
    {
      "epoch": 0.48284625158831,
      "grad_norm": 6.497286796569824,
      "learning_rate": 2e-05,
      "loss": 0.7781,
      "step": 1520
    },
    {
      "epoch": 0.48602287166454894,
      "grad_norm": 3.46614408493042,
      "learning_rate": 2e-05,
      "loss": 0.772,
      "step": 1530
    },
    {
      "epoch": 0.4891994917407878,
      "grad_norm": 3.886023759841919,
      "learning_rate": 2e-05,
      "loss": 0.7873,
      "step": 1540
    },
    {
      "epoch": 0.4923761118170267,
      "grad_norm": 7.272862911224365,
      "learning_rate": 2e-05,
      "loss": 0.8025,
      "step": 1550
    },
    {
      "epoch": 0.49555273189326554,
      "grad_norm": 4.81460428237915,
      "learning_rate": 2e-05,
      "loss": 0.8123,
      "step": 1560
    },
    {
      "epoch": 0.49872935196950446,
      "grad_norm": 4.087132930755615,
      "learning_rate": 2e-05,
      "loss": 0.8206,
      "step": 1570
    },
    {
      "epoch": 0.5003176620076238,
      "eval_loss": 1.7331726551055908,
      "eval_mse": 1.7322897648988274,
      "eval_pearson": 0.4649075710565212,
      "eval_runtime": 21.0521,
      "eval_samples_per_second": 1024.128,
      "eval_spearmanr": 0.4546173609492045,
      "eval_steps_per_second": 4.038,
      "step": 1575
    },
    {
      "epoch": 0.5019059720457433,
      "grad_norm": 3.913445234298706,
      "learning_rate": 2e-05,
      "loss": 0.7935,
      "step": 1580
    },
    {
      "epoch": 0.5050825921219823,
      "grad_norm": 4.4319939613342285,
      "learning_rate": 2e-05,
      "loss": 0.7795,
      "step": 1590
    },
    {
      "epoch": 0.5082592121982211,
      "grad_norm": 4.058106422424316,
      "learning_rate": 2e-05,
      "loss": 0.7839,
      "step": 1600
    },
    {
      "epoch": 0.51143583227446,
      "grad_norm": 4.3425750732421875,
      "learning_rate": 2e-05,
      "loss": 0.7355,
      "step": 1610
    },
    {
      "epoch": 0.5146124523506989,
      "grad_norm": 7.141615867614746,
      "learning_rate": 2e-05,
      "loss": 0.7716,
      "step": 1620
    },
    {
      "epoch": 0.5177890724269377,
      "grad_norm": 8.0413818359375,
      "learning_rate": 2e-05,
      "loss": 0.7803,
      "step": 1630
    },
    {
      "epoch": 0.5209656925031766,
      "grad_norm": 12.77387523651123,
      "learning_rate": 2e-05,
      "loss": 0.819,
      "step": 1640
    },
    {
      "epoch": 0.5241423125794155,
      "grad_norm": 10.0193452835083,
      "learning_rate": 2e-05,
      "loss": 0.8109,
      "step": 1650
    },
    {
      "epoch": 0.5273189326556544,
      "grad_norm": 5.328230381011963,
      "learning_rate": 2e-05,
      "loss": 0.7723,
      "step": 1660
    },
    {
      "epoch": 0.5304955527318933,
      "grad_norm": 3.1867377758026123,
      "learning_rate": 2e-05,
      "loss": 0.7379,
      "step": 1670
    },
    {
      "epoch": 0.5336721728081322,
      "grad_norm": 5.357115268707275,
      "learning_rate": 2e-05,
      "loss": 0.7726,
      "step": 1680
    },
    {
      "epoch": 0.536848792884371,
      "grad_norm": 3.5576162338256836,
      "learning_rate": 2e-05,
      "loss": 0.7537,
      "step": 1690
    },
    {
      "epoch": 0.5400254129606099,
      "grad_norm": 3.7924089431762695,
      "learning_rate": 2e-05,
      "loss": 0.738,
      "step": 1700
    },
    {
      "epoch": 0.5432020330368488,
      "grad_norm": 9.547410011291504,
      "learning_rate": 2e-05,
      "loss": 0.7605,
      "step": 1710
    },
    {
      "epoch": 0.5463786531130876,
      "grad_norm": 8.309102058410645,
      "learning_rate": 2e-05,
      "loss": 0.7508,
      "step": 1720
    },
    {
      "epoch": 0.5495552731893265,
      "grad_norm": 3.3865890502929688,
      "learning_rate": 2e-05,
      "loss": 0.7635,
      "step": 1730
    },
    {
      "epoch": 0.5527318932655655,
      "grad_norm": 4.991755962371826,
      "learning_rate": 2e-05,
      "loss": 0.7644,
      "step": 1740
    },
    {
      "epoch": 0.5559085133418044,
      "grad_norm": 4.156182289123535,
      "learning_rate": 2e-05,
      "loss": 0.7273,
      "step": 1750
    },
    {
      "epoch": 0.5590851334180432,
      "grad_norm": 3.5740976333618164,
      "learning_rate": 2e-05,
      "loss": 0.8004,
      "step": 1760
    },
    {
      "epoch": 0.5622617534942821,
      "grad_norm": 2.7808878421783447,
      "learning_rate": 2e-05,
      "loss": 0.7535,
      "step": 1770
    },
    {
      "epoch": 0.565438373570521,
      "grad_norm": 2.961747646331787,
      "learning_rate": 2e-05,
      "loss": 0.7849,
      "step": 1780
    },
    {
      "epoch": 0.5686149936467598,
      "grad_norm": 7.0623369216918945,
      "learning_rate": 2e-05,
      "loss": 0.7438,
      "step": 1790
    },
    {
      "epoch": 0.5717916137229987,
      "grad_norm": 4.20172119140625,
      "learning_rate": 2e-05,
      "loss": 0.7655,
      "step": 1800
    },
    {
      "epoch": 0.5749682337992376,
      "grad_norm": 5.837213039398193,
      "learning_rate": 2e-05,
      "loss": 0.7566,
      "step": 1810
    },
    {
      "epoch": 0.5781448538754765,
      "grad_norm": 3.300539493560791,
      "learning_rate": 2e-05,
      "loss": 0.7343,
      "step": 1820
    },
    {
      "epoch": 0.5813214739517154,
      "grad_norm": 5.316587448120117,
      "learning_rate": 2e-05,
      "loss": 0.7078,
      "step": 1830
    },
    {
      "epoch": 0.5844980940279543,
      "grad_norm": 3.017282485961914,
      "learning_rate": 2e-05,
      "loss": 0.7267,
      "step": 1840
    },
    {
      "epoch": 0.5876747141041931,
      "grad_norm": 3.288627862930298,
      "learning_rate": 2e-05,
      "loss": 0.7359,
      "step": 1850
    },
    {
      "epoch": 0.590851334180432,
      "grad_norm": 3.221799373626709,
      "learning_rate": 2e-05,
      "loss": 0.6723,
      "step": 1860
    },
    {
      "epoch": 0.5940279542566709,
      "grad_norm": 3.2016708850860596,
      "learning_rate": 2e-05,
      "loss": 0.7206,
      "step": 1870
    },
    {
      "epoch": 0.5972045743329097,
      "grad_norm": 6.798167705535889,
      "learning_rate": 2e-05,
      "loss": 0.7304,
      "step": 1880
    },
    {
      "epoch": 0.6003811944091486,
      "grad_norm": 5.053493976593018,
      "learning_rate": 2e-05,
      "loss": 0.7729,
      "step": 1890
    },
    {
      "epoch": 0.6003811944091486,
      "eval_loss": 2.1555848121643066,
      "eval_mse": 2.1544159372352714,
      "eval_pearson": 0.4605632123711076,
      "eval_runtime": 20.992,
      "eval_samples_per_second": 1027.06,
      "eval_spearmanr": 0.4472218048065664,
      "eval_steps_per_second": 4.049,
      "step": 1890
    },
    {
      "epoch": 0.6035578144853876,
      "grad_norm": 3.350639820098877,
      "learning_rate": 2e-05,
      "loss": 0.7343,
      "step": 1900
    },
    {
      "epoch": 0.6067344345616265,
      "grad_norm": 5.255520820617676,
      "learning_rate": 2e-05,
      "loss": 0.7289,
      "step": 1910
    },
    {
      "epoch": 0.6099110546378653,
      "grad_norm": 3.3122739791870117,
      "learning_rate": 2e-05,
      "loss": 0.7138,
      "step": 1920
    },
    {
      "epoch": 0.6130876747141042,
      "grad_norm": 7.260948657989502,
      "learning_rate": 2e-05,
      "loss": 0.7564,
      "step": 1930
    },
    {
      "epoch": 0.6162642947903431,
      "grad_norm": 5.708258152008057,
      "learning_rate": 2e-05,
      "loss": 0.7418,
      "step": 1940
    },
    {
      "epoch": 0.6194409148665819,
      "grad_norm": 4.21168851852417,
      "learning_rate": 2e-05,
      "loss": 0.7096,
      "step": 1950
    },
    {
      "epoch": 0.6226175349428208,
      "grad_norm": 7.603250503540039,
      "learning_rate": 2e-05,
      "loss": 0.7151,
      "step": 1960
    },
    {
      "epoch": 0.6257941550190598,
      "grad_norm": 3.6511013507843018,
      "learning_rate": 2e-05,
      "loss": 0.6881,
      "step": 1970
    },
    {
      "epoch": 0.6289707750952986,
      "grad_norm": 3.3381710052490234,
      "learning_rate": 2e-05,
      "loss": 0.7286,
      "step": 1980
    },
    {
      "epoch": 0.6321473951715375,
      "grad_norm": 4.070122241973877,
      "learning_rate": 2e-05,
      "loss": 0.7435,
      "step": 1990
    },
    {
      "epoch": 0.6353240152477764,
      "grad_norm": 4.108591556549072,
      "learning_rate": 2e-05,
      "loss": 0.7191,
      "step": 2000
    },
    {
      "epoch": 0.6385006353240152,
      "grad_norm": 3.710186004638672,
      "learning_rate": 2e-05,
      "loss": 0.746,
      "step": 2010
    },
    {
      "epoch": 0.6416772554002541,
      "grad_norm": 3.363760232925415,
      "learning_rate": 2e-05,
      "loss": 0.6862,
      "step": 2020
    },
    {
      "epoch": 0.644853875476493,
      "grad_norm": 5.879563331604004,
      "learning_rate": 2e-05,
      "loss": 0.7078,
      "step": 2030
    },
    {
      "epoch": 0.6480304955527318,
      "grad_norm": 3.844386577606201,
      "learning_rate": 2e-05,
      "loss": 0.7296,
      "step": 2040
    },
    {
      "epoch": 0.6512071156289708,
      "grad_norm": 7.639307498931885,
      "learning_rate": 2e-05,
      "loss": 0.6943,
      "step": 2050
    },
    {
      "epoch": 0.6543837357052097,
      "grad_norm": 3.3547229766845703,
      "learning_rate": 2e-05,
      "loss": 0.7081,
      "step": 2060
    },
    {
      "epoch": 0.6575603557814486,
      "grad_norm": 5.47993803024292,
      "learning_rate": 2e-05,
      "loss": 0.7091,
      "step": 2070
    },
    {
      "epoch": 0.6607369758576874,
      "grad_norm": 6.184176445007324,
      "learning_rate": 2e-05,
      "loss": 0.671,
      "step": 2080
    },
    {
      "epoch": 0.6639135959339263,
      "grad_norm": 3.669919967651367,
      "learning_rate": 2e-05,
      "loss": 0.712,
      "step": 2090
    },
    {
      "epoch": 0.6670902160101652,
      "grad_norm": 29.456035614013672,
      "learning_rate": 2e-05,
      "loss": 0.6696,
      "step": 2100
    },
    {
      "epoch": 0.670266836086404,
      "grad_norm": 3.8629355430603027,
      "learning_rate": 2e-05,
      "loss": 0.6848,
      "step": 2110
    },
    {
      "epoch": 0.6734434561626429,
      "grad_norm": 6.476571559906006,
      "learning_rate": 2e-05,
      "loss": 0.6897,
      "step": 2120
    },
    {
      "epoch": 0.6766200762388819,
      "grad_norm": 3.5782928466796875,
      "learning_rate": 2e-05,
      "loss": 0.7062,
      "step": 2130
    },
    {
      "epoch": 0.6797966963151207,
      "grad_norm": 5.702664852142334,
      "learning_rate": 2e-05,
      "loss": 0.6853,
      "step": 2140
    },
    {
      "epoch": 0.6829733163913596,
      "grad_norm": 3.99365234375,
      "learning_rate": 2e-05,
      "loss": 0.7074,
      "step": 2150
    },
    {
      "epoch": 0.6861499364675985,
      "grad_norm": 3.3203864097595215,
      "learning_rate": 2e-05,
      "loss": 0.6908,
      "step": 2160
    },
    {
      "epoch": 0.6893265565438373,
      "grad_norm": 4.018142223358154,
      "learning_rate": 2e-05,
      "loss": 0.6987,
      "step": 2170
    },
    {
      "epoch": 0.6925031766200762,
      "grad_norm": 9.108452796936035,
      "learning_rate": 2e-05,
      "loss": 0.6821,
      "step": 2180
    },
    {
      "epoch": 0.6956797966963151,
      "grad_norm": 6.279295444488525,
      "learning_rate": 2e-05,
      "loss": 0.6835,
      "step": 2190
    },
    {
      "epoch": 0.6988564167725541,
      "grad_norm": 3.9637367725372314,
      "learning_rate": 2e-05,
      "loss": 0.7217,
      "step": 2200
    },
    {
      "epoch": 0.7004447268106735,
      "eval_loss": 1.9742586612701416,
      "eval_mse": 1.9738076307114512,
      "eval_pearson": 0.43455682286224806,
      "eval_runtime": 20.968,
      "eval_samples_per_second": 1028.235,
      "eval_spearmanr": 0.41033786423545593,
      "eval_steps_per_second": 4.054,
      "step": 2205
    },
    {
      "epoch": 0.7020330368487929,
      "grad_norm": 5.5899434089660645,
      "learning_rate": 2e-05,
      "loss": 0.7122,
      "step": 2210
    },
    {
      "epoch": 0.7052096569250318,
      "grad_norm": 5.029947757720947,
      "learning_rate": 2e-05,
      "loss": 0.678,
      "step": 2220
    },
    {
      "epoch": 0.7083862770012707,
      "grad_norm": 3.7911036014556885,
      "learning_rate": 2e-05,
      "loss": 0.6639,
      "step": 2230
    },
    {
      "epoch": 0.7115628970775095,
      "grad_norm": 3.3265762329101562,
      "learning_rate": 2e-05,
      "loss": 0.684,
      "step": 2240
    },
    {
      "epoch": 0.7147395171537484,
      "grad_norm": 4.239485740661621,
      "learning_rate": 2e-05,
      "loss": 0.7104,
      "step": 2250
    },
    {
      "epoch": 0.7179161372299873,
      "grad_norm": 6.412047386169434,
      "learning_rate": 2e-05,
      "loss": 0.699,
      "step": 2260
    },
    {
      "epoch": 0.7210927573062261,
      "grad_norm": 5.269157886505127,
      "learning_rate": 2e-05,
      "loss": 0.6822,
      "step": 2270
    },
    {
      "epoch": 0.7242693773824651,
      "grad_norm": 3.8606643676757812,
      "learning_rate": 2e-05,
      "loss": 0.7304,
      "step": 2280
    },
    {
      "epoch": 0.727445997458704,
      "grad_norm": 5.791884899139404,
      "learning_rate": 2e-05,
      "loss": 0.6944,
      "step": 2290
    },
    {
      "epoch": 0.7306226175349428,
      "grad_norm": 4.026009559631348,
      "learning_rate": 2e-05,
      "loss": 0.7181,
      "step": 2300
    },
    {
      "epoch": 0.7337992376111817,
      "grad_norm": 3.5223991870880127,
      "learning_rate": 2e-05,
      "loss": 0.7024,
      "step": 2310
    },
    {
      "epoch": 0.7369758576874206,
      "grad_norm": 4.010108470916748,
      "learning_rate": 2e-05,
      "loss": 0.6838,
      "step": 2320
    },
    {
      "epoch": 0.7401524777636594,
      "grad_norm": 4.3402791023254395,
      "learning_rate": 2e-05,
      "loss": 0.6551,
      "step": 2330
    },
    {
      "epoch": 0.7433290978398983,
      "grad_norm": 3.9370343685150146,
      "learning_rate": 2e-05,
      "loss": 0.6764,
      "step": 2340
    },
    {
      "epoch": 0.7465057179161372,
      "grad_norm": 5.526285171508789,
      "learning_rate": 2e-05,
      "loss": 0.6686,
      "step": 2350
    },
    {
      "epoch": 0.7496823379923762,
      "grad_norm": 3.4031107425689697,
      "learning_rate": 2e-05,
      "loss": 0.6617,
      "step": 2360
    },
    {
      "epoch": 0.752858958068615,
      "grad_norm": 4.02889347076416,
      "learning_rate": 2e-05,
      "loss": 0.6834,
      "step": 2370
    },
    {
      "epoch": 0.7560355781448539,
      "grad_norm": 4.3755598068237305,
      "learning_rate": 2e-05,
      "loss": 0.7031,
      "step": 2380
    },
    {
      "epoch": 0.7592121982210928,
      "grad_norm": 3.4484691619873047,
      "learning_rate": 2e-05,
      "loss": 0.6345,
      "step": 2390
    },
    {
      "epoch": 0.7623888182973316,
      "grad_norm": 6.089108467102051,
      "learning_rate": 2e-05,
      "loss": 0.6616,
      "step": 2400
    },
    {
      "epoch": 0.7655654383735705,
      "grad_norm": 4.151254653930664,
      "learning_rate": 2e-05,
      "loss": 0.7029,
      "step": 2410
    },
    {
      "epoch": 0.7687420584498094,
      "grad_norm": 2.733734130859375,
      "learning_rate": 2e-05,
      "loss": 0.668,
      "step": 2420
    },
    {
      "epoch": 0.7719186785260482,
      "grad_norm": 3.7817466259002686,
      "learning_rate": 2e-05,
      "loss": 0.6753,
      "step": 2430
    },
    {
      "epoch": 0.7750952986022872,
      "grad_norm": 4.278485298156738,
      "learning_rate": 2e-05,
      "loss": 0.6807,
      "step": 2440
    },
    {
      "epoch": 0.7782719186785261,
      "grad_norm": 5.265995025634766,
      "learning_rate": 2e-05,
      "loss": 0.6553,
      "step": 2450
    },
    {
      "epoch": 0.7814485387547649,
      "grad_norm": 3.2464261054992676,
      "learning_rate": 2e-05,
      "loss": 0.6653,
      "step": 2460
    },
    {
      "epoch": 0.7846251588310038,
      "grad_norm": 5.813297748565674,
      "learning_rate": 2e-05,
      "loss": 0.6784,
      "step": 2470
    },
    {
      "epoch": 0.7878017789072427,
      "grad_norm": 3.6597213745117188,
      "learning_rate": 2e-05,
      "loss": 0.6415,
      "step": 2480
    },
    {
      "epoch": 0.7909783989834815,
      "grad_norm": 7.618241786956787,
      "learning_rate": 2e-05,
      "loss": 0.6757,
      "step": 2490
    },
    {
      "epoch": 0.7941550190597204,
      "grad_norm": 4.7698869705200195,
      "learning_rate": 2e-05,
      "loss": 0.6788,
      "step": 2500
    },
    {
      "epoch": 0.7973316391359594,
      "grad_norm": 4.782868385314941,
      "learning_rate": 2e-05,
      "loss": 0.6515,
      "step": 2510
    },
    {
      "epoch": 0.8005082592121983,
      "grad_norm": 3.234818935394287,
      "learning_rate": 2e-05,
      "loss": 0.6608,
      "step": 2520
    },
    {
      "epoch": 0.8005082592121983,
      "eval_loss": 1.8843692541122437,
      "eval_mse": 1.8837146086462795,
      "eval_pearson": 0.448392504571606,
      "eval_runtime": 20.943,
      "eval_samples_per_second": 1029.463,
      "eval_spearmanr": 0.4296779273921014,
      "eval_steps_per_second": 4.059,
      "step": 2520
    },
    {
      "epoch": 0.8036848792884371,
      "grad_norm": 4.025857925415039,
      "learning_rate": 2e-05,
      "loss": 0.6533,
      "step": 2530
    },
    {
      "epoch": 0.806861499364676,
      "grad_norm": 5.641546726226807,
      "learning_rate": 2e-05,
      "loss": 0.6795,
      "step": 2540
    },
    {
      "epoch": 0.8100381194409149,
      "grad_norm": 3.797182321548462,
      "learning_rate": 2e-05,
      "loss": 0.6792,
      "step": 2550
    },
    {
      "epoch": 0.8132147395171537,
      "grad_norm": 5.8034186363220215,
      "learning_rate": 2e-05,
      "loss": 0.6677,
      "step": 2560
    },
    {
      "epoch": 0.8163913595933926,
      "grad_norm": 8.95903205871582,
      "learning_rate": 2e-05,
      "loss": 0.6819,
      "step": 2570
    },
    {
      "epoch": 0.8195679796696315,
      "grad_norm": 3.315450429916382,
      "learning_rate": 2e-05,
      "loss": 0.6166,
      "step": 2580
    },
    {
      "epoch": 0.8227445997458704,
      "grad_norm": 6.538461685180664,
      "learning_rate": 2e-05,
      "loss": 0.6604,
      "step": 2590
    },
    {
      "epoch": 0.8259212198221093,
      "grad_norm": 4.278208255767822,
      "learning_rate": 2e-05,
      "loss": 0.6838,
      "step": 2600
    },
    {
      "epoch": 0.8290978398983482,
      "grad_norm": 3.2401316165924072,
      "learning_rate": 2e-05,
      "loss": 0.6741,
      "step": 2610
    },
    {
      "epoch": 0.832274459974587,
      "grad_norm": 4.648869514465332,
      "learning_rate": 2e-05,
      "loss": 0.6156,
      "step": 2620
    },
    {
      "epoch": 0.8354510800508259,
      "grad_norm": 4.125024795532227,
      "learning_rate": 2e-05,
      "loss": 0.6355,
      "step": 2630
    },
    {
      "epoch": 0.8386277001270648,
      "grad_norm": 2.80131196975708,
      "learning_rate": 2e-05,
      "loss": 0.647,
      "step": 2640
    },
    {
      "epoch": 0.8418043202033036,
      "grad_norm": 4.2255330085754395,
      "learning_rate": 2e-05,
      "loss": 0.6247,
      "step": 2650
    },
    {
      "epoch": 0.8449809402795425,
      "grad_norm": 4.2025346755981445,
      "learning_rate": 2e-05,
      "loss": 0.6354,
      "step": 2660
    },
    {
      "epoch": 0.8481575603557815,
      "grad_norm": 2.9295623302459717,
      "learning_rate": 2e-05,
      "loss": 0.6414,
      "step": 2670
    },
    {
      "epoch": 0.8513341804320204,
      "grad_norm": 3.4423677921295166,
      "learning_rate": 2e-05,
      "loss": 0.6317,
      "step": 2680
    },
    {
      "epoch": 0.8545108005082592,
      "grad_norm": 6.074214458465576,
      "learning_rate": 2e-05,
      "loss": 0.6506,
      "step": 2690
    },
    {
      "epoch": 0.8576874205844981,
      "grad_norm": 3.5384418964385986,
      "learning_rate": 2e-05,
      "loss": 0.6331,
      "step": 2700
    },
    {
      "epoch": 0.860864040660737,
      "grad_norm": 5.127457141876221,
      "learning_rate": 2e-05,
      "loss": 0.6149,
      "step": 2710
    },
    {
      "epoch": 0.8640406607369758,
      "grad_norm": 4.088214874267578,
      "learning_rate": 2e-05,
      "loss": 0.6479,
      "step": 2720
    },
    {
      "epoch": 0.8672172808132147,
      "grad_norm": 5.694389343261719,
      "learning_rate": 2e-05,
      "loss": 0.6201,
      "step": 2730
    },
    {
      "epoch": 0.8703939008894537,
      "grad_norm": 3.3473081588745117,
      "learning_rate": 2e-05,
      "loss": 0.6367,
      "step": 2740
    },
    {
      "epoch": 0.8735705209656925,
      "grad_norm": 6.215909004211426,
      "learning_rate": 2e-05,
      "loss": 0.6221,
      "step": 2750
    },
    {
      "epoch": 0.8767471410419314,
      "grad_norm": 4.398200511932373,
      "learning_rate": 2e-05,
      "loss": 0.5961,
      "step": 2760
    },
    {
      "epoch": 0.8799237611181703,
      "grad_norm": 4.922428131103516,
      "learning_rate": 2e-05,
      "loss": 0.626,
      "step": 2770
    },
    {
      "epoch": 0.8831003811944091,
      "grad_norm": 3.6328632831573486,
      "learning_rate": 2e-05,
      "loss": 0.6476,
      "step": 2780
    },
    {
      "epoch": 0.886277001270648,
      "grad_norm": 7.135413646697998,
      "learning_rate": 2e-05,
      "loss": 0.6414,
      "step": 2790
    },
    {
      "epoch": 0.8894536213468869,
      "grad_norm": 3.9023797512054443,
      "learning_rate": 2e-05,
      "loss": 0.6556,
      "step": 2800
    },
    {
      "epoch": 0.8926302414231257,
      "grad_norm": 3.5584990978240967,
      "learning_rate": 2e-05,
      "loss": 0.6324,
      "step": 2810
    },
    {
      "epoch": 0.8958068614993647,
      "grad_norm": 4.506717681884766,
      "learning_rate": 2e-05,
      "loss": 0.6193,
      "step": 2820
    },
    {
      "epoch": 0.8989834815756036,
      "grad_norm": 4.826307773590088,
      "learning_rate": 2e-05,
      "loss": 0.6388,
      "step": 2830
    },
    {
      "epoch": 0.900571791613723,
      "eval_loss": 1.8767364025115967,
      "eval_mse": 1.875420881932211,
      "eval_pearson": 0.4453531611052308,
      "eval_runtime": 20.9496,
      "eval_samples_per_second": 1029.137,
      "eval_spearmanr": 0.42809273606095155,
      "eval_steps_per_second": 4.057,
      "step": 2835
    },
    {
      "epoch": 0.9021601016518425,
      "grad_norm": 4.660782814025879,
      "learning_rate": 2e-05,
      "loss": 0.6124,
      "step": 2840
    },
    {
      "epoch": 0.9053367217280813,
      "grad_norm": 4.2706499099731445,
      "learning_rate": 2e-05,
      "loss": 0.5969,
      "step": 2850
    },
    {
      "epoch": 0.9085133418043202,
      "grad_norm": 4.211018085479736,
      "learning_rate": 2e-05,
      "loss": 0.6414,
      "step": 2860
    },
    {
      "epoch": 0.9116899618805591,
      "grad_norm": 7.5686116218566895,
      "learning_rate": 2e-05,
      "loss": 0.6226,
      "step": 2870
    },
    {
      "epoch": 0.9148665819567979,
      "grad_norm": 7.05577278137207,
      "learning_rate": 2e-05,
      "loss": 0.6551,
      "step": 2880
    },
    {
      "epoch": 0.9180432020330368,
      "grad_norm": 3.567023992538452,
      "learning_rate": 2e-05,
      "loss": 0.6375,
      "step": 2890
    },
    {
      "epoch": 0.9212198221092758,
      "grad_norm": 5.4999189376831055,
      "learning_rate": 2e-05,
      "loss": 0.6141,
      "step": 2900
    },
    {
      "epoch": 0.9243964421855146,
      "grad_norm": 5.631507396697998,
      "learning_rate": 2e-05,
      "loss": 0.5934,
      "step": 2910
    },
    {
      "epoch": 0.9275730622617535,
      "grad_norm": 3.483912229537964,
      "learning_rate": 2e-05,
      "loss": 0.6143,
      "step": 2920
    },
    {
      "epoch": 0.9307496823379924,
      "grad_norm": 3.4075686931610107,
      "learning_rate": 2e-05,
      "loss": 0.6579,
      "step": 2930
    },
    {
      "epoch": 0.9339263024142312,
      "grad_norm": 3.1097095012664795,
      "learning_rate": 2e-05,
      "loss": 0.6285,
      "step": 2940
    },
    {
      "epoch": 0.9371029224904701,
      "grad_norm": 4.897862434387207,
      "learning_rate": 2e-05,
      "loss": 0.5865,
      "step": 2950
    },
    {
      "epoch": 0.940279542566709,
      "grad_norm": 5.884397506713867,
      "learning_rate": 2e-05,
      "loss": 0.6384,
      "step": 2960
    },
    {
      "epoch": 0.9434561626429478,
      "grad_norm": 5.360279560089111,
      "learning_rate": 2e-05,
      "loss": 0.6272,
      "step": 2970
    },
    {
      "epoch": 0.9466327827191868,
      "grad_norm": 3.631948709487915,
      "learning_rate": 2e-05,
      "loss": 0.622,
      "step": 2980
    },
    {
      "epoch": 0.9498094027954257,
      "grad_norm": 4.598989963531494,
      "learning_rate": 2e-05,
      "loss": 0.6011,
      "step": 2990
    },
    {
      "epoch": 0.9529860228716646,
      "grad_norm": 3.9110934734344482,
      "learning_rate": 2e-05,
      "loss": 0.608,
      "step": 3000
    },
    {
      "epoch": 0.9561626429479034,
      "grad_norm": 8.986289024353027,
      "learning_rate": 2e-05,
      "loss": 0.6012,
      "step": 3010
    },
    {
      "epoch": 0.9593392630241423,
      "grad_norm": 3.8630330562591553,
      "learning_rate": 2e-05,
      "loss": 0.6026,
      "step": 3020
    },
    {
      "epoch": 0.9625158831003812,
      "grad_norm": 3.9615185260772705,
      "learning_rate": 2e-05,
      "loss": 0.6035,
      "step": 3030
    },
    {
      "epoch": 0.96569250317662,
      "grad_norm": 4.100024223327637,
      "learning_rate": 2e-05,
      "loss": 0.6055,
      "step": 3040
    },
    {
      "epoch": 0.968869123252859,
      "grad_norm": 4.228113651275635,
      "learning_rate": 2e-05,
      "loss": 0.6025,
      "step": 3050
    },
    {
      "epoch": 0.9720457433290979,
      "grad_norm": 6.646990776062012,
      "learning_rate": 2e-05,
      "loss": 0.6164,
      "step": 3060
    },
    {
      "epoch": 0.9752223634053367,
      "grad_norm": 4.521121501922607,
      "learning_rate": 2e-05,
      "loss": 0.6046,
      "step": 3070
    },
    {
      "epoch": 0.9783989834815756,
      "grad_norm": 4.376370906829834,
      "learning_rate": 2e-05,
      "loss": 0.6311,
      "step": 3080
    },
    {
      "epoch": 0.9815756035578145,
      "grad_norm": 4.064891815185547,
      "learning_rate": 2e-05,
      "loss": 0.6097,
      "step": 3090
    },
    {
      "epoch": 0.9847522236340533,
      "grad_norm": 3.631924629211426,
      "learning_rate": 2e-05,
      "loss": 0.5995,
      "step": 3100
    },
    {
      "epoch": 0.9879288437102922,
      "grad_norm": 3.5814456939697266,
      "learning_rate": 2e-05,
      "loss": 0.5692,
      "step": 3110
    },
    {
      "epoch": 0.9911054637865311,
      "grad_norm": 3.201612949371338,
      "learning_rate": 2e-05,
      "loss": 0.5788,
      "step": 3120
    },
    {
      "epoch": 0.9942820838627701,
      "grad_norm": 6.188814640045166,
      "learning_rate": 2e-05,
      "loss": 0.6087,
      "step": 3130
    },
    {
      "epoch": 0.9974587039390089,
      "grad_norm": 5.162104606628418,
      "learning_rate": 2e-05,
      "loss": 0.5947,
      "step": 3140
    },
    {
      "epoch": 1.0006353240152477,
      "grad_norm": 4.030823230743408,
      "learning_rate": 2e-05,
      "loss": 0.5857,
      "step": 3150
    },
    {
      "epoch": 1.0006353240152477,
      "eval_loss": 1.920541524887085,
      "eval_mse": 1.9191167754233436,
      "eval_pearson": 0.459511272450955,
      "eval_runtime": 21.0449,
      "eval_samples_per_second": 1024.475,
      "eval_spearmanr": 0.44992934260154716,
      "eval_steps_per_second": 4.039,
      "step": 3150
    },
    {
      "epoch": 1.0038119440914866,
      "grad_norm": 2.8808753490448,
      "learning_rate": 2e-05,
      "loss": 0.5517,
      "step": 3160
    },
    {
      "epoch": 1.0069885641677256,
      "grad_norm": 4.343062877655029,
      "learning_rate": 2e-05,
      "loss": 0.5809,
      "step": 3170
    },
    {
      "epoch": 1.0101651842439645,
      "grad_norm": 3.71439528465271,
      "learning_rate": 2e-05,
      "loss": 0.5405,
      "step": 3180
    },
    {
      "epoch": 1.0133418043202034,
      "grad_norm": 3.9014084339141846,
      "learning_rate": 2e-05,
      "loss": 0.5215,
      "step": 3190
    },
    {
      "epoch": 1.0165184243964422,
      "grad_norm": 4.094655513763428,
      "learning_rate": 2e-05,
      "loss": 0.5335,
      "step": 3200
    },
    {
      "epoch": 1.0196950444726811,
      "grad_norm": 3.6637394428253174,
      "learning_rate": 2e-05,
      "loss": 0.5328,
      "step": 3210
    },
    {
      "epoch": 1.02287166454892,
      "grad_norm": 3.6453914642333984,
      "learning_rate": 2e-05,
      "loss": 0.5735,
      "step": 3220
    },
    {
      "epoch": 1.0260482846251588,
      "grad_norm": 4.6907057762146,
      "learning_rate": 2e-05,
      "loss": 0.5426,
      "step": 3230
    },
    {
      "epoch": 1.0292249047013977,
      "grad_norm": 3.7817609310150146,
      "learning_rate": 2e-05,
      "loss": 0.5196,
      "step": 3240
    },
    {
      "epoch": 1.0324015247776366,
      "grad_norm": 4.807611465454102,
      "learning_rate": 2e-05,
      "loss": 0.5512,
      "step": 3250
    },
    {
      "epoch": 1.0355781448538754,
      "grad_norm": 5.904694080352783,
      "learning_rate": 2e-05,
      "loss": 0.5461,
      "step": 3260
    },
    {
      "epoch": 1.0387547649301143,
      "grad_norm": 3.3421761989593506,
      "learning_rate": 2e-05,
      "loss": 0.5727,
      "step": 3270
    },
    {
      "epoch": 1.0419313850063532,
      "grad_norm": 5.9115447998046875,
      "learning_rate": 2e-05,
      "loss": 0.5722,
      "step": 3280
    },
    {
      "epoch": 1.045108005082592,
      "grad_norm": 8.895461082458496,
      "learning_rate": 2e-05,
      "loss": 0.5626,
      "step": 3290
    },
    {
      "epoch": 1.048284625158831,
      "grad_norm": 6.09803581237793,
      "learning_rate": 2e-05,
      "loss": 0.5636,
      "step": 3300
    },
    {
      "epoch": 1.0514612452350698,
      "grad_norm": 7.061208248138428,
      "learning_rate": 2e-05,
      "loss": 0.5763,
      "step": 3310
    },
    {
      "epoch": 1.0546378653113089,
      "grad_norm": 3.7712457180023193,
      "learning_rate": 2e-05,
      "loss": 0.5171,
      "step": 3320
    },
    {
      "epoch": 1.0578144853875477,
      "grad_norm": 3.7711095809936523,
      "learning_rate": 2e-05,
      "loss": 0.5554,
      "step": 3330
    },
    {
      "epoch": 1.0609911054637866,
      "grad_norm": 6.848146438598633,
      "learning_rate": 2e-05,
      "loss": 0.5147,
      "step": 3340
    },
    {
      "epoch": 1.0641677255400255,
      "grad_norm": 4.304545879364014,
      "learning_rate": 2e-05,
      "loss": 0.5447,
      "step": 3350
    },
    {
      "epoch": 1.0673443456162643,
      "grad_norm": 4.091097354888916,
      "learning_rate": 2e-05,
      "loss": 0.557,
      "step": 3360
    },
    {
      "epoch": 1.0705209656925032,
      "grad_norm": 5.994009494781494,
      "learning_rate": 2e-05,
      "loss": 0.4985,
      "step": 3370
    },
    {
      "epoch": 1.073697585768742,
      "grad_norm": 4.994138240814209,
      "learning_rate": 2e-05,
      "loss": 0.5216,
      "step": 3380
    },
    {
      "epoch": 1.076874205844981,
      "grad_norm": 4.465786457061768,
      "learning_rate": 2e-05,
      "loss": 0.5139,
      "step": 3390
    },
    {
      "epoch": 1.0800508259212198,
      "grad_norm": 6.072381019592285,
      "learning_rate": 2e-05,
      "loss": 0.5386,
      "step": 3400
    },
    {
      "epoch": 1.0832274459974587,
      "grad_norm": 4.342925071716309,
      "learning_rate": 2e-05,
      "loss": 0.5337,
      "step": 3410
    },
    {
      "epoch": 1.0864040660736975,
      "grad_norm": 5.239419937133789,
      "learning_rate": 2e-05,
      "loss": 0.5215,
      "step": 3420
    },
    {
      "epoch": 1.0895806861499364,
      "grad_norm": 6.112219333648682,
      "learning_rate": 2e-05,
      "loss": 0.5615,
      "step": 3430
    },
    {
      "epoch": 1.0927573062261753,
      "grad_norm": 3.8467085361480713,
      "learning_rate": 2e-05,
      "loss": 0.5204,
      "step": 3440
    },
    {
      "epoch": 1.0959339263024142,
      "grad_norm": 4.57859468460083,
      "learning_rate": 2e-05,
      "loss": 0.5205,
      "step": 3450
    },
    {
      "epoch": 1.099110546378653,
      "grad_norm": 5.157961845397949,
      "learning_rate": 2e-05,
      "loss": 0.5521,
      "step": 3460
    },
    {
      "epoch": 1.1006988564167726,
      "eval_loss": 2.071171283721924,
      "eval_mse": 2.0701123008259152,
      "eval_pearson": 0.45186179674342786,
      "eval_runtime": 21.1554,
      "eval_samples_per_second": 1019.126,
      "eval_spearmanr": 0.4388676810132584,
      "eval_steps_per_second": 4.018,
      "step": 3465
    },
    {
      "epoch": 1.102287166454892,
      "grad_norm": 4.462590217590332,
      "learning_rate": 2e-05,
      "loss": 0.5318,
      "step": 3470
    },
    {
      "epoch": 1.105463786531131,
      "grad_norm": 3.7436506748199463,
      "learning_rate": 2e-05,
      "loss": 0.5552,
      "step": 3480
    },
    {
      "epoch": 1.1086404066073698,
      "grad_norm": 3.7293505668640137,
      "learning_rate": 2e-05,
      "loss": 0.5283,
      "step": 3490
    },
    {
      "epoch": 1.1118170266836087,
      "grad_norm": 5.044747829437256,
      "learning_rate": 2e-05,
      "loss": 0.5396,
      "step": 3500
    },
    {
      "epoch": 1.1149936467598476,
      "grad_norm": 3.565300226211548,
      "learning_rate": 2e-05,
      "loss": 0.4983,
      "step": 3510
    },
    {
      "epoch": 1.1181702668360864,
      "grad_norm": 3.4682281017303467,
      "learning_rate": 2e-05,
      "loss": 0.507,
      "step": 3520
    },
    {
      "epoch": 1.1213468869123253,
      "grad_norm": 4.124659538269043,
      "learning_rate": 2e-05,
      "loss": 0.55,
      "step": 3530
    },
    {
      "epoch": 1.1245235069885642,
      "grad_norm": 4.144728183746338,
      "learning_rate": 2e-05,
      "loss": 0.5528,
      "step": 3540
    },
    {
      "epoch": 1.127700127064803,
      "grad_norm": 3.8690009117126465,
      "learning_rate": 2e-05,
      "loss": 0.4937,
      "step": 3550
    },
    {
      "epoch": 1.130876747141042,
      "grad_norm": 5.1243205070495605,
      "learning_rate": 2e-05,
      "loss": 0.5504,
      "step": 3560
    },
    {
      "epoch": 1.1340533672172808,
      "grad_norm": 3.2725534439086914,
      "learning_rate": 2e-05,
      "loss": 0.5217,
      "step": 3570
    },
    {
      "epoch": 1.1372299872935197,
      "grad_norm": 5.377074241638184,
      "learning_rate": 2e-05,
      "loss": 0.5567,
      "step": 3580
    },
    {
      "epoch": 1.1404066073697585,
      "grad_norm": 6.727631092071533,
      "learning_rate": 2e-05,
      "loss": 0.508,
      "step": 3590
    },
    {
      "epoch": 1.1435832274459974,
      "grad_norm": 4.739048480987549,
      "learning_rate": 2e-05,
      "loss": 0.5462,
      "step": 3600
    },
    {
      "epoch": 1.1467598475222363,
      "grad_norm": 4.820936679840088,
      "learning_rate": 2e-05,
      "loss": 0.5396,
      "step": 3610
    },
    {
      "epoch": 1.1499364675984753,
      "grad_norm": 4.342735290527344,
      "learning_rate": 2e-05,
      "loss": 0.548,
      "step": 3620
    },
    {
      "epoch": 1.153113087674714,
      "grad_norm": 4.187570095062256,
      "learning_rate": 2e-05,
      "loss": 0.5121,
      "step": 3630
    },
    {
      "epoch": 1.156289707750953,
      "grad_norm": 3.9586429595947266,
      "learning_rate": 2e-05,
      "loss": 0.5167,
      "step": 3640
    },
    {
      "epoch": 1.159466327827192,
      "grad_norm": 3.118283748626709,
      "learning_rate": 2e-05,
      "loss": 0.5067,
      "step": 3650
    },
    {
      "epoch": 1.1626429479034308,
      "grad_norm": 10.155055046081543,
      "learning_rate": 2e-05,
      "loss": 0.5405,
      "step": 3660
    },
    {
      "epoch": 1.1658195679796697,
      "grad_norm": 3.6638007164001465,
      "learning_rate": 2e-05,
      "loss": 0.5501,
      "step": 3670
    },
    {
      "epoch": 1.1689961880559085,
      "grad_norm": 5.156478404998779,
      "learning_rate": 2e-05,
      "loss": 0.5554,
      "step": 3680
    },
    {
      "epoch": 1.1721728081321474,
      "grad_norm": 3.2490389347076416,
      "learning_rate": 2e-05,
      "loss": 0.5056,
      "step": 3690
    },
    {
      "epoch": 1.1753494282083863,
      "grad_norm": 3.797043561935425,
      "learning_rate": 2e-05,
      "loss": 0.5139,
      "step": 3700
    },
    {
      "epoch": 1.1785260482846251,
      "grad_norm": 3.889270305633545,
      "learning_rate": 2e-05,
      "loss": 0.4943,
      "step": 3710
    },
    {
      "epoch": 1.181702668360864,
      "grad_norm": 3.55600905418396,
      "learning_rate": 2e-05,
      "loss": 0.5318,
      "step": 3720
    },
    {
      "epoch": 1.1848792884371029,
      "grad_norm": 5.077720642089844,
      "learning_rate": 2e-05,
      "loss": 0.4998,
      "step": 3730
    },
    {
      "epoch": 1.1880559085133418,
      "grad_norm": 4.121164798736572,
      "learning_rate": 2e-05,
      "loss": 0.4964,
      "step": 3740
    },
    {
      "epoch": 1.1912325285895806,
      "grad_norm": 4.5166015625,
      "learning_rate": 2e-05,
      "loss": 0.5095,
      "step": 3750
    },
    {
      "epoch": 1.1944091486658195,
      "grad_norm": 4.257815361022949,
      "learning_rate": 2e-05,
      "loss": 0.4944,
      "step": 3760
    },
    {
      "epoch": 1.1975857687420584,
      "grad_norm": 6.253177642822266,
      "learning_rate": 2e-05,
      "loss": 0.5298,
      "step": 3770
    },
    {
      "epoch": 1.2007623888182972,
      "grad_norm": 4.1409478187561035,
      "learning_rate": 2e-05,
      "loss": 0.5629,
      "step": 3780
    },
    {
      "epoch": 1.2007623888182972,
      "eval_loss": 1.9561522006988525,
      "eval_mse": 1.955341234432744,
      "eval_pearson": 0.46046241198091475,
      "eval_runtime": 20.9502,
      "eval_samples_per_second": 1029.106,
      "eval_spearmanr": 0.4482418338199883,
      "eval_steps_per_second": 4.057,
      "step": 3780
    },
    {
      "epoch": 1.2039390088945363,
      "grad_norm": 3.28908371925354,
      "learning_rate": 2e-05,
      "loss": 0.5151,
      "step": 3790
    },
    {
      "epoch": 1.2071156289707752,
      "grad_norm": 4.578044891357422,
      "learning_rate": 2e-05,
      "loss": 0.5011,
      "step": 3800
    },
    {
      "epoch": 1.210292249047014,
      "grad_norm": 3.6123762130737305,
      "learning_rate": 2e-05,
      "loss": 0.52,
      "step": 3810
    },
    {
      "epoch": 1.213468869123253,
      "grad_norm": 4.250359058380127,
      "learning_rate": 2e-05,
      "loss": 0.5142,
      "step": 3820
    },
    {
      "epoch": 1.2166454891994918,
      "grad_norm": 3.413147211074829,
      "learning_rate": 2e-05,
      "loss": 0.537,
      "step": 3830
    },
    {
      "epoch": 1.2198221092757306,
      "grad_norm": 5.657536029815674,
      "learning_rate": 2e-05,
      "loss": 0.5341,
      "step": 3840
    },
    {
      "epoch": 1.2229987293519695,
      "grad_norm": 3.9150993824005127,
      "learning_rate": 2e-05,
      "loss": 0.5072,
      "step": 3850
    },
    {
      "epoch": 1.2261753494282084,
      "grad_norm": 3.8997371196746826,
      "learning_rate": 2e-05,
      "loss": 0.5128,
      "step": 3860
    },
    {
      "epoch": 1.2293519695044473,
      "grad_norm": 3.938157558441162,
      "learning_rate": 2e-05,
      "loss": 0.4751,
      "step": 3870
    },
    {
      "epoch": 1.2325285895806861,
      "grad_norm": 3.9253203868865967,
      "learning_rate": 2e-05,
      "loss": 0.5376,
      "step": 3880
    },
    {
      "epoch": 1.235705209656925,
      "grad_norm": 5.950531959533691,
      "learning_rate": 2e-05,
      "loss": 0.5109,
      "step": 3890
    },
    {
      "epoch": 1.2388818297331639,
      "grad_norm": 5.146865367889404,
      "learning_rate": 2e-05,
      "loss": 0.5046,
      "step": 3900
    },
    {
      "epoch": 1.2420584498094027,
      "grad_norm": 4.645552158355713,
      "learning_rate": 2e-05,
      "loss": 0.5347,
      "step": 3910
    },
    {
      "epoch": 1.2452350698856416,
      "grad_norm": 4.385765075683594,
      "learning_rate": 2e-05,
      "loss": 0.4932,
      "step": 3920
    },
    {
      "epoch": 1.2484116899618805,
      "grad_norm": 6.738862037658691,
      "learning_rate": 2e-05,
      "loss": 0.4946,
      "step": 3930
    },
    {
      "epoch": 1.2515883100381195,
      "grad_norm": 4.531278133392334,
      "learning_rate": 2e-05,
      "loss": 0.5219,
      "step": 3940
    },
    {
      "epoch": 1.2547649301143582,
      "grad_norm": 4.3796515464782715,
      "learning_rate": 2e-05,
      "loss": 0.4592,
      "step": 3950
    },
    {
      "epoch": 1.2579415501905973,
      "grad_norm": 3.7307026386260986,
      "learning_rate": 2e-05,
      "loss": 0.4996,
      "step": 3960
    },
    {
      "epoch": 1.2611181702668361,
      "grad_norm": 3.5378189086914062,
      "learning_rate": 2e-05,
      "loss": 0.5437,
      "step": 3970
    },
    {
      "epoch": 1.264294790343075,
      "grad_norm": 3.850538730621338,
      "learning_rate": 2e-05,
      "loss": 0.52,
      "step": 3980
    },
    {
      "epoch": 1.2674714104193139,
      "grad_norm": 3.6939926147460938,
      "learning_rate": 2e-05,
      "loss": 0.5047,
      "step": 3990
    },
    {
      "epoch": 1.2706480304955527,
      "grad_norm": 4.411185264587402,
      "learning_rate": 2e-05,
      "loss": 0.4838,
      "step": 4000
    },
    {
      "epoch": 1.2738246505717916,
      "grad_norm": 3.8614249229431152,
      "learning_rate": 2e-05,
      "loss": 0.4749,
      "step": 4010
    },
    {
      "epoch": 1.2770012706480305,
      "grad_norm": 3.0205914974212646,
      "learning_rate": 2e-05,
      "loss": 0.5474,
      "step": 4020
    },
    {
      "epoch": 1.2801778907242694,
      "grad_norm": 4.750476837158203,
      "learning_rate": 2e-05,
      "loss": 0.4954,
      "step": 4030
    },
    {
      "epoch": 1.2833545108005082,
      "grad_norm": 4.384004592895508,
      "learning_rate": 2e-05,
      "loss": 0.5111,
      "step": 4040
    },
    {
      "epoch": 1.286531130876747,
      "grad_norm": 3.976759672164917,
      "learning_rate": 2e-05,
      "loss": 0.5053,
      "step": 4050
    },
    {
      "epoch": 1.289707750952986,
      "grad_norm": 3.339120388031006,
      "learning_rate": 2e-05,
      "loss": 0.4672,
      "step": 4060
    },
    {
      "epoch": 1.2928843710292248,
      "grad_norm": 3.827554941177368,
      "learning_rate": 2e-05,
      "loss": 0.5271,
      "step": 4070
    },
    {
      "epoch": 1.2960609911054637,
      "grad_norm": 4.425243377685547,
      "learning_rate": 2e-05,
      "loss": 0.4908,
      "step": 4080
    },
    {
      "epoch": 1.2992376111817028,
      "grad_norm": 4.657439231872559,
      "learning_rate": 2e-05,
      "loss": 0.4946,
      "step": 4090
    },
    {
      "epoch": 1.300825921219822,
      "eval_loss": 1.904611349105835,
      "eval_mse": 1.9023189287017581,
      "eval_pearson": 0.4512541327956447,
      "eval_runtime": 20.8473,
      "eval_samples_per_second": 1034.188,
      "eval_spearmanr": 0.44218175759520895,
      "eval_steps_per_second": 4.077,
      "step": 4095
    },
    {
      "epoch": 1.3024142312579414,
      "grad_norm": 2.9277215003967285,
      "learning_rate": 2e-05,
      "loss": 0.4958,
      "step": 4100
    },
    {
      "epoch": 1.3055908513341805,
      "grad_norm": 4.338184833526611,
      "learning_rate": 2e-05,
      "loss": 0.473,
      "step": 4110
    },
    {
      "epoch": 1.3087674714104194,
      "grad_norm": 4.000386714935303,
      "learning_rate": 2e-05,
      "loss": 0.4725,
      "step": 4120
    },
    {
      "epoch": 1.3119440914866582,
      "grad_norm": 4.551592826843262,
      "learning_rate": 2e-05,
      "loss": 0.5084,
      "step": 4130
    },
    {
      "epoch": 1.3151207115628971,
      "grad_norm": 5.57088041305542,
      "learning_rate": 2e-05,
      "loss": 0.5344,
      "step": 4140
    },
    {
      "epoch": 1.318297331639136,
      "grad_norm": 3.393904447555542,
      "learning_rate": 2e-05,
      "loss": 0.5014,
      "step": 4150
    },
    {
      "epoch": 1.3214739517153749,
      "grad_norm": 5.394770622253418,
      "learning_rate": 2e-05,
      "loss": 0.5041,
      "step": 4160
    },
    {
      "epoch": 1.3246505717916137,
      "grad_norm": 3.3825442790985107,
      "learning_rate": 2e-05,
      "loss": 0.4744,
      "step": 4170
    },
    {
      "epoch": 1.3278271918678526,
      "grad_norm": 8.38036823272705,
      "learning_rate": 2e-05,
      "loss": 0.5257,
      "step": 4180
    },
    {
      "epoch": 1.3310038119440915,
      "grad_norm": 7.137537956237793,
      "learning_rate": 2e-05,
      "loss": 0.4908,
      "step": 4190
    },
    {
      "epoch": 1.3341804320203303,
      "grad_norm": 5.187898635864258,
      "learning_rate": 2e-05,
      "loss": 0.4751,
      "step": 4200
    },
    {
      "epoch": 1.3373570520965692,
      "grad_norm": 3.7854080200195312,
      "learning_rate": 2e-05,
      "loss": 0.4949,
      "step": 4210
    },
    {
      "epoch": 1.340533672172808,
      "grad_norm": 4.668229579925537,
      "learning_rate": 2e-05,
      "loss": 0.4776,
      "step": 4220
    },
    {
      "epoch": 1.343710292249047,
      "grad_norm": 4.000870227813721,
      "learning_rate": 2e-05,
      "loss": 0.49,
      "step": 4230
    },
    {
      "epoch": 1.346886912325286,
      "grad_norm": 3.256464719772339,
      "learning_rate": 2e-05,
      "loss": 0.4603,
      "step": 4240
    },
    {
      "epoch": 1.3500635324015247,
      "grad_norm": 3.785737991333008,
      "learning_rate": 2e-05,
      "loss": 0.4677,
      "step": 4250
    },
    {
      "epoch": 1.3532401524777637,
      "grad_norm": 3.8543570041656494,
      "learning_rate": 2e-05,
      "loss": 0.492,
      "step": 4260
    },
    {
      "epoch": 1.3564167725540026,
      "grad_norm": 3.5913829803466797,
      "learning_rate": 2e-05,
      "loss": 0.5112,
      "step": 4270
    },
    {
      "epoch": 1.3595933926302415,
      "grad_norm": 4.662125110626221,
      "learning_rate": 2e-05,
      "loss": 0.469,
      "step": 4280
    },
    {
      "epoch": 1.3627700127064803,
      "grad_norm": 3.856447458267212,
      "learning_rate": 2e-05,
      "loss": 0.4798,
      "step": 4290
    },
    {
      "epoch": 1.3659466327827192,
      "grad_norm": 3.777315855026245,
      "learning_rate": 2e-05,
      "loss": 0.5165,
      "step": 4300
    },
    {
      "epoch": 1.369123252858958,
      "grad_norm": 5.84978723526001,
      "learning_rate": 2e-05,
      "loss": 0.4649,
      "step": 4310
    },
    {
      "epoch": 1.372299872935197,
      "grad_norm": 5.233149528503418,
      "learning_rate": 2e-05,
      "loss": 0.4561,
      "step": 4320
    },
    {
      "epoch": 1.3754764930114358,
      "grad_norm": 7.822786331176758,
      "learning_rate": 2e-05,
      "loss": 0.4971,
      "step": 4330
    },
    {
      "epoch": 1.3786531130876747,
      "grad_norm": 6.38986349105835,
      "learning_rate": 2e-05,
      "loss": 0.5022,
      "step": 4340
    },
    {
      "epoch": 1.3818297331639136,
      "grad_norm": 4.259394645690918,
      "learning_rate": 2e-05,
      "loss": 0.4885,
      "step": 4350
    },
    {
      "epoch": 1.3850063532401524,
      "grad_norm": 3.4552900791168213,
      "learning_rate": 2e-05,
      "loss": 0.4755,
      "step": 4360
    },
    {
      "epoch": 1.3881829733163913,
      "grad_norm": 4.775784492492676,
      "learning_rate": 2e-05,
      "loss": 0.4796,
      "step": 4370
    },
    {
      "epoch": 1.3913595933926302,
      "grad_norm": 4.352939605712891,
      "learning_rate": 2e-05,
      "loss": 0.4498,
      "step": 4380
    },
    {
      "epoch": 1.3945362134688692,
      "grad_norm": 3.352900981903076,
      "learning_rate": 2e-05,
      "loss": 0.469,
      "step": 4390
    },
    {
      "epoch": 1.397712833545108,
      "grad_norm": 3.3636786937713623,
      "learning_rate": 2e-05,
      "loss": 0.4629,
      "step": 4400
    },
    {
      "epoch": 1.400889453621347,
      "grad_norm": 4.616709232330322,
      "learning_rate": 2e-05,
      "loss": 0.4829,
      "step": 4410
    },
    {
      "epoch": 1.400889453621347,
      "eval_loss": 1.8297613859176636,
      "eval_mse": 1.8288091743368386,
      "eval_pearson": 0.45165560934869575,
      "eval_runtime": 20.9612,
      "eval_samples_per_second": 1028.568,
      "eval_spearmanr": 0.44125606286840957,
      "eval_steps_per_second": 4.055,
      "step": 4410
    },
    {
      "epoch": 1.4040660736975858,
      "grad_norm": 3.5320558547973633,
      "learning_rate": 2e-05,
      "loss": 0.502,
      "step": 4420
    },
    {
      "epoch": 1.4072426937738247,
      "grad_norm": 3.203502655029297,
      "learning_rate": 2e-05,
      "loss": 0.4467,
      "step": 4430
    },
    {
      "epoch": 1.4104193138500636,
      "grad_norm": 5.960835933685303,
      "learning_rate": 2e-05,
      "loss": 0.5061,
      "step": 4440
    },
    {
      "epoch": 1.4135959339263025,
      "grad_norm": 6.529360294342041,
      "learning_rate": 2e-05,
      "loss": 0.4938,
      "step": 4450
    },
    {
      "epoch": 1.4167725540025413,
      "grad_norm": 4.023168087005615,
      "learning_rate": 2e-05,
      "loss": 0.4588,
      "step": 4460
    },
    {
      "epoch": 1.4199491740787802,
      "grad_norm": 3.5245656967163086,
      "learning_rate": 2e-05,
      "loss": 0.4487,
      "step": 4470
    },
    {
      "epoch": 1.423125794155019,
      "grad_norm": 5.870619297027588,
      "learning_rate": 2e-05,
      "loss": 0.4736,
      "step": 4480
    },
    {
      "epoch": 1.426302414231258,
      "grad_norm": 4.9798431396484375,
      "learning_rate": 2e-05,
      "loss": 0.4539,
      "step": 4490
    },
    {
      "epoch": 1.4294790343074968,
      "grad_norm": 5.953068733215332,
      "learning_rate": 2e-05,
      "loss": 0.47,
      "step": 4500
    },
    {
      "epoch": 1.4326556543837357,
      "grad_norm": 5.815986633300781,
      "learning_rate": 2e-05,
      "loss": 0.4486,
      "step": 4510
    },
    {
      "epoch": 1.4358322744599745,
      "grad_norm": 4.534429550170898,
      "learning_rate": 2e-05,
      "loss": 0.4708,
      "step": 4520
    },
    {
      "epoch": 1.4390088945362134,
      "grad_norm": 3.618729591369629,
      "learning_rate": 2e-05,
      "loss": 0.4683,
      "step": 4530
    },
    {
      "epoch": 1.4421855146124525,
      "grad_norm": 5.701501369476318,
      "learning_rate": 2e-05,
      "loss": 0.4587,
      "step": 4540
    },
    {
      "epoch": 1.4453621346886911,
      "grad_norm": 3.8840079307556152,
      "learning_rate": 2e-05,
      "loss": 0.4919,
      "step": 4550
    },
    {
      "epoch": 1.4485387547649302,
      "grad_norm": 3.2747464179992676,
      "learning_rate": 2e-05,
      "loss": 0.4663,
      "step": 4560
    },
    {
      "epoch": 1.4517153748411689,
      "grad_norm": 3.4552788734436035,
      "learning_rate": 2e-05,
      "loss": 0.4622,
      "step": 4570
    },
    {
      "epoch": 1.454891994917408,
      "grad_norm": 3.883559226989746,
      "learning_rate": 2e-05,
      "loss": 0.4741,
      "step": 4580
    },
    {
      "epoch": 1.4580686149936468,
      "grad_norm": 3.941246271133423,
      "learning_rate": 2e-05,
      "loss": 0.4394,
      "step": 4590
    },
    {
      "epoch": 1.4612452350698857,
      "grad_norm": 4.051551342010498,
      "learning_rate": 2e-05,
      "loss": 0.4657,
      "step": 4600
    },
    {
      "epoch": 1.4644218551461246,
      "grad_norm": 10.07449722290039,
      "learning_rate": 2e-05,
      "loss": 0.491,
      "step": 4610
    },
    {
      "epoch": 1.4675984752223634,
      "grad_norm": 5.049393177032471,
      "learning_rate": 2e-05,
      "loss": 0.4897,
      "step": 4620
    },
    {
      "epoch": 1.4707750952986023,
      "grad_norm": 3.5650506019592285,
      "learning_rate": 2e-05,
      "loss": 0.4672,
      "step": 4630
    },
    {
      "epoch": 1.4739517153748412,
      "grad_norm": 4.712182521820068,
      "learning_rate": 2e-05,
      "loss": 0.4708,
      "step": 4640
    },
    {
      "epoch": 1.47712833545108,
      "grad_norm": 3.7638566493988037,
      "learning_rate": 2e-05,
      "loss": 0.4305,
      "step": 4650
    },
    {
      "epoch": 1.4803049555273189,
      "grad_norm": 3.289870262145996,
      "learning_rate": 2e-05,
      "loss": 0.4322,
      "step": 4660
    },
    {
      "epoch": 1.4834815756035578,
      "grad_norm": 5.279167175292969,
      "learning_rate": 2e-05,
      "loss": 0.465,
      "step": 4670
    },
    {
      "epoch": 1.4866581956797966,
      "grad_norm": 6.262819290161133,
      "learning_rate": 2e-05,
      "loss": 0.473,
      "step": 4680
    },
    {
      "epoch": 1.4898348157560357,
      "grad_norm": 7.965264797210693,
      "learning_rate": 2e-05,
      "loss": 0.4783,
      "step": 4690
    },
    {
      "epoch": 1.4930114358322744,
      "grad_norm": 4.786180019378662,
      "learning_rate": 2e-05,
      "loss": 0.4394,
      "step": 4700
    },
    {
      "epoch": 1.4961880559085134,
      "grad_norm": 3.1750645637512207,
      "learning_rate": 2e-05,
      "loss": 0.4526,
      "step": 4710
    },
    {
      "epoch": 1.499364675984752,
      "grad_norm": 5.034577369689941,
      "learning_rate": 2e-05,
      "loss": 0.4572,
      "step": 4720
    },
    {
      "epoch": 1.5009529860228716,
      "eval_loss": 1.6555919647216797,
      "eval_mse": 1.6543964457423435,
      "eval_pearson": 0.43991053987735435,
      "eval_runtime": 21.1445,
      "eval_samples_per_second": 1019.652,
      "eval_spearmanr": 0.43242328592904267,
      "eval_steps_per_second": 4.02,
      "step": 4725
    },
    {
      "epoch": 1.5025412960609912,
      "grad_norm": 3.1913676261901855,
      "learning_rate": 2e-05,
      "loss": 0.4856,
      "step": 4730
    },
    {
      "epoch": 1.5057179161372298,
      "grad_norm": 4.615609645843506,
      "learning_rate": 2e-05,
      "loss": 0.4427,
      "step": 4740
    },
    {
      "epoch": 1.508894536213469,
      "grad_norm": 3.8084733486175537,
      "learning_rate": 2e-05,
      "loss": 0.4496,
      "step": 4750
    },
    {
      "epoch": 1.5120711562897078,
      "grad_norm": 6.332023620605469,
      "learning_rate": 2e-05,
      "loss": 0.4959,
      "step": 4760
    },
    {
      "epoch": 1.5152477763659467,
      "grad_norm": 4.3846588134765625,
      "learning_rate": 2e-05,
      "loss": 0.4474,
      "step": 4770
    },
    {
      "epoch": 1.5184243964421855,
      "grad_norm": 6.731326580047607,
      "learning_rate": 2e-05,
      "loss": 0.457,
      "step": 4780
    },
    {
      "epoch": 1.5216010165184244,
      "grad_norm": 3.7773451805114746,
      "learning_rate": 2e-05,
      "loss": 0.4938,
      "step": 4790
    },
    {
      "epoch": 1.5247776365946633,
      "grad_norm": 4.191238880157471,
      "learning_rate": 2e-05,
      "loss": 0.4555,
      "step": 4800
    },
    {
      "epoch": 1.5279542566709021,
      "grad_norm": 4.17323112487793,
      "learning_rate": 2e-05,
      "loss": 0.4513,
      "step": 4810
    },
    {
      "epoch": 1.531130876747141,
      "grad_norm": 3.6412322521209717,
      "learning_rate": 2e-05,
      "loss": 0.4633,
      "step": 4820
    },
    {
      "epoch": 1.5343074968233799,
      "grad_norm": 4.6997809410095215,
      "learning_rate": 2e-05,
      "loss": 0.4319,
      "step": 4830
    },
    {
      "epoch": 1.537484116899619,
      "grad_norm": 4.25459623336792,
      "learning_rate": 2e-05,
      "loss": 0.4733,
      "step": 4840
    },
    {
      "epoch": 1.5406607369758576,
      "grad_norm": 3.3784306049346924,
      "learning_rate": 2e-05,
      "loss": 0.4223,
      "step": 4850
    },
    {
      "epoch": 1.5438373570520967,
      "grad_norm": 3.5753965377807617,
      "learning_rate": 2e-05,
      "loss": 0.4279,
      "step": 4860
    },
    {
      "epoch": 1.5470139771283353,
      "grad_norm": 3.594855785369873,
      "learning_rate": 2e-05,
      "loss": 0.4163,
      "step": 4870
    },
    {
      "epoch": 1.5501905972045744,
      "grad_norm": 2.9432730674743652,
      "learning_rate": 2e-05,
      "loss": 0.4586,
      "step": 4880
    },
    {
      "epoch": 1.553367217280813,
      "grad_norm": 3.6772942543029785,
      "learning_rate": 2e-05,
      "loss": 0.4306,
      "step": 4890
    },
    {
      "epoch": 1.5565438373570522,
      "grad_norm": 4.239747047424316,
      "learning_rate": 2e-05,
      "loss": 0.4292,
      "step": 4900
    },
    {
      "epoch": 1.559720457433291,
      "grad_norm": 4.793656826019287,
      "learning_rate": 2e-05,
      "loss": 0.4653,
      "step": 4910
    },
    {
      "epoch": 1.5628970775095299,
      "grad_norm": 4.219391822814941,
      "learning_rate": 2e-05,
      "loss": 0.4243,
      "step": 4920
    },
    {
      "epoch": 1.5660736975857688,
      "grad_norm": 3.347705364227295,
      "learning_rate": 2e-05,
      "loss": 0.4391,
      "step": 4930
    },
    {
      "epoch": 1.5692503176620076,
      "grad_norm": 2.874540090560913,
      "learning_rate": 2e-05,
      "loss": 0.4586,
      "step": 4940
    },
    {
      "epoch": 1.5724269377382465,
      "grad_norm": 4.374900817871094,
      "learning_rate": 2e-05,
      "loss": 0.4516,
      "step": 4950
    },
    {
      "epoch": 1.5756035578144854,
      "grad_norm": 3.4908342361450195,
      "learning_rate": 2e-05,
      "loss": 0.4284,
      "step": 4960
    },
    {
      "epoch": 1.5787801778907242,
      "grad_norm": 3.0620923042297363,
      "learning_rate": 2e-05,
      "loss": 0.3971,
      "step": 4970
    },
    {
      "epoch": 1.581956797966963,
      "grad_norm": 3.4375438690185547,
      "learning_rate": 2e-05,
      "loss": 0.4428,
      "step": 4980
    },
    {
      "epoch": 1.5851334180432022,
      "grad_norm": 3.538367509841919,
      "learning_rate": 2e-05,
      "loss": 0.4527,
      "step": 4990
    },
    {
      "epoch": 1.5883100381194408,
      "grad_norm": 3.07437801361084,
      "learning_rate": 2e-05,
      "loss": 0.4591,
      "step": 5000
    },
    {
      "epoch": 1.59148665819568,
      "grad_norm": 4.576995849609375,
      "learning_rate": 2e-05,
      "loss": 0.4094,
      "step": 5010
    },
    {
      "epoch": 1.5946632782719186,
      "grad_norm": 4.769122123718262,
      "learning_rate": 2e-05,
      "loss": 0.4399,
      "step": 5020
    },
    {
      "epoch": 1.5978398983481577,
      "grad_norm": 8.146119117736816,
      "learning_rate": 2e-05,
      "loss": 0.4341,
      "step": 5030
    },
    {
      "epoch": 1.6010165184243963,
      "grad_norm": 4.86060094833374,
      "learning_rate": 2e-05,
      "loss": 0.4597,
      "step": 5040
    },
    {
      "epoch": 1.6010165184243963,
      "eval_loss": 1.7392070293426514,
      "eval_mse": 1.738632166695285,
      "eval_pearson": 0.45633494976625744,
      "eval_runtime": 20.9541,
      "eval_samples_per_second": 1028.916,
      "eval_spearmanr": 0.4461722596425553,
      "eval_steps_per_second": 4.056,
      "step": 5040
    },
    {
      "epoch": 1.6041931385006354,
      "grad_norm": 4.301446914672852,
      "learning_rate": 2e-05,
      "loss": 0.4194,
      "step": 5050
    },
    {
      "epoch": 1.6073697585768743,
      "grad_norm": 4.412772178649902,
      "learning_rate": 2e-05,
      "loss": 0.4479,
      "step": 5060
    },
    {
      "epoch": 1.6105463786531131,
      "grad_norm": 5.283125400543213,
      "learning_rate": 2e-05,
      "loss": 0.4657,
      "step": 5070
    },
    {
      "epoch": 1.613722998729352,
      "grad_norm": 3.341946601867676,
      "learning_rate": 2e-05,
      "loss": 0.4421,
      "step": 5080
    },
    {
      "epoch": 1.6168996188055909,
      "grad_norm": 3.0117690563201904,
      "learning_rate": 2e-05,
      "loss": 0.4,
      "step": 5090
    },
    {
      "epoch": 1.6200762388818297,
      "grad_norm": 3.743648052215576,
      "learning_rate": 2e-05,
      "loss": 0.427,
      "step": 5100
    },
    {
      "epoch": 1.6232528589580686,
      "grad_norm": 4.107281684875488,
      "learning_rate": 2e-05,
      "loss": 0.4401,
      "step": 5110
    },
    {
      "epoch": 1.6264294790343075,
      "grad_norm": 2.9857089519500732,
      "learning_rate": 2e-05,
      "loss": 0.4741,
      "step": 5120
    },
    {
      "epoch": 1.6296060991105463,
      "grad_norm": 4.080428123474121,
      "learning_rate": 2e-05,
      "loss": 0.4723,
      "step": 5130
    },
    {
      "epoch": 1.6327827191867854,
      "grad_norm": 6.171986103057861,
      "learning_rate": 2e-05,
      "loss": 0.4255,
      "step": 5140
    },
    {
      "epoch": 1.635959339263024,
      "grad_norm": 3.5114099979400635,
      "learning_rate": 2e-05,
      "loss": 0.4437,
      "step": 5150
    },
    {
      "epoch": 1.6391359593392631,
      "grad_norm": 4.263001441955566,
      "learning_rate": 2e-05,
      "loss": 0.4503,
      "step": 5160
    },
    {
      "epoch": 1.6423125794155018,
      "grad_norm": 3.345731019973755,
      "learning_rate": 2e-05,
      "loss": 0.4376,
      "step": 5170
    },
    {
      "epoch": 1.6454891994917409,
      "grad_norm": 4.602128028869629,
      "learning_rate": 2e-05,
      "loss": 0.413,
      "step": 5180
    },
    {
      "epoch": 1.6486658195679795,
      "grad_norm": 3.6260874271392822,
      "learning_rate": 2e-05,
      "loss": 0.3974,
      "step": 5190
    },
    {
      "epoch": 1.6518424396442186,
      "grad_norm": 4.02785062789917,
      "learning_rate": 2e-05,
      "loss": 0.4653,
      "step": 5200
    },
    {
      "epoch": 1.6550190597204575,
      "grad_norm": 3.072639226913452,
      "learning_rate": 2e-05,
      "loss": 0.4309,
      "step": 5210
    },
    {
      "epoch": 1.6581956797966964,
      "grad_norm": 4.461144924163818,
      "learning_rate": 2e-05,
      "loss": 0.4219,
      "step": 5220
    },
    {
      "epoch": 1.6613722998729352,
      "grad_norm": 3.819798707962036,
      "learning_rate": 2e-05,
      "loss": 0.4525,
      "step": 5230
    },
    {
      "epoch": 1.664548919949174,
      "grad_norm": 6.1277360916137695,
      "learning_rate": 2e-05,
      "loss": 0.4798,
      "step": 5240
    },
    {
      "epoch": 1.667725540025413,
      "grad_norm": 3.8212924003601074,
      "learning_rate": 2e-05,
      "loss": 0.4291,
      "step": 5250
    },
    {
      "epoch": 1.6709021601016518,
      "grad_norm": 4.325523376464844,
      "learning_rate": 2e-05,
      "loss": 0.4063,
      "step": 5260
    },
    {
      "epoch": 1.6740787801778907,
      "grad_norm": 3.91949725151062,
      "learning_rate": 2e-05,
      "loss": 0.4379,
      "step": 5270
    },
    {
      "epoch": 1.6772554002541296,
      "grad_norm": 3.771449565887451,
      "learning_rate": 2e-05,
      "loss": 0.4511,
      "step": 5280
    },
    {
      "epoch": 1.6804320203303686,
      "grad_norm": 4.773737907409668,
      "learning_rate": 2e-05,
      "loss": 0.4457,
      "step": 5290
    },
    {
      "epoch": 1.6836086404066073,
      "grad_norm": 4.766421794891357,
      "learning_rate": 2e-05,
      "loss": 0.4327,
      "step": 5300
    },
    {
      "epoch": 1.6867852604828464,
      "grad_norm": 5.279891014099121,
      "learning_rate": 2e-05,
      "loss": 0.4276,
      "step": 5310
    },
    {
      "epoch": 1.689961880559085,
      "grad_norm": 3.627497911453247,
      "learning_rate": 2e-05,
      "loss": 0.3868,
      "step": 5320
    },
    {
      "epoch": 1.6931385006353241,
      "grad_norm": 2.9355521202087402,
      "learning_rate": 2e-05,
      "loss": 0.4572,
      "step": 5330
    },
    {
      "epoch": 1.6963151207115628,
      "grad_norm": 2.896610975265503,
      "learning_rate": 2e-05,
      "loss": 0.4394,
      "step": 5340
    },
    {
      "epoch": 1.6994917407878019,
      "grad_norm": 4.318401336669922,
      "learning_rate": 2e-05,
      "loss": 0.4494,
      "step": 5350
    },
    {
      "epoch": 1.7010800508259212,
      "eval_loss": 1.8237440586090088,
      "eval_mse": 1.8224129378021539,
      "eval_pearson": 0.45039780223977455,
      "eval_runtime": 21.0525,
      "eval_samples_per_second": 1024.107,
      "eval_spearmanr": 0.4418371056648854,
      "eval_steps_per_second": 4.038,
      "step": 5355
    },
    {
      "epoch": 1.7026683608640405,
      "grad_norm": 3.2672486305236816,
      "learning_rate": 2e-05,
      "loss": 0.3812,
      "step": 5360
    },
    {
      "epoch": 1.7058449809402796,
      "grad_norm": 3.2817161083221436,
      "learning_rate": 2e-05,
      "loss": 0.4222,
      "step": 5370
    },
    {
      "epoch": 1.7090216010165185,
      "grad_norm": 6.19587516784668,
      "learning_rate": 2e-05,
      "loss": 0.4419,
      "step": 5380
    },
    {
      "epoch": 1.7121982210927573,
      "grad_norm": 3.789090633392334,
      "learning_rate": 2e-05,
      "loss": 0.4326,
      "step": 5390
    },
    {
      "epoch": 1.7153748411689962,
      "grad_norm": 4.534907341003418,
      "learning_rate": 2e-05,
      "loss": 0.4234,
      "step": 5400
    },
    {
      "epoch": 1.718551461245235,
      "grad_norm": 3.5682528018951416,
      "learning_rate": 2e-05,
      "loss": 0.418,
      "step": 5410
    },
    {
      "epoch": 1.721728081321474,
      "grad_norm": 5.344198226928711,
      "learning_rate": 2e-05,
      "loss": 0.4209,
      "step": 5420
    },
    {
      "epoch": 1.7249047013977128,
      "grad_norm": 4.714031219482422,
      "learning_rate": 2e-05,
      "loss": 0.3991,
      "step": 5430
    },
    {
      "epoch": 1.7280813214739519,
      "grad_norm": 5.35324764251709,
      "learning_rate": 2e-05,
      "loss": 0.4362,
      "step": 5440
    },
    {
      "epoch": 1.7312579415501905,
      "grad_norm": 4.344095706939697,
      "learning_rate": 2e-05,
      "loss": 0.3962,
      "step": 5450
    },
    {
      "epoch": 1.7344345616264296,
      "grad_norm": 4.6620049476623535,
      "learning_rate": 2e-05,
      "loss": 0.4423,
      "step": 5460
    },
    {
      "epoch": 1.7376111817026683,
      "grad_norm": 6.955480575561523,
      "learning_rate": 2e-05,
      "loss": 0.4023,
      "step": 5470
    },
    {
      "epoch": 1.7407878017789074,
      "grad_norm": 4.089348316192627,
      "learning_rate": 2e-05,
      "loss": 0.3933,
      "step": 5480
    },
    {
      "epoch": 1.743964421855146,
      "grad_norm": 4.441503524780273,
      "learning_rate": 2e-05,
      "loss": 0.4146,
      "step": 5490
    },
    {
      "epoch": 1.747141041931385,
      "grad_norm": 3.0330312252044678,
      "learning_rate": 2e-05,
      "loss": 0.453,
      "step": 5500
    },
    {
      "epoch": 1.7503176620076237,
      "grad_norm": 4.955244064331055,
      "learning_rate": 2e-05,
      "loss": 0.4139,
      "step": 5510
    },
    {
      "epoch": 1.7534942820838628,
      "grad_norm": 5.954678535461426,
      "learning_rate": 2e-05,
      "loss": 0.4087,
      "step": 5520
    },
    {
      "epoch": 1.7566709021601017,
      "grad_norm": 3.7548365592956543,
      "learning_rate": 2e-05,
      "loss": 0.419,
      "step": 5530
    },
    {
      "epoch": 1.7598475222363406,
      "grad_norm": 3.4777631759643555,
      "learning_rate": 2e-05,
      "loss": 0.3994,
      "step": 5540
    },
    {
      "epoch": 1.7630241423125794,
      "grad_norm": 5.090317726135254,
      "learning_rate": 2e-05,
      "loss": 0.4058,
      "step": 5550
    },
    {
      "epoch": 1.7662007623888183,
      "grad_norm": 4.739583969116211,
      "learning_rate": 2e-05,
      "loss": 0.4047,
      "step": 5560
    },
    {
      "epoch": 1.7693773824650572,
      "grad_norm": 4.71114444732666,
      "learning_rate": 2e-05,
      "loss": 0.4437,
      "step": 5570
    },
    {
      "epoch": 1.772554002541296,
      "grad_norm": 4.770493030548096,
      "learning_rate": 2e-05,
      "loss": 0.3815,
      "step": 5580
    },
    {
      "epoch": 1.775730622617535,
      "grad_norm": 3.934793710708618,
      "learning_rate": 2e-05,
      "loss": 0.427,
      "step": 5590
    },
    {
      "epoch": 1.7789072426937738,
      "grad_norm": 3.7614080905914307,
      "learning_rate": 2e-05,
      "loss": 0.4309,
      "step": 5600
    },
    {
      "epoch": 1.7820838627700128,
      "grad_norm": 3.3825860023498535,
      "learning_rate": 2e-05,
      "loss": 0.4451,
      "step": 5610
    },
    {
      "epoch": 1.7852604828462515,
      "grad_norm": 4.356903553009033,
      "learning_rate": 2e-05,
      "loss": 0.4188,
      "step": 5620
    },
    {
      "epoch": 1.7884371029224906,
      "grad_norm": 3.534095287322998,
      "learning_rate": 2e-05,
      "loss": 0.4255,
      "step": 5630
    },
    {
      "epoch": 1.7916137229987292,
      "grad_norm": 4.878359794616699,
      "learning_rate": 2e-05,
      "loss": 0.3989,
      "step": 5640
    },
    {
      "epoch": 1.7947903430749683,
      "grad_norm": 3.5004515647888184,
      "learning_rate": 2e-05,
      "loss": 0.4139,
      "step": 5650
    },
    {
      "epoch": 1.797966963151207,
      "grad_norm": 4.319774627685547,
      "learning_rate": 2e-05,
      "loss": 0.4288,
      "step": 5660
    },
    {
      "epoch": 1.801143583227446,
      "grad_norm": 4.423202037811279,
      "learning_rate": 2e-05,
      "loss": 0.4193,
      "step": 5670
    },
    {
      "epoch": 1.801143583227446,
      "eval_loss": 1.68681001663208,
      "eval_mse": 1.6851568478379046,
      "eval_pearson": 0.41683739443853357,
      "eval_runtime": 21.0614,
      "eval_samples_per_second": 1023.673,
      "eval_spearmanr": 0.4115760145791074,
      "eval_steps_per_second": 4.036,
      "step": 5670
    },
    {
      "epoch": 1.804320203303685,
      "grad_norm": 3.2236812114715576,
      "learning_rate": 2e-05,
      "loss": 0.3989,
      "step": 5680
    },
    {
      "epoch": 1.8074968233799238,
      "grad_norm": 3.9231760501861572,
      "learning_rate": 2e-05,
      "loss": 0.4029,
      "step": 5690
    },
    {
      "epoch": 1.8106734434561627,
      "grad_norm": 2.7821433544158936,
      "learning_rate": 2e-05,
      "loss": 0.4171,
      "step": 5700
    },
    {
      "epoch": 1.8138500635324015,
      "grad_norm": 3.4140899181365967,
      "learning_rate": 2e-05,
      "loss": 0.411,
      "step": 5710
    },
    {
      "epoch": 1.8170266836086404,
      "grad_norm": 4.913980007171631,
      "learning_rate": 2e-05,
      "loss": 0.4176,
      "step": 5720
    },
    {
      "epoch": 1.8202033036848793,
      "grad_norm": 3.5098752975463867,
      "learning_rate": 2e-05,
      "loss": 0.3812,
      "step": 5730
    },
    {
      "epoch": 1.8233799237611181,
      "grad_norm": 4.558460235595703,
      "learning_rate": 2e-05,
      "loss": 0.4163,
      "step": 5740
    },
    {
      "epoch": 1.826556543837357,
      "grad_norm": 3.5285844802856445,
      "learning_rate": 2e-05,
      "loss": 0.3788,
      "step": 5750
    },
    {
      "epoch": 1.829733163913596,
      "grad_norm": 4.524538516998291,
      "learning_rate": 2e-05,
      "loss": 0.3735,
      "step": 5760
    },
    {
      "epoch": 1.8329097839898347,
      "grad_norm": 4.154550552368164,
      "learning_rate": 2e-05,
      "loss": 0.4182,
      "step": 5770
    },
    {
      "epoch": 1.8360864040660738,
      "grad_norm": 4.319825172424316,
      "learning_rate": 2e-05,
      "loss": 0.4092,
      "step": 5780
    },
    {
      "epoch": 1.8392630241423125,
      "grad_norm": 3.9304986000061035,
      "learning_rate": 2e-05,
      "loss": 0.4162,
      "step": 5790
    },
    {
      "epoch": 1.8424396442185516,
      "grad_norm": 4.088283061981201,
      "learning_rate": 2e-05,
      "loss": 0.3856,
      "step": 5800
    },
    {
      "epoch": 1.8456162642947902,
      "grad_norm": 4.287428379058838,
      "learning_rate": 2e-05,
      "loss": 0.3837,
      "step": 5810
    },
    {
      "epoch": 1.8487928843710293,
      "grad_norm": 3.094475507736206,
      "learning_rate": 2e-05,
      "loss": 0.39,
      "step": 5820
    },
    {
      "epoch": 1.8519695044472682,
      "grad_norm": 3.4195642471313477,
      "learning_rate": 2e-05,
      "loss": 0.406,
      "step": 5830
    },
    {
      "epoch": 1.855146124523507,
      "grad_norm": 3.6041176319122314,
      "learning_rate": 2e-05,
      "loss": 0.387,
      "step": 5840
    },
    {
      "epoch": 1.858322744599746,
      "grad_norm": 3.6559321880340576,
      "learning_rate": 2e-05,
      "loss": 0.4057,
      "step": 5850
    },
    {
      "epoch": 1.8614993646759848,
      "grad_norm": 5.060196876525879,
      "learning_rate": 2e-05,
      "loss": 0.4119,
      "step": 5860
    },
    {
      "epoch": 1.8646759847522236,
      "grad_norm": 3.1578469276428223,
      "learning_rate": 2e-05,
      "loss": 0.386,
      "step": 5870
    },
    {
      "epoch": 1.8678526048284625,
      "grad_norm": 3.485340118408203,
      "learning_rate": 2e-05,
      "loss": 0.3798,
      "step": 5880
    },
    {
      "epoch": 1.8710292249047014,
      "grad_norm": 4.364165782928467,
      "learning_rate": 2e-05,
      "loss": 0.3995,
      "step": 5890
    },
    {
      "epoch": 1.8742058449809402,
      "grad_norm": 3.1417689323425293,
      "learning_rate": 2e-05,
      "loss": 0.4183,
      "step": 5900
    },
    {
      "epoch": 1.8773824650571793,
      "grad_norm": 3.707756280899048,
      "learning_rate": 2e-05,
      "loss": 0.3973,
      "step": 5910
    },
    {
      "epoch": 1.880559085133418,
      "grad_norm": 4.021154880523682,
      "learning_rate": 2e-05,
      "loss": 0.4028,
      "step": 5920
    },
    {
      "epoch": 1.883735705209657,
      "grad_norm": 5.453032970428467,
      "learning_rate": 2e-05,
      "loss": 0.4054,
      "step": 5930
    },
    {
      "epoch": 1.8869123252858957,
      "grad_norm": 2.985502243041992,
      "learning_rate": 2e-05,
      "loss": 0.3753,
      "step": 5940
    },
    {
      "epoch": 1.8900889453621348,
      "grad_norm": 3.7939605712890625,
      "learning_rate": 2e-05,
      "loss": 0.3964,
      "step": 5950
    },
    {
      "epoch": 1.8932655654383734,
      "grad_norm": 3.089805841445923,
      "learning_rate": 2e-05,
      "loss": 0.4039,
      "step": 5960
    },
    {
      "epoch": 1.8964421855146125,
      "grad_norm": 3.0438098907470703,
      "learning_rate": 2e-05,
      "loss": 0.3894,
      "step": 5970
    },
    {
      "epoch": 1.8996188055908514,
      "grad_norm": 5.146917819976807,
      "learning_rate": 2e-05,
      "loss": 0.3692,
      "step": 5980
    },
    {
      "epoch": 1.9012071156289707,
      "eval_loss": 2.409196615219116,
      "eval_mse": 2.407901274494426,
      "eval_pearson": 0.4152282149623877,
      "eval_runtime": 20.9861,
      "eval_samples_per_second": 1027.348,
      "eval_spearmanr": 0.4103543708396147,
      "eval_steps_per_second": 4.05,
      "step": 5985
    },
    {
      "epoch": 1.9027954256670903,
      "grad_norm": 5.082085609436035,
      "learning_rate": 2e-05,
      "loss": 0.3897,
      "step": 5990
    },
    {
      "epoch": 1.9059720457433291,
      "grad_norm": 4.291292190551758,
      "learning_rate": 2e-05,
      "loss": 0.3877,
      "step": 6000
    },
    {
      "epoch": 1.909148665819568,
      "grad_norm": 3.3044960498809814,
      "learning_rate": 2e-05,
      "loss": 0.3965,
      "step": 6010
    },
    {
      "epoch": 1.9123252858958069,
      "grad_norm": 3.468862771987915,
      "learning_rate": 2e-05,
      "loss": 0.3857,
      "step": 6020
    },
    {
      "epoch": 1.9155019059720457,
      "grad_norm": 2.9843127727508545,
      "learning_rate": 2e-05,
      "loss": 0.406,
      "step": 6030
    },
    {
      "epoch": 1.9186785260482846,
      "grad_norm": 3.072406768798828,
      "learning_rate": 2e-05,
      "loss": 0.3629,
      "step": 6040
    },
    {
      "epoch": 1.9218551461245235,
      "grad_norm": 4.376337051391602,
      "learning_rate": 2e-05,
      "loss": 0.387,
      "step": 6050
    },
    {
      "epoch": 1.9250317662007626,
      "grad_norm": 4.359591960906982,
      "learning_rate": 2e-05,
      "loss": 0.384,
      "step": 6060
    },
    {
      "epoch": 1.9282083862770012,
      "grad_norm": 4.222086429595947,
      "learning_rate": 2e-05,
      "loss": 0.4024,
      "step": 6070
    },
    {
      "epoch": 1.9313850063532403,
      "grad_norm": 2.8981738090515137,
      "learning_rate": 2e-05,
      "loss": 0.3766,
      "step": 6080
    },
    {
      "epoch": 1.934561626429479,
      "grad_norm": 4.076464653015137,
      "learning_rate": 2e-05,
      "loss": 0.3803,
      "step": 6090
    },
    {
      "epoch": 1.937738246505718,
      "grad_norm": 3.141685962677002,
      "learning_rate": 2e-05,
      "loss": 0.3623,
      "step": 6100
    },
    {
      "epoch": 1.9409148665819567,
      "grad_norm": 3.2237164974212646,
      "learning_rate": 2e-05,
      "loss": 0.3943,
      "step": 6110
    },
    {
      "epoch": 1.9440914866581958,
      "grad_norm": 3.8109309673309326,
      "learning_rate": 2e-05,
      "loss": 0.3868,
      "step": 6120
    },
    {
      "epoch": 1.9472681067344344,
      "grad_norm": 6.68056058883667,
      "learning_rate": 2e-05,
      "loss": 0.3676,
      "step": 6130
    },
    {
      "epoch": 1.9504447268106735,
      "grad_norm": 5.41143274307251,
      "learning_rate": 2e-05,
      "loss": 0.3593,
      "step": 6140
    },
    {
      "epoch": 1.9536213468869124,
      "grad_norm": 3.7187087535858154,
      "learning_rate": 2e-05,
      "loss": 0.3778,
      "step": 6150
    },
    {
      "epoch": 1.9567979669631512,
      "grad_norm": 3.861199378967285,
      "learning_rate": 2e-05,
      "loss": 0.3913,
      "step": 6160
    },
    {
      "epoch": 1.95997458703939,
      "grad_norm": 3.7114319801330566,
      "learning_rate": 2e-05,
      "loss": 0.3868,
      "step": 6170
    },
    {
      "epoch": 1.963151207115629,
      "grad_norm": 3.36190128326416,
      "learning_rate": 2e-05,
      "loss": 0.3715,
      "step": 6180
    },
    {
      "epoch": 1.9663278271918678,
      "grad_norm": 3.8180882930755615,
      "learning_rate": 2e-05,
      "loss": 0.3761,
      "step": 6190
    },
    {
      "epoch": 1.9695044472681067,
      "grad_norm": 4.740658760070801,
      "learning_rate": 2e-05,
      "loss": 0.4025,
      "step": 6200
    },
    {
      "epoch": 1.9726810673443456,
      "grad_norm": 4.744024276733398,
      "learning_rate": 2e-05,
      "loss": 0.3613,
      "step": 6210
    },
    {
      "epoch": 1.9758576874205844,
      "grad_norm": 3.364912748336792,
      "learning_rate": 2e-05,
      "loss": 0.4029,
      "step": 6220
    },
    {
      "epoch": 1.9790343074968235,
      "grad_norm": 3.6376543045043945,
      "learning_rate": 2e-05,
      "loss": 0.396,
      "step": 6230
    },
    {
      "epoch": 1.9822109275730622,
      "grad_norm": 3.8720409870147705,
      "learning_rate": 2e-05,
      "loss": 0.3638,
      "step": 6240
    },
    {
      "epoch": 1.9853875476493013,
      "grad_norm": 4.015488624572754,
      "learning_rate": 2e-05,
      "loss": 0.3658,
      "step": 6250
    },
    {
      "epoch": 1.98856416772554,
      "grad_norm": 3.212768793106079,
      "learning_rate": 2e-05,
      "loss": 0.3557,
      "step": 6260
    },
    {
      "epoch": 1.991740787801779,
      "grad_norm": 3.5932555198669434,
      "learning_rate": 2e-05,
      "loss": 0.3892,
      "step": 6270
    },
    {
      "epoch": 1.9949174078780176,
      "grad_norm": 3.367762327194214,
      "learning_rate": 2e-05,
      "loss": 0.3887,
      "step": 6280
    },
    {
      "epoch": 1.9980940279542567,
      "grad_norm": 3.89199161529541,
      "learning_rate": 2e-05,
      "loss": 0.3761,
      "step": 6290
    },
    {
      "epoch": 2.0012706480304954,
      "grad_norm": 3.8482272624969482,
      "learning_rate": 2e-05,
      "loss": 0.3754,
      "step": 6300
    },
    {
      "epoch": 2.0012706480304954,
      "eval_loss": 1.5324081182479858,
      "eval_mse": 1.5309971844110506,
      "eval_pearson": 0.4299425286588892,
      "eval_runtime": 20.9628,
      "eval_samples_per_second": 1028.487,
      "eval_spearmanr": 0.425361909531844,
      "eval_steps_per_second": 4.055,
      "step": 6300
    },
    {
      "epoch": 2.0044472681067345,
      "grad_norm": 4.997333526611328,
      "learning_rate": 2e-05,
      "loss": 0.3461,
      "step": 6310
    },
    {
      "epoch": 2.007623888182973,
      "grad_norm": 2.712430715560913,
      "learning_rate": 2e-05,
      "loss": 0.366,
      "step": 6320
    },
    {
      "epoch": 2.010800508259212,
      "grad_norm": 4.151648998260498,
      "learning_rate": 2e-05,
      "loss": 0.3572,
      "step": 6330
    },
    {
      "epoch": 2.0139771283354513,
      "grad_norm": 4.058177947998047,
      "learning_rate": 2e-05,
      "loss": 0.3464,
      "step": 6340
    },
    {
      "epoch": 2.01715374841169,
      "grad_norm": 2.8518645763397217,
      "learning_rate": 2e-05,
      "loss": 0.3233,
      "step": 6350
    },
    {
      "epoch": 2.020330368487929,
      "grad_norm": 3.0951626300811768,
      "learning_rate": 2e-05,
      "loss": 0.3184,
      "step": 6360
    },
    {
      "epoch": 2.0235069885641677,
      "grad_norm": 3.651493787765503,
      "learning_rate": 2e-05,
      "loss": 0.3354,
      "step": 6370
    },
    {
      "epoch": 2.0266836086404068,
      "grad_norm": 3.7773633003234863,
      "learning_rate": 2e-05,
      "loss": 0.3406,
      "step": 6380
    },
    {
      "epoch": 2.0298602287166454,
      "grad_norm": 2.9919350147247314,
      "learning_rate": 2e-05,
      "loss": 0.3372,
      "step": 6390
    },
    {
      "epoch": 2.0330368487928845,
      "grad_norm": 2.6513047218322754,
      "learning_rate": 2e-05,
      "loss": 0.3279,
      "step": 6400
    },
    {
      "epoch": 2.036213468869123,
      "grad_norm": 3.523130178451538,
      "learning_rate": 2e-05,
      "loss": 0.3288,
      "step": 6410
    },
    {
      "epoch": 2.0393900889453622,
      "grad_norm": 4.870430946350098,
      "learning_rate": 2e-05,
      "loss": 0.3585,
      "step": 6420
    },
    {
      "epoch": 2.042566709021601,
      "grad_norm": 4.219918727874756,
      "learning_rate": 2e-05,
      "loss": 0.3433,
      "step": 6430
    },
    {
      "epoch": 2.04574332909784,
      "grad_norm": 3.2337801456451416,
      "learning_rate": 2e-05,
      "loss": 0.3508,
      "step": 6440
    },
    {
      "epoch": 2.0489199491740786,
      "grad_norm": 3.3010241985321045,
      "learning_rate": 2e-05,
      "loss": 0.3231,
      "step": 6450
    },
    {
      "epoch": 2.0520965692503177,
      "grad_norm": 11.999056816101074,
      "learning_rate": 2e-05,
      "loss": 0.3584,
      "step": 6460
    },
    {
      "epoch": 2.0552731893265563,
      "grad_norm": 4.348931312561035,
      "learning_rate": 2e-05,
      "loss": 0.3309,
      "step": 6470
    },
    {
      "epoch": 2.0584498094027954,
      "grad_norm": 3.5433876514434814,
      "learning_rate": 2e-05,
      "loss": 0.3372,
      "step": 6480
    },
    {
      "epoch": 2.0616264294790345,
      "grad_norm": 3.74810528755188,
      "learning_rate": 2e-05,
      "loss": 0.3524,
      "step": 6490
    },
    {
      "epoch": 2.064803049555273,
      "grad_norm": 3.3859636783599854,
      "learning_rate": 2e-05,
      "loss": 0.3255,
      "step": 6500
    },
    {
      "epoch": 2.0679796696315123,
      "grad_norm": 3.274477958679199,
      "learning_rate": 2e-05,
      "loss": 0.3202,
      "step": 6510
    },
    {
      "epoch": 2.071156289707751,
      "grad_norm": 4.219608783721924,
      "learning_rate": 2e-05,
      "loss": 0.3184,
      "step": 6520
    },
    {
      "epoch": 2.07433290978399,
      "grad_norm": 2.9076881408691406,
      "learning_rate": 2e-05,
      "loss": 0.3328,
      "step": 6530
    },
    {
      "epoch": 2.0775095298602286,
      "grad_norm": 3.516784906387329,
      "learning_rate": 2e-05,
      "loss": 0.344,
      "step": 6540
    },
    {
      "epoch": 2.0806861499364677,
      "grad_norm": 3.033940553665161,
      "learning_rate": 2e-05,
      "loss": 0.3328,
      "step": 6550
    },
    {
      "epoch": 2.0838627700127064,
      "grad_norm": 4.569103717803955,
      "learning_rate": 2e-05,
      "loss": 0.3407,
      "step": 6560
    },
    {
      "epoch": 2.0870393900889455,
      "grad_norm": 3.8842062950134277,
      "learning_rate": 2e-05,
      "loss": 0.3063,
      "step": 6570
    },
    {
      "epoch": 2.090216010165184,
      "grad_norm": 3.1386735439300537,
      "learning_rate": 2e-05,
      "loss": 0.3166,
      "step": 6580
    },
    {
      "epoch": 2.093392630241423,
      "grad_norm": 5.7535223960876465,
      "learning_rate": 2e-05,
      "loss": 0.3265,
      "step": 6590
    },
    {
      "epoch": 2.096569250317662,
      "grad_norm": 3.763667583465576,
      "learning_rate": 2e-05,
      "loss": 0.3963,
      "step": 6600
    },
    {
      "epoch": 2.099745870393901,
      "grad_norm": 4.361255168914795,
      "learning_rate": 2e-05,
      "loss": 0.3138,
      "step": 6610
    },
    {
      "epoch": 2.1013341804320205,
      "eval_loss": 1.7700914144515991,
      "eval_mse": 1.7687017429516356,
      "eval_pearson": 0.4319344920145477,
      "eval_runtime": 21.0732,
      "eval_samples_per_second": 1023.1,
      "eval_spearmanr": 0.42273645335996335,
      "eval_steps_per_second": 4.034,
      "step": 6615
    },
    {
      "epoch": 2.1029224904701396,
      "grad_norm": 5.023381233215332,
      "learning_rate": 2e-05,
      "loss": 0.364,
      "step": 6620
    },
    {
      "epoch": 2.1060991105463787,
      "grad_norm": 3.1682615280151367,
      "learning_rate": 2e-05,
      "loss": 0.3419,
      "step": 6630
    },
    {
      "epoch": 2.1092757306226178,
      "grad_norm": 3.486237049102783,
      "learning_rate": 2e-05,
      "loss": 0.3281,
      "step": 6640
    },
    {
      "epoch": 2.1124523506988564,
      "grad_norm": 4.257582187652588,
      "learning_rate": 2e-05,
      "loss": 0.3391,
      "step": 6650
    },
    {
      "epoch": 2.1156289707750955,
      "grad_norm": 3.73846697807312,
      "learning_rate": 2e-05,
      "loss": 0.3521,
      "step": 6660
    },
    {
      "epoch": 2.118805590851334,
      "grad_norm": 4.4336347579956055,
      "learning_rate": 2e-05,
      "loss": 0.3222,
      "step": 6670
    },
    {
      "epoch": 2.121982210927573,
      "grad_norm": 8.979869842529297,
      "learning_rate": 2e-05,
      "loss": 0.3683,
      "step": 6680
    },
    {
      "epoch": 2.125158831003812,
      "grad_norm": 4.286839962005615,
      "learning_rate": 2e-05,
      "loss": 0.3318,
      "step": 6690
    },
    {
      "epoch": 2.128335451080051,
      "grad_norm": 3.4742276668548584,
      "learning_rate": 2e-05,
      "loss": 0.3128,
      "step": 6700
    },
    {
      "epoch": 2.1315120711562896,
      "grad_norm": 3.4061126708984375,
      "learning_rate": 2e-05,
      "loss": 0.3267,
      "step": 6710
    },
    {
      "epoch": 2.1346886912325287,
      "grad_norm": 5.003134727478027,
      "learning_rate": 2e-05,
      "loss": 0.3413,
      "step": 6720
    },
    {
      "epoch": 2.1378653113087673,
      "grad_norm": 3.373333215713501,
      "learning_rate": 2e-05,
      "loss": 0.3298,
      "step": 6730
    },
    {
      "epoch": 2.1410419313850064,
      "grad_norm": 3.5140609741210938,
      "learning_rate": 2e-05,
      "loss": 0.3316,
      "step": 6740
    },
    {
      "epoch": 2.144218551461245,
      "grad_norm": 4.316902160644531,
      "learning_rate": 2e-05,
      "loss": 0.307,
      "step": 6750
    },
    {
      "epoch": 2.147395171537484,
      "grad_norm": 4.244209289550781,
      "learning_rate": 2e-05,
      "loss": 0.3564,
      "step": 6760
    },
    {
      "epoch": 2.150571791613723,
      "grad_norm": 4.072957515716553,
      "learning_rate": 2e-05,
      "loss": 0.3411,
      "step": 6770
    },
    {
      "epoch": 2.153748411689962,
      "grad_norm": 5.2361273765563965,
      "learning_rate": 2e-05,
      "loss": 0.3567,
      "step": 6780
    },
    {
      "epoch": 2.1569250317662005,
      "grad_norm": 3.8521971702575684,
      "learning_rate": 2e-05,
      "loss": 0.3425,
      "step": 6790
    },
    {
      "epoch": 2.1601016518424396,
      "grad_norm": 4.741084098815918,
      "learning_rate": 2e-05,
      "loss": 0.3428,
      "step": 6800
    },
    {
      "epoch": 2.1632782719186787,
      "grad_norm": 3.1793816089630127,
      "learning_rate": 2e-05,
      "loss": 0.3527,
      "step": 6810
    },
    {
      "epoch": 2.1664548919949174,
      "grad_norm": 2.8577167987823486,
      "learning_rate": 2e-05,
      "loss": 0.3116,
      "step": 6820
    },
    {
      "epoch": 2.1696315120711565,
      "grad_norm": 3.6697709560394287,
      "learning_rate": 2e-05,
      "loss": 0.3395,
      "step": 6830
    },
    {
      "epoch": 2.172808132147395,
      "grad_norm": 5.26312780380249,
      "learning_rate": 2e-05,
      "loss": 0.2991,
      "step": 6840
    },
    {
      "epoch": 2.175984752223634,
      "grad_norm": 3.8249425888061523,
      "learning_rate": 2e-05,
      "loss": 0.3161,
      "step": 6850
    },
    {
      "epoch": 2.179161372299873,
      "grad_norm": 4.021222114562988,
      "learning_rate": 2e-05,
      "loss": 0.3242,
      "step": 6860
    },
    {
      "epoch": 2.182337992376112,
      "grad_norm": 3.4902398586273193,
      "learning_rate": 2e-05,
      "loss": 0.3272,
      "step": 6870
    },
    {
      "epoch": 2.1855146124523506,
      "grad_norm": 2.918503522872925,
      "learning_rate": 2e-05,
      "loss": 0.3241,
      "step": 6880
    },
    {
      "epoch": 2.1886912325285897,
      "grad_norm": 2.87440824508667,
      "learning_rate": 2e-05,
      "loss": 0.3292,
      "step": 6890
    },
    {
      "epoch": 2.1918678526048283,
      "grad_norm": 4.66793966293335,
      "learning_rate": 2e-05,
      "loss": 0.3287,
      "step": 6900
    },
    {
      "epoch": 2.1950444726810674,
      "grad_norm": 2.9992268085479736,
      "learning_rate": 2e-05,
      "loss": 0.3125,
      "step": 6910
    },
    {
      "epoch": 2.198221092757306,
      "grad_norm": 4.693375110626221,
      "learning_rate": 2e-05,
      "loss": 0.3422,
      "step": 6920
    },
    {
      "epoch": 2.201397712833545,
      "grad_norm": 3.5200979709625244,
      "learning_rate": 2e-05,
      "loss": 0.3414,
      "step": 6930
    },
    {
      "epoch": 2.201397712833545,
      "eval_loss": 1.7358758449554443,
      "eval_mse": 1.7343973964729202,
      "eval_pearson": 0.3894510350325094,
      "eval_runtime": 21.0613,
      "eval_samples_per_second": 1023.681,
      "eval_spearmanr": 0.3852050536499099,
      "eval_steps_per_second": 4.036,
      "step": 6930
    },
    {
      "epoch": 2.204574332909784,
      "grad_norm": 3.553967237472534,
      "learning_rate": 2e-05,
      "loss": 0.3154,
      "step": 6940
    },
    {
      "epoch": 2.207750952986023,
      "grad_norm": 3.172473669052124,
      "learning_rate": 2e-05,
      "loss": 0.3238,
      "step": 6950
    },
    {
      "epoch": 2.210927573062262,
      "grad_norm": 4.873626232147217,
      "learning_rate": 2e-05,
      "loss": 0.3247,
      "step": 6960
    },
    {
      "epoch": 2.2141041931385006,
      "grad_norm": 2.836535930633545,
      "learning_rate": 2e-05,
      "loss": 0.3295,
      "step": 6970
    },
    {
      "epoch": 2.2172808132147397,
      "grad_norm": 4.014500617980957,
      "learning_rate": 2e-05,
      "loss": 0.3441,
      "step": 6980
    },
    {
      "epoch": 2.2204574332909783,
      "grad_norm": 5.580926895141602,
      "learning_rate": 2e-05,
      "loss": 0.3556,
      "step": 6990
    },
    {
      "epoch": 2.2236340533672174,
      "grad_norm": 4.290802478790283,
      "learning_rate": 2e-05,
      "loss": 0.3285,
      "step": 7000
    },
    {
      "epoch": 2.226810673443456,
      "grad_norm": 2.6677024364471436,
      "learning_rate": 2e-05,
      "loss": 0.307,
      "step": 7010
    },
    {
      "epoch": 2.229987293519695,
      "grad_norm": 4.044354438781738,
      "learning_rate": 2e-05,
      "loss": 0.3099,
      "step": 7020
    },
    {
      "epoch": 2.233163913595934,
      "grad_norm": 3.5153794288635254,
      "learning_rate": 2e-05,
      "loss": 0.2969,
      "step": 7030
    },
    {
      "epoch": 2.236340533672173,
      "grad_norm": 5.293027400970459,
      "learning_rate": 2e-05,
      "loss": 0.3215,
      "step": 7040
    },
    {
      "epoch": 2.2395171537484115,
      "grad_norm": 3.3990867137908936,
      "learning_rate": 2e-05,
      "loss": 0.3219,
      "step": 7050
    },
    {
      "epoch": 2.2426937738246506,
      "grad_norm": 3.6655783653259277,
      "learning_rate": 2e-05,
      "loss": 0.3055,
      "step": 7060
    },
    {
      "epoch": 2.2458703939008893,
      "grad_norm": 4.666078090667725,
      "learning_rate": 2e-05,
      "loss": 0.3272,
      "step": 7070
    },
    {
      "epoch": 2.2490470139771284,
      "grad_norm": 3.097029447555542,
      "learning_rate": 2e-05,
      "loss": 0.3153,
      "step": 7080
    },
    {
      "epoch": 2.252223634053367,
      "grad_norm": 3.7616565227508545,
      "learning_rate": 2e-05,
      "loss": 0.3159,
      "step": 7090
    },
    {
      "epoch": 2.255400254129606,
      "grad_norm": 4.855223655700684,
      "learning_rate": 2e-05,
      "loss": 0.3199,
      "step": 7100
    },
    {
      "epoch": 2.258576874205845,
      "grad_norm": 3.0523924827575684,
      "learning_rate": 2e-05,
      "loss": 0.3219,
      "step": 7110
    },
    {
      "epoch": 2.261753494282084,
      "grad_norm": 2.9767799377441406,
      "learning_rate": 2e-05,
      "loss": 0.3314,
      "step": 7120
    },
    {
      "epoch": 2.264930114358323,
      "grad_norm": 3.517336368560791,
      "learning_rate": 2e-05,
      "loss": 0.3,
      "step": 7130
    },
    {
      "epoch": 2.2681067344345616,
      "grad_norm": 4.655422210693359,
      "learning_rate": 2e-05,
      "loss": 0.3143,
      "step": 7140
    },
    {
      "epoch": 2.2712833545108007,
      "grad_norm": 2.330357313156128,
      "learning_rate": 2e-05,
      "loss": 0.3227,
      "step": 7150
    },
    {
      "epoch": 2.2744599745870393,
      "grad_norm": 3.228217840194702,
      "learning_rate": 2e-05,
      "loss": 0.3183,
      "step": 7160
    },
    {
      "epoch": 2.2776365946632784,
      "grad_norm": 4.029611110687256,
      "learning_rate": 2e-05,
      "loss": 0.3178,
      "step": 7170
    },
    {
      "epoch": 2.280813214739517,
      "grad_norm": 3.297917127609253,
      "learning_rate": 2e-05,
      "loss": 0.3228,
      "step": 7180
    },
    {
      "epoch": 2.283989834815756,
      "grad_norm": 3.743655204772949,
      "learning_rate": 2e-05,
      "loss": 0.3361,
      "step": 7190
    },
    {
      "epoch": 2.2871664548919948,
      "grad_norm": 3.714214563369751,
      "learning_rate": 2e-05,
      "loss": 0.3224,
      "step": 7200
    },
    {
      "epoch": 2.290343074968234,
      "grad_norm": 2.8897008895874023,
      "learning_rate": 2e-05,
      "loss": 0.3051,
      "step": 7210
    },
    {
      "epoch": 2.2935196950444725,
      "grad_norm": 3.0278854370117188,
      "learning_rate": 2e-05,
      "loss": 0.287,
      "step": 7220
    },
    {
      "epoch": 2.2966963151207116,
      "grad_norm": 4.588321685791016,
      "learning_rate": 2e-05,
      "loss": 0.3267,
      "step": 7230
    },
    {
      "epoch": 2.2998729351969507,
      "grad_norm": 3.8723440170288086,
      "learning_rate": 2e-05,
      "loss": 0.3248,
      "step": 7240
    },
    {
      "epoch": 2.30146124523507,
      "eval_loss": 1.8286395072937012,
      "eval_mse": 1.8280647451006193,
      "eval_pearson": 0.40667389584896435,
      "eval_runtime": 20.8493,
      "eval_samples_per_second": 1034.086,
      "eval_spearmanr": 0.4055440277951282,
      "eval_steps_per_second": 4.077,
      "step": 7245
    },
    {
      "epoch": 2.3030495552731893,
      "grad_norm": 3.3690662384033203,
      "learning_rate": 2e-05,
      "loss": 0.2984,
      "step": 7250
    },
    {
      "epoch": 2.306226175349428,
      "grad_norm": 4.5256171226501465,
      "learning_rate": 2e-05,
      "loss": 0.3296,
      "step": 7260
    },
    {
      "epoch": 2.309402795425667,
      "grad_norm": 4.220268726348877,
      "learning_rate": 2e-05,
      "loss": 0.3232,
      "step": 7270
    },
    {
      "epoch": 2.312579415501906,
      "grad_norm": 3.012953996658325,
      "learning_rate": 2e-05,
      "loss": 0.309,
      "step": 7280
    },
    {
      "epoch": 2.315756035578145,
      "grad_norm": 4.178468704223633,
      "learning_rate": 2e-05,
      "loss": 0.3385,
      "step": 7290
    },
    {
      "epoch": 2.318932655654384,
      "grad_norm": 3.4104015827178955,
      "learning_rate": 2e-05,
      "loss": 0.3165,
      "step": 7300
    },
    {
      "epoch": 2.3221092757306225,
      "grad_norm": 3.1451263427734375,
      "learning_rate": 2e-05,
      "loss": 0.296,
      "step": 7310
    },
    {
      "epoch": 2.3252858958068616,
      "grad_norm": 3.9933345317840576,
      "learning_rate": 2e-05,
      "loss": 0.3162,
      "step": 7320
    },
    {
      "epoch": 2.3284625158831003,
      "grad_norm": 4.172810077667236,
      "learning_rate": 2e-05,
      "loss": 0.3038,
      "step": 7330
    },
    {
      "epoch": 2.3316391359593394,
      "grad_norm": 5.8789472579956055,
      "learning_rate": 2e-05,
      "loss": 0.3122,
      "step": 7340
    },
    {
      "epoch": 2.334815756035578,
      "grad_norm": 3.386197090148926,
      "learning_rate": 2e-05,
      "loss": 0.3142,
      "step": 7350
    },
    {
      "epoch": 2.337992376111817,
      "grad_norm": 4.674314022064209,
      "learning_rate": 2e-05,
      "loss": 0.321,
      "step": 7360
    },
    {
      "epoch": 2.3411689961880557,
      "grad_norm": 4.013875961303711,
      "learning_rate": 2e-05,
      "loss": 0.3137,
      "step": 7370
    },
    {
      "epoch": 2.344345616264295,
      "grad_norm": 3.4709174633026123,
      "learning_rate": 2e-05,
      "loss": 0.3068,
      "step": 7380
    },
    {
      "epoch": 2.3475222363405335,
      "grad_norm": 3.9194633960723877,
      "learning_rate": 2e-05,
      "loss": 0.3148,
      "step": 7390
    },
    {
      "epoch": 2.3506988564167726,
      "grad_norm": 2.802370548248291,
      "learning_rate": 2e-05,
      "loss": 0.3072,
      "step": 7400
    },
    {
      "epoch": 2.3538754764930117,
      "grad_norm": 3.459306001663208,
      "learning_rate": 2e-05,
      "loss": 0.3059,
      "step": 7410
    },
    {
      "epoch": 2.3570520965692503,
      "grad_norm": 3.754518747329712,
      "learning_rate": 2e-05,
      "loss": 0.305,
      "step": 7420
    },
    {
      "epoch": 2.3602287166454894,
      "grad_norm": 3.4654693603515625,
      "learning_rate": 2e-05,
      "loss": 0.3115,
      "step": 7430
    },
    {
      "epoch": 2.363405336721728,
      "grad_norm": 3.116976022720337,
      "learning_rate": 2e-05,
      "loss": 0.3248,
      "step": 7440
    },
    {
      "epoch": 2.366581956797967,
      "grad_norm": 7.003861427307129,
      "learning_rate": 2e-05,
      "loss": 0.314,
      "step": 7450
    },
    {
      "epoch": 2.3697585768742058,
      "grad_norm": 5.144872665405273,
      "learning_rate": 2e-05,
      "loss": 0.3038,
      "step": 7460
    },
    {
      "epoch": 2.372935196950445,
      "grad_norm": 3.707010269165039,
      "learning_rate": 2e-05,
      "loss": 0.3046,
      "step": 7470
    },
    {
      "epoch": 2.3761118170266835,
      "grad_norm": 3.464641809463501,
      "learning_rate": 2e-05,
      "loss": 0.3097,
      "step": 7480
    },
    {
      "epoch": 2.3792884371029226,
      "grad_norm": 5.047520160675049,
      "learning_rate": 2e-05,
      "loss": 0.3324,
      "step": 7490
    },
    {
      "epoch": 2.3824650571791612,
      "grad_norm": 5.353788375854492,
      "learning_rate": 2e-05,
      "loss": 0.3083,
      "step": 7500
    },
    {
      "epoch": 2.3856416772554003,
      "grad_norm": 3.3236923217773438,
      "learning_rate": 2e-05,
      "loss": 0.316,
      "step": 7510
    },
    {
      "epoch": 2.388818297331639,
      "grad_norm": 4.444792747497559,
      "learning_rate": 2e-05,
      "loss": 0.3086,
      "step": 7520
    },
    {
      "epoch": 2.391994917407878,
      "grad_norm": 4.285708427429199,
      "learning_rate": 2e-05,
      "loss": 0.3282,
      "step": 7530
    },
    {
      "epoch": 2.3951715374841167,
      "grad_norm": 4.0028157234191895,
      "learning_rate": 2e-05,
      "loss": 0.3031,
      "step": 7540
    },
    {
      "epoch": 2.398348157560356,
      "grad_norm": 3.199981451034546,
      "learning_rate": 2e-05,
      "loss": 0.3009,
      "step": 7550
    },
    {
      "epoch": 2.4015247776365944,
      "grad_norm": 3.070631742477417,
      "learning_rate": 2e-05,
      "loss": 0.3125,
      "step": 7560
    },
    {
      "epoch": 2.4015247776365944,
      "eval_loss": 1.8288960456848145,
      "eval_mse": 1.827679789751934,
      "eval_pearson": 0.4039155662627848,
      "eval_runtime": 21.0641,
      "eval_samples_per_second": 1023.544,
      "eval_spearmanr": 0.4015137704674759,
      "eval_steps_per_second": 4.035,
      "step": 7560
    },
    {
      "epoch": 2.4047013977128335,
      "grad_norm": 3.7775588035583496,
      "learning_rate": 2e-05,
      "loss": 0.3227,
      "step": 7570
    },
    {
      "epoch": 2.4078780177890726,
      "grad_norm": 3.2612102031707764,
      "learning_rate": 2e-05,
      "loss": 0.3109,
      "step": 7580
    },
    {
      "epoch": 2.4110546378653113,
      "grad_norm": 3.3454365730285645,
      "learning_rate": 2e-05,
      "loss": 0.3065,
      "step": 7590
    },
    {
      "epoch": 2.4142312579415504,
      "grad_norm": 2.9859120845794678,
      "learning_rate": 2e-05,
      "loss": 0.2835,
      "step": 7600
    },
    {
      "epoch": 2.417407878017789,
      "grad_norm": 4.034391403198242,
      "learning_rate": 2e-05,
      "loss": 0.3094,
      "step": 7610
    },
    {
      "epoch": 2.420584498094028,
      "grad_norm": 2.7862980365753174,
      "learning_rate": 2e-05,
      "loss": 0.3092,
      "step": 7620
    },
    {
      "epoch": 2.4237611181702667,
      "grad_norm": 3.625905752182007,
      "learning_rate": 2e-05,
      "loss": 0.3094,
      "step": 7630
    },
    {
      "epoch": 2.426937738246506,
      "grad_norm": 3.644625425338745,
      "learning_rate": 2e-05,
      "loss": 0.291,
      "step": 7640
    },
    {
      "epoch": 2.4301143583227445,
      "grad_norm": 4.074687957763672,
      "learning_rate": 2e-05,
      "loss": 0.3135,
      "step": 7650
    },
    {
      "epoch": 2.4332909783989836,
      "grad_norm": 2.635709524154663,
      "learning_rate": 2e-05,
      "loss": 0.296,
      "step": 7660
    },
    {
      "epoch": 2.436467598475222,
      "grad_norm": 3.7964773178100586,
      "learning_rate": 2e-05,
      "loss": 0.3404,
      "step": 7670
    },
    {
      "epoch": 2.4396442185514613,
      "grad_norm": 2.408463478088379,
      "learning_rate": 2e-05,
      "loss": 0.298,
      "step": 7680
    },
    {
      "epoch": 2.4428208386277,
      "grad_norm": 5.596233367919922,
      "learning_rate": 2e-05,
      "loss": 0.3129,
      "step": 7690
    },
    {
      "epoch": 2.445997458703939,
      "grad_norm": 2.6994524002075195,
      "learning_rate": 2e-05,
      "loss": 0.3218,
      "step": 7700
    },
    {
      "epoch": 2.449174078780178,
      "grad_norm": 3.924006223678589,
      "learning_rate": 2e-05,
      "loss": 0.3126,
      "step": 7710
    },
    {
      "epoch": 2.4523506988564168,
      "grad_norm": 3.489511251449585,
      "learning_rate": 2e-05,
      "loss": 0.2986,
      "step": 7720
    },
    {
      "epoch": 2.4555273189326554,
      "grad_norm": 4.24034309387207,
      "learning_rate": 2e-05,
      "loss": 0.3118,
      "step": 7730
    },
    {
      "epoch": 2.4587039390088945,
      "grad_norm": 4.726108074188232,
      "learning_rate": 2e-05,
      "loss": 0.3179,
      "step": 7740
    },
    {
      "epoch": 2.4618805590851336,
      "grad_norm": 4.395129203796387,
      "learning_rate": 2e-05,
      "loss": 0.3016,
      "step": 7750
    },
    {
      "epoch": 2.4650571791613722,
      "grad_norm": 4.521108150482178,
      "learning_rate": 2e-05,
      "loss": 0.3241,
      "step": 7760
    },
    {
      "epoch": 2.4682337992376113,
      "grad_norm": 2.5993900299072266,
      "learning_rate": 2e-05,
      "loss": 0.3103,
      "step": 7770
    },
    {
      "epoch": 2.47141041931385,
      "grad_norm": 2.9472274780273438,
      "learning_rate": 2e-05,
      "loss": 0.2912,
      "step": 7780
    },
    {
      "epoch": 2.474587039390089,
      "grad_norm": 3.4274466037750244,
      "learning_rate": 2e-05,
      "loss": 0.2936,
      "step": 7790
    },
    {
      "epoch": 2.4777636594663277,
      "grad_norm": 5.500731945037842,
      "learning_rate": 2e-05,
      "loss": 0.3039,
      "step": 7800
    },
    {
      "epoch": 2.480940279542567,
      "grad_norm": 4.086122035980225,
      "learning_rate": 2e-05,
      "loss": 0.2999,
      "step": 7810
    },
    {
      "epoch": 2.4841168996188054,
      "grad_norm": 2.5464446544647217,
      "learning_rate": 2e-05,
      "loss": 0.3088,
      "step": 7820
    },
    {
      "epoch": 2.4872935196950445,
      "grad_norm": 3.4798941612243652,
      "learning_rate": 2e-05,
      "loss": 0.291,
      "step": 7830
    },
    {
      "epoch": 2.490470139771283,
      "grad_norm": 2.525982141494751,
      "learning_rate": 2e-05,
      "loss": 0.297,
      "step": 7840
    },
    {
      "epoch": 2.4936467598475223,
      "grad_norm": 4.956206321716309,
      "learning_rate": 2e-05,
      "loss": 0.2922,
      "step": 7850
    },
    {
      "epoch": 2.496823379923761,
      "grad_norm": 3.6501243114471436,
      "learning_rate": 2e-05,
      "loss": 0.2952,
      "step": 7860
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.9241747856140137,
      "learning_rate": 2e-05,
      "loss": 0.2922,
      "step": 7870
    },
    {
      "epoch": 2.5015883100381195,
      "eval_loss": 1.9053466320037842,
      "eval_mse": 1.9043459978721802,
      "eval_pearson": 0.41163024061384934,
      "eval_runtime": 20.9371,
      "eval_samples_per_second": 1029.752,
      "eval_spearmanr": 0.41029914956643865,
      "eval_steps_per_second": 4.06,
      "step": 7875
    },
    {
      "epoch": 2.503176620076239,
      "grad_norm": 2.7619476318359375,
      "learning_rate": 2e-05,
      "loss": 0.3097,
      "step": 7880
    },
    {
      "epoch": 2.5063532401524777,
      "grad_norm": 3.061901807785034,
      "learning_rate": 2e-05,
      "loss": 0.2924,
      "step": 7890
    },
    {
      "epoch": 2.5095298602287164,
      "grad_norm": 2.6954262256622314,
      "learning_rate": 2e-05,
      "loss": 0.2791,
      "step": 7900
    },
    {
      "epoch": 2.5127064803049555,
      "grad_norm": 3.8304929733276367,
      "learning_rate": 2e-05,
      "loss": 0.2831,
      "step": 7910
    },
    {
      "epoch": 2.5158831003811946,
      "grad_norm": 3.610806465148926,
      "learning_rate": 2e-05,
      "loss": 0.2778,
      "step": 7920
    },
    {
      "epoch": 2.519059720457433,
      "grad_norm": 2.671459197998047,
      "learning_rate": 2e-05,
      "loss": 0.2805,
      "step": 7930
    },
    {
      "epoch": 2.5222363405336723,
      "grad_norm": 5.8958282470703125,
      "learning_rate": 2e-05,
      "loss": 0.2988,
      "step": 7940
    },
    {
      "epoch": 2.525412960609911,
      "grad_norm": 4.420055389404297,
      "learning_rate": 2e-05,
      "loss": 0.2894,
      "step": 7950
    },
    {
      "epoch": 2.52858958068615,
      "grad_norm": 3.4843356609344482,
      "learning_rate": 2e-05,
      "loss": 0.2935,
      "step": 7960
    },
    {
      "epoch": 2.5317662007623887,
      "grad_norm": 3.228463649749756,
      "learning_rate": 2e-05,
      "loss": 0.3064,
      "step": 7970
    },
    {
      "epoch": 2.5349428208386278,
      "grad_norm": 2.953939437866211,
      "learning_rate": 2e-05,
      "loss": 0.2945,
      "step": 7980
    },
    {
      "epoch": 2.5381194409148664,
      "grad_norm": 3.2718074321746826,
      "learning_rate": 2e-05,
      "loss": 0.3028,
      "step": 7990
    },
    {
      "epoch": 2.5412960609911055,
      "grad_norm": 3.646794080734253,
      "learning_rate": 2e-05,
      "loss": 0.2875,
      "step": 8000
    },
    {
      "epoch": 2.5444726810673446,
      "grad_norm": 2.383570671081543,
      "learning_rate": 2e-05,
      "loss": 0.3157,
      "step": 8010
    },
    {
      "epoch": 2.5476493011435832,
      "grad_norm": 3.379009962081909,
      "learning_rate": 2e-05,
      "loss": 0.279,
      "step": 8020
    },
    {
      "epoch": 2.550825921219822,
      "grad_norm": 3.4204139709472656,
      "learning_rate": 2e-05,
      "loss": 0.2907,
      "step": 8030
    },
    {
      "epoch": 2.554002541296061,
      "grad_norm": 2.815058946609497,
      "learning_rate": 2e-05,
      "loss": 0.2701,
      "step": 8040
    },
    {
      "epoch": 2.5571791613723,
      "grad_norm": 3.295876979827881,
      "learning_rate": 2e-05,
      "loss": 0.3083,
      "step": 8050
    },
    {
      "epoch": 2.5603557814485387,
      "grad_norm": 6.980191707611084,
      "learning_rate": 2e-05,
      "loss": 0.2918,
      "step": 8060
    },
    {
      "epoch": 2.563532401524778,
      "grad_norm": 2.7986011505126953,
      "learning_rate": 2e-05,
      "loss": 0.2828,
      "step": 8070
    },
    {
      "epoch": 2.5667090216010164,
      "grad_norm": 4.7217583656311035,
      "learning_rate": 2e-05,
      "loss": 0.2983,
      "step": 8080
    },
    {
      "epoch": 2.5698856416772555,
      "grad_norm": 2.660914182662964,
      "learning_rate": 2e-05,
      "loss": 0.2782,
      "step": 8090
    },
    {
      "epoch": 2.573062261753494,
      "grad_norm": 4.80010986328125,
      "learning_rate": 2e-05,
      "loss": 0.2927,
      "step": 8100
    },
    {
      "epoch": 2.5762388818297333,
      "grad_norm": 4.976133823394775,
      "learning_rate": 2e-05,
      "loss": 0.2824,
      "step": 8110
    },
    {
      "epoch": 2.579415501905972,
      "grad_norm": 4.5943803787231445,
      "learning_rate": 2e-05,
      "loss": 0.2971,
      "step": 8120
    },
    {
      "epoch": 2.582592121982211,
      "grad_norm": 3.0418472290039062,
      "learning_rate": 2e-05,
      "loss": 0.3011,
      "step": 8130
    },
    {
      "epoch": 2.5857687420584496,
      "grad_norm": 5.822489261627197,
      "learning_rate": 2e-05,
      "loss": 0.3023,
      "step": 8140
    },
    {
      "epoch": 2.5889453621346887,
      "grad_norm": 3.6778452396392822,
      "learning_rate": 2e-05,
      "loss": 0.2743,
      "step": 8150
    },
    {
      "epoch": 2.5921219822109274,
      "grad_norm": 4.344014644622803,
      "learning_rate": 2e-05,
      "loss": 0.293,
      "step": 8160
    },
    {
      "epoch": 2.5952986022871665,
      "grad_norm": 2.2369322776794434,
      "learning_rate": 2e-05,
      "loss": 0.2727,
      "step": 8170
    },
    {
      "epoch": 2.5984752223634056,
      "grad_norm": 3.2712533473968506,
      "learning_rate": 2e-05,
      "loss": 0.2824,
      "step": 8180
    },
    {
      "epoch": 2.601651842439644,
      "grad_norm": 3.575397253036499,
      "learning_rate": 2e-05,
      "loss": 0.2903,
      "step": 8190
    },
    {
      "epoch": 2.601651842439644,
      "eval_loss": 1.7667744159698486,
      "eval_mse": 1.765129027241955,
      "eval_pearson": 0.42106935738838625,
      "eval_runtime": 21.1669,
      "eval_samples_per_second": 1018.573,
      "eval_spearmanr": 0.41941699615486194,
      "eval_steps_per_second": 4.016,
      "step": 8190
    },
    {
      "epoch": 2.604828462515883,
      "grad_norm": 2.578019142150879,
      "learning_rate": 2e-05,
      "loss": 0.2768,
      "step": 8200
    },
    {
      "epoch": 2.608005082592122,
      "grad_norm": 3.7636466026306152,
      "learning_rate": 2e-05,
      "loss": 0.2868,
      "step": 8210
    },
    {
      "epoch": 2.611181702668361,
      "grad_norm": 2.914856195449829,
      "learning_rate": 2e-05,
      "loss": 0.2801,
      "step": 8220
    },
    {
      "epoch": 2.6143583227445997,
      "grad_norm": 4.509111404418945,
      "learning_rate": 2e-05,
      "loss": 0.3015,
      "step": 8230
    },
    {
      "epoch": 2.6175349428208388,
      "grad_norm": 3.9969322681427,
      "learning_rate": 2e-05,
      "loss": 0.2815,
      "step": 8240
    },
    {
      "epoch": 2.6207115628970774,
      "grad_norm": 4.027764797210693,
      "learning_rate": 2e-05,
      "loss": 0.301,
      "step": 8250
    },
    {
      "epoch": 2.6238881829733165,
      "grad_norm": 2.931150436401367,
      "learning_rate": 2e-05,
      "loss": 0.282,
      "step": 8260
    },
    {
      "epoch": 2.627064803049555,
      "grad_norm": 3.142935037612915,
      "learning_rate": 2e-05,
      "loss": 0.2888,
      "step": 8270
    },
    {
      "epoch": 2.6302414231257942,
      "grad_norm": 4.72607946395874,
      "learning_rate": 2e-05,
      "loss": 0.2788,
      "step": 8280
    },
    {
      "epoch": 2.633418043202033,
      "grad_norm": 3.823162794113159,
      "learning_rate": 2e-05,
      "loss": 0.2767,
      "step": 8290
    },
    {
      "epoch": 2.636594663278272,
      "grad_norm": 4.107354640960693,
      "learning_rate": 2e-05,
      "loss": 0.2742,
      "step": 8300
    },
    {
      "epoch": 2.639771283354511,
      "grad_norm": 3.239654302597046,
      "learning_rate": 2e-05,
      "loss": 0.2957,
      "step": 8310
    },
    {
      "epoch": 2.6429479034307497,
      "grad_norm": 3.1887049674987793,
      "learning_rate": 2e-05,
      "loss": 0.276,
      "step": 8320
    },
    {
      "epoch": 2.6461245235069883,
      "grad_norm": 3.049337148666382,
      "learning_rate": 2e-05,
      "loss": 0.2709,
      "step": 8330
    },
    {
      "epoch": 2.6493011435832274,
      "grad_norm": 3.1697051525115967,
      "learning_rate": 2e-05,
      "loss": 0.2743,
      "step": 8340
    },
    {
      "epoch": 2.6524777636594665,
      "grad_norm": 3.364968776702881,
      "learning_rate": 2e-05,
      "loss": 0.2821,
      "step": 8350
    },
    {
      "epoch": 2.655654383735705,
      "grad_norm": 2.6984505653381348,
      "learning_rate": 2e-05,
      "loss": 0.2604,
      "step": 8360
    },
    {
      "epoch": 2.658831003811944,
      "grad_norm": 3.1899967193603516,
      "learning_rate": 2e-05,
      "loss": 0.285,
      "step": 8370
    },
    {
      "epoch": 2.662007623888183,
      "grad_norm": 2.9613513946533203,
      "learning_rate": 2e-05,
      "loss": 0.2995,
      "step": 8380
    },
    {
      "epoch": 2.665184243964422,
      "grad_norm": 2.9602303504943848,
      "learning_rate": 2e-05,
      "loss": 0.2951,
      "step": 8390
    },
    {
      "epoch": 2.6683608640406606,
      "grad_norm": 3.7589035034179688,
      "learning_rate": 2e-05,
      "loss": 0.2802,
      "step": 8400
    },
    {
      "epoch": 2.6715374841168997,
      "grad_norm": 3.4439241886138916,
      "learning_rate": 2e-05,
      "loss": 0.2721,
      "step": 8410
    },
    {
      "epoch": 2.6747141041931384,
      "grad_norm": 3.5613088607788086,
      "learning_rate": 2e-05,
      "loss": 0.2838,
      "step": 8420
    },
    {
      "epoch": 2.6778907242693775,
      "grad_norm": 2.9375228881835938,
      "learning_rate": 2e-05,
      "loss": 0.3026,
      "step": 8430
    },
    {
      "epoch": 2.681067344345616,
      "grad_norm": 4.570002555847168,
      "learning_rate": 2e-05,
      "loss": 0.2669,
      "step": 8440
    },
    {
      "epoch": 2.684243964421855,
      "grad_norm": 3.7788548469543457,
      "learning_rate": 2e-05,
      "loss": 0.2722,
      "step": 8450
    },
    {
      "epoch": 2.687420584498094,
      "grad_norm": 3.520000696182251,
      "learning_rate": 2e-05,
      "loss": 0.2991,
      "step": 8460
    },
    {
      "epoch": 2.690597204574333,
      "grad_norm": 3.7317607402801514,
      "learning_rate": 2e-05,
      "loss": 0.2693,
      "step": 8470
    },
    {
      "epoch": 2.693773824650572,
      "grad_norm": 3.233596086502075,
      "learning_rate": 2e-05,
      "loss": 0.2688,
      "step": 8480
    },
    {
      "epoch": 2.6969504447268107,
      "grad_norm": 4.483689308166504,
      "learning_rate": 2e-05,
      "loss": 0.26,
      "step": 8490
    },
    {
      "epoch": 2.7001270648030493,
      "grad_norm": 2.4909579753875732,
      "learning_rate": 2e-05,
      "loss": 0.2831,
      "step": 8500
    },
    {
      "epoch": 2.701715374841169,
      "eval_loss": 1.7423121929168701,
      "eval_mse": 1.7407212102346827,
      "eval_pearson": 0.4107110593109706,
      "eval_runtime": 20.9746,
      "eval_samples_per_second": 1027.911,
      "eval_spearmanr": 0.407948273769525,
      "eval_steps_per_second": 4.053,
      "step": 8505
    },
    {
      "epoch": 2.7033036848792884,
      "grad_norm": 3.571420669555664,
      "learning_rate": 2e-05,
      "loss": 0.2741,
      "step": 8510
    },
    {
      "epoch": 2.7064803049555275,
      "grad_norm": 2.870864152908325,
      "learning_rate": 2e-05,
      "loss": 0.2745,
      "step": 8520
    },
    {
      "epoch": 2.709656925031766,
      "grad_norm": 3.798492670059204,
      "learning_rate": 2e-05,
      "loss": 0.2845,
      "step": 8530
    },
    {
      "epoch": 2.7128335451080052,
      "grad_norm": 3.8844876289367676,
      "learning_rate": 2e-05,
      "loss": 0.2643,
      "step": 8540
    },
    {
      "epoch": 2.716010165184244,
      "grad_norm": 5.316924095153809,
      "learning_rate": 2e-05,
      "loss": 0.2822,
      "step": 8550
    },
    {
      "epoch": 2.719186785260483,
      "grad_norm": 3.4426486492156982,
      "learning_rate": 2e-05,
      "loss": 0.2874,
      "step": 8560
    },
    {
      "epoch": 2.7223634053367216,
      "grad_norm": 4.225093364715576,
      "learning_rate": 2e-05,
      "loss": 0.2675,
      "step": 8570
    },
    {
      "epoch": 2.7255400254129607,
      "grad_norm": 3.1144347190856934,
      "learning_rate": 2e-05,
      "loss": 0.2643,
      "step": 8580
    },
    {
      "epoch": 2.7287166454891993,
      "grad_norm": 3.5335307121276855,
      "learning_rate": 2e-05,
      "loss": 0.2716,
      "step": 8590
    },
    {
      "epoch": 2.7318932655654384,
      "grad_norm": 3.5676286220550537,
      "learning_rate": 2e-05,
      "loss": 0.2628,
      "step": 8600
    },
    {
      "epoch": 2.7350698856416775,
      "grad_norm": 3.474148988723755,
      "learning_rate": 2e-05,
      "loss": 0.2608,
      "step": 8610
    },
    {
      "epoch": 2.738246505717916,
      "grad_norm": 9.399161338806152,
      "learning_rate": 2e-05,
      "loss": 0.284,
      "step": 8620
    },
    {
      "epoch": 2.741423125794155,
      "grad_norm": 3.4708845615386963,
      "learning_rate": 2e-05,
      "loss": 0.29,
      "step": 8630
    },
    {
      "epoch": 2.744599745870394,
      "grad_norm": 3.927830934524536,
      "learning_rate": 2e-05,
      "loss": 0.2607,
      "step": 8640
    },
    {
      "epoch": 2.747776365946633,
      "grad_norm": 4.090804576873779,
      "learning_rate": 2e-05,
      "loss": 0.2615,
      "step": 8650
    },
    {
      "epoch": 2.7509529860228716,
      "grad_norm": 2.4917900562286377,
      "learning_rate": 2e-05,
      "loss": 0.2354,
      "step": 8660
    },
    {
      "epoch": 2.7541296060991103,
      "grad_norm": 2.5947139263153076,
      "learning_rate": 2e-05,
      "loss": 0.2787,
      "step": 8670
    },
    {
      "epoch": 2.7573062261753494,
      "grad_norm": 3.290353775024414,
      "learning_rate": 2e-05,
      "loss": 0.2897,
      "step": 8680
    },
    {
      "epoch": 2.7604828462515885,
      "grad_norm": 2.746670722961426,
      "learning_rate": 2e-05,
      "loss": 0.2686,
      "step": 8690
    },
    {
      "epoch": 2.763659466327827,
      "grad_norm": 3.5549492835998535,
      "learning_rate": 2e-05,
      "loss": 0.2852,
      "step": 8700
    },
    {
      "epoch": 2.766836086404066,
      "grad_norm": 3.137869119644165,
      "learning_rate": 2e-05,
      "loss": 0.2676,
      "step": 8710
    },
    {
      "epoch": 2.770012706480305,
      "grad_norm": 4.542569637298584,
      "learning_rate": 2e-05,
      "loss": 0.2802,
      "step": 8720
    },
    {
      "epoch": 2.773189326556544,
      "grad_norm": 3.4597063064575195,
      "learning_rate": 2e-05,
      "loss": 0.2481,
      "step": 8730
    },
    {
      "epoch": 2.7763659466327826,
      "grad_norm": 4.124399662017822,
      "learning_rate": 2e-05,
      "loss": 0.2645,
      "step": 8740
    },
    {
      "epoch": 2.7795425667090217,
      "grad_norm": 5.309525012969971,
      "learning_rate": 2e-05,
      "loss": 0.276,
      "step": 8750
    },
    {
      "epoch": 2.7827191867852603,
      "grad_norm": 2.835420608520508,
      "learning_rate": 2e-05,
      "loss": 0.2624,
      "step": 8760
    },
    {
      "epoch": 2.7858958068614994,
      "grad_norm": 3.8976004123687744,
      "learning_rate": 2e-05,
      "loss": 0.2709,
      "step": 8770
    },
    {
      "epoch": 2.7890724269377385,
      "grad_norm": 3.747339963912964,
      "learning_rate": 2e-05,
      "loss": 0.296,
      "step": 8780
    },
    {
      "epoch": 2.792249047013977,
      "grad_norm": 3.3649721145629883,
      "learning_rate": 2e-05,
      "loss": 0.2509,
      "step": 8790
    },
    {
      "epoch": 2.795425667090216,
      "grad_norm": 4.320361137390137,
      "learning_rate": 2e-05,
      "loss": 0.2759,
      "step": 8800
    },
    {
      "epoch": 2.798602287166455,
      "grad_norm": 3.8096742630004883,
      "learning_rate": 2e-05,
      "loss": 0.2484,
      "step": 8810
    },
    {
      "epoch": 2.801778907242694,
      "grad_norm": 2.8977458477020264,
      "learning_rate": 2e-05,
      "loss": 0.2747,
      "step": 8820
    },
    {
      "epoch": 2.801778907242694,
      "eval_loss": 2.0004796981811523,
      "eval_mse": 1.9996344030898665,
      "eval_pearson": 0.404574565121531,
      "eval_runtime": 20.861,
      "eval_samples_per_second": 1033.507,
      "eval_spearmanr": 0.4038611705928899,
      "eval_steps_per_second": 4.075,
      "step": 8820
    },
    {
      "epoch": 2.8049555273189326,
      "grad_norm": 2.510965347290039,
      "learning_rate": 2e-05,
      "loss": 0.2787,
      "step": 8830
    },
    {
      "epoch": 2.8081321473951717,
      "grad_norm": 3.135451316833496,
      "learning_rate": 2e-05,
      "loss": 0.2538,
      "step": 8840
    },
    {
      "epoch": 2.8113087674714103,
      "grad_norm": 3.6487467288970947,
      "learning_rate": 2e-05,
      "loss": 0.2556,
      "step": 8850
    },
    {
      "epoch": 2.8144853875476494,
      "grad_norm": 3.041562080383301,
      "learning_rate": 2e-05,
      "loss": 0.2661,
      "step": 8860
    },
    {
      "epoch": 2.817662007623888,
      "grad_norm": 3.1891653537750244,
      "learning_rate": 2e-05,
      "loss": 0.2814,
      "step": 8870
    },
    {
      "epoch": 2.820838627700127,
      "grad_norm": 5.41468620300293,
      "learning_rate": 2e-05,
      "loss": 0.292,
      "step": 8880
    },
    {
      "epoch": 2.824015247776366,
      "grad_norm": 3.811276435852051,
      "learning_rate": 2e-05,
      "loss": 0.2667,
      "step": 8890
    },
    {
      "epoch": 2.827191867852605,
      "grad_norm": 3.1562411785125732,
      "learning_rate": 2e-05,
      "loss": 0.2562,
      "step": 8900
    },
    {
      "epoch": 2.8303684879288435,
      "grad_norm": 2.2936270236968994,
      "learning_rate": 2e-05,
      "loss": 0.2557,
      "step": 8910
    },
    {
      "epoch": 2.8335451080050826,
      "grad_norm": 2.7553365230560303,
      "learning_rate": 2e-05,
      "loss": 0.2698,
      "step": 8920
    },
    {
      "epoch": 2.8367217280813213,
      "grad_norm": 2.195826530456543,
      "learning_rate": 2e-05,
      "loss": 0.2737,
      "step": 8930
    },
    {
      "epoch": 2.8398983481575604,
      "grad_norm": 4.176576137542725,
      "learning_rate": 2e-05,
      "loss": 0.2635,
      "step": 8940
    },
    {
      "epoch": 2.8430749682337995,
      "grad_norm": 4.863821983337402,
      "learning_rate": 2e-05,
      "loss": 0.3031,
      "step": 8950
    },
    {
      "epoch": 2.846251588310038,
      "grad_norm": 3.356868028640747,
      "learning_rate": 2e-05,
      "loss": 0.2588,
      "step": 8960
    },
    {
      "epoch": 2.8494282083862768,
      "grad_norm": 3.9186270236968994,
      "learning_rate": 2e-05,
      "loss": 0.2651,
      "step": 8970
    },
    {
      "epoch": 2.852604828462516,
      "grad_norm": 5.017434597015381,
      "learning_rate": 2e-05,
      "loss": 0.3033,
      "step": 8980
    },
    {
      "epoch": 2.855781448538755,
      "grad_norm": 2.4781312942504883,
      "learning_rate": 2e-05,
      "loss": 0.2685,
      "step": 8990
    },
    {
      "epoch": 2.8589580686149936,
      "grad_norm": 4.509230613708496,
      "learning_rate": 2e-05,
      "loss": 0.2809,
      "step": 9000
    },
    {
      "epoch": 2.8621346886912327,
      "grad_norm": 4.466365814208984,
      "learning_rate": 2e-05,
      "loss": 0.2916,
      "step": 9010
    },
    {
      "epoch": 2.8653113087674713,
      "grad_norm": 4.114477157592773,
      "learning_rate": 2e-05,
      "loss": 0.2677,
      "step": 9020
    },
    {
      "epoch": 2.8684879288437104,
      "grad_norm": 2.857661724090576,
      "learning_rate": 2e-05,
      "loss": 0.2667,
      "step": 9030
    },
    {
      "epoch": 2.871664548919949,
      "grad_norm": 3.3470799922943115,
      "learning_rate": 2e-05,
      "loss": 0.2511,
      "step": 9040
    },
    {
      "epoch": 2.874841168996188,
      "grad_norm": 3.474970579147339,
      "learning_rate": 2e-05,
      "loss": 0.2638,
      "step": 9050
    },
    {
      "epoch": 2.878017789072427,
      "grad_norm": 2.45515513420105,
      "learning_rate": 2e-05,
      "loss": 0.2759,
      "step": 9060
    },
    {
      "epoch": 2.881194409148666,
      "grad_norm": 2.8467671871185303,
      "learning_rate": 2e-05,
      "loss": 0.2446,
      "step": 9070
    },
    {
      "epoch": 2.884371029224905,
      "grad_norm": 2.614818811416626,
      "learning_rate": 2e-05,
      "loss": 0.2623,
      "step": 9080
    },
    {
      "epoch": 2.8875476493011436,
      "grad_norm": 3.4058218002319336,
      "learning_rate": 2e-05,
      "loss": 0.2602,
      "step": 9090
    },
    {
      "epoch": 2.8907242693773822,
      "grad_norm": 3.4632434844970703,
      "learning_rate": 2e-05,
      "loss": 0.2715,
      "step": 9100
    },
    {
      "epoch": 2.8939008894536213,
      "grad_norm": 2.857682228088379,
      "learning_rate": 2e-05,
      "loss": 0.2646,
      "step": 9110
    },
    {
      "epoch": 2.8970775095298604,
      "grad_norm": 5.0221757888793945,
      "learning_rate": 2e-05,
      "loss": 0.2536,
      "step": 9120
    },
    {
      "epoch": 2.900254129606099,
      "grad_norm": 3.514763593673706,
      "learning_rate": 2e-05,
      "loss": 0.259,
      "step": 9130
    },
    {
      "epoch": 2.9018424396442186,
      "eval_loss": 1.889666199684143,
      "eval_mse": 1.887739983377961,
      "eval_pearson": 0.4150162001058149,
      "eval_runtime": 20.9891,
      "eval_samples_per_second": 1027.199,
      "eval_spearmanr": 0.4131262256641622,
      "eval_steps_per_second": 4.05,
      "step": 9135
    },
    {
      "epoch": 2.9034307496823377,
      "grad_norm": 3.3300821781158447,
      "learning_rate": 2e-05,
      "loss": 0.2787,
      "step": 9140
    },
    {
      "epoch": 2.906607369758577,
      "grad_norm": 3.342034101486206,
      "learning_rate": 2e-05,
      "loss": 0.2384,
      "step": 9150
    },
    {
      "epoch": 2.909783989834816,
      "grad_norm": 3.3230419158935547,
      "learning_rate": 2e-05,
      "loss": 0.2505,
      "step": 9160
    },
    {
      "epoch": 2.9129606099110545,
      "grad_norm": 2.095200538635254,
      "learning_rate": 2e-05,
      "loss": 0.2577,
      "step": 9170
    },
    {
      "epoch": 2.9161372299872936,
      "grad_norm": 2.391916513442993,
      "learning_rate": 2e-05,
      "loss": 0.2652,
      "step": 9180
    },
    {
      "epoch": 2.9193138500635323,
      "grad_norm": 4.25920295715332,
      "learning_rate": 2e-05,
      "loss": 0.2659,
      "step": 9190
    },
    {
      "epoch": 2.9224904701397714,
      "grad_norm": 2.8915205001831055,
      "learning_rate": 2e-05,
      "loss": 0.2639,
      "step": 9200
    },
    {
      "epoch": 2.92566709021601,
      "grad_norm": 3.851752758026123,
      "learning_rate": 2e-05,
      "loss": 0.2628,
      "step": 9210
    },
    {
      "epoch": 2.928843710292249,
      "grad_norm": 3.7632133960723877,
      "learning_rate": 2e-05,
      "loss": 0.2659,
      "step": 9220
    },
    {
      "epoch": 2.9320203303684877,
      "grad_norm": 3.755741834640503,
      "learning_rate": 2e-05,
      "loss": 0.2687,
      "step": 9230
    },
    {
      "epoch": 2.935196950444727,
      "grad_norm": 2.9958384037017822,
      "learning_rate": 2e-05,
      "loss": 0.2638,
      "step": 9240
    },
    {
      "epoch": 2.938373570520966,
      "grad_norm": 3.0323071479797363,
      "learning_rate": 2e-05,
      "loss": 0.2506,
      "step": 9250
    },
    {
      "epoch": 2.9415501905972046,
      "grad_norm": 3.7485122680664062,
      "learning_rate": 2e-05,
      "loss": 0.2696,
      "step": 9260
    },
    {
      "epoch": 2.944726810673443,
      "grad_norm": 2.236943006515503,
      "learning_rate": 2e-05,
      "loss": 0.2598,
      "step": 9270
    },
    {
      "epoch": 2.9479034307496823,
      "grad_norm": 4.119748592376709,
      "learning_rate": 2e-05,
      "loss": 0.2786,
      "step": 9280
    },
    {
      "epoch": 2.9510800508259214,
      "grad_norm": 2.8778293132781982,
      "learning_rate": 2e-05,
      "loss": 0.2589,
      "step": 9290
    },
    {
      "epoch": 2.95425667090216,
      "grad_norm": 4.708333969116211,
      "learning_rate": 2e-05,
      "loss": 0.2774,
      "step": 9300
    },
    {
      "epoch": 2.957433290978399,
      "grad_norm": 3.557844638824463,
      "learning_rate": 2e-05,
      "loss": 0.2734,
      "step": 9310
    },
    {
      "epoch": 2.9606099110546378,
      "grad_norm": 3.3579814434051514,
      "learning_rate": 2e-05,
      "loss": 0.2624,
      "step": 9320
    },
    {
      "epoch": 2.963786531130877,
      "grad_norm": 5.656414985656738,
      "learning_rate": 2e-05,
      "loss": 0.2686,
      "step": 9330
    },
    {
      "epoch": 2.9669631512071155,
      "grad_norm": 3.6449642181396484,
      "learning_rate": 2e-05,
      "loss": 0.2555,
      "step": 9340
    },
    {
      "epoch": 2.9701397712833546,
      "grad_norm": 3.8248820304870605,
      "learning_rate": 2e-05,
      "loss": 0.2624,
      "step": 9350
    },
    {
      "epoch": 2.9733163913595932,
      "grad_norm": 3.8582048416137695,
      "learning_rate": 2e-05,
      "loss": 0.2513,
      "step": 9360
    },
    {
      "epoch": 2.9764930114358323,
      "grad_norm": 2.9456708431243896,
      "learning_rate": 2e-05,
      "loss": 0.2526,
      "step": 9370
    },
    {
      "epoch": 2.9796696315120714,
      "grad_norm": 3.3341267108917236,
      "learning_rate": 2e-05,
      "loss": 0.2587,
      "step": 9380
    },
    {
      "epoch": 2.98284625158831,
      "grad_norm": 2.7733912467956543,
      "learning_rate": 2e-05,
      "loss": 0.2516,
      "step": 9390
    },
    {
      "epoch": 2.9860228716645487,
      "grad_norm": 3.3317959308624268,
      "learning_rate": 2e-05,
      "loss": 0.2387,
      "step": 9400
    },
    {
      "epoch": 2.989199491740788,
      "grad_norm": 3.4897072315216064,
      "learning_rate": 2e-05,
      "loss": 0.2465,
      "step": 9410
    },
    {
      "epoch": 2.992376111817027,
      "grad_norm": 3.2123591899871826,
      "learning_rate": 2e-05,
      "loss": 0.2594,
      "step": 9420
    },
    {
      "epoch": 2.9955527318932655,
      "grad_norm": 3.616391658782959,
      "learning_rate": 2e-05,
      "loss": 0.2628,
      "step": 9430
    },
    {
      "epoch": 2.998729351969504,
      "grad_norm": 2.862250804901123,
      "learning_rate": 2e-05,
      "loss": 0.2425,
      "step": 9440
    },
    {
      "epoch": 3.0019059720457433,
      "grad_norm": 3.4370813369750977,
      "learning_rate": 2e-05,
      "loss": 0.2574,
      "step": 9450
    },
    {
      "epoch": 3.0019059720457433,
      "eval_loss": 1.7875010967254639,
      "eval_mse": 1.786107460521369,
      "eval_pearson": 0.4077410568036832,
      "eval_runtime": 21.0789,
      "eval_samples_per_second": 1022.824,
      "eval_spearmanr": 0.407259253634793,
      "eval_steps_per_second": 4.032,
      "step": 9450
    },
    {
      "epoch": 3.0050825921219824,
      "grad_norm": 2.3946096897125244,
      "learning_rate": 2e-05,
      "loss": 0.2329,
      "step": 9460
    },
    {
      "epoch": 3.008259212198221,
      "grad_norm": 3.6338648796081543,
      "learning_rate": 2e-05,
      "loss": 0.2216,
      "step": 9470
    },
    {
      "epoch": 3.01143583227446,
      "grad_norm": 3.470154047012329,
      "learning_rate": 2e-05,
      "loss": 0.2367,
      "step": 9480
    },
    {
      "epoch": 3.0146124523506987,
      "grad_norm": 2.8946070671081543,
      "learning_rate": 2e-05,
      "loss": 0.2369,
      "step": 9490
    },
    {
      "epoch": 3.017789072426938,
      "grad_norm": 2.2465593814849854,
      "learning_rate": 2e-05,
      "loss": 0.2268,
      "step": 9500
    },
    {
      "epoch": 3.0209656925031765,
      "grad_norm": 2.4817092418670654,
      "learning_rate": 2e-05,
      "loss": 0.2259,
      "step": 9510
    },
    {
      "epoch": 3.0241423125794156,
      "grad_norm": 3.9695699214935303,
      "learning_rate": 2e-05,
      "loss": 0.2187,
      "step": 9520
    },
    {
      "epoch": 3.027318932655654,
      "grad_norm": 4.058683395385742,
      "learning_rate": 2e-05,
      "loss": 0.2394,
      "step": 9530
    },
    {
      "epoch": 3.0304955527318933,
      "grad_norm": 3.402048110961914,
      "learning_rate": 2e-05,
      "loss": 0.2375,
      "step": 9540
    },
    {
      "epoch": 3.033672172808132,
      "grad_norm": 3.6825435161590576,
      "learning_rate": 2e-05,
      "loss": 0.2174,
      "step": 9550
    },
    {
      "epoch": 3.036848792884371,
      "grad_norm": 2.426624298095703,
      "learning_rate": 2e-05,
      "loss": 0.2353,
      "step": 9560
    },
    {
      "epoch": 3.04002541296061,
      "grad_norm": 3.5706958770751953,
      "learning_rate": 2e-05,
      "loss": 0.2239,
      "step": 9570
    },
    {
      "epoch": 3.0432020330368488,
      "grad_norm": 3.565152406692505,
      "learning_rate": 2e-05,
      "loss": 0.2317,
      "step": 9580
    },
    {
      "epoch": 3.046378653113088,
      "grad_norm": 3.153310537338257,
      "learning_rate": 2e-05,
      "loss": 0.2111,
      "step": 9590
    },
    {
      "epoch": 3.0495552731893265,
      "grad_norm": 2.9517621994018555,
      "learning_rate": 2e-05,
      "loss": 0.2308,
      "step": 9600
    },
    {
      "epoch": 3.0527318932655656,
      "grad_norm": 2.456609010696411,
      "learning_rate": 2e-05,
      "loss": 0.2186,
      "step": 9610
    },
    {
      "epoch": 3.0559085133418042,
      "grad_norm": 3.2435977458953857,
      "learning_rate": 2e-05,
      "loss": 0.2263,
      "step": 9620
    },
    {
      "epoch": 3.0590851334180433,
      "grad_norm": 3.714907169342041,
      "learning_rate": 2e-05,
      "loss": 0.2351,
      "step": 9630
    },
    {
      "epoch": 3.062261753494282,
      "grad_norm": 3.7492778301239014,
      "learning_rate": 2e-05,
      "loss": 0.222,
      "step": 9640
    },
    {
      "epoch": 3.065438373570521,
      "grad_norm": 3.197789430618286,
      "learning_rate": 2e-05,
      "loss": 0.2381,
      "step": 9650
    },
    {
      "epoch": 3.0686149936467597,
      "grad_norm": 3.953557014465332,
      "learning_rate": 2e-05,
      "loss": 0.2588,
      "step": 9660
    },
    {
      "epoch": 3.071791613722999,
      "grad_norm": 3.887730836868286,
      "learning_rate": 2e-05,
      "loss": 0.2239,
      "step": 9670
    },
    {
      "epoch": 3.0749682337992374,
      "grad_norm": 2.6692607402801514,
      "learning_rate": 2e-05,
      "loss": 0.2362,
      "step": 9680
    },
    {
      "epoch": 3.0781448538754765,
      "grad_norm": 2.233119249343872,
      "learning_rate": 2e-05,
      "loss": 0.2141,
      "step": 9690
    },
    {
      "epoch": 3.081321473951715,
      "grad_norm": 2.8592233657836914,
      "learning_rate": 2e-05,
      "loss": 0.2238,
      "step": 9700
    },
    {
      "epoch": 3.0844980940279543,
      "grad_norm": 2.1496763229370117,
      "learning_rate": 2e-05,
      "loss": 0.2172,
      "step": 9710
    },
    {
      "epoch": 3.0876747141041934,
      "grad_norm": 3.570326328277588,
      "learning_rate": 2e-05,
      "loss": 0.2362,
      "step": 9720
    },
    {
      "epoch": 3.090851334180432,
      "grad_norm": 2.668790102005005,
      "learning_rate": 2e-05,
      "loss": 0.2321,
      "step": 9730
    },
    {
      "epoch": 3.094027954256671,
      "grad_norm": 3.0492420196533203,
      "learning_rate": 2e-05,
      "loss": 0.2278,
      "step": 9740
    },
    {
      "epoch": 3.0972045743329097,
      "grad_norm": 2.6095688343048096,
      "learning_rate": 2e-05,
      "loss": 0.2441,
      "step": 9750
    },
    {
      "epoch": 3.100381194409149,
      "grad_norm": 3.2059803009033203,
      "learning_rate": 2e-05,
      "loss": 0.2277,
      "step": 9760
    },
    {
      "epoch": 3.101969504447268,
      "eval_loss": 1.8172353506088257,
      "eval_mse": 1.816117022964751,
      "eval_pearson": 0.3887694369292265,
      "eval_runtime": 20.9608,
      "eval_samples_per_second": 1028.586,
      "eval_spearmanr": 0.38436461755139206,
      "eval_steps_per_second": 4.055,
      "step": 9765
    },
    {
      "epoch": 3.1035578144853875,
      "grad_norm": 3.973508358001709,
      "learning_rate": 2e-05,
      "loss": 0.2269,
      "step": 9770
    },
    {
      "epoch": 3.1067344345616266,
      "grad_norm": 3.90811824798584,
      "learning_rate": 2e-05,
      "loss": 0.2351,
      "step": 9780
    },
    {
      "epoch": 3.109911054637865,
      "grad_norm": 3.72556209564209,
      "learning_rate": 2e-05,
      "loss": 0.2212,
      "step": 9790
    },
    {
      "epoch": 3.1130876747141043,
      "grad_norm": 4.764963150024414,
      "learning_rate": 2e-05,
      "loss": 0.2326,
      "step": 9800
    },
    {
      "epoch": 3.116264294790343,
      "grad_norm": 2.7356479167938232,
      "learning_rate": 2e-05,
      "loss": 0.2195,
      "step": 9810
    },
    {
      "epoch": 3.119440914866582,
      "grad_norm": 4.243790626525879,
      "learning_rate": 2e-05,
      "loss": 0.2351,
      "step": 9820
    },
    {
      "epoch": 3.1226175349428207,
      "grad_norm": 2.996192693710327,
      "learning_rate": 2e-05,
      "loss": 0.2409,
      "step": 9830
    },
    {
      "epoch": 3.1257941550190598,
      "grad_norm": 2.8408291339874268,
      "learning_rate": 2e-05,
      "loss": 0.2218,
      "step": 9840
    },
    {
      "epoch": 3.1289707750952984,
      "grad_norm": 3.6525638103485107,
      "learning_rate": 2e-05,
      "loss": 0.2261,
      "step": 9850
    },
    {
      "epoch": 3.1321473951715375,
      "grad_norm": 2.4730329513549805,
      "learning_rate": 2e-05,
      "loss": 0.2218,
      "step": 9860
    },
    {
      "epoch": 3.135324015247776,
      "grad_norm": 2.920926094055176,
      "learning_rate": 2e-05,
      "loss": 0.2173,
      "step": 9870
    },
    {
      "epoch": 3.1385006353240152,
      "grad_norm": 3.0772597789764404,
      "learning_rate": 2e-05,
      "loss": 0.2223,
      "step": 9880
    },
    {
      "epoch": 3.1416772554002543,
      "grad_norm": 4.171317100524902,
      "learning_rate": 2e-05,
      "loss": 0.2235,
      "step": 9890
    },
    {
      "epoch": 3.144853875476493,
      "grad_norm": 3.4756834506988525,
      "learning_rate": 2e-05,
      "loss": 0.2264,
      "step": 9900
    },
    {
      "epoch": 3.148030495552732,
      "grad_norm": 2.275515556335449,
      "learning_rate": 2e-05,
      "loss": 0.2209,
      "step": 9910
    },
    {
      "epoch": 3.1512071156289707,
      "grad_norm": 3.5325441360473633,
      "learning_rate": 2e-05,
      "loss": 0.2309,
      "step": 9920
    },
    {
      "epoch": 3.15438373570521,
      "grad_norm": 3.1952719688415527,
      "learning_rate": 2e-05,
      "loss": 0.2271,
      "step": 9930
    },
    {
      "epoch": 3.1575603557814484,
      "grad_norm": 4.322828769683838,
      "learning_rate": 2e-05,
      "loss": 0.2264,
      "step": 9940
    },
    {
      "epoch": 3.1607369758576875,
      "grad_norm": 2.66691517829895,
      "learning_rate": 2e-05,
      "loss": 0.2363,
      "step": 9950
    },
    {
      "epoch": 3.163913595933926,
      "grad_norm": 4.413153648376465,
      "learning_rate": 2e-05,
      "loss": 0.2157,
      "step": 9960
    },
    {
      "epoch": 3.1670902160101653,
      "grad_norm": 2.894611120223999,
      "learning_rate": 2e-05,
      "loss": 0.2357,
      "step": 9970
    },
    {
      "epoch": 3.170266836086404,
      "grad_norm": 2.763627529144287,
      "learning_rate": 2e-05,
      "loss": 0.2395,
      "step": 9980
    },
    {
      "epoch": 3.173443456162643,
      "grad_norm": 4.542568206787109,
      "learning_rate": 2e-05,
      "loss": 0.2427,
      "step": 9990
    },
    {
      "epoch": 3.1766200762388817,
      "grad_norm": 4.79934024810791,
      "learning_rate": 2e-05,
      "loss": 0.259,
      "step": 10000
    },
    {
      "epoch": 3.1797966963151207,
      "grad_norm": 3.2727749347686768,
      "learning_rate": 2e-05,
      "loss": 0.2395,
      "step": 10010
    },
    {
      "epoch": 3.1829733163913594,
      "grad_norm": 3.0848867893218994,
      "learning_rate": 2e-05,
      "loss": 0.2117,
      "step": 10020
    },
    {
      "epoch": 3.1861499364675985,
      "grad_norm": 3.0101311206817627,
      "learning_rate": 2e-05,
      "loss": 0.2327,
      "step": 10030
    },
    {
      "epoch": 3.189326556543837,
      "grad_norm": 2.314528226852417,
      "learning_rate": 2e-05,
      "loss": 0.2233,
      "step": 10040
    },
    {
      "epoch": 3.192503176620076,
      "grad_norm": 3.27897047996521,
      "learning_rate": 2e-05,
      "loss": 0.2374,
      "step": 10050
    },
    {
      "epoch": 3.1956797966963153,
      "grad_norm": 4.188076019287109,
      "learning_rate": 2e-05,
      "loss": 0.2185,
      "step": 10060
    },
    {
      "epoch": 3.198856416772554,
      "grad_norm": 2.2093634605407715,
      "learning_rate": 2e-05,
      "loss": 0.2261,
      "step": 10070
    },
    {
      "epoch": 3.202033036848793,
      "grad_norm": 3.878624677658081,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 10080
    },
    {
      "epoch": 3.202033036848793,
      "eval_loss": 1.765011191368103,
      "eval_mse": 1.763574501026097,
      "eval_pearson": 0.4183406341272813,
      "eval_runtime": 20.872,
      "eval_samples_per_second": 1032.963,
      "eval_spearmanr": 0.4161769224461227,
      "eval_steps_per_second": 4.072,
      "step": 10080
    },
    {
      "epoch": 3.2052096569250317,
      "grad_norm": 2.665010690689087,
      "learning_rate": 2e-05,
      "loss": 0.2229,
      "step": 10090
    },
    {
      "epoch": 3.2083862770012708,
      "grad_norm": 3.033679962158203,
      "learning_rate": 2e-05,
      "loss": 0.2202,
      "step": 10100
    },
    {
      "epoch": 3.2115628970775094,
      "grad_norm": 2.920483112335205,
      "learning_rate": 2e-05,
      "loss": 0.2273,
      "step": 10110
    },
    {
      "epoch": 3.2147395171537485,
      "grad_norm": 2.1169755458831787,
      "learning_rate": 2e-05,
      "loss": 0.216,
      "step": 10120
    },
    {
      "epoch": 3.217916137229987,
      "grad_norm": 5.162678241729736,
      "learning_rate": 2e-05,
      "loss": 0.2458,
      "step": 10130
    },
    {
      "epoch": 3.2210927573062262,
      "grad_norm": 3.8215348720550537,
      "learning_rate": 2e-05,
      "loss": 0.2293,
      "step": 10140
    },
    {
      "epoch": 3.224269377382465,
      "grad_norm": 3.560049533843994,
      "learning_rate": 2e-05,
      "loss": 0.2211,
      "step": 10150
    },
    {
      "epoch": 3.227445997458704,
      "grad_norm": 2.851344108581543,
      "learning_rate": 2e-05,
      "loss": 0.2255,
      "step": 10160
    },
    {
      "epoch": 3.2306226175349426,
      "grad_norm": 2.2838916778564453,
      "learning_rate": 2e-05,
      "loss": 0.2324,
      "step": 10170
    },
    {
      "epoch": 3.2337992376111817,
      "grad_norm": 2.6617813110351562,
      "learning_rate": 2e-05,
      "loss": 0.2217,
      "step": 10180
    },
    {
      "epoch": 3.236975857687421,
      "grad_norm": 3.1306331157684326,
      "learning_rate": 2e-05,
      "loss": 0.2107,
      "step": 10190
    },
    {
      "epoch": 3.2401524777636594,
      "grad_norm": 2.575202465057373,
      "learning_rate": 2e-05,
      "loss": 0.2295,
      "step": 10200
    },
    {
      "epoch": 3.2433290978398985,
      "grad_norm": 3.3574581146240234,
      "learning_rate": 2e-05,
      "loss": 0.2135,
      "step": 10210
    },
    {
      "epoch": 3.246505717916137,
      "grad_norm": 3.0604734420776367,
      "learning_rate": 2e-05,
      "loss": 0.2051,
      "step": 10220
    },
    {
      "epoch": 3.2496823379923763,
      "grad_norm": 4.033308029174805,
      "learning_rate": 2e-05,
      "loss": 0.2249,
      "step": 10230
    },
    {
      "epoch": 3.252858958068615,
      "grad_norm": 2.9587626457214355,
      "learning_rate": 2e-05,
      "loss": 0.2203,
      "step": 10240
    },
    {
      "epoch": 3.256035578144854,
      "grad_norm": 2.964970111846924,
      "learning_rate": 2e-05,
      "loss": 0.2096,
      "step": 10250
    },
    {
      "epoch": 3.2592121982210926,
      "grad_norm": 2.855210781097412,
      "learning_rate": 2e-05,
      "loss": 0.2152,
      "step": 10260
    },
    {
      "epoch": 3.2623888182973317,
      "grad_norm": 2.065070152282715,
      "learning_rate": 2e-05,
      "loss": 0.2002,
      "step": 10270
    },
    {
      "epoch": 3.2655654383735704,
      "grad_norm": 4.280394077301025,
      "learning_rate": 2e-05,
      "loss": 0.2262,
      "step": 10280
    },
    {
      "epoch": 3.2687420584498095,
      "grad_norm": 2.525974750518799,
      "learning_rate": 2e-05,
      "loss": 0.218,
      "step": 10290
    },
    {
      "epoch": 3.271918678526048,
      "grad_norm": 3.7064757347106934,
      "learning_rate": 2e-05,
      "loss": 0.2024,
      "step": 10300
    },
    {
      "epoch": 3.275095298602287,
      "grad_norm": 2.92193603515625,
      "learning_rate": 2e-05,
      "loss": 0.228,
      "step": 10310
    },
    {
      "epoch": 3.2782719186785263,
      "grad_norm": 3.543195962905884,
      "learning_rate": 2e-05,
      "loss": 0.2241,
      "step": 10320
    },
    {
      "epoch": 3.281448538754765,
      "grad_norm": 2.400196075439453,
      "learning_rate": 2e-05,
      "loss": 0.2015,
      "step": 10330
    },
    {
      "epoch": 3.2846251588310036,
      "grad_norm": 3.483915328979492,
      "learning_rate": 2e-05,
      "loss": 0.2208,
      "step": 10340
    },
    {
      "epoch": 3.2878017789072427,
      "grad_norm": 3.1360106468200684,
      "learning_rate": 2e-05,
      "loss": 0.2124,
      "step": 10350
    },
    {
      "epoch": 3.2909783989834818,
      "grad_norm": 2.9591963291168213,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 10360
    },
    {
      "epoch": 3.2941550190597204,
      "grad_norm": 2.660616874694824,
      "learning_rate": 2e-05,
      "loss": 0.2178,
      "step": 10370
    },
    {
      "epoch": 3.2973316391359595,
      "grad_norm": 2.140669107437134,
      "learning_rate": 2e-05,
      "loss": 0.2131,
      "step": 10380
    },
    {
      "epoch": 3.300508259212198,
      "grad_norm": 3.0472350120544434,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 10390
    },
    {
      "epoch": 3.3020965692503177,
      "eval_loss": 1.7846345901489258,
      "eval_mse": 1.7831259382133704,
      "eval_pearson": 0.4074031937657335,
      "eval_runtime": 21.1943,
      "eval_samples_per_second": 1017.253,
      "eval_spearmanr": 0.40473374716988564,
      "eval_steps_per_second": 4.011,
      "step": 10395
    },
    {
      "epoch": 3.3036848792884372,
      "grad_norm": 2.5071861743927,
      "learning_rate": 2e-05,
      "loss": 0.2207,
      "step": 10400
    },
    {
      "epoch": 3.306861499364676,
      "grad_norm": 3.192103862762451,
      "learning_rate": 2e-05,
      "loss": 0.2361,
      "step": 10410
    },
    {
      "epoch": 3.310038119440915,
      "grad_norm": 3.7705495357513428,
      "learning_rate": 2e-05,
      "loss": 0.233,
      "step": 10420
    },
    {
      "epoch": 3.3132147395171536,
      "grad_norm": 3.6162827014923096,
      "learning_rate": 2e-05,
      "loss": 0.23,
      "step": 10430
    },
    {
      "epoch": 3.3163913595933927,
      "grad_norm": 2.178027868270874,
      "learning_rate": 2e-05,
      "loss": 0.2012,
      "step": 10440
    },
    {
      "epoch": 3.3195679796696314,
      "grad_norm": 3.2662010192871094,
      "learning_rate": 2e-05,
      "loss": 0.2365,
      "step": 10450
    },
    {
      "epoch": 3.3227445997458704,
      "grad_norm": 2.887310028076172,
      "learning_rate": 2e-05,
      "loss": 0.2171,
      "step": 10460
    },
    {
      "epoch": 3.325921219822109,
      "grad_norm": 2.5440940856933594,
      "learning_rate": 2e-05,
      "loss": 0.204,
      "step": 10470
    },
    {
      "epoch": 3.329097839898348,
      "grad_norm": 3.774993896484375,
      "learning_rate": 2e-05,
      "loss": 0.2181,
      "step": 10480
    },
    {
      "epoch": 3.3322744599745873,
      "grad_norm": 4.90976095199585,
      "learning_rate": 2e-05,
      "loss": 0.2296,
      "step": 10490
    },
    {
      "epoch": 3.335451080050826,
      "grad_norm": 2.038914918899536,
      "learning_rate": 2e-05,
      "loss": 0.2088,
      "step": 10500
    },
    {
      "epoch": 3.3386277001270646,
      "grad_norm": 2.557741165161133,
      "learning_rate": 2e-05,
      "loss": 0.2059,
      "step": 10510
    },
    {
      "epoch": 3.3418043202033036,
      "grad_norm": 2.519057035446167,
      "learning_rate": 2e-05,
      "loss": 0.2179,
      "step": 10520
    },
    {
      "epoch": 3.3449809402795427,
      "grad_norm": 4.4573974609375,
      "learning_rate": 2e-05,
      "loss": 0.2143,
      "step": 10530
    },
    {
      "epoch": 3.3481575603557814,
      "grad_norm": 3.3971638679504395,
      "learning_rate": 2e-05,
      "loss": 0.2115,
      "step": 10540
    },
    {
      "epoch": 3.3513341804320205,
      "grad_norm": 2.820972204208374,
      "learning_rate": 2e-05,
      "loss": 0.2163,
      "step": 10550
    },
    {
      "epoch": 3.354510800508259,
      "grad_norm": 2.7978053092956543,
      "learning_rate": 2e-05,
      "loss": 0.2196,
      "step": 10560
    },
    {
      "epoch": 3.357687420584498,
      "grad_norm": 4.200861930847168,
      "learning_rate": 2e-05,
      "loss": 0.2179,
      "step": 10570
    },
    {
      "epoch": 3.360864040660737,
      "grad_norm": 3.173243522644043,
      "learning_rate": 2e-05,
      "loss": 0.2242,
      "step": 10580
    },
    {
      "epoch": 3.364040660736976,
      "grad_norm": 2.7507777214050293,
      "learning_rate": 2e-05,
      "loss": 0.2137,
      "step": 10590
    },
    {
      "epoch": 3.3672172808132146,
      "grad_norm": 8.122220039367676,
      "learning_rate": 2e-05,
      "loss": 0.2116,
      "step": 10600
    },
    {
      "epoch": 3.3703939008894537,
      "grad_norm": 4.09391975402832,
      "learning_rate": 2e-05,
      "loss": 0.2141,
      "step": 10610
    },
    {
      "epoch": 3.3735705209656923,
      "grad_norm": 6.24989652633667,
      "learning_rate": 2e-05,
      "loss": 0.216,
      "step": 10620
    },
    {
      "epoch": 3.3767471410419314,
      "grad_norm": 2.2628824710845947,
      "learning_rate": 2e-05,
      "loss": 0.209,
      "step": 10630
    },
    {
      "epoch": 3.37992376111817,
      "grad_norm": 2.684957504272461,
      "learning_rate": 2e-05,
      "loss": 0.2241,
      "step": 10640
    },
    {
      "epoch": 3.383100381194409,
      "grad_norm": 2.8773257732391357,
      "learning_rate": 2e-05,
      "loss": 0.2002,
      "step": 10650
    },
    {
      "epoch": 3.3862770012706482,
      "grad_norm": 3.957995891571045,
      "learning_rate": 2e-05,
      "loss": 0.2415,
      "step": 10660
    },
    {
      "epoch": 3.389453621346887,
      "grad_norm": 3.2934229373931885,
      "learning_rate": 2e-05,
      "loss": 0.2374,
      "step": 10670
    },
    {
      "epoch": 3.392630241423126,
      "grad_norm": 2.686769723892212,
      "learning_rate": 2e-05,
      "loss": 0.2321,
      "step": 10680
    },
    {
      "epoch": 3.3958068614993646,
      "grad_norm": 2.9742302894592285,
      "learning_rate": 2e-05,
      "loss": 0.2176,
      "step": 10690
    },
    {
      "epoch": 3.3989834815756037,
      "grad_norm": 2.390333652496338,
      "learning_rate": 2e-05,
      "loss": 0.2089,
      "step": 10700
    },
    {
      "epoch": 3.4021601016518423,
      "grad_norm": 2.5817503929138184,
      "learning_rate": 2e-05,
      "loss": 0.2176,
      "step": 10710
    },
    {
      "epoch": 3.4021601016518423,
      "eval_loss": 1.6859549283981323,
      "eval_mse": 1.684737196586316,
      "eval_pearson": 0.457476411717944,
      "eval_runtime": 20.8732,
      "eval_samples_per_second": 1032.905,
      "eval_spearmanr": 0.45177032576203074,
      "eval_steps_per_second": 4.072,
      "step": 10710
    },
    {
      "epoch": 3.4053367217280814,
      "grad_norm": 4.009512901306152,
      "learning_rate": 2e-05,
      "loss": 0.2107,
      "step": 10720
    },
    {
      "epoch": 3.40851334180432,
      "grad_norm": 4.251843452453613,
      "learning_rate": 2e-05,
      "loss": 0.2048,
      "step": 10730
    },
    {
      "epoch": 3.411689961880559,
      "grad_norm": 3.485589027404785,
      "learning_rate": 2e-05,
      "loss": 0.2052,
      "step": 10740
    },
    {
      "epoch": 3.414866581956798,
      "grad_norm": 2.2894394397735596,
      "learning_rate": 2e-05,
      "loss": 0.2233,
      "step": 10750
    },
    {
      "epoch": 3.418043202033037,
      "grad_norm": 2.156787157058716,
      "learning_rate": 2e-05,
      "loss": 0.1985,
      "step": 10760
    },
    {
      "epoch": 3.4212198221092756,
      "grad_norm": 2.628117799758911,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 10770
    },
    {
      "epoch": 3.4243964421855146,
      "grad_norm": 2.7022218704223633,
      "learning_rate": 2e-05,
      "loss": 0.2104,
      "step": 10780
    },
    {
      "epoch": 3.4275730622617537,
      "grad_norm": 5.8504204750061035,
      "learning_rate": 2e-05,
      "loss": 0.2273,
      "step": 10790
    },
    {
      "epoch": 3.4307496823379924,
      "grad_norm": 2.9750494956970215,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 10800
    },
    {
      "epoch": 3.433926302414231,
      "grad_norm": 2.802844762802124,
      "learning_rate": 2e-05,
      "loss": 0.2056,
      "step": 10810
    },
    {
      "epoch": 3.43710292249047,
      "grad_norm": 2.0708227157592773,
      "learning_rate": 2e-05,
      "loss": 0.2137,
      "step": 10820
    },
    {
      "epoch": 3.440279542566709,
      "grad_norm": 2.4605329036712646,
      "learning_rate": 2e-05,
      "loss": 0.1987,
      "step": 10830
    },
    {
      "epoch": 3.443456162642948,
      "grad_norm": 3.1406259536743164,
      "learning_rate": 2e-05,
      "loss": 0.2152,
      "step": 10840
    },
    {
      "epoch": 3.446632782719187,
      "grad_norm": 2.981250286102295,
      "learning_rate": 2e-05,
      "loss": 0.215,
      "step": 10850
    },
    {
      "epoch": 3.4498094027954256,
      "grad_norm": 3.288848876953125,
      "learning_rate": 2e-05,
      "loss": 0.2083,
      "step": 10860
    },
    {
      "epoch": 3.4529860228716647,
      "grad_norm": 2.860619068145752,
      "learning_rate": 2e-05,
      "loss": 0.2171,
      "step": 10870
    },
    {
      "epoch": 3.4561626429479033,
      "grad_norm": 2.6720497608184814,
      "learning_rate": 2e-05,
      "loss": 0.2073,
      "step": 10880
    },
    {
      "epoch": 3.4593392630241424,
      "grad_norm": 2.371722936630249,
      "learning_rate": 2e-05,
      "loss": 0.2116,
      "step": 10890
    },
    {
      "epoch": 3.462515883100381,
      "grad_norm": 2.7400810718536377,
      "learning_rate": 2e-05,
      "loss": 0.2161,
      "step": 10900
    },
    {
      "epoch": 3.46569250317662,
      "grad_norm": 2.878258466720581,
      "learning_rate": 2e-05,
      "loss": 0.2094,
      "step": 10910
    },
    {
      "epoch": 3.468869123252859,
      "grad_norm": 3.885406732559204,
      "learning_rate": 2e-05,
      "loss": 0.2219,
      "step": 10920
    },
    {
      "epoch": 3.472045743329098,
      "grad_norm": 4.351571559906006,
      "learning_rate": 2e-05,
      "loss": 0.2255,
      "step": 10930
    },
    {
      "epoch": 3.4752223634053365,
      "grad_norm": 3.094216823577881,
      "learning_rate": 2e-05,
      "loss": 0.202,
      "step": 10940
    },
    {
      "epoch": 3.4783989834815756,
      "grad_norm": 3.8344337940216064,
      "learning_rate": 2e-05,
      "loss": 0.2083,
      "step": 10950
    },
    {
      "epoch": 3.4815756035578147,
      "grad_norm": 2.6872589588165283,
      "learning_rate": 2e-05,
      "loss": 0.2177,
      "step": 10960
    },
    {
      "epoch": 3.4847522236340533,
      "grad_norm": 2.438215732574463,
      "learning_rate": 2e-05,
      "loss": 0.2139,
      "step": 10970
    },
    {
      "epoch": 3.4879288437102924,
      "grad_norm": 3.2489006519317627,
      "learning_rate": 2e-05,
      "loss": 0.2097,
      "step": 10980
    },
    {
      "epoch": 3.491105463786531,
      "grad_norm": 1.943954586982727,
      "learning_rate": 2e-05,
      "loss": 0.2039,
      "step": 10990
    },
    {
      "epoch": 3.49428208386277,
      "grad_norm": 3.014626979827881,
      "learning_rate": 2e-05,
      "loss": 0.219,
      "step": 11000
    },
    {
      "epoch": 3.497458703939009,
      "grad_norm": 3.481919288635254,
      "learning_rate": 2e-05,
      "loss": 0.2039,
      "step": 11010
    },
    {
      "epoch": 3.500635324015248,
      "grad_norm": 3.251291275024414,
      "learning_rate": 2e-05,
      "loss": 0.2091,
      "step": 11020
    },
    {
      "epoch": 3.502223634053367,
      "eval_loss": 1.8205788135528564,
      "eval_mse": 1.8193959113092943,
      "eval_pearson": 0.43650068280466353,
      "eval_runtime": 21.0468,
      "eval_samples_per_second": 1024.382,
      "eval_spearmanr": 0.43327753599821234,
      "eval_steps_per_second": 4.039,
      "step": 11025
    },
    {
      "epoch": 3.5038119440914866,
      "grad_norm": 2.538656711578369,
      "learning_rate": 2e-05,
      "loss": 0.1942,
      "step": 11030
    },
    {
      "epoch": 3.5069885641677256,
      "grad_norm": 2.7338883876800537,
      "learning_rate": 2e-05,
      "loss": 0.2033,
      "step": 11040
    },
    {
      "epoch": 3.5101651842439643,
      "grad_norm": 3.737560987472534,
      "learning_rate": 2e-05,
      "loss": 0.2206,
      "step": 11050
    },
    {
      "epoch": 3.5133418043202034,
      "grad_norm": 3.077277183532715,
      "learning_rate": 2e-05,
      "loss": 0.2173,
      "step": 11060
    },
    {
      "epoch": 3.516518424396442,
      "grad_norm": 2.460134506225586,
      "learning_rate": 2e-05,
      "loss": 0.2065,
      "step": 11070
    },
    {
      "epoch": 3.519695044472681,
      "grad_norm": 3.0355608463287354,
      "learning_rate": 2e-05,
      "loss": 0.2065,
      "step": 11080
    },
    {
      "epoch": 3.52287166454892,
      "grad_norm": 2.8701882362365723,
      "learning_rate": 2e-05,
      "loss": 0.2159,
      "step": 11090
    },
    {
      "epoch": 3.526048284625159,
      "grad_norm": 2.179858684539795,
      "learning_rate": 2e-05,
      "loss": 0.2004,
      "step": 11100
    },
    {
      "epoch": 3.5292249047013975,
      "grad_norm": 2.413541793823242,
      "learning_rate": 2e-05,
      "loss": 0.2028,
      "step": 11110
    },
    {
      "epoch": 3.5324015247776366,
      "grad_norm": 3.343964099884033,
      "learning_rate": 2e-05,
      "loss": 0.2124,
      "step": 11120
    },
    {
      "epoch": 3.5355781448538757,
      "grad_norm": 1.847710132598877,
      "learning_rate": 2e-05,
      "loss": 0.1982,
      "step": 11130
    },
    {
      "epoch": 3.5387547649301143,
      "grad_norm": 2.8601505756378174,
      "learning_rate": 2e-05,
      "loss": 0.1957,
      "step": 11140
    },
    {
      "epoch": 3.5419313850063534,
      "grad_norm": 3.525635242462158,
      "learning_rate": 2e-05,
      "loss": 0.2038,
      "step": 11150
    },
    {
      "epoch": 3.545108005082592,
      "grad_norm": 4.258574485778809,
      "learning_rate": 2e-05,
      "loss": 0.1994,
      "step": 11160
    },
    {
      "epoch": 3.548284625158831,
      "grad_norm": 2.4065158367156982,
      "learning_rate": 2e-05,
      "loss": 0.2132,
      "step": 11170
    },
    {
      "epoch": 3.55146124523507,
      "grad_norm": 3.462231159210205,
      "learning_rate": 2e-05,
      "loss": 0.2193,
      "step": 11180
    },
    {
      "epoch": 3.554637865311309,
      "grad_norm": 3.0265471935272217,
      "learning_rate": 2e-05,
      "loss": 0.2125,
      "step": 11190
    },
    {
      "epoch": 3.5578144853875475,
      "grad_norm": 2.623699188232422,
      "learning_rate": 2e-05,
      "loss": 0.203,
      "step": 11200
    },
    {
      "epoch": 3.5609911054637866,
      "grad_norm": 3.030550718307495,
      "learning_rate": 2e-05,
      "loss": 0.21,
      "step": 11210
    },
    {
      "epoch": 3.5641677255400253,
      "grad_norm": 3.823986530303955,
      "learning_rate": 2e-05,
      "loss": 0.2001,
      "step": 11220
    },
    {
      "epoch": 3.5673443456162643,
      "grad_norm": 4.680149078369141,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 11230
    },
    {
      "epoch": 3.570520965692503,
      "grad_norm": 2.435485363006592,
      "learning_rate": 2e-05,
      "loss": 0.1881,
      "step": 11240
    },
    {
      "epoch": 3.573697585768742,
      "grad_norm": 3.4596025943756104,
      "learning_rate": 2e-05,
      "loss": 0.2068,
      "step": 11250
    },
    {
      "epoch": 3.576874205844981,
      "grad_norm": 1.7320520877838135,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 11260
    },
    {
      "epoch": 3.58005082592122,
      "grad_norm": 2.265493392944336,
      "learning_rate": 2e-05,
      "loss": 0.1997,
      "step": 11270
    },
    {
      "epoch": 3.5832274459974585,
      "grad_norm": 3.8226468563079834,
      "learning_rate": 2e-05,
      "loss": 0.2261,
      "step": 11280
    },
    {
      "epoch": 3.5864040660736975,
      "grad_norm": 3.9629979133605957,
      "learning_rate": 2e-05,
      "loss": 0.2068,
      "step": 11290
    },
    {
      "epoch": 3.5895806861499366,
      "grad_norm": 4.073217391967773,
      "learning_rate": 2e-05,
      "loss": 0.2046,
      "step": 11300
    },
    {
      "epoch": 3.5927573062261753,
      "grad_norm": 3.2781782150268555,
      "learning_rate": 2e-05,
      "loss": 0.1987,
      "step": 11310
    },
    {
      "epoch": 3.5959339263024144,
      "grad_norm": 3.0118939876556396,
      "learning_rate": 2e-05,
      "loss": 0.2166,
      "step": 11320
    },
    {
      "epoch": 3.599110546378653,
      "grad_norm": 2.154402256011963,
      "learning_rate": 2e-05,
      "loss": 0.2012,
      "step": 11330
    },
    {
      "epoch": 3.602287166454892,
      "grad_norm": 2.200242519378662,
      "learning_rate": 2e-05,
      "loss": 0.2278,
      "step": 11340
    },
    {
      "epoch": 3.602287166454892,
      "eval_loss": 1.6927770376205444,
      "eval_mse": 1.6917647178407502,
      "eval_pearson": 0.4029097873757187,
      "eval_runtime": 21.0634,
      "eval_samples_per_second": 1023.575,
      "eval_spearmanr": 0.3988698877200054,
      "eval_steps_per_second": 4.035,
      "step": 11340
    },
    {
      "epoch": 3.6054637865311308,
      "grad_norm": 1.9333387613296509,
      "learning_rate": 2e-05,
      "loss": 0.2208,
      "step": 11350
    },
    {
      "epoch": 3.60864040660737,
      "grad_norm": 3.0553700923919678,
      "learning_rate": 2e-05,
      "loss": 0.2117,
      "step": 11360
    },
    {
      "epoch": 3.6118170266836085,
      "grad_norm": 2.247231960296631,
      "learning_rate": 2e-05,
      "loss": 0.1902,
      "step": 11370
    },
    {
      "epoch": 3.6149936467598476,
      "grad_norm": 2.1994566917419434,
      "learning_rate": 2e-05,
      "loss": 0.2243,
      "step": 11380
    },
    {
      "epoch": 3.6181702668360867,
      "grad_norm": 1.9180128574371338,
      "learning_rate": 2e-05,
      "loss": 0.1958,
      "step": 11390
    },
    {
      "epoch": 3.6213468869123253,
      "grad_norm": 2.2180838584899902,
      "learning_rate": 2e-05,
      "loss": 0.2006,
      "step": 11400
    },
    {
      "epoch": 3.624523506988564,
      "grad_norm": 3.051724910736084,
      "learning_rate": 2e-05,
      "loss": 0.1972,
      "step": 11410
    },
    {
      "epoch": 3.627700127064803,
      "grad_norm": 2.7778775691986084,
      "learning_rate": 2e-05,
      "loss": 0.2115,
      "step": 11420
    },
    {
      "epoch": 3.630876747141042,
      "grad_norm": 2.6172878742218018,
      "learning_rate": 2e-05,
      "loss": 0.189,
      "step": 11430
    },
    {
      "epoch": 3.634053367217281,
      "grad_norm": 3.6251778602600098,
      "learning_rate": 2e-05,
      "loss": 0.2097,
      "step": 11440
    },
    {
      "epoch": 3.6372299872935194,
      "grad_norm": 2.4727766513824463,
      "learning_rate": 2e-05,
      "loss": 0.2025,
      "step": 11450
    },
    {
      "epoch": 3.6404066073697585,
      "grad_norm": 2.9940690994262695,
      "learning_rate": 2e-05,
      "loss": 0.1954,
      "step": 11460
    },
    {
      "epoch": 3.6435832274459976,
      "grad_norm": 2.5714807510375977,
      "learning_rate": 2e-05,
      "loss": 0.1991,
      "step": 11470
    },
    {
      "epoch": 3.6467598475222363,
      "grad_norm": 2.3408265113830566,
      "learning_rate": 2e-05,
      "loss": 0.2045,
      "step": 11480
    },
    {
      "epoch": 3.6499364675984753,
      "grad_norm": 2.912882089614868,
      "learning_rate": 2e-05,
      "loss": 0.2,
      "step": 11490
    },
    {
      "epoch": 3.653113087674714,
      "grad_norm": 3.719907522201538,
      "learning_rate": 2e-05,
      "loss": 0.1898,
      "step": 11500
    },
    {
      "epoch": 3.656289707750953,
      "grad_norm": 2.417173147201538,
      "learning_rate": 2e-05,
      "loss": 0.1967,
      "step": 11510
    },
    {
      "epoch": 3.6594663278271917,
      "grad_norm": 2.3007001876831055,
      "learning_rate": 2e-05,
      "loss": 0.2031,
      "step": 11520
    },
    {
      "epoch": 3.662642947903431,
      "grad_norm": 2.160078287124634,
      "learning_rate": 2e-05,
      "loss": 0.1948,
      "step": 11530
    },
    {
      "epoch": 3.6658195679796695,
      "grad_norm": 2.870339870452881,
      "learning_rate": 2e-05,
      "loss": 0.1981,
      "step": 11540
    },
    {
      "epoch": 3.6689961880559085,
      "grad_norm": 3.477931261062622,
      "learning_rate": 2e-05,
      "loss": 0.2164,
      "step": 11550
    },
    {
      "epoch": 3.6721728081321476,
      "grad_norm": 2.38228702545166,
      "learning_rate": 2e-05,
      "loss": 0.1922,
      "step": 11560
    },
    {
      "epoch": 3.6753494282083863,
      "grad_norm": 2.9705872535705566,
      "learning_rate": 2e-05,
      "loss": 0.1973,
      "step": 11570
    },
    {
      "epoch": 3.678526048284625,
      "grad_norm": 3.1615259647369385,
      "learning_rate": 2e-05,
      "loss": 0.2208,
      "step": 11580
    },
    {
      "epoch": 3.681702668360864,
      "grad_norm": 2.0312395095825195,
      "learning_rate": 2e-05,
      "loss": 0.2056,
      "step": 11590
    },
    {
      "epoch": 3.684879288437103,
      "grad_norm": 2.4534220695495605,
      "learning_rate": 2e-05,
      "loss": 0.2043,
      "step": 11600
    },
    {
      "epoch": 3.6880559085133418,
      "grad_norm": 2.376504421234131,
      "learning_rate": 2e-05,
      "loss": 0.1799,
      "step": 11610
    },
    {
      "epoch": 3.691232528589581,
      "grad_norm": 2.3049609661102295,
      "learning_rate": 2e-05,
      "loss": 0.1931,
      "step": 11620
    },
    {
      "epoch": 3.6944091486658195,
      "grad_norm": 2.3880717754364014,
      "learning_rate": 2e-05,
      "loss": 0.2043,
      "step": 11630
    },
    {
      "epoch": 3.6975857687420586,
      "grad_norm": 4.089442253112793,
      "learning_rate": 2e-05,
      "loss": 0.2043,
      "step": 11640
    },
    {
      "epoch": 3.700762388818297,
      "grad_norm": 3.9802870750427246,
      "learning_rate": 2e-05,
      "loss": 0.2064,
      "step": 11650
    },
    {
      "epoch": 3.7023506988564168,
      "eval_loss": 1.8027637004852295,
      "eval_mse": 1.8017944680544131,
      "eval_pearson": 0.4065984204758052,
      "eval_runtime": 21.1593,
      "eval_samples_per_second": 1018.938,
      "eval_spearmanr": 0.4034698459380352,
      "eval_steps_per_second": 4.017,
      "step": 11655
    },
    {
      "epoch": 3.7039390088945363,
      "grad_norm": 3.430603265762329,
      "learning_rate": 2e-05,
      "loss": 0.2009,
      "step": 11660
    },
    {
      "epoch": 3.707115628970775,
      "grad_norm": 2.627434730529785,
      "learning_rate": 2e-05,
      "loss": 0.2001,
      "step": 11670
    },
    {
      "epoch": 3.710292249047014,
      "grad_norm": 3.5364856719970703,
      "learning_rate": 2e-05,
      "loss": 0.2151,
      "step": 11680
    },
    {
      "epoch": 3.713468869123253,
      "grad_norm": 3.4284565448760986,
      "learning_rate": 2e-05,
      "loss": 0.197,
      "step": 11690
    },
    {
      "epoch": 3.716645489199492,
      "grad_norm": 2.6317203044891357,
      "learning_rate": 2e-05,
      "loss": 0.2032,
      "step": 11700
    },
    {
      "epoch": 3.7198221092757304,
      "grad_norm": 3.230733871459961,
      "learning_rate": 2e-05,
      "loss": 0.2078,
      "step": 11710
    },
    {
      "epoch": 3.7229987293519695,
      "grad_norm": 3.3341710567474365,
      "learning_rate": 2e-05,
      "loss": 0.1951,
      "step": 11720
    },
    {
      "epoch": 3.7261753494282086,
      "grad_norm": 4.641509056091309,
      "learning_rate": 2e-05,
      "loss": 0.2324,
      "step": 11730
    },
    {
      "epoch": 3.7293519695044473,
      "grad_norm": 2.4114933013916016,
      "learning_rate": 2e-05,
      "loss": 0.1864,
      "step": 11740
    },
    {
      "epoch": 3.732528589580686,
      "grad_norm": 2.4200377464294434,
      "learning_rate": 2e-05,
      "loss": 0.1947,
      "step": 11750
    },
    {
      "epoch": 3.735705209656925,
      "grad_norm": 2.778357744216919,
      "learning_rate": 2e-05,
      "loss": 0.1952,
      "step": 11760
    },
    {
      "epoch": 3.738881829733164,
      "grad_norm": 2.421680450439453,
      "learning_rate": 2e-05,
      "loss": 0.1956,
      "step": 11770
    },
    {
      "epoch": 3.7420584498094027,
      "grad_norm": 3.842000722885132,
      "learning_rate": 2e-05,
      "loss": 0.1905,
      "step": 11780
    },
    {
      "epoch": 3.745235069885642,
      "grad_norm": 3.5807125568389893,
      "learning_rate": 2e-05,
      "loss": 0.1955,
      "step": 11790
    },
    {
      "epoch": 3.7484116899618805,
      "grad_norm": 2.9838109016418457,
      "learning_rate": 2e-05,
      "loss": 0.1891,
      "step": 11800
    },
    {
      "epoch": 3.7515883100381195,
      "grad_norm": 2.0077435970306396,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 11810
    },
    {
      "epoch": 3.754764930114358,
      "grad_norm": 2.8858835697174072,
      "learning_rate": 2e-05,
      "loss": 0.1997,
      "step": 11820
    },
    {
      "epoch": 3.7579415501905973,
      "grad_norm": 2.345975399017334,
      "learning_rate": 2e-05,
      "loss": 0.1988,
      "step": 11830
    },
    {
      "epoch": 3.761118170266836,
      "grad_norm": 2.589146137237549,
      "learning_rate": 2e-05,
      "loss": 0.2092,
      "step": 11840
    },
    {
      "epoch": 3.764294790343075,
      "grad_norm": 3.794268846511841,
      "learning_rate": 2e-05,
      "loss": 0.2003,
      "step": 11850
    },
    {
      "epoch": 3.767471410419314,
      "grad_norm": 2.7064125537872314,
      "learning_rate": 2e-05,
      "loss": 0.1915,
      "step": 11860
    },
    {
      "epoch": 3.7706480304955527,
      "grad_norm": 2.101619005203247,
      "learning_rate": 2e-05,
      "loss": 0.1852,
      "step": 11870
    },
    {
      "epoch": 3.7738246505717914,
      "grad_norm": 3.673476219177246,
      "learning_rate": 2e-05,
      "loss": 0.1961,
      "step": 11880
    },
    {
      "epoch": 3.7770012706480305,
      "grad_norm": 2.2939341068267822,
      "learning_rate": 2e-05,
      "loss": 0.1925,
      "step": 11890
    },
    {
      "epoch": 3.7801778907242696,
      "grad_norm": 2.5866129398345947,
      "learning_rate": 2e-05,
      "loss": 0.2017,
      "step": 11900
    },
    {
      "epoch": 3.783354510800508,
      "grad_norm": 2.670820713043213,
      "learning_rate": 2e-05,
      "loss": 0.177,
      "step": 11910
    },
    {
      "epoch": 3.786531130876747,
      "grad_norm": 1.7998095750808716,
      "learning_rate": 2e-05,
      "loss": 0.1955,
      "step": 11920
    },
    {
      "epoch": 3.789707750952986,
      "grad_norm": 2.9982149600982666,
      "learning_rate": 2e-05,
      "loss": 0.1863,
      "step": 11930
    },
    {
      "epoch": 3.792884371029225,
      "grad_norm": 3.0232737064361572,
      "learning_rate": 2e-05,
      "loss": 0.2007,
      "step": 11940
    },
    {
      "epoch": 3.7960609911054637,
      "grad_norm": 2.1592748165130615,
      "learning_rate": 2e-05,
      "loss": 0.1911,
      "step": 11950
    },
    {
      "epoch": 3.799237611181703,
      "grad_norm": 3.113879919052124,
      "learning_rate": 2e-05,
      "loss": 0.1856,
      "step": 11960
    },
    {
      "epoch": 3.8024142312579414,
      "grad_norm": 1.8053675889968872,
      "learning_rate": 2e-05,
      "loss": 0.1917,
      "step": 11970
    },
    {
      "epoch": 3.8024142312579414,
      "eval_loss": 1.7199218273162842,
      "eval_mse": 1.7186541053851814,
      "eval_pearson": 0.3807594519239138,
      "eval_runtime": 21.0595,
      "eval_samples_per_second": 1023.768,
      "eval_spearmanr": 0.37973632267267926,
      "eval_steps_per_second": 4.036,
      "step": 11970
    },
    {
      "epoch": 3.8055908513341805,
      "grad_norm": 2.372039794921875,
      "learning_rate": 2e-05,
      "loss": 0.1939,
      "step": 11980
    },
    {
      "epoch": 3.808767471410419,
      "grad_norm": 3.1513800621032715,
      "learning_rate": 2e-05,
      "loss": 0.1958,
      "step": 11990
    },
    {
      "epoch": 3.8119440914866582,
      "grad_norm": 1.8363041877746582,
      "learning_rate": 2e-05,
      "loss": 0.1764,
      "step": 12000
    },
    {
      "epoch": 3.815120711562897,
      "grad_norm": 1.8886609077453613,
      "learning_rate": 2e-05,
      "loss": 0.1985,
      "step": 12010
    },
    {
      "epoch": 3.818297331639136,
      "grad_norm": 3.166513681411743,
      "learning_rate": 2e-05,
      "loss": 0.2042,
      "step": 12020
    },
    {
      "epoch": 3.821473951715375,
      "grad_norm": 2.805931806564331,
      "learning_rate": 2e-05,
      "loss": 0.1995,
      "step": 12030
    },
    {
      "epoch": 3.8246505717916137,
      "grad_norm": 3.937608003616333,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 12040
    },
    {
      "epoch": 3.8278271918678524,
      "grad_norm": 2.4035298824310303,
      "learning_rate": 2e-05,
      "loss": 0.2035,
      "step": 12050
    },
    {
      "epoch": 3.8310038119440915,
      "grad_norm": 3.285388231277466,
      "learning_rate": 2e-05,
      "loss": 0.1962,
      "step": 12060
    },
    {
      "epoch": 3.8341804320203305,
      "grad_norm": 2.909250020980835,
      "learning_rate": 2e-05,
      "loss": 0.2032,
      "step": 12070
    },
    {
      "epoch": 3.837357052096569,
      "grad_norm": 2.667973756790161,
      "learning_rate": 2e-05,
      "loss": 0.2025,
      "step": 12080
    },
    {
      "epoch": 3.8405336721728083,
      "grad_norm": 3.2282333374023438,
      "learning_rate": 2e-05,
      "loss": 0.1964,
      "step": 12090
    },
    {
      "epoch": 3.843710292249047,
      "grad_norm": 3.2469749450683594,
      "learning_rate": 2e-05,
      "loss": 0.1959,
      "step": 12100
    },
    {
      "epoch": 3.846886912325286,
      "grad_norm": 2.8451950550079346,
      "learning_rate": 2e-05,
      "loss": 0.1827,
      "step": 12110
    },
    {
      "epoch": 3.8500635324015247,
      "grad_norm": 3.219726324081421,
      "learning_rate": 2e-05,
      "loss": 0.2101,
      "step": 12120
    },
    {
      "epoch": 3.8532401524777637,
      "grad_norm": 2.6662485599517822,
      "learning_rate": 2e-05,
      "loss": 0.192,
      "step": 12130
    },
    {
      "epoch": 3.8564167725540024,
      "grad_norm": 3.065372943878174,
      "learning_rate": 2e-05,
      "loss": 0.1821,
      "step": 12140
    },
    {
      "epoch": 3.8595933926302415,
      "grad_norm": 2.925560712814331,
      "learning_rate": 2e-05,
      "loss": 0.1987,
      "step": 12150
    },
    {
      "epoch": 3.8627700127064806,
      "grad_norm": 2.876173257827759,
      "learning_rate": 2e-05,
      "loss": 0.1923,
      "step": 12160
    },
    {
      "epoch": 3.865946632782719,
      "grad_norm": 2.6837921142578125,
      "learning_rate": 2e-05,
      "loss": 0.1952,
      "step": 12170
    },
    {
      "epoch": 3.869123252858958,
      "grad_norm": 2.2386820316314697,
      "learning_rate": 2e-05,
      "loss": 0.2067,
      "step": 12180
    },
    {
      "epoch": 3.872299872935197,
      "grad_norm": 3.0897560119628906,
      "learning_rate": 2e-05,
      "loss": 0.2038,
      "step": 12190
    },
    {
      "epoch": 3.875476493011436,
      "grad_norm": 2.196995973587036,
      "learning_rate": 2e-05,
      "loss": 0.1844,
      "step": 12200
    },
    {
      "epoch": 3.8786531130876747,
      "grad_norm": 2.496647357940674,
      "learning_rate": 2e-05,
      "loss": 0.1959,
      "step": 12210
    },
    {
      "epoch": 3.8818297331639133,
      "grad_norm": 2.482942581176758,
      "learning_rate": 2e-05,
      "loss": 0.1896,
      "step": 12220
    },
    {
      "epoch": 3.8850063532401524,
      "grad_norm": 3.4233715534210205,
      "learning_rate": 2e-05,
      "loss": 0.1972,
      "step": 12230
    },
    {
      "epoch": 3.8881829733163915,
      "grad_norm": 2.9184556007385254,
      "learning_rate": 2e-05,
      "loss": 0.188,
      "step": 12240
    },
    {
      "epoch": 3.89135959339263,
      "grad_norm": 1.9983853101730347,
      "learning_rate": 2e-05,
      "loss": 0.1958,
      "step": 12250
    },
    {
      "epoch": 3.8945362134688692,
      "grad_norm": 2.069824695587158,
      "learning_rate": 2e-05,
      "loss": 0.1781,
      "step": 12260
    },
    {
      "epoch": 3.897712833545108,
      "grad_norm": 2.2110793590545654,
      "learning_rate": 2e-05,
      "loss": 0.1881,
      "step": 12270
    },
    {
      "epoch": 3.900889453621347,
      "grad_norm": 3.352032423019409,
      "learning_rate": 2e-05,
      "loss": 0.1964,
      "step": 12280
    },
    {
      "epoch": 3.9024777636594665,
      "eval_loss": 1.797168493270874,
      "eval_mse": 1.7964126251402945,
      "eval_pearson": 0.4173777262839412,
      "eval_runtime": 20.9494,
      "eval_samples_per_second": 1029.147,
      "eval_spearmanr": 0.41483619096935104,
      "eval_steps_per_second": 4.057,
      "step": 12285
    },
    {
      "epoch": 3.9040660736975856,
      "grad_norm": 2.7090587615966797,
      "learning_rate": 2e-05,
      "loss": 0.1889,
      "step": 12290
    },
    {
      "epoch": 3.9072426937738247,
      "grad_norm": 2.3792788982391357,
      "learning_rate": 2e-05,
      "loss": 0.1807,
      "step": 12300
    },
    {
      "epoch": 3.9104193138500634,
      "grad_norm": 2.0199174880981445,
      "learning_rate": 2e-05,
      "loss": 0.1916,
      "step": 12310
    },
    {
      "epoch": 3.9135959339263025,
      "grad_norm": 2.5627567768096924,
      "learning_rate": 2e-05,
      "loss": 0.1804,
      "step": 12320
    },
    {
      "epoch": 3.9167725540025415,
      "grad_norm": 2.2652883529663086,
      "learning_rate": 2e-05,
      "loss": 0.1935,
      "step": 12330
    },
    {
      "epoch": 3.91994917407878,
      "grad_norm": 2.53814697265625,
      "learning_rate": 2e-05,
      "loss": 0.1974,
      "step": 12340
    },
    {
      "epoch": 3.923125794155019,
      "grad_norm": 2.159897565841675,
      "learning_rate": 2e-05,
      "loss": 0.1781,
      "step": 12350
    },
    {
      "epoch": 3.926302414231258,
      "grad_norm": 2.4523470401763916,
      "learning_rate": 2e-05,
      "loss": 0.1837,
      "step": 12360
    },
    {
      "epoch": 3.929479034307497,
      "grad_norm": 2.3874990940093994,
      "learning_rate": 2e-05,
      "loss": 0.182,
      "step": 12370
    },
    {
      "epoch": 3.9326556543837357,
      "grad_norm": 2.0222537517547607,
      "learning_rate": 2e-05,
      "loss": 0.1897,
      "step": 12380
    },
    {
      "epoch": 3.9358322744599747,
      "grad_norm": 3.286623954772949,
      "learning_rate": 2e-05,
      "loss": 0.1931,
      "step": 12390
    },
    {
      "epoch": 3.9390088945362134,
      "grad_norm": 4.9442458152771,
      "learning_rate": 2e-05,
      "loss": 0.2072,
      "step": 12400
    },
    {
      "epoch": 3.9421855146124525,
      "grad_norm": 3.2376840114593506,
      "learning_rate": 2e-05,
      "loss": 0.178,
      "step": 12410
    },
    {
      "epoch": 3.945362134688691,
      "grad_norm": 2.6476340293884277,
      "learning_rate": 2e-05,
      "loss": 0.1906,
      "step": 12420
    },
    {
      "epoch": 3.94853875476493,
      "grad_norm": 2.416086435317993,
      "learning_rate": 2e-05,
      "loss": 0.1975,
      "step": 12430
    },
    {
      "epoch": 3.951715374841169,
      "grad_norm": 2.9773616790771484,
      "learning_rate": 2e-05,
      "loss": 0.18,
      "step": 12440
    },
    {
      "epoch": 3.954891994917408,
      "grad_norm": 2.8729166984558105,
      "learning_rate": 2e-05,
      "loss": 0.1831,
      "step": 12450
    },
    {
      "epoch": 3.9580686149936466,
      "grad_norm": 2.802509307861328,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 12460
    },
    {
      "epoch": 3.9612452350698857,
      "grad_norm": 2.147059917449951,
      "learning_rate": 2e-05,
      "loss": 0.2021,
      "step": 12470
    },
    {
      "epoch": 3.9644218551461243,
      "grad_norm": 2.0026638507843018,
      "learning_rate": 2e-05,
      "loss": 0.1896,
      "step": 12480
    },
    {
      "epoch": 3.9675984752223634,
      "grad_norm": 2.8366007804870605,
      "learning_rate": 2e-05,
      "loss": 0.1977,
      "step": 12490
    },
    {
      "epoch": 3.9707750952986025,
      "grad_norm": 3.178828716278076,
      "learning_rate": 2e-05,
      "loss": 0.1997,
      "step": 12500
    },
    {
      "epoch": 3.973951715374841,
      "grad_norm": 2.3758325576782227,
      "learning_rate": 2e-05,
      "loss": 0.1865,
      "step": 12510
    },
    {
      "epoch": 3.97712833545108,
      "grad_norm": 3.717837333679199,
      "learning_rate": 2e-05,
      "loss": 0.1733,
      "step": 12520
    },
    {
      "epoch": 3.980304955527319,
      "grad_norm": 11.307995796203613,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 12530
    },
    {
      "epoch": 3.983481575603558,
      "grad_norm": 2.792184829711914,
      "learning_rate": 2e-05,
      "loss": 0.1932,
      "step": 12540
    },
    {
      "epoch": 3.9866581956797966,
      "grad_norm": 3.240691661834717,
      "learning_rate": 2e-05,
      "loss": 0.1872,
      "step": 12550
    },
    {
      "epoch": 3.9898348157560357,
      "grad_norm": 3.781907081604004,
      "learning_rate": 2e-05,
      "loss": 0.2055,
      "step": 12560
    },
    {
      "epoch": 3.9930114358322744,
      "grad_norm": 2.766594409942627,
      "learning_rate": 2e-05,
      "loss": 0.1847,
      "step": 12570
    },
    {
      "epoch": 3.9961880559085134,
      "grad_norm": 2.4611165523529053,
      "learning_rate": 2e-05,
      "loss": 0.1972,
      "step": 12580
    },
    {
      "epoch": 3.999364675984752,
      "grad_norm": 2.030372381210327,
      "learning_rate": 2e-05,
      "loss": 0.1914,
      "step": 12590
    },
    {
      "epoch": 4.002541296060991,
      "grad_norm": 3.5981788635253906,
      "learning_rate": 2e-05,
      "loss": 0.1586,
      "step": 12600
    },
    {
      "epoch": 4.002541296060991,
      "eval_loss": 1.719434142112732,
      "eval_mse": 1.7180137013128383,
      "eval_pearson": 0.40994270832507285,
      "eval_runtime": 20.8601,
      "eval_samples_per_second": 1033.55,
      "eval_spearmanr": 0.40798501578543506,
      "eval_steps_per_second": 4.075,
      "step": 12600
    },
    {
      "epoch": 4.00571791613723,
      "grad_norm": 2.6813371181488037,
      "learning_rate": 2e-05,
      "loss": 0.1616,
      "step": 12610
    },
    {
      "epoch": 4.008894536213469,
      "grad_norm": 2.7600033283233643,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 12620
    },
    {
      "epoch": 4.012071156289708,
      "grad_norm": 2.9744791984558105,
      "learning_rate": 2e-05,
      "loss": 0.1681,
      "step": 12630
    },
    {
      "epoch": 4.015247776365946,
      "grad_norm": 2.595630168914795,
      "learning_rate": 2e-05,
      "loss": 0.1645,
      "step": 12640
    },
    {
      "epoch": 4.018424396442185,
      "grad_norm": 2.150660514831543,
      "learning_rate": 2e-05,
      "loss": 0.1744,
      "step": 12650
    },
    {
      "epoch": 4.021601016518424,
      "grad_norm": 2.377171516418457,
      "learning_rate": 2e-05,
      "loss": 0.1661,
      "step": 12660
    },
    {
      "epoch": 4.0247776365946635,
      "grad_norm": 2.1664180755615234,
      "learning_rate": 2e-05,
      "loss": 0.1739,
      "step": 12670
    },
    {
      "epoch": 4.027954256670903,
      "grad_norm": 2.04655122756958,
      "learning_rate": 2e-05,
      "loss": 0.1608,
      "step": 12680
    },
    {
      "epoch": 4.031130876747141,
      "grad_norm": 1.9071297645568848,
      "learning_rate": 2e-05,
      "loss": 0.1579,
      "step": 12690
    },
    {
      "epoch": 4.03430749682338,
      "grad_norm": 5.7149248123168945,
      "learning_rate": 2e-05,
      "loss": 0.1739,
      "step": 12700
    },
    {
      "epoch": 4.037484116899619,
      "grad_norm": 3.1992266178131104,
      "learning_rate": 2e-05,
      "loss": 0.1831,
      "step": 12710
    },
    {
      "epoch": 4.040660736975858,
      "grad_norm": 2.2474019527435303,
      "learning_rate": 2e-05,
      "loss": 0.1849,
      "step": 12720
    },
    {
      "epoch": 4.043837357052096,
      "grad_norm": 3.077971935272217,
      "learning_rate": 2e-05,
      "loss": 0.1688,
      "step": 12730
    },
    {
      "epoch": 4.047013977128335,
      "grad_norm": 3.5274055004119873,
      "learning_rate": 2e-05,
      "loss": 0.1564,
      "step": 12740
    },
    {
      "epoch": 4.050190597204574,
      "grad_norm": 2.51436710357666,
      "learning_rate": 2e-05,
      "loss": 0.166,
      "step": 12750
    },
    {
      "epoch": 4.0533672172808135,
      "grad_norm": 2.4796457290649414,
      "learning_rate": 2e-05,
      "loss": 0.165,
      "step": 12760
    },
    {
      "epoch": 4.056543837357052,
      "grad_norm": 2.876727342605591,
      "learning_rate": 2e-05,
      "loss": 0.1698,
      "step": 12770
    },
    {
      "epoch": 4.059720457433291,
      "grad_norm": 1.9764022827148438,
      "learning_rate": 2e-05,
      "loss": 0.1565,
      "step": 12780
    },
    {
      "epoch": 4.06289707750953,
      "grad_norm": 2.909414291381836,
      "learning_rate": 2e-05,
      "loss": 0.1759,
      "step": 12790
    },
    {
      "epoch": 4.066073697585769,
      "grad_norm": 4.346346378326416,
      "learning_rate": 2e-05,
      "loss": 0.1666,
      "step": 12800
    },
    {
      "epoch": 4.069250317662007,
      "grad_norm": 1.554752230644226,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 12810
    },
    {
      "epoch": 4.072426937738246,
      "grad_norm": 1.8517117500305176,
      "learning_rate": 2e-05,
      "loss": 0.1828,
      "step": 12820
    },
    {
      "epoch": 4.075603557814485,
      "grad_norm": 2.5141642093658447,
      "learning_rate": 2e-05,
      "loss": 0.1557,
      "step": 12830
    },
    {
      "epoch": 4.0787801778907244,
      "grad_norm": 2.3095288276672363,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 12840
    },
    {
      "epoch": 4.0819567979669635,
      "grad_norm": 3.0449604988098145,
      "learning_rate": 2e-05,
      "loss": 0.182,
      "step": 12850
    },
    {
      "epoch": 4.085133418043202,
      "grad_norm": 1.7172373533248901,
      "learning_rate": 2e-05,
      "loss": 0.1661,
      "step": 12860
    },
    {
      "epoch": 4.088310038119441,
      "grad_norm": 2.189490556716919,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 12870
    },
    {
      "epoch": 4.09148665819568,
      "grad_norm": 2.3670949935913086,
      "learning_rate": 2e-05,
      "loss": 0.177,
      "step": 12880
    },
    {
      "epoch": 4.094663278271919,
      "grad_norm": 2.3953769207000732,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 12890
    },
    {
      "epoch": 4.097839898348157,
      "grad_norm": 2.070521593093872,
      "learning_rate": 2e-05,
      "loss": 0.1688,
      "step": 12900
    },
    {
      "epoch": 4.101016518424396,
      "grad_norm": 2.4995248317718506,
      "learning_rate": 2e-05,
      "loss": 0.1529,
      "step": 12910
    },
    {
      "epoch": 4.102604828462516,
      "eval_loss": 1.7270668745040894,
      "eval_mse": 1.7258224333079224,
      "eval_pearson": 0.40369939924143994,
      "eval_runtime": 20.9566,
      "eval_samples_per_second": 1028.794,
      "eval_spearmanr": 0.40050666188831247,
      "eval_steps_per_second": 4.056,
      "step": 12915
    },
    {
      "epoch": 4.104193138500635,
      "grad_norm": 2.3201911449432373,
      "learning_rate": 2e-05,
      "loss": 0.159,
      "step": 12920
    },
    {
      "epoch": 4.1073697585768745,
      "grad_norm": 2.197577714920044,
      "learning_rate": 2e-05,
      "loss": 0.1672,
      "step": 12930
    },
    {
      "epoch": 4.110546378653113,
      "grad_norm": 1.9971994161605835,
      "learning_rate": 2e-05,
      "loss": 0.1648,
      "step": 12940
    },
    {
      "epoch": 4.113722998729352,
      "grad_norm": 2.3784947395324707,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 12950
    },
    {
      "epoch": 4.116899618805591,
      "grad_norm": 4.246170520782471,
      "learning_rate": 2e-05,
      "loss": 0.1774,
      "step": 12960
    },
    {
      "epoch": 4.12007623888183,
      "grad_norm": 2.291928768157959,
      "learning_rate": 2e-05,
      "loss": 0.1703,
      "step": 12970
    },
    {
      "epoch": 4.123252858958069,
      "grad_norm": 2.2288084030151367,
      "learning_rate": 2e-05,
      "loss": 0.1656,
      "step": 12980
    },
    {
      "epoch": 4.126429479034307,
      "grad_norm": 4.122826099395752,
      "learning_rate": 2e-05,
      "loss": 0.1673,
      "step": 12990
    },
    {
      "epoch": 4.129606099110546,
      "grad_norm": 3.2107279300689697,
      "learning_rate": 2e-05,
      "loss": 0.1788,
      "step": 13000
    },
    {
      "epoch": 4.132782719186785,
      "grad_norm": 1.8238416910171509,
      "learning_rate": 2e-05,
      "loss": 0.1612,
      "step": 13010
    },
    {
      "epoch": 4.1359593392630245,
      "grad_norm": 3.547684669494629,
      "learning_rate": 2e-05,
      "loss": 0.1871,
      "step": 13020
    },
    {
      "epoch": 4.139135959339263,
      "grad_norm": 2.6420514583587646,
      "learning_rate": 2e-05,
      "loss": 0.1785,
      "step": 13030
    },
    {
      "epoch": 4.142312579415502,
      "grad_norm": 1.912774920463562,
      "learning_rate": 2e-05,
      "loss": 0.1686,
      "step": 13040
    },
    {
      "epoch": 4.145489199491741,
      "grad_norm": 3.0087718963623047,
      "learning_rate": 2e-05,
      "loss": 0.1663,
      "step": 13050
    },
    {
      "epoch": 4.14866581956798,
      "grad_norm": 2.463958263397217,
      "learning_rate": 2e-05,
      "loss": 0.1749,
      "step": 13060
    },
    {
      "epoch": 4.151842439644218,
      "grad_norm": 3.9082224369049072,
      "learning_rate": 2e-05,
      "loss": 0.1691,
      "step": 13070
    },
    {
      "epoch": 4.155019059720457,
      "grad_norm": 2.011322498321533,
      "learning_rate": 2e-05,
      "loss": 0.1728,
      "step": 13080
    },
    {
      "epoch": 4.158195679796696,
      "grad_norm": 3.755474328994751,
      "learning_rate": 2e-05,
      "loss": 0.1762,
      "step": 13090
    },
    {
      "epoch": 4.161372299872935,
      "grad_norm": 3.0848307609558105,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 13100
    },
    {
      "epoch": 4.1645489199491745,
      "grad_norm": 4.415066719055176,
      "learning_rate": 2e-05,
      "loss": 0.1875,
      "step": 13110
    },
    {
      "epoch": 4.167725540025413,
      "grad_norm": 6.237598896026611,
      "learning_rate": 2e-05,
      "loss": 0.1706,
      "step": 13120
    },
    {
      "epoch": 4.170902160101652,
      "grad_norm": 2.8897628784179688,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 13130
    },
    {
      "epoch": 4.174078780177891,
      "grad_norm": 2.8088788986206055,
      "learning_rate": 2e-05,
      "loss": 0.1609,
      "step": 13140
    },
    {
      "epoch": 4.17725540025413,
      "grad_norm": 2.182234287261963,
      "learning_rate": 2e-05,
      "loss": 0.1744,
      "step": 13150
    },
    {
      "epoch": 4.180432020330368,
      "grad_norm": 2.0803399085998535,
      "learning_rate": 2e-05,
      "loss": 0.1659,
      "step": 13160
    },
    {
      "epoch": 4.183608640406607,
      "grad_norm": 2.3240885734558105,
      "learning_rate": 2e-05,
      "loss": 0.1856,
      "step": 13170
    },
    {
      "epoch": 4.186785260482846,
      "grad_norm": 2.1017050743103027,
      "learning_rate": 2e-05,
      "loss": 0.1746,
      "step": 13180
    },
    {
      "epoch": 4.1899618805590855,
      "grad_norm": 1.7493911981582642,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 13190
    },
    {
      "epoch": 4.193138500635324,
      "grad_norm": 2.3536746501922607,
      "learning_rate": 2e-05,
      "loss": 0.1686,
      "step": 13200
    },
    {
      "epoch": 4.196315120711563,
      "grad_norm": 4.319462776184082,
      "learning_rate": 2e-05,
      "loss": 0.1702,
      "step": 13210
    },
    {
      "epoch": 4.199491740787802,
      "grad_norm": 34.11221694946289,
      "learning_rate": 2e-05,
      "loss": 0.182,
      "step": 13220
    },
    {
      "epoch": 4.202668360864041,
      "grad_norm": 2.6573219299316406,
      "learning_rate": 2e-05,
      "loss": 0.1828,
      "step": 13230
    },
    {
      "epoch": 4.202668360864041,
      "eval_loss": 1.709825038909912,
      "eval_mse": 1.7084382969091243,
      "eval_pearson": 0.4239711777305685,
      "eval_runtime": 20.8411,
      "eval_samples_per_second": 1034.494,
      "eval_spearmanr": 0.422519716785231,
      "eval_steps_per_second": 4.078,
      "step": 13230
    },
    {
      "epoch": 4.205844980940279,
      "grad_norm": 2.2078919410705566,
      "learning_rate": 2e-05,
      "loss": 0.1577,
      "step": 13240
    },
    {
      "epoch": 4.209021601016518,
      "grad_norm": 2.4290316104888916,
      "learning_rate": 2e-05,
      "loss": 0.1608,
      "step": 13250
    },
    {
      "epoch": 4.212198221092757,
      "grad_norm": 1.880593180656433,
      "learning_rate": 2e-05,
      "loss": 0.172,
      "step": 13260
    },
    {
      "epoch": 4.215374841168996,
      "grad_norm": 2.517854690551758,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 13270
    },
    {
      "epoch": 4.2185514612452355,
      "grad_norm": 3.288969039916992,
      "learning_rate": 2e-05,
      "loss": 0.1741,
      "step": 13280
    },
    {
      "epoch": 4.221728081321474,
      "grad_norm": 4.484128475189209,
      "learning_rate": 2e-05,
      "loss": 0.1688,
      "step": 13290
    },
    {
      "epoch": 4.224904701397713,
      "grad_norm": 2.002479314804077,
      "learning_rate": 2e-05,
      "loss": 0.1656,
      "step": 13300
    },
    {
      "epoch": 4.228081321473952,
      "grad_norm": 3.824051856994629,
      "learning_rate": 2e-05,
      "loss": 0.1631,
      "step": 13310
    },
    {
      "epoch": 4.231257941550191,
      "grad_norm": 3.2868175506591797,
      "learning_rate": 2e-05,
      "loss": 0.1794,
      "step": 13320
    },
    {
      "epoch": 4.234434561626429,
      "grad_norm": 3.501511335372925,
      "learning_rate": 2e-05,
      "loss": 0.1612,
      "step": 13330
    },
    {
      "epoch": 4.237611181702668,
      "grad_norm": 13.442681312561035,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 13340
    },
    {
      "epoch": 4.240787801778907,
      "grad_norm": 2.571655750274658,
      "learning_rate": 2e-05,
      "loss": 0.1764,
      "step": 13350
    },
    {
      "epoch": 4.243964421855146,
      "grad_norm": 7.126721382141113,
      "learning_rate": 2e-05,
      "loss": 0.1648,
      "step": 13360
    },
    {
      "epoch": 4.247141041931385,
      "grad_norm": 1.9929304122924805,
      "learning_rate": 2e-05,
      "loss": 0.1623,
      "step": 13370
    },
    {
      "epoch": 4.250317662007624,
      "grad_norm": 2.683565139770508,
      "learning_rate": 2e-05,
      "loss": 0.176,
      "step": 13380
    },
    {
      "epoch": 4.253494282083863,
      "grad_norm": 1.918164849281311,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 13390
    },
    {
      "epoch": 4.256670902160102,
      "grad_norm": 2.5857787132263184,
      "learning_rate": 2e-05,
      "loss": 0.1478,
      "step": 13400
    },
    {
      "epoch": 4.25984752223634,
      "grad_norm": 2.159451723098755,
      "learning_rate": 2e-05,
      "loss": 0.1602,
      "step": 13410
    },
    {
      "epoch": 4.263024142312579,
      "grad_norm": 3.1054487228393555,
      "learning_rate": 2e-05,
      "loss": 0.1612,
      "step": 13420
    },
    {
      "epoch": 4.266200762388818,
      "grad_norm": 2.313448429107666,
      "learning_rate": 2e-05,
      "loss": 0.1663,
      "step": 13430
    },
    {
      "epoch": 4.269377382465057,
      "grad_norm": 2.399768114089966,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 13440
    },
    {
      "epoch": 4.2725540025412965,
      "grad_norm": 2.127173662185669,
      "learning_rate": 2e-05,
      "loss": 0.155,
      "step": 13450
    },
    {
      "epoch": 4.275730622617535,
      "grad_norm": 1.645073652267456,
      "learning_rate": 2e-05,
      "loss": 0.1473,
      "step": 13460
    },
    {
      "epoch": 4.278907242693774,
      "grad_norm": 2.5621497631073,
      "learning_rate": 2e-05,
      "loss": 0.1763,
      "step": 13470
    },
    {
      "epoch": 4.282083862770013,
      "grad_norm": 1.9842020273208618,
      "learning_rate": 2e-05,
      "loss": 0.1602,
      "step": 13480
    },
    {
      "epoch": 4.285260482846252,
      "grad_norm": 2.405153512954712,
      "learning_rate": 2e-05,
      "loss": 0.1764,
      "step": 13490
    },
    {
      "epoch": 4.28843710292249,
      "grad_norm": 3.371049165725708,
      "learning_rate": 2e-05,
      "loss": 0.1684,
      "step": 13500
    },
    {
      "epoch": 4.291613722998729,
      "grad_norm": 10.624505996704102,
      "learning_rate": 2e-05,
      "loss": 0.1645,
      "step": 13510
    },
    {
      "epoch": 4.294790343074968,
      "grad_norm": 2.8362393379211426,
      "learning_rate": 2e-05,
      "loss": 0.1601,
      "step": 13520
    },
    {
      "epoch": 4.297966963151207,
      "grad_norm": 4.034559726715088,
      "learning_rate": 2e-05,
      "loss": 0.1714,
      "step": 13530
    },
    {
      "epoch": 4.301143583227446,
      "grad_norm": 1.6837050914764404,
      "learning_rate": 2e-05,
      "loss": 0.1596,
      "step": 13540
    },
    {
      "epoch": 4.302731893265565,
      "eval_loss": 1.72792387008667,
      "eval_mse": 1.7269227681443067,
      "eval_pearson": 0.40565686538525214,
      "eval_runtime": 21.0585,
      "eval_samples_per_second": 1023.813,
      "eval_spearmanr": 0.40346720168748973,
      "eval_steps_per_second": 4.036,
      "step": 13545
    },
    {
      "epoch": 4.304320203303685,
      "grad_norm": 2.6848130226135254,
      "learning_rate": 2e-05,
      "loss": 0.1763,
      "step": 13550
    },
    {
      "epoch": 4.307496823379924,
      "grad_norm": 2.824908971786499,
      "learning_rate": 2e-05,
      "loss": 0.17,
      "step": 13560
    },
    {
      "epoch": 4.310673443456163,
      "grad_norm": 2.3002445697784424,
      "learning_rate": 2e-05,
      "loss": 0.1672,
      "step": 13570
    },
    {
      "epoch": 4.313850063532401,
      "grad_norm": 3.3178365230560303,
      "learning_rate": 2e-05,
      "loss": 0.1693,
      "step": 13580
    },
    {
      "epoch": 4.31702668360864,
      "grad_norm": 2.680577278137207,
      "learning_rate": 2e-05,
      "loss": 0.16,
      "step": 13590
    },
    {
      "epoch": 4.320203303684879,
      "grad_norm": 2.9242234230041504,
      "learning_rate": 2e-05,
      "loss": 0.1612,
      "step": 13600
    },
    {
      "epoch": 4.323379923761118,
      "grad_norm": 2.1841635704040527,
      "learning_rate": 2e-05,
      "loss": 0.1546,
      "step": 13610
    },
    {
      "epoch": 4.326556543837357,
      "grad_norm": 2.113302230834961,
      "learning_rate": 2e-05,
      "loss": 0.1627,
      "step": 13620
    },
    {
      "epoch": 4.329733163913596,
      "grad_norm": 2.201087713241577,
      "learning_rate": 2e-05,
      "loss": 0.1696,
      "step": 13630
    },
    {
      "epoch": 4.332909783989835,
      "grad_norm": 2.9318978786468506,
      "learning_rate": 2e-05,
      "loss": 0.1596,
      "step": 13640
    },
    {
      "epoch": 4.336086404066074,
      "grad_norm": 1.9519648551940918,
      "learning_rate": 2e-05,
      "loss": 0.1638,
      "step": 13650
    },
    {
      "epoch": 4.339263024142313,
      "grad_norm": 1.8555757999420166,
      "learning_rate": 2e-05,
      "loss": 0.1725,
      "step": 13660
    },
    {
      "epoch": 4.342439644218551,
      "grad_norm": 2.732764720916748,
      "learning_rate": 2e-05,
      "loss": 0.1671,
      "step": 13670
    },
    {
      "epoch": 4.34561626429479,
      "grad_norm": 1.7662278413772583,
      "learning_rate": 2e-05,
      "loss": 0.1701,
      "step": 13680
    },
    {
      "epoch": 4.348792884371029,
      "grad_norm": 2.0117745399475098,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 13690
    },
    {
      "epoch": 4.351969504447268,
      "grad_norm": 3.422910690307617,
      "learning_rate": 2e-05,
      "loss": 0.1682,
      "step": 13700
    },
    {
      "epoch": 4.355146124523507,
      "grad_norm": 3.897986888885498,
      "learning_rate": 2e-05,
      "loss": 0.1644,
      "step": 13710
    },
    {
      "epoch": 4.358322744599746,
      "grad_norm": 2.6587107181549072,
      "learning_rate": 2e-05,
      "loss": 0.1601,
      "step": 13720
    },
    {
      "epoch": 4.361499364675985,
      "grad_norm": 2.654414653778076,
      "learning_rate": 2e-05,
      "loss": 0.1782,
      "step": 13730
    },
    {
      "epoch": 4.364675984752224,
      "grad_norm": 2.467900514602661,
      "learning_rate": 2e-05,
      "loss": 0.1726,
      "step": 13740
    },
    {
      "epoch": 4.367852604828462,
      "grad_norm": 2.618293285369873,
      "learning_rate": 2e-05,
      "loss": 0.1816,
      "step": 13750
    },
    {
      "epoch": 4.371029224904701,
      "grad_norm": 3.419365644454956,
      "learning_rate": 2e-05,
      "loss": 0.1575,
      "step": 13760
    },
    {
      "epoch": 4.37420584498094,
      "grad_norm": 2.3186087608337402,
      "learning_rate": 2e-05,
      "loss": 0.1565,
      "step": 13770
    },
    {
      "epoch": 4.377382465057179,
      "grad_norm": 1.9741525650024414,
      "learning_rate": 2e-05,
      "loss": 0.1447,
      "step": 13780
    },
    {
      "epoch": 4.380559085133418,
      "grad_norm": 2.2583584785461426,
      "learning_rate": 2e-05,
      "loss": 0.1465,
      "step": 13790
    },
    {
      "epoch": 4.383735705209657,
      "grad_norm": 3.3373773097991943,
      "learning_rate": 2e-05,
      "loss": 0.1701,
      "step": 13800
    },
    {
      "epoch": 4.386912325285896,
      "grad_norm": 2.1175737380981445,
      "learning_rate": 2e-05,
      "loss": 0.1766,
      "step": 13810
    },
    {
      "epoch": 4.390088945362135,
      "grad_norm": 2.1077523231506348,
      "learning_rate": 2e-05,
      "loss": 0.1852,
      "step": 13820
    },
    {
      "epoch": 4.393265565438374,
      "grad_norm": 1.6512750387191772,
      "learning_rate": 2e-05,
      "loss": 0.1674,
      "step": 13830
    },
    {
      "epoch": 4.396442185514612,
      "grad_norm": 3.381021022796631,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 13840
    },
    {
      "epoch": 4.399618805590851,
      "grad_norm": 1.8231226205825806,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 13850
    },
    {
      "epoch": 4.40279542566709,
      "grad_norm": 3.251600503921509,
      "learning_rate": 2e-05,
      "loss": 0.172,
      "step": 13860
    },
    {
      "epoch": 4.40279542566709,
      "eval_loss": 1.7879269123077393,
      "eval_mse": 1.7864626682755678,
      "eval_pearson": 0.3962521832825613,
      "eval_runtime": 20.9739,
      "eval_samples_per_second": 1027.942,
      "eval_spearmanr": 0.39255762927384136,
      "eval_steps_per_second": 4.053,
      "step": 13860
    },
    {
      "epoch": 4.405972045743329,
      "grad_norm": 2.9443166255950928,
      "learning_rate": 2e-05,
      "loss": 0.1626,
      "step": 13870
    },
    {
      "epoch": 4.409148665819568,
      "grad_norm": 1.9941762685775757,
      "learning_rate": 2e-05,
      "loss": 0.1594,
      "step": 13880
    },
    {
      "epoch": 4.412325285895807,
      "grad_norm": 2.091804027557373,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 13890
    },
    {
      "epoch": 4.415501905972046,
      "grad_norm": 2.2780632972717285,
      "learning_rate": 2e-05,
      "loss": 0.1615,
      "step": 13900
    },
    {
      "epoch": 4.418678526048285,
      "grad_norm": 2.9292702674865723,
      "learning_rate": 2e-05,
      "loss": 0.1702,
      "step": 13910
    },
    {
      "epoch": 4.421855146124524,
      "grad_norm": 2.439195156097412,
      "learning_rate": 2e-05,
      "loss": 0.1578,
      "step": 13920
    },
    {
      "epoch": 4.425031766200762,
      "grad_norm": 2.7261319160461426,
      "learning_rate": 2e-05,
      "loss": 0.1636,
      "step": 13930
    },
    {
      "epoch": 4.428208386277001,
      "grad_norm": 3.9469919204711914,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 13940
    },
    {
      "epoch": 4.43138500635324,
      "grad_norm": 2.6512842178344727,
      "learning_rate": 2e-05,
      "loss": 0.1519,
      "step": 13950
    },
    {
      "epoch": 4.434561626429479,
      "grad_norm": 2.4634673595428467,
      "learning_rate": 2e-05,
      "loss": 0.1623,
      "step": 13960
    },
    {
      "epoch": 4.437738246505718,
      "grad_norm": 2.211742639541626,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 13970
    },
    {
      "epoch": 4.440914866581957,
      "grad_norm": 2.5762836933135986,
      "learning_rate": 2e-05,
      "loss": 0.1589,
      "step": 13980
    },
    {
      "epoch": 4.444091486658196,
      "grad_norm": 4.2182393074035645,
      "learning_rate": 2e-05,
      "loss": 0.1691,
      "step": 13990
    },
    {
      "epoch": 4.447268106734435,
      "grad_norm": 2.688175678253174,
      "learning_rate": 2e-05,
      "loss": 0.1704,
      "step": 14000
    },
    {
      "epoch": 4.450444726810673,
      "grad_norm": 2.7192940711975098,
      "learning_rate": 2e-05,
      "loss": 0.1917,
      "step": 14010
    },
    {
      "epoch": 4.453621346886912,
      "grad_norm": 1.6439645290374756,
      "learning_rate": 2e-05,
      "loss": 0.1539,
      "step": 14020
    },
    {
      "epoch": 4.456797966963151,
      "grad_norm": 2.096090793609619,
      "learning_rate": 2e-05,
      "loss": 0.1515,
      "step": 14030
    },
    {
      "epoch": 4.45997458703939,
      "grad_norm": 2.530884265899658,
      "learning_rate": 2e-05,
      "loss": 0.1562,
      "step": 14040
    },
    {
      "epoch": 4.463151207115629,
      "grad_norm": 2.212120294570923,
      "learning_rate": 2e-05,
      "loss": 0.1515,
      "step": 14050
    },
    {
      "epoch": 4.466327827191868,
      "grad_norm": 1.7652995586395264,
      "learning_rate": 2e-05,
      "loss": 0.1606,
      "step": 14060
    },
    {
      "epoch": 4.469504447268107,
      "grad_norm": 2.505284547805786,
      "learning_rate": 2e-05,
      "loss": 0.1711,
      "step": 14070
    },
    {
      "epoch": 4.472681067344346,
      "grad_norm": 2.523336887359619,
      "learning_rate": 2e-05,
      "loss": 0.1725,
      "step": 14080
    },
    {
      "epoch": 4.475857687420585,
      "grad_norm": 3.0782809257507324,
      "learning_rate": 2e-05,
      "loss": 0.1705,
      "step": 14090
    },
    {
      "epoch": 4.479034307496823,
      "grad_norm": 2.1079165935516357,
      "learning_rate": 2e-05,
      "loss": 0.1673,
      "step": 14100
    },
    {
      "epoch": 4.482210927573062,
      "grad_norm": 2.348526954650879,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 14110
    },
    {
      "epoch": 4.485387547649301,
      "grad_norm": 5.007017135620117,
      "learning_rate": 2e-05,
      "loss": 0.1554,
      "step": 14120
    },
    {
      "epoch": 4.48856416772554,
      "grad_norm": 1.7387794256210327,
      "learning_rate": 2e-05,
      "loss": 0.1604,
      "step": 14130
    },
    {
      "epoch": 4.4917407878017785,
      "grad_norm": 2.346198320388794,
      "learning_rate": 2e-05,
      "loss": 0.1462,
      "step": 14140
    },
    {
      "epoch": 4.494917407878018,
      "grad_norm": 3.6724741458892822,
      "learning_rate": 2e-05,
      "loss": 0.1635,
      "step": 14150
    },
    {
      "epoch": 4.498094027954257,
      "grad_norm": 2.265444755554199,
      "learning_rate": 2e-05,
      "loss": 0.1739,
      "step": 14160
    },
    {
      "epoch": 4.501270648030496,
      "grad_norm": 2.138016939163208,
      "learning_rate": 2e-05,
      "loss": 0.1523,
      "step": 14170
    },
    {
      "epoch": 4.502858958068615,
      "eval_loss": 1.8449493646621704,
      "eval_mse": 1.8425351707214672,
      "eval_pearson": 0.41259766588338087,
      "eval_runtime": 20.8579,
      "eval_samples_per_second": 1033.661,
      "eval_spearmanr": 0.41138858088048386,
      "eval_steps_per_second": 4.075,
      "step": 14175
    },
    {
      "epoch": 4.504447268106734,
      "grad_norm": 2.2210161685943604,
      "learning_rate": 2e-05,
      "loss": 0.1553,
      "step": 14180
    },
    {
      "epoch": 4.507623888182973,
      "grad_norm": 4.658461093902588,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 14190
    },
    {
      "epoch": 4.510800508259212,
      "grad_norm": 3.5512163639068604,
      "learning_rate": 2e-05,
      "loss": 0.1694,
      "step": 14200
    },
    {
      "epoch": 4.513977128335451,
      "grad_norm": 2.215221643447876,
      "learning_rate": 2e-05,
      "loss": 0.1614,
      "step": 14210
    },
    {
      "epoch": 4.51715374841169,
      "grad_norm": 1.8517506122589111,
      "learning_rate": 2e-05,
      "loss": 0.1572,
      "step": 14220
    },
    {
      "epoch": 4.520330368487929,
      "grad_norm": 2.2377710342407227,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 14230
    },
    {
      "epoch": 4.523506988564168,
      "grad_norm": 5.018138885498047,
      "learning_rate": 2e-05,
      "loss": 0.1486,
      "step": 14240
    },
    {
      "epoch": 4.526683608640407,
      "grad_norm": 3.9251325130462646,
      "learning_rate": 2e-05,
      "loss": 0.1625,
      "step": 14250
    },
    {
      "epoch": 4.529860228716646,
      "grad_norm": 3.1286585330963135,
      "learning_rate": 2e-05,
      "loss": 0.1799,
      "step": 14260
    },
    {
      "epoch": 4.533036848792884,
      "grad_norm": 1.6613706350326538,
      "learning_rate": 2e-05,
      "loss": 0.1519,
      "step": 14270
    },
    {
      "epoch": 4.536213468869123,
      "grad_norm": 2.3336336612701416,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 14280
    },
    {
      "epoch": 4.539390088945362,
      "grad_norm": 1.9381206035614014,
      "learning_rate": 2e-05,
      "loss": 0.1585,
      "step": 14290
    },
    {
      "epoch": 4.542566709021601,
      "grad_norm": 2.364943504333496,
      "learning_rate": 2e-05,
      "loss": 0.1526,
      "step": 14300
    },
    {
      "epoch": 4.5457433290978395,
      "grad_norm": 2.780709743499756,
      "learning_rate": 2e-05,
      "loss": 0.1557,
      "step": 14310
    },
    {
      "epoch": 4.548919949174079,
      "grad_norm": 1.9464606046676636,
      "learning_rate": 2e-05,
      "loss": 0.1658,
      "step": 14320
    },
    {
      "epoch": 4.552096569250318,
      "grad_norm": 2.3874754905700684,
      "learning_rate": 2e-05,
      "loss": 0.1528,
      "step": 14330
    },
    {
      "epoch": 4.555273189326557,
      "grad_norm": 2.1522655487060547,
      "learning_rate": 2e-05,
      "loss": 0.1549,
      "step": 14340
    },
    {
      "epoch": 4.558449809402795,
      "grad_norm": 2.5261106491088867,
      "learning_rate": 2e-05,
      "loss": 0.1491,
      "step": 14350
    },
    {
      "epoch": 4.561626429479034,
      "grad_norm": 2.115961790084839,
      "learning_rate": 2e-05,
      "loss": 0.162,
      "step": 14360
    },
    {
      "epoch": 4.564803049555273,
      "grad_norm": 2.689319133758545,
      "learning_rate": 2e-05,
      "loss": 0.1722,
      "step": 14370
    },
    {
      "epoch": 4.567979669631512,
      "grad_norm": 1.890648603439331,
      "learning_rate": 2e-05,
      "loss": 0.1497,
      "step": 14380
    },
    {
      "epoch": 4.571156289707751,
      "grad_norm": 2.620800733566284,
      "learning_rate": 2e-05,
      "loss": 0.1586,
      "step": 14390
    },
    {
      "epoch": 4.5743329097839895,
      "grad_norm": 2.70072340965271,
      "learning_rate": 2e-05,
      "loss": 0.1696,
      "step": 14400
    },
    {
      "epoch": 4.577509529860229,
      "grad_norm": 2.6487090587615967,
      "learning_rate": 2e-05,
      "loss": 0.1603,
      "step": 14410
    },
    {
      "epoch": 4.580686149936468,
      "grad_norm": 2.494898796081543,
      "learning_rate": 2e-05,
      "loss": 0.1588,
      "step": 14420
    },
    {
      "epoch": 4.583862770012707,
      "grad_norm": 3.172987937927246,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 14430
    },
    {
      "epoch": 4.587039390088945,
      "grad_norm": 2.1166269779205322,
      "learning_rate": 2e-05,
      "loss": 0.1814,
      "step": 14440
    },
    {
      "epoch": 4.590216010165184,
      "grad_norm": 2.0654003620147705,
      "learning_rate": 2e-05,
      "loss": 0.1483,
      "step": 14450
    },
    {
      "epoch": 4.593392630241423,
      "grad_norm": 2.5902786254882812,
      "learning_rate": 2e-05,
      "loss": 0.1407,
      "step": 14460
    },
    {
      "epoch": 4.596569250317662,
      "grad_norm": 2.344473123550415,
      "learning_rate": 2e-05,
      "loss": 0.1553,
      "step": 14470
    },
    {
      "epoch": 4.599745870393901,
      "grad_norm": 2.1018295288085938,
      "learning_rate": 2e-05,
      "loss": 0.1555,
      "step": 14480
    },
    {
      "epoch": 4.60292249047014,
      "grad_norm": 2.6575636863708496,
      "learning_rate": 2e-05,
      "loss": 0.1582,
      "step": 14490
    },
    {
      "epoch": 4.60292249047014,
      "eval_loss": 1.716693639755249,
      "eval_mse": 1.7155409156197963,
      "eval_pearson": 0.42701806663771946,
      "eval_runtime": 21.0569,
      "eval_samples_per_second": 1023.891,
      "eval_spearmanr": 0.4273770067329388,
      "eval_steps_per_second": 4.037,
      "step": 14490
    },
    {
      "epoch": 4.606099110546379,
      "grad_norm": 2.9278199672698975,
      "learning_rate": 2e-05,
      "loss": 0.1586,
      "step": 14500
    },
    {
      "epoch": 4.609275730622618,
      "grad_norm": 2.297667980194092,
      "learning_rate": 2e-05,
      "loss": 0.1382,
      "step": 14510
    },
    {
      "epoch": 4.612452350698856,
      "grad_norm": 2.3938472270965576,
      "learning_rate": 2e-05,
      "loss": 0.1513,
      "step": 14520
    },
    {
      "epoch": 4.615628970775095,
      "grad_norm": 2.4941213130950928,
      "learning_rate": 2e-05,
      "loss": 0.1602,
      "step": 14530
    },
    {
      "epoch": 4.618805590851334,
      "grad_norm": 2.619755983352661,
      "learning_rate": 2e-05,
      "loss": 0.1649,
      "step": 14540
    },
    {
      "epoch": 4.621982210927573,
      "grad_norm": 2.9936530590057373,
      "learning_rate": 2e-05,
      "loss": 0.1585,
      "step": 14550
    },
    {
      "epoch": 4.625158831003812,
      "grad_norm": 4.633026123046875,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 14560
    },
    {
      "epoch": 4.6283354510800505,
      "grad_norm": 13.095866203308105,
      "learning_rate": 2e-05,
      "loss": 0.1587,
      "step": 14570
    },
    {
      "epoch": 4.63151207115629,
      "grad_norm": 2.189802885055542,
      "learning_rate": 2e-05,
      "loss": 0.1634,
      "step": 14580
    },
    {
      "epoch": 4.634688691232529,
      "grad_norm": 3.741309642791748,
      "learning_rate": 2e-05,
      "loss": 0.1617,
      "step": 14590
    },
    {
      "epoch": 4.637865311308768,
      "grad_norm": 7.141267776489258,
      "learning_rate": 2e-05,
      "loss": 0.1723,
      "step": 14600
    },
    {
      "epoch": 4.641041931385006,
      "grad_norm": 1.868186354637146,
      "learning_rate": 2e-05,
      "loss": 0.1518,
      "step": 14610
    },
    {
      "epoch": 4.644218551461245,
      "grad_norm": 3.055295705795288,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 14620
    },
    {
      "epoch": 4.647395171537484,
      "grad_norm": 2.2587831020355225,
      "learning_rate": 2e-05,
      "loss": 0.1544,
      "step": 14630
    },
    {
      "epoch": 4.650571791613723,
      "grad_norm": 2.771751880645752,
      "learning_rate": 2e-05,
      "loss": 0.1675,
      "step": 14640
    },
    {
      "epoch": 4.653748411689962,
      "grad_norm": 3.844743251800537,
      "learning_rate": 2e-05,
      "loss": 0.1589,
      "step": 14650
    },
    {
      "epoch": 4.6569250317662005,
      "grad_norm": 2.9046242237091064,
      "learning_rate": 2e-05,
      "loss": 0.1506,
      "step": 14660
    },
    {
      "epoch": 4.66010165184244,
      "grad_norm": 2.095828056335449,
      "learning_rate": 2e-05,
      "loss": 0.1441,
      "step": 14670
    },
    {
      "epoch": 4.663278271918679,
      "grad_norm": 1.225833535194397,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 14680
    },
    {
      "epoch": 4.666454891994917,
      "grad_norm": 2.940051555633545,
      "learning_rate": 2e-05,
      "loss": 0.155,
      "step": 14690
    },
    {
      "epoch": 4.669631512071156,
      "grad_norm": 2.770918369293213,
      "learning_rate": 2e-05,
      "loss": 0.153,
      "step": 14700
    },
    {
      "epoch": 4.672808132147395,
      "grad_norm": 2.316300392150879,
      "learning_rate": 2e-05,
      "loss": 0.1548,
      "step": 14710
    },
    {
      "epoch": 4.675984752223634,
      "grad_norm": 2.0903475284576416,
      "learning_rate": 2e-05,
      "loss": 0.1688,
      "step": 14720
    },
    {
      "epoch": 4.679161372299873,
      "grad_norm": 1.5198711156845093,
      "learning_rate": 2e-05,
      "loss": 0.1563,
      "step": 14730
    },
    {
      "epoch": 4.6823379923761115,
      "grad_norm": 3.766944646835327,
      "learning_rate": 2e-05,
      "loss": 0.1599,
      "step": 14740
    },
    {
      "epoch": 4.685514612452351,
      "grad_norm": 2.6773781776428223,
      "learning_rate": 2e-05,
      "loss": 0.1761,
      "step": 14750
    },
    {
      "epoch": 4.68869123252859,
      "grad_norm": 4.790326118469238,
      "learning_rate": 2e-05,
      "loss": 0.1494,
      "step": 14760
    },
    {
      "epoch": 4.691867852604829,
      "grad_norm": 1.9023778438568115,
      "learning_rate": 2e-05,
      "loss": 0.1594,
      "step": 14770
    },
    {
      "epoch": 4.695044472681067,
      "grad_norm": 1.9849185943603516,
      "learning_rate": 2e-05,
      "loss": 0.1594,
      "step": 14780
    },
    {
      "epoch": 4.698221092757306,
      "grad_norm": 3.1147282123565674,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 14790
    },
    {
      "epoch": 4.701397712833545,
      "grad_norm": 3.7818191051483154,
      "learning_rate": 2e-05,
      "loss": 0.1611,
      "step": 14800
    },
    {
      "epoch": 4.702986022871665,
      "eval_loss": 1.854593276977539,
      "eval_mse": 1.853546177257191,
      "eval_pearson": 0.39327190079973007,
      "eval_runtime": 21.1653,
      "eval_samples_per_second": 1018.65,
      "eval_spearmanr": 0.3902780962258572,
      "eval_steps_per_second": 4.016,
      "step": 14805
    },
    {
      "epoch": 4.704574332909784,
      "grad_norm": 1.9173588752746582,
      "learning_rate": 2e-05,
      "loss": 0.1536,
      "step": 14810
    },
    {
      "epoch": 4.707750952986023,
      "grad_norm": 3.886838436126709,
      "learning_rate": 2e-05,
      "loss": 0.1386,
      "step": 14820
    },
    {
      "epoch": 4.7109275730622615,
      "grad_norm": 2.1712567806243896,
      "learning_rate": 2e-05,
      "loss": 0.1531,
      "step": 14830
    },
    {
      "epoch": 4.714104193138501,
      "grad_norm": 1.9970775842666626,
      "learning_rate": 2e-05,
      "loss": 0.1609,
      "step": 14840
    },
    {
      "epoch": 4.71728081321474,
      "grad_norm": 2.777873992919922,
      "learning_rate": 2e-05,
      "loss": 0.15,
      "step": 14850
    },
    {
      "epoch": 4.720457433290979,
      "grad_norm": 2.7257301807403564,
      "learning_rate": 2e-05,
      "loss": 0.1522,
      "step": 14860
    },
    {
      "epoch": 4.723634053367217,
      "grad_norm": 2.54972767829895,
      "learning_rate": 2e-05,
      "loss": 0.176,
      "step": 14870
    },
    {
      "epoch": 4.726810673443456,
      "grad_norm": 2.6213409900665283,
      "learning_rate": 2e-05,
      "loss": 0.1506,
      "step": 14880
    },
    {
      "epoch": 4.729987293519695,
      "grad_norm": 1.8436945676803589,
      "learning_rate": 2e-05,
      "loss": 0.1406,
      "step": 14890
    },
    {
      "epoch": 4.733163913595934,
      "grad_norm": 2.3692526817321777,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 14900
    },
    {
      "epoch": 4.7363405336721724,
      "grad_norm": 2.1065406799316406,
      "learning_rate": 2e-05,
      "loss": 0.1692,
      "step": 14910
    },
    {
      "epoch": 4.7395171537484115,
      "grad_norm": 2.823227882385254,
      "learning_rate": 2e-05,
      "loss": 0.154,
      "step": 14920
    },
    {
      "epoch": 4.742693773824651,
      "grad_norm": 3.0376312732696533,
      "learning_rate": 2e-05,
      "loss": 0.1648,
      "step": 14930
    },
    {
      "epoch": 4.74587039390089,
      "grad_norm": 2.1139163970947266,
      "learning_rate": 2e-05,
      "loss": 0.1579,
      "step": 14940
    },
    {
      "epoch": 4.749047013977128,
      "grad_norm": 1.765358805656433,
      "learning_rate": 2e-05,
      "loss": 0.145,
      "step": 14950
    },
    {
      "epoch": 4.752223634053367,
      "grad_norm": 1.8073080778121948,
      "learning_rate": 2e-05,
      "loss": 0.1502,
      "step": 14960
    },
    {
      "epoch": 4.755400254129606,
      "grad_norm": 1.7230790853500366,
      "learning_rate": 2e-05,
      "loss": 0.1436,
      "step": 14970
    },
    {
      "epoch": 4.758576874205845,
      "grad_norm": 2.144404411315918,
      "learning_rate": 2e-05,
      "loss": 0.1634,
      "step": 14980
    },
    {
      "epoch": 4.761753494282084,
      "grad_norm": 1.5809842348098755,
      "learning_rate": 2e-05,
      "loss": 0.1457,
      "step": 14990
    },
    {
      "epoch": 4.7649301143583225,
      "grad_norm": 1.7979416847229004,
      "learning_rate": 2e-05,
      "loss": 0.1499,
      "step": 15000
    },
    {
      "epoch": 4.768106734434562,
      "grad_norm": 1.900939702987671,
      "learning_rate": 2e-05,
      "loss": 0.1516,
      "step": 15010
    },
    {
      "epoch": 4.771283354510801,
      "grad_norm": 6.695561408996582,
      "learning_rate": 2e-05,
      "loss": 0.1562,
      "step": 15020
    },
    {
      "epoch": 4.77445997458704,
      "grad_norm": 4.6441330909729,
      "learning_rate": 2e-05,
      "loss": 0.1668,
      "step": 15030
    },
    {
      "epoch": 4.777636594663278,
      "grad_norm": 2.4696412086486816,
      "learning_rate": 2e-05,
      "loss": 0.1608,
      "step": 15040
    },
    {
      "epoch": 4.780813214739517,
      "grad_norm": 2.755692720413208,
      "learning_rate": 2e-05,
      "loss": 0.1631,
      "step": 15050
    },
    {
      "epoch": 4.783989834815756,
      "grad_norm": 2.011277437210083,
      "learning_rate": 2e-05,
      "loss": 0.1441,
      "step": 15060
    },
    {
      "epoch": 4.787166454891995,
      "grad_norm": 1.8624227046966553,
      "learning_rate": 2e-05,
      "loss": 0.1467,
      "step": 15070
    },
    {
      "epoch": 4.790343074968233,
      "grad_norm": 2.311300754547119,
      "learning_rate": 2e-05,
      "loss": 0.1549,
      "step": 15080
    },
    {
      "epoch": 4.7935196950444725,
      "grad_norm": 2.427292585372925,
      "learning_rate": 2e-05,
      "loss": 0.157,
      "step": 15090
    },
    {
      "epoch": 4.796696315120712,
      "grad_norm": 2.1805765628814697,
      "learning_rate": 2e-05,
      "loss": 0.1411,
      "step": 15100
    },
    {
      "epoch": 4.799872935196951,
      "grad_norm": 2.479762315750122,
      "learning_rate": 2e-05,
      "loss": 0.1487,
      "step": 15110
    },
    {
      "epoch": 4.803049555273189,
      "grad_norm": 2.064199447631836,
      "learning_rate": 2e-05,
      "loss": 0.1564,
      "step": 15120
    },
    {
      "epoch": 4.803049555273189,
      "eval_loss": 1.7504733800888062,
      "eval_mse": 1.7495636233372238,
      "eval_pearson": 0.4067052905482965,
      "eval_runtime": 21.0624,
      "eval_samples_per_second": 1023.623,
      "eval_spearmanr": 0.4081037994085108,
      "eval_steps_per_second": 4.036,
      "step": 15120
    },
    {
      "epoch": 4.806226175349428,
      "grad_norm": 3.3405277729034424,
      "learning_rate": 2e-05,
      "loss": 0.1526,
      "step": 15130
    },
    {
      "epoch": 4.809402795425667,
      "grad_norm": 2.4062132835388184,
      "learning_rate": 2e-05,
      "loss": 0.1615,
      "step": 15140
    },
    {
      "epoch": 4.812579415501906,
      "grad_norm": 2.4385929107666016,
      "learning_rate": 2e-05,
      "loss": 0.142,
      "step": 15150
    },
    {
      "epoch": 4.815756035578145,
      "grad_norm": 1.8938192129135132,
      "learning_rate": 2e-05,
      "loss": 0.154,
      "step": 15160
    },
    {
      "epoch": 4.8189326556543834,
      "grad_norm": 3.065094232559204,
      "learning_rate": 2e-05,
      "loss": 0.1568,
      "step": 15170
    },
    {
      "epoch": 4.8221092757306225,
      "grad_norm": 1.8142725229263306,
      "learning_rate": 2e-05,
      "loss": 0.1405,
      "step": 15180
    },
    {
      "epoch": 4.825285895806862,
      "grad_norm": 2.3021583557128906,
      "learning_rate": 2e-05,
      "loss": 0.1423,
      "step": 15190
    },
    {
      "epoch": 4.828462515883101,
      "grad_norm": 2.565776824951172,
      "learning_rate": 2e-05,
      "loss": 0.1492,
      "step": 15200
    },
    {
      "epoch": 4.831639135959339,
      "grad_norm": 2.462022542953491,
      "learning_rate": 2e-05,
      "loss": 0.1463,
      "step": 15210
    },
    {
      "epoch": 4.834815756035578,
      "grad_norm": 2.6864354610443115,
      "learning_rate": 2e-05,
      "loss": 0.1562,
      "step": 15220
    },
    {
      "epoch": 4.837992376111817,
      "grad_norm": 4.1562676429748535,
      "learning_rate": 2e-05,
      "loss": 0.1521,
      "step": 15230
    },
    {
      "epoch": 4.841168996188056,
      "grad_norm": 2.59262752532959,
      "learning_rate": 2e-05,
      "loss": 0.1497,
      "step": 15240
    },
    {
      "epoch": 4.844345616264295,
      "grad_norm": 1.6498615741729736,
      "learning_rate": 2e-05,
      "loss": 0.1609,
      "step": 15250
    },
    {
      "epoch": 4.8475222363405335,
      "grad_norm": 3.1896419525146484,
      "learning_rate": 2e-05,
      "loss": 0.1489,
      "step": 15260
    },
    {
      "epoch": 4.850698856416773,
      "grad_norm": 3.2115707397460938,
      "learning_rate": 2e-05,
      "loss": 0.1555,
      "step": 15270
    },
    {
      "epoch": 4.853875476493012,
      "grad_norm": 2.0996928215026855,
      "learning_rate": 2e-05,
      "loss": 0.156,
      "step": 15280
    },
    {
      "epoch": 4.85705209656925,
      "grad_norm": 2.1262545585632324,
      "learning_rate": 2e-05,
      "loss": 0.1445,
      "step": 15290
    },
    {
      "epoch": 4.860228716645489,
      "grad_norm": 2.06921648979187,
      "learning_rate": 2e-05,
      "loss": 0.1499,
      "step": 15300
    },
    {
      "epoch": 4.863405336721728,
      "grad_norm": 3.592977523803711,
      "learning_rate": 2e-05,
      "loss": 0.161,
      "step": 15310
    },
    {
      "epoch": 4.866581956797967,
      "grad_norm": 2.9930498600006104,
      "learning_rate": 2e-05,
      "loss": 0.1625,
      "step": 15320
    },
    {
      "epoch": 4.869758576874206,
      "grad_norm": 2.1742777824401855,
      "learning_rate": 2e-05,
      "loss": 0.1501,
      "step": 15330
    },
    {
      "epoch": 4.872935196950444,
      "grad_norm": 1.749268889427185,
      "learning_rate": 2e-05,
      "loss": 0.14,
      "step": 15340
    },
    {
      "epoch": 4.8761118170266835,
      "grad_norm": 2.0000596046447754,
      "learning_rate": 2e-05,
      "loss": 0.1582,
      "step": 15350
    },
    {
      "epoch": 4.879288437102923,
      "grad_norm": 4.40285587310791,
      "learning_rate": 2e-05,
      "loss": 0.1623,
      "step": 15360
    },
    {
      "epoch": 4.882465057179162,
      "grad_norm": 3.865800380706787,
      "learning_rate": 2e-05,
      "loss": 0.1538,
      "step": 15370
    },
    {
      "epoch": 4.8856416772554,
      "grad_norm": 2.9020628929138184,
      "learning_rate": 2e-05,
      "loss": 0.1421,
      "step": 15380
    },
    {
      "epoch": 4.888818297331639,
      "grad_norm": 4.6536946296691895,
      "learning_rate": 2e-05,
      "loss": 0.1718,
      "step": 15390
    },
    {
      "epoch": 4.891994917407878,
      "grad_norm": 4.077261924743652,
      "learning_rate": 2e-05,
      "loss": 0.1535,
      "step": 15400
    },
    {
      "epoch": 4.895171537484117,
      "grad_norm": 1.3455760478973389,
      "learning_rate": 2e-05,
      "loss": 0.1357,
      "step": 15410
    },
    {
      "epoch": 4.898348157560356,
      "grad_norm": 2.9146316051483154,
      "learning_rate": 2e-05,
      "loss": 0.1624,
      "step": 15420
    },
    {
      "epoch": 4.901524777636594,
      "grad_norm": 3.010305881500244,
      "learning_rate": 2e-05,
      "loss": 0.1538,
      "step": 15430
    },
    {
      "epoch": 4.903113087674714,
      "eval_loss": 1.773337721824646,
      "eval_mse": 1.769477300146847,
      "eval_pearson": 0.40487265648372417,
      "eval_runtime": 21.0595,
      "eval_samples_per_second": 1023.768,
      "eval_spearmanr": 0.40770564605278675,
      "eval_steps_per_second": 4.036,
      "step": 15435
    },
    {
      "epoch": 4.9047013977128335,
      "grad_norm": 3.1402645111083984,
      "learning_rate": 2e-05,
      "loss": 0.1518,
      "step": 15440
    },
    {
      "epoch": 4.907878017789073,
      "grad_norm": 2.1504313945770264,
      "learning_rate": 2e-05,
      "loss": 0.1473,
      "step": 15450
    },
    {
      "epoch": 4.911054637865311,
      "grad_norm": 1.6433912515640259,
      "learning_rate": 2e-05,
      "loss": 0.1487,
      "step": 15460
    },
    {
      "epoch": 4.91423125794155,
      "grad_norm": 1.9825234413146973,
      "learning_rate": 2e-05,
      "loss": 0.1379,
      "step": 15470
    },
    {
      "epoch": 4.917407878017789,
      "grad_norm": 1.9100933074951172,
      "learning_rate": 2e-05,
      "loss": 0.1489,
      "step": 15480
    },
    {
      "epoch": 4.920584498094028,
      "grad_norm": 3.0357961654663086,
      "learning_rate": 2e-05,
      "loss": 0.1448,
      "step": 15490
    },
    {
      "epoch": 4.923761118170267,
      "grad_norm": 1.7340620756149292,
      "learning_rate": 2e-05,
      "loss": 0.1635,
      "step": 15500
    },
    {
      "epoch": 4.926937738246505,
      "grad_norm": 1.9339556694030762,
      "learning_rate": 2e-05,
      "loss": 0.1471,
      "step": 15510
    },
    {
      "epoch": 4.9301143583227445,
      "grad_norm": 1.9485570192337036,
      "learning_rate": 2e-05,
      "loss": 0.1543,
      "step": 15520
    },
    {
      "epoch": 4.933290978398984,
      "grad_norm": 5.541880130767822,
      "learning_rate": 2e-05,
      "loss": 0.1585,
      "step": 15530
    },
    {
      "epoch": 4.936467598475223,
      "grad_norm": 2.3132083415985107,
      "learning_rate": 2e-05,
      "loss": 0.1617,
      "step": 15540
    },
    {
      "epoch": 4.939644218551461,
      "grad_norm": 2.1641528606414795,
      "learning_rate": 2e-05,
      "loss": 0.1436,
      "step": 15550
    },
    {
      "epoch": 4.9428208386277,
      "grad_norm": 2.2349941730499268,
      "learning_rate": 2e-05,
      "loss": 0.1498,
      "step": 15560
    },
    {
      "epoch": 4.945997458703939,
      "grad_norm": 1.6306867599487305,
      "learning_rate": 2e-05,
      "loss": 0.1425,
      "step": 15570
    },
    {
      "epoch": 4.949174078780178,
      "grad_norm": 4.221702575683594,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 15580
    },
    {
      "epoch": 4.952350698856417,
      "grad_norm": 1.775002360343933,
      "learning_rate": 2e-05,
      "loss": 0.149,
      "step": 15590
    },
    {
      "epoch": 4.955527318932655,
      "grad_norm": 1.7786039113998413,
      "learning_rate": 2e-05,
      "loss": 0.1618,
      "step": 15600
    },
    {
      "epoch": 4.9587039390088945,
      "grad_norm": 1.5645909309387207,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 15610
    },
    {
      "epoch": 4.961880559085134,
      "grad_norm": 2.809967279434204,
      "learning_rate": 2e-05,
      "loss": 0.1395,
      "step": 15620
    },
    {
      "epoch": 4.965057179161373,
      "grad_norm": 2.762544870376587,
      "learning_rate": 2e-05,
      "loss": 0.1471,
      "step": 15630
    },
    {
      "epoch": 4.968233799237611,
      "grad_norm": 3.331145763397217,
      "learning_rate": 2e-05,
      "loss": 0.1581,
      "step": 15640
    },
    {
      "epoch": 4.97141041931385,
      "grad_norm": 2.077014923095703,
      "learning_rate": 2e-05,
      "loss": 0.1595,
      "step": 15650
    },
    {
      "epoch": 4.974587039390089,
      "grad_norm": 1.526539921760559,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 15660
    },
    {
      "epoch": 4.977763659466328,
      "grad_norm": 3.0483152866363525,
      "learning_rate": 2e-05,
      "loss": 0.1471,
      "step": 15670
    },
    {
      "epoch": 4.980940279542566,
      "grad_norm": 3.0240023136138916,
      "learning_rate": 2e-05,
      "loss": 0.1363,
      "step": 15680
    },
    {
      "epoch": 4.984116899618805,
      "grad_norm": 2.039275646209717,
      "learning_rate": 2e-05,
      "loss": 0.1364,
      "step": 15690
    },
    {
      "epoch": 4.9872935196950445,
      "grad_norm": 2.6167805194854736,
      "learning_rate": 2e-05,
      "loss": 0.1518,
      "step": 15700
    },
    {
      "epoch": 4.990470139771284,
      "grad_norm": 1.9691141843795776,
      "learning_rate": 2e-05,
      "loss": 0.1598,
      "step": 15710
    },
    {
      "epoch": 4.993646759847522,
      "grad_norm": 2.6183488368988037,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 15720
    },
    {
      "epoch": 4.996823379923761,
      "grad_norm": 1.8698614835739136,
      "learning_rate": 2e-05,
      "loss": 0.1597,
      "step": 15730
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.84296452999115,
      "learning_rate": 2e-05,
      "loss": 0.1373,
      "step": 15740
    },
    {
      "epoch": 5.003176620076239,
      "grad_norm": 3.329864501953125,
      "learning_rate": 2e-05,
      "loss": 0.1236,
      "step": 15750
    },
    {
      "epoch": 5.003176620076239,
      "eval_loss": 1.8301056623458862,
      "eval_mse": 1.828619332107368,
      "eval_pearson": 0.41996342091212147,
      "eval_runtime": 21.0667,
      "eval_samples_per_second": 1023.418,
      "eval_spearmanr": 0.4198197243684757,
      "eval_steps_per_second": 4.035,
      "step": 15750
    },
    {
      "epoch": 5.006353240152478,
      "grad_norm": 1.9003238677978516,
      "learning_rate": 2e-05,
      "loss": 0.1309,
      "step": 15760
    },
    {
      "epoch": 5.009529860228716,
      "grad_norm": 1.7873033285140991,
      "learning_rate": 2e-05,
      "loss": 0.141,
      "step": 15770
    },
    {
      "epoch": 5.0127064803049555,
      "grad_norm": 5.4332780838012695,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 15780
    },
    {
      "epoch": 5.015883100381195,
      "grad_norm": 2.1290228366851807,
      "learning_rate": 2e-05,
      "loss": 0.1401,
      "step": 15790
    },
    {
      "epoch": 5.019059720457434,
      "grad_norm": 2.1954879760742188,
      "learning_rate": 2e-05,
      "loss": 0.1255,
      "step": 15800
    },
    {
      "epoch": 5.022236340533672,
      "grad_norm": 2.139390230178833,
      "learning_rate": 2e-05,
      "loss": 0.1399,
      "step": 15810
    },
    {
      "epoch": 5.025412960609911,
      "grad_norm": 1.6496727466583252,
      "learning_rate": 2e-05,
      "loss": 0.1391,
      "step": 15820
    },
    {
      "epoch": 5.02858958068615,
      "grad_norm": 2.009247303009033,
      "learning_rate": 2e-05,
      "loss": 0.134,
      "step": 15830
    },
    {
      "epoch": 5.031766200762389,
      "grad_norm": 2.263753652572632,
      "learning_rate": 2e-05,
      "loss": 0.1183,
      "step": 15840
    },
    {
      "epoch": 5.034942820838627,
      "grad_norm": 1.6283295154571533,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 15850
    },
    {
      "epoch": 5.038119440914866,
      "grad_norm": 1.9893810749053955,
      "learning_rate": 2e-05,
      "loss": 0.1367,
      "step": 15860
    },
    {
      "epoch": 5.0412960609911055,
      "grad_norm": 2.0739240646362305,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 15870
    },
    {
      "epoch": 5.044472681067345,
      "grad_norm": 2.2227745056152344,
      "learning_rate": 2e-05,
      "loss": 0.1348,
      "step": 15880
    },
    {
      "epoch": 5.047649301143583,
      "grad_norm": 2.5183069705963135,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 15890
    },
    {
      "epoch": 5.050825921219822,
      "grad_norm": 2.111870765686035,
      "learning_rate": 2e-05,
      "loss": 0.1262,
      "step": 15900
    },
    {
      "epoch": 5.054002541296061,
      "grad_norm": 3.167736053466797,
      "learning_rate": 2e-05,
      "loss": 0.1243,
      "step": 15910
    },
    {
      "epoch": 5.0571791613723,
      "grad_norm": 1.9354820251464844,
      "learning_rate": 2e-05,
      "loss": 0.1248,
      "step": 15920
    },
    {
      "epoch": 5.060355781448539,
      "grad_norm": 2.207418203353882,
      "learning_rate": 2e-05,
      "loss": 0.1369,
      "step": 15930
    },
    {
      "epoch": 5.063532401524777,
      "grad_norm": 1.8310811519622803,
      "learning_rate": 2e-05,
      "loss": 0.1246,
      "step": 15940
    },
    {
      "epoch": 5.066709021601016,
      "grad_norm": 1.528505802154541,
      "learning_rate": 2e-05,
      "loss": 0.1392,
      "step": 15950
    },
    {
      "epoch": 5.0698856416772555,
      "grad_norm": 1.6576141119003296,
      "learning_rate": 2e-05,
      "loss": 0.1512,
      "step": 15960
    },
    {
      "epoch": 5.073062261753495,
      "grad_norm": 1.9923837184906006,
      "learning_rate": 2e-05,
      "loss": 0.1352,
      "step": 15970
    },
    {
      "epoch": 5.076238881829733,
      "grad_norm": 2.352912425994873,
      "learning_rate": 2e-05,
      "loss": 0.1312,
      "step": 15980
    },
    {
      "epoch": 5.079415501905972,
      "grad_norm": 3.1042556762695312,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 15990
    },
    {
      "epoch": 5.082592121982211,
      "grad_norm": 2.259533166885376,
      "learning_rate": 2e-05,
      "loss": 0.1387,
      "step": 16000
    },
    {
      "epoch": 5.08576874205845,
      "grad_norm": 2.141660690307617,
      "learning_rate": 2e-05,
      "loss": 0.131,
      "step": 16010
    },
    {
      "epoch": 5.088945362134688,
      "grad_norm": 1.919095754623413,
      "learning_rate": 2e-05,
      "loss": 0.1355,
      "step": 16020
    },
    {
      "epoch": 5.092121982210927,
      "grad_norm": 3.361171007156372,
      "learning_rate": 2e-05,
      "loss": 0.1452,
      "step": 16030
    },
    {
      "epoch": 5.0952986022871665,
      "grad_norm": 2.613351583480835,
      "learning_rate": 2e-05,
      "loss": 0.1527,
      "step": 16040
    },
    {
      "epoch": 5.098475222363406,
      "grad_norm": 3.1456820964813232,
      "learning_rate": 2e-05,
      "loss": 0.1466,
      "step": 16050
    },
    {
      "epoch": 5.101651842439645,
      "grad_norm": 1.5367436408996582,
      "learning_rate": 2e-05,
      "loss": 0.1283,
      "step": 16060
    },
    {
      "epoch": 5.103240152477763,
      "eval_loss": 1.774552822113037,
      "eval_mse": 1.7733886878985863,
      "eval_pearson": 0.40666961391193873,
      "eval_runtime": 21.0585,
      "eval_samples_per_second": 1023.817,
      "eval_spearmanr": 0.4071497737197207,
      "eval_steps_per_second": 4.036,
      "step": 16065
    },
    {
      "epoch": 5.104828462515883,
      "grad_norm": 1.605639934539795,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 16070
    },
    {
      "epoch": 5.108005082592122,
      "grad_norm": 2.080705165863037,
      "learning_rate": 2e-05,
      "loss": 0.1358,
      "step": 16080
    },
    {
      "epoch": 5.111181702668361,
      "grad_norm": 1.4741086959838867,
      "learning_rate": 2e-05,
      "loss": 0.1201,
      "step": 16090
    },
    {
      "epoch": 5.1143583227446,
      "grad_norm": 1.6189875602722168,
      "learning_rate": 2e-05,
      "loss": 0.1451,
      "step": 16100
    },
    {
      "epoch": 5.117534942820838,
      "grad_norm": 2.65240740776062,
      "learning_rate": 2e-05,
      "loss": 0.1319,
      "step": 16110
    },
    {
      "epoch": 5.120711562897077,
      "grad_norm": 2.8937160968780518,
      "learning_rate": 2e-05,
      "loss": 0.138,
      "step": 16120
    },
    {
      "epoch": 5.1238881829733165,
      "grad_norm": 1.917930245399475,
      "learning_rate": 2e-05,
      "loss": 0.1345,
      "step": 16130
    },
    {
      "epoch": 5.127064803049556,
      "grad_norm": 1.6864491701126099,
      "learning_rate": 2e-05,
      "loss": 0.1407,
      "step": 16140
    },
    {
      "epoch": 5.130241423125794,
      "grad_norm": 2.1697967052459717,
      "learning_rate": 2e-05,
      "loss": 0.148,
      "step": 16150
    },
    {
      "epoch": 5.133418043202033,
      "grad_norm": 7.652432441711426,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 16160
    },
    {
      "epoch": 5.136594663278272,
      "grad_norm": 2.8243720531463623,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 16170
    },
    {
      "epoch": 5.139771283354511,
      "grad_norm": 1.5559524297714233,
      "learning_rate": 2e-05,
      "loss": 0.1414,
      "step": 16180
    },
    {
      "epoch": 5.142947903430749,
      "grad_norm": 1.9510773420333862,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 16190
    },
    {
      "epoch": 5.146124523506988,
      "grad_norm": 2.458156108856201,
      "learning_rate": 2e-05,
      "loss": 0.1332,
      "step": 16200
    },
    {
      "epoch": 5.149301143583227,
      "grad_norm": 1.8987481594085693,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 16210
    },
    {
      "epoch": 5.1524777636594665,
      "grad_norm": 2.1155707836151123,
      "learning_rate": 2e-05,
      "loss": 0.1294,
      "step": 16220
    },
    {
      "epoch": 5.155654383735706,
      "grad_norm": 2.3124847412109375,
      "learning_rate": 2e-05,
      "loss": 0.1281,
      "step": 16230
    },
    {
      "epoch": 5.158831003811944,
      "grad_norm": 1.8267852067947388,
      "learning_rate": 2e-05,
      "loss": 0.1342,
      "step": 16240
    },
    {
      "epoch": 5.162007623888183,
      "grad_norm": 2.2863657474517822,
      "learning_rate": 2e-05,
      "loss": 0.1301,
      "step": 16250
    },
    {
      "epoch": 5.165184243964422,
      "grad_norm": 4.030402660369873,
      "learning_rate": 2e-05,
      "loss": 0.123,
      "step": 16260
    },
    {
      "epoch": 5.168360864040661,
      "grad_norm": 2.8083558082580566,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 16270
    },
    {
      "epoch": 5.171537484116899,
      "grad_norm": 2.0060126781463623,
      "learning_rate": 2e-05,
      "loss": 0.1236,
      "step": 16280
    },
    {
      "epoch": 5.174714104193138,
      "grad_norm": 5.77162504196167,
      "learning_rate": 2e-05,
      "loss": 0.1304,
      "step": 16290
    },
    {
      "epoch": 5.1778907242693775,
      "grad_norm": 2.0838441848754883,
      "learning_rate": 2e-05,
      "loss": 0.1251,
      "step": 16300
    },
    {
      "epoch": 5.1810673443456166,
      "grad_norm": 1.836683750152588,
      "learning_rate": 2e-05,
      "loss": 0.1426,
      "step": 16310
    },
    {
      "epoch": 5.184243964421855,
      "grad_norm": 2.2389588356018066,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 16320
    },
    {
      "epoch": 5.187420584498094,
      "grad_norm": 2.3268988132476807,
      "learning_rate": 2e-05,
      "loss": 0.1398,
      "step": 16330
    },
    {
      "epoch": 5.190597204574333,
      "grad_norm": 3.0547890663146973,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 16340
    },
    {
      "epoch": 5.193773824650572,
      "grad_norm": 1.3149374723434448,
      "learning_rate": 2e-05,
      "loss": 0.1259,
      "step": 16350
    },
    {
      "epoch": 5.196950444726811,
      "grad_norm": 1.9613208770751953,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 16360
    },
    {
      "epoch": 5.200127064803049,
      "grad_norm": 2.4060211181640625,
      "learning_rate": 2e-05,
      "loss": 0.1419,
      "step": 16370
    },
    {
      "epoch": 5.203303684879288,
      "grad_norm": 1.5341850519180298,
      "learning_rate": 2e-05,
      "loss": 0.1313,
      "step": 16380
    },
    {
      "epoch": 5.203303684879288,
      "eval_loss": 1.722756266593933,
      "eval_mse": 1.7217067827873194,
      "eval_pearson": 0.4288533470079571,
      "eval_runtime": 20.8901,
      "eval_samples_per_second": 1032.067,
      "eval_spearmanr": 0.42612273131837675,
      "eval_steps_per_second": 4.069,
      "step": 16380
    },
    {
      "epoch": 5.2064803049555275,
      "grad_norm": 3.7899386882781982,
      "learning_rate": 2e-05,
      "loss": 0.1429,
      "step": 16390
    },
    {
      "epoch": 5.209656925031767,
      "grad_norm": 2.867540121078491,
      "learning_rate": 2e-05,
      "loss": 0.1279,
      "step": 16400
    },
    {
      "epoch": 5.212833545108005,
      "grad_norm": 3.0925745964050293,
      "learning_rate": 2e-05,
      "loss": 0.1266,
      "step": 16410
    },
    {
      "epoch": 5.216010165184244,
      "grad_norm": 2.270517110824585,
      "learning_rate": 2e-05,
      "loss": 0.1278,
      "step": 16420
    },
    {
      "epoch": 5.219186785260483,
      "grad_norm": 2.0340309143066406,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 16430
    },
    {
      "epoch": 5.222363405336722,
      "grad_norm": 2.56394362449646,
      "learning_rate": 2e-05,
      "loss": 0.1336,
      "step": 16440
    },
    {
      "epoch": 5.22554002541296,
      "grad_norm": 2.763824462890625,
      "learning_rate": 2e-05,
      "loss": 0.1431,
      "step": 16450
    },
    {
      "epoch": 5.228716645489199,
      "grad_norm": 1.8705947399139404,
      "learning_rate": 2e-05,
      "loss": 0.1237,
      "step": 16460
    },
    {
      "epoch": 5.231893265565438,
      "grad_norm": 1.6700416803359985,
      "learning_rate": 2e-05,
      "loss": 0.1405,
      "step": 16470
    },
    {
      "epoch": 5.2350698856416775,
      "grad_norm": 2.066378593444824,
      "learning_rate": 2e-05,
      "loss": 0.1532,
      "step": 16480
    },
    {
      "epoch": 5.238246505717916,
      "grad_norm": 1.446465015411377,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 16490
    },
    {
      "epoch": 5.241423125794155,
      "grad_norm": 2.0772221088409424,
      "learning_rate": 2e-05,
      "loss": 0.1318,
      "step": 16500
    },
    {
      "epoch": 5.244599745870394,
      "grad_norm": 2.9060821533203125,
      "learning_rate": 2e-05,
      "loss": 0.1377,
      "step": 16510
    },
    {
      "epoch": 5.247776365946633,
      "grad_norm": 3.485450267791748,
      "learning_rate": 2e-05,
      "loss": 0.139,
      "step": 16520
    },
    {
      "epoch": 5.250952986022872,
      "grad_norm": 2.317107677459717,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 16530
    },
    {
      "epoch": 5.25412960609911,
      "grad_norm": 2.3237228393554688,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 16540
    },
    {
      "epoch": 5.257306226175349,
      "grad_norm": 1.526504397392273,
      "learning_rate": 2e-05,
      "loss": 0.1397,
      "step": 16550
    },
    {
      "epoch": 5.2604828462515885,
      "grad_norm": 1.2299220561981201,
      "learning_rate": 2e-05,
      "loss": 0.1138,
      "step": 16560
    },
    {
      "epoch": 5.2636594663278276,
      "grad_norm": 1.5418604612350464,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 16570
    },
    {
      "epoch": 5.266836086404066,
      "grad_norm": 1.9087433815002441,
      "learning_rate": 2e-05,
      "loss": 0.146,
      "step": 16580
    },
    {
      "epoch": 5.270012706480305,
      "grad_norm": 1.6612164974212646,
      "learning_rate": 2e-05,
      "loss": 0.1367,
      "step": 16590
    },
    {
      "epoch": 5.273189326556544,
      "grad_norm": 1.4772111177444458,
      "learning_rate": 2e-05,
      "loss": 0.1386,
      "step": 16600
    },
    {
      "epoch": 5.276365946632783,
      "grad_norm": 3.4407005310058594,
      "learning_rate": 2e-05,
      "loss": 0.13,
      "step": 16610
    },
    {
      "epoch": 5.279542566709021,
      "grad_norm": 1.547157645225525,
      "learning_rate": 2e-05,
      "loss": 0.1309,
      "step": 16620
    },
    {
      "epoch": 5.28271918678526,
      "grad_norm": 2.102008104324341,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 16630
    },
    {
      "epoch": 5.285895806861499,
      "grad_norm": 2.915254592895508,
      "learning_rate": 2e-05,
      "loss": 0.136,
      "step": 16640
    },
    {
      "epoch": 5.2890724269377385,
      "grad_norm": 1.6479566097259521,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 16650
    },
    {
      "epoch": 5.292249047013977,
      "grad_norm": 1.557251214981079,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 16660
    },
    {
      "epoch": 5.295425667090216,
      "grad_norm": 2.099331855773926,
      "learning_rate": 2e-05,
      "loss": 0.1264,
      "step": 16670
    },
    {
      "epoch": 5.298602287166455,
      "grad_norm": 2.255720376968384,
      "learning_rate": 2e-05,
      "loss": 0.1373,
      "step": 16680
    },
    {
      "epoch": 5.301778907242694,
      "grad_norm": 1.9058165550231934,
      "learning_rate": 2e-05,
      "loss": 0.1259,
      "step": 16690
    },
    {
      "epoch": 5.3033672172808135,
      "eval_loss": 1.6074790954589844,
      "eval_mse": 1.6061859071033562,
      "eval_pearson": 0.44010958498145775,
      "eval_runtime": 20.9551,
      "eval_samples_per_second": 1028.867,
      "eval_spearmanr": 0.43595647919220176,
      "eval_steps_per_second": 4.056,
      "step": 16695
    },
    {
      "epoch": 5.304955527318933,
      "grad_norm": 1.6901655197143555,
      "learning_rate": 2e-05,
      "loss": 0.1333,
      "step": 16700
    },
    {
      "epoch": 5.308132147395171,
      "grad_norm": 2.471900463104248,
      "learning_rate": 2e-05,
      "loss": 0.1413,
      "step": 16710
    },
    {
      "epoch": 5.31130876747141,
      "grad_norm": 1.8389960527420044,
      "learning_rate": 2e-05,
      "loss": 0.127,
      "step": 16720
    },
    {
      "epoch": 5.314485387547649,
      "grad_norm": 2.3931267261505127,
      "learning_rate": 2e-05,
      "loss": 0.1295,
      "step": 16730
    },
    {
      "epoch": 5.3176620076238885,
      "grad_norm": 3.036071300506592,
      "learning_rate": 2e-05,
      "loss": 0.1136,
      "step": 16740
    },
    {
      "epoch": 5.320838627700127,
      "grad_norm": 4.914029121398926,
      "learning_rate": 2e-05,
      "loss": 0.126,
      "step": 16750
    },
    {
      "epoch": 5.324015247776366,
      "grad_norm": 4.152422904968262,
      "learning_rate": 2e-05,
      "loss": 0.1398,
      "step": 16760
    },
    {
      "epoch": 5.327191867852605,
      "grad_norm": 1.3221204280853271,
      "learning_rate": 2e-05,
      "loss": 0.1379,
      "step": 16770
    },
    {
      "epoch": 5.330368487928844,
      "grad_norm": 1.6145544052124023,
      "learning_rate": 2e-05,
      "loss": 0.131,
      "step": 16780
    },
    {
      "epoch": 5.333545108005082,
      "grad_norm": 1.890474796295166,
      "learning_rate": 2e-05,
      "loss": 0.1433,
      "step": 16790
    },
    {
      "epoch": 5.336721728081321,
      "grad_norm": 3.4543941020965576,
      "learning_rate": 2e-05,
      "loss": 0.1299,
      "step": 16800
    },
    {
      "epoch": 5.33989834815756,
      "grad_norm": 1.8986315727233887,
      "learning_rate": 2e-05,
      "loss": 0.134,
      "step": 16810
    },
    {
      "epoch": 5.3430749682337995,
      "grad_norm": 2.0092484951019287,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 16820
    },
    {
      "epoch": 5.346251588310038,
      "grad_norm": 2.579637289047241,
      "learning_rate": 2e-05,
      "loss": 0.1269,
      "step": 16830
    },
    {
      "epoch": 5.349428208386277,
      "grad_norm": 2.0302047729492188,
      "learning_rate": 2e-05,
      "loss": 0.1193,
      "step": 16840
    },
    {
      "epoch": 5.352604828462516,
      "grad_norm": 2.5641562938690186,
      "learning_rate": 2e-05,
      "loss": 0.1197,
      "step": 16850
    },
    {
      "epoch": 5.355781448538755,
      "grad_norm": 1.7035775184631348,
      "learning_rate": 2e-05,
      "loss": 0.1367,
      "step": 16860
    },
    {
      "epoch": 5.358958068614994,
      "grad_norm": 2.8543224334716797,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 16870
    },
    {
      "epoch": 5.362134688691232,
      "grad_norm": 2.724372386932373,
      "learning_rate": 2e-05,
      "loss": 0.1478,
      "step": 16880
    },
    {
      "epoch": 5.365311308767471,
      "grad_norm": 1.6902892589569092,
      "learning_rate": 2e-05,
      "loss": 0.1269,
      "step": 16890
    },
    {
      "epoch": 5.36848792884371,
      "grad_norm": 1.6522656679153442,
      "learning_rate": 2e-05,
      "loss": 0.1409,
      "step": 16900
    },
    {
      "epoch": 5.3716645489199495,
      "grad_norm": 2.556776285171509,
      "learning_rate": 2e-05,
      "loss": 0.1314,
      "step": 16910
    },
    {
      "epoch": 5.374841168996188,
      "grad_norm": 2.399205446243286,
      "learning_rate": 2e-05,
      "loss": 0.1307,
      "step": 16920
    },
    {
      "epoch": 5.378017789072427,
      "grad_norm": 2.3183958530426025,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 16930
    },
    {
      "epoch": 5.381194409148666,
      "grad_norm": 2.9025163650512695,
      "learning_rate": 2e-05,
      "loss": 0.1287,
      "step": 16940
    },
    {
      "epoch": 5.384371029224905,
      "grad_norm": 2.1620426177978516,
      "learning_rate": 2e-05,
      "loss": 0.1464,
      "step": 16950
    },
    {
      "epoch": 5.387547649301144,
      "grad_norm": 1.783491611480713,
      "learning_rate": 2e-05,
      "loss": 0.1354,
      "step": 16960
    },
    {
      "epoch": 5.390724269377382,
      "grad_norm": 2.6249821186065674,
      "learning_rate": 2e-05,
      "loss": 0.1349,
      "step": 16970
    },
    {
      "epoch": 5.393900889453621,
      "grad_norm": 1.7403451204299927,
      "learning_rate": 2e-05,
      "loss": 0.1421,
      "step": 16980
    },
    {
      "epoch": 5.39707750952986,
      "grad_norm": 1.5459660291671753,
      "learning_rate": 2e-05,
      "loss": 0.1226,
      "step": 16990
    },
    {
      "epoch": 5.4002541296060995,
      "grad_norm": 3.759077310562134,
      "learning_rate": 2e-05,
      "loss": 0.1282,
      "step": 17000
    },
    {
      "epoch": 5.403430749682338,
      "grad_norm": 2.9333274364471436,
      "learning_rate": 2e-05,
      "loss": 0.1367,
      "step": 17010
    },
    {
      "epoch": 5.403430749682338,
      "eval_loss": 1.9325498342514038,
      "eval_mse": 1.9317032949048645,
      "eval_pearson": 0.3797078704076753,
      "eval_runtime": 20.8589,
      "eval_samples_per_second": 1033.613,
      "eval_spearmanr": 0.3787163477609836,
      "eval_steps_per_second": 4.075,
      "step": 17010
    },
    {
      "epoch": 5.406607369758577,
      "grad_norm": 2.1358895301818848,
      "learning_rate": 2e-05,
      "loss": 0.1273,
      "step": 17020
    },
    {
      "epoch": 5.409783989834816,
      "grad_norm": 2.362813711166382,
      "learning_rate": 2e-05,
      "loss": 0.127,
      "step": 17030
    },
    {
      "epoch": 5.412960609911055,
      "grad_norm": 4.063168048858643,
      "learning_rate": 2e-05,
      "loss": 0.1285,
      "step": 17040
    },
    {
      "epoch": 5.416137229987293,
      "grad_norm": 1.5365574359893799,
      "learning_rate": 2e-05,
      "loss": 0.1305,
      "step": 17050
    },
    {
      "epoch": 5.419313850063532,
      "grad_norm": 1.86458158493042,
      "learning_rate": 2e-05,
      "loss": 0.1246,
      "step": 17060
    },
    {
      "epoch": 5.422490470139771,
      "grad_norm": 2.349369764328003,
      "learning_rate": 2e-05,
      "loss": 0.1249,
      "step": 17070
    },
    {
      "epoch": 5.4256670902160105,
      "grad_norm": 1.2309606075286865,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 17080
    },
    {
      "epoch": 5.428843710292249,
      "grad_norm": 2.530585765838623,
      "learning_rate": 2e-05,
      "loss": 0.1368,
      "step": 17090
    },
    {
      "epoch": 5.432020330368488,
      "grad_norm": 1.8345787525177002,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 17100
    },
    {
      "epoch": 5.435196950444727,
      "grad_norm": 2.5247600078582764,
      "learning_rate": 2e-05,
      "loss": 0.1297,
      "step": 17110
    },
    {
      "epoch": 5.438373570520966,
      "grad_norm": 1.1325321197509766,
      "learning_rate": 2e-05,
      "loss": 0.1247,
      "step": 17120
    },
    {
      "epoch": 5.441550190597205,
      "grad_norm": 1.7408015727996826,
      "learning_rate": 2e-05,
      "loss": 0.1261,
      "step": 17130
    },
    {
      "epoch": 5.444726810673443,
      "grad_norm": 2.1495168209075928,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 17140
    },
    {
      "epoch": 5.447903430749682,
      "grad_norm": 2.1799867153167725,
      "learning_rate": 2e-05,
      "loss": 0.1281,
      "step": 17150
    },
    {
      "epoch": 5.451080050825921,
      "grad_norm": 1.824899435043335,
      "learning_rate": 2e-05,
      "loss": 0.1253,
      "step": 17160
    },
    {
      "epoch": 5.4542566709021605,
      "grad_norm": 1.6243667602539062,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 17170
    },
    {
      "epoch": 5.457433290978399,
      "grad_norm": 2.1173667907714844,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 17180
    },
    {
      "epoch": 5.460609911054638,
      "grad_norm": 2.6477041244506836,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 17190
    },
    {
      "epoch": 5.463786531130877,
      "grad_norm": 2.488189220428467,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 17200
    },
    {
      "epoch": 5.466963151207116,
      "grad_norm": 2.0604264736175537,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 17210
    },
    {
      "epoch": 5.470139771283354,
      "grad_norm": 1.8247836828231812,
      "learning_rate": 2e-05,
      "loss": 0.1292,
      "step": 17220
    },
    {
      "epoch": 5.473316391359593,
      "grad_norm": 1.347488284111023,
      "learning_rate": 2e-05,
      "loss": 0.1171,
      "step": 17230
    },
    {
      "epoch": 5.476493011435832,
      "grad_norm": 5.498874187469482,
      "learning_rate": 2e-05,
      "loss": 0.1412,
      "step": 17240
    },
    {
      "epoch": 5.479669631512071,
      "grad_norm": 2.378919839859009,
      "learning_rate": 2e-05,
      "loss": 0.1315,
      "step": 17250
    },
    {
      "epoch": 5.48284625158831,
      "grad_norm": 1.9670180082321167,
      "learning_rate": 2e-05,
      "loss": 0.132,
      "step": 17260
    },
    {
      "epoch": 5.486022871664549,
      "grad_norm": 2.336143970489502,
      "learning_rate": 2e-05,
      "loss": 0.1227,
      "step": 17270
    },
    {
      "epoch": 5.489199491740788,
      "grad_norm": 2.524946451187134,
      "learning_rate": 2e-05,
      "loss": 0.1262,
      "step": 17280
    },
    {
      "epoch": 5.492376111817027,
      "grad_norm": 2.0216171741485596,
      "learning_rate": 2e-05,
      "loss": 0.1244,
      "step": 17290
    },
    {
      "epoch": 5.495552731893266,
      "grad_norm": 3.9445433616638184,
      "learning_rate": 2e-05,
      "loss": 0.1327,
      "step": 17300
    },
    {
      "epoch": 5.498729351969504,
      "grad_norm": 1.6041233539581299,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 17310
    },
    {
      "epoch": 5.501905972045743,
      "grad_norm": 1.125525951385498,
      "learning_rate": 2e-05,
      "loss": 0.1195,
      "step": 17320
    },
    {
      "epoch": 5.503494282083863,
      "eval_loss": 1.8737255334854126,
      "eval_mse": 1.8725663873945848,
      "eval_pearson": 0.3676528501466242,
      "eval_runtime": 20.9969,
      "eval_samples_per_second": 1026.817,
      "eval_spearmanr": 0.36689458759618204,
      "eval_steps_per_second": 4.048,
      "step": 17325
    },
    {
      "epoch": 5.505082592121982,
      "grad_norm": 2.2956767082214355,
      "learning_rate": 2e-05,
      "loss": 0.1334,
      "step": 17330
    },
    {
      "epoch": 5.5082592121982215,
      "grad_norm": 3.429908275604248,
      "learning_rate": 2e-05,
      "loss": 0.1284,
      "step": 17340
    },
    {
      "epoch": 5.51143583227446,
      "grad_norm": 1.9152491092681885,
      "learning_rate": 2e-05,
      "loss": 0.1242,
      "step": 17350
    },
    {
      "epoch": 5.514612452350699,
      "grad_norm": 1.4332395792007446,
      "learning_rate": 2e-05,
      "loss": 0.1281,
      "step": 17360
    },
    {
      "epoch": 5.517789072426938,
      "grad_norm": 2.046154737472534,
      "learning_rate": 2e-05,
      "loss": 0.1372,
      "step": 17370
    },
    {
      "epoch": 5.520965692503177,
      "grad_norm": 1.7291148900985718,
      "learning_rate": 2e-05,
      "loss": 0.1215,
      "step": 17380
    },
    {
      "epoch": 5.524142312579415,
      "grad_norm": 2.07077956199646,
      "learning_rate": 2e-05,
      "loss": 0.1249,
      "step": 17390
    },
    {
      "epoch": 5.527318932655654,
      "grad_norm": 1.6307597160339355,
      "learning_rate": 2e-05,
      "loss": 0.1306,
      "step": 17400
    },
    {
      "epoch": 5.530495552731893,
      "grad_norm": 3.2550559043884277,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 17410
    },
    {
      "epoch": 5.533672172808132,
      "grad_norm": 2.2970833778381348,
      "learning_rate": 2e-05,
      "loss": 0.1522,
      "step": 17420
    },
    {
      "epoch": 5.536848792884371,
      "grad_norm": 1.1947746276855469,
      "learning_rate": 2e-05,
      "loss": 0.1285,
      "step": 17430
    },
    {
      "epoch": 5.54002541296061,
      "grad_norm": 1.8324329853057861,
      "learning_rate": 2e-05,
      "loss": 0.1298,
      "step": 17440
    },
    {
      "epoch": 5.543202033036849,
      "grad_norm": 1.5811842679977417,
      "learning_rate": 2e-05,
      "loss": 0.1233,
      "step": 17450
    },
    {
      "epoch": 5.546378653113088,
      "grad_norm": 2.727764844894409,
      "learning_rate": 2e-05,
      "loss": 0.123,
      "step": 17460
    },
    {
      "epoch": 5.549555273189327,
      "grad_norm": 3.332010269165039,
      "learning_rate": 2e-05,
      "loss": 0.1391,
      "step": 17470
    },
    {
      "epoch": 5.552731893265565,
      "grad_norm": 1.6439464092254639,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 17480
    },
    {
      "epoch": 5.555908513341804,
      "grad_norm": 1.4399724006652832,
      "learning_rate": 2e-05,
      "loss": 0.127,
      "step": 17490
    },
    {
      "epoch": 5.559085133418043,
      "grad_norm": 1.987649917602539,
      "learning_rate": 2e-05,
      "loss": 0.1403,
      "step": 17500
    },
    {
      "epoch": 5.562261753494282,
      "grad_norm": 1.9356294870376587,
      "learning_rate": 2e-05,
      "loss": 0.1426,
      "step": 17510
    },
    {
      "epoch": 5.565438373570521,
      "grad_norm": 1.7227710485458374,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 17520
    },
    {
      "epoch": 5.56861499364676,
      "grad_norm": 2.443511962890625,
      "learning_rate": 2e-05,
      "loss": 0.1345,
      "step": 17530
    },
    {
      "epoch": 5.571791613722999,
      "grad_norm": 2.0837562084198,
      "learning_rate": 2e-05,
      "loss": 0.1265,
      "step": 17540
    },
    {
      "epoch": 5.574968233799238,
      "grad_norm": 2.261763095855713,
      "learning_rate": 2e-05,
      "loss": 0.124,
      "step": 17550
    },
    {
      "epoch": 5.578144853875477,
      "grad_norm": 2.305191993713379,
      "learning_rate": 2e-05,
      "loss": 0.1241,
      "step": 17560
    },
    {
      "epoch": 5.581321473951715,
      "grad_norm": 1.7782723903656006,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 17570
    },
    {
      "epoch": 5.584498094027954,
      "grad_norm": 1.989324927330017,
      "learning_rate": 2e-05,
      "loss": 0.1143,
      "step": 17580
    },
    {
      "epoch": 5.587674714104193,
      "grad_norm": 1.9061968326568604,
      "learning_rate": 2e-05,
      "loss": 0.1364,
      "step": 17590
    },
    {
      "epoch": 5.590851334180432,
      "grad_norm": 2.478336811065674,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 17600
    },
    {
      "epoch": 5.594027954256671,
      "grad_norm": 2.434136152267456,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 17610
    },
    {
      "epoch": 5.59720457433291,
      "grad_norm": 1.5611317157745361,
      "learning_rate": 2e-05,
      "loss": 0.13,
      "step": 17620
    },
    {
      "epoch": 5.600381194409149,
      "grad_norm": 1.4117532968521118,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 17630
    },
    {
      "epoch": 5.603557814485388,
      "grad_norm": 1.8407032489776611,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 17640
    },
    {
      "epoch": 5.603557814485388,
      "eval_loss": 1.7616517543792725,
      "eval_mse": 1.760627502302513,
      "eval_pearson": 0.41528153876101304,
      "eval_runtime": 20.9682,
      "eval_samples_per_second": 1028.226,
      "eval_spearmanr": 0.4132125287426988,
      "eval_steps_per_second": 4.054,
      "step": 17640
    },
    {
      "epoch": 5.606734434561626,
      "grad_norm": 1.931295394897461,
      "learning_rate": 2e-05,
      "loss": 0.1302,
      "step": 17650
    },
    {
      "epoch": 5.609911054637865,
      "grad_norm": 1.787310242652893,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 17660
    },
    {
      "epoch": 5.613087674714104,
      "grad_norm": 2.3913838863372803,
      "learning_rate": 2e-05,
      "loss": 0.1278,
      "step": 17670
    },
    {
      "epoch": 5.616264294790343,
      "grad_norm": 1.6005154848098755,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 17680
    },
    {
      "epoch": 5.619440914866582,
      "grad_norm": 2.406752109527588,
      "learning_rate": 2e-05,
      "loss": 0.1205,
      "step": 17690
    },
    {
      "epoch": 5.622617534942821,
      "grad_norm": 4.176156044006348,
      "learning_rate": 2e-05,
      "loss": 0.1337,
      "step": 17700
    },
    {
      "epoch": 5.62579415501906,
      "grad_norm": 2.435990333557129,
      "learning_rate": 2e-05,
      "loss": 0.1327,
      "step": 17710
    },
    {
      "epoch": 5.628970775095299,
      "grad_norm": 2.8091530799865723,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 17720
    },
    {
      "epoch": 5.632147395171538,
      "grad_norm": 1.8870017528533936,
      "learning_rate": 2e-05,
      "loss": 0.1303,
      "step": 17730
    },
    {
      "epoch": 5.635324015247776,
      "grad_norm": 1.6248842477798462,
      "learning_rate": 2e-05,
      "loss": 0.133,
      "step": 17740
    },
    {
      "epoch": 5.638500635324015,
      "grad_norm": 3.0108695030212402,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 17750
    },
    {
      "epoch": 5.641677255400254,
      "grad_norm": 2.1768476963043213,
      "learning_rate": 2e-05,
      "loss": 0.1169,
      "step": 17760
    },
    {
      "epoch": 5.6448538754764925,
      "grad_norm": 1.6869808435440063,
      "learning_rate": 2e-05,
      "loss": 0.1178,
      "step": 17770
    },
    {
      "epoch": 5.648030495552732,
      "grad_norm": 2.430114984512329,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 17780
    },
    {
      "epoch": 5.651207115628971,
      "grad_norm": 1.469566822052002,
      "learning_rate": 2e-05,
      "loss": 0.1137,
      "step": 17790
    },
    {
      "epoch": 5.65438373570521,
      "grad_norm": 2.1295220851898193,
      "learning_rate": 2e-05,
      "loss": 0.1287,
      "step": 17800
    },
    {
      "epoch": 5.657560355781449,
      "grad_norm": 2.303269624710083,
      "learning_rate": 2e-05,
      "loss": 0.1284,
      "step": 17810
    },
    {
      "epoch": 5.660736975857687,
      "grad_norm": 1.8236910104751587,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 17820
    },
    {
      "epoch": 5.663913595933926,
      "grad_norm": 1.5083372592926025,
      "learning_rate": 2e-05,
      "loss": 0.1129,
      "step": 17830
    },
    {
      "epoch": 5.667090216010165,
      "grad_norm": 1.7853009700775146,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 17840
    },
    {
      "epoch": 5.670266836086404,
      "grad_norm": 1.5494053363800049,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 17850
    },
    {
      "epoch": 5.673443456162643,
      "grad_norm": 1.313722848892212,
      "learning_rate": 2e-05,
      "loss": 0.126,
      "step": 17860
    },
    {
      "epoch": 5.676620076238882,
      "grad_norm": 3.792076587677002,
      "learning_rate": 2e-05,
      "loss": 0.1289,
      "step": 17870
    },
    {
      "epoch": 5.679796696315121,
      "grad_norm": 3.4166321754455566,
      "learning_rate": 2e-05,
      "loss": 0.1237,
      "step": 17880
    },
    {
      "epoch": 5.68297331639136,
      "grad_norm": 1.5768516063690186,
      "learning_rate": 2e-05,
      "loss": 0.1297,
      "step": 17890
    },
    {
      "epoch": 5.686149936467599,
      "grad_norm": 1.9793492555618286,
      "learning_rate": 2e-05,
      "loss": 0.118,
      "step": 17900
    },
    {
      "epoch": 5.689326556543837,
      "grad_norm": 2.707571029663086,
      "learning_rate": 2e-05,
      "loss": 0.1234,
      "step": 17910
    },
    {
      "epoch": 5.692503176620076,
      "grad_norm": 1.9418518543243408,
      "learning_rate": 2e-05,
      "loss": 0.1378,
      "step": 17920
    },
    {
      "epoch": 5.695679796696315,
      "grad_norm": 2.5602316856384277,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 17930
    },
    {
      "epoch": 5.698856416772554,
      "grad_norm": 2.188904047012329,
      "learning_rate": 2e-05,
      "loss": 0.1272,
      "step": 17940
    },
    {
      "epoch": 5.702033036848793,
      "grad_norm": 1.6416712999343872,
      "learning_rate": 2e-05,
      "loss": 0.1307,
      "step": 17950
    },
    {
      "epoch": 5.703621346886912,
      "eval_loss": 1.7579762935638428,
      "eval_mse": 1.7568406165088484,
      "eval_pearson": 0.4122925682748418,
      "eval_runtime": 21.0606,
      "eval_samples_per_second": 1023.71,
      "eval_spearmanr": 0.4138639502790463,
      "eval_steps_per_second": 4.036,
      "step": 17955
    },
    {
      "epoch": 5.705209656925032,
      "grad_norm": 1.7635726928710938,
      "learning_rate": 2e-05,
      "loss": 0.1272,
      "step": 17960
    },
    {
      "epoch": 5.708386277001271,
      "grad_norm": 2.9217231273651123,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 17970
    },
    {
      "epoch": 5.71156289707751,
      "grad_norm": 2.704408645629883,
      "learning_rate": 2e-05,
      "loss": 0.1299,
      "step": 17980
    },
    {
      "epoch": 5.714739517153748,
      "grad_norm": 3.015064001083374,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 17990
    },
    {
      "epoch": 5.717916137229987,
      "grad_norm": 3.4405553340911865,
      "learning_rate": 2e-05,
      "loss": 0.1275,
      "step": 18000
    },
    {
      "epoch": 5.721092757306226,
      "grad_norm": 1.5829628705978394,
      "learning_rate": 2e-05,
      "loss": 0.117,
      "step": 18010
    },
    {
      "epoch": 5.724269377382465,
      "grad_norm": 1.9172927141189575,
      "learning_rate": 2e-05,
      "loss": 0.1232,
      "step": 18020
    },
    {
      "epoch": 5.7274459974587035,
      "grad_norm": 2.0002222061157227,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 18030
    },
    {
      "epoch": 5.730622617534943,
      "grad_norm": 2.0391664505004883,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 18040
    },
    {
      "epoch": 5.733799237611182,
      "grad_norm": 2.386430263519287,
      "learning_rate": 2e-05,
      "loss": 0.1267,
      "step": 18050
    },
    {
      "epoch": 5.736975857687421,
      "grad_norm": 2.531942844390869,
      "learning_rate": 2e-05,
      "loss": 0.1232,
      "step": 18060
    },
    {
      "epoch": 5.74015247776366,
      "grad_norm": 1.6034973859786987,
      "learning_rate": 2e-05,
      "loss": 0.1234,
      "step": 18070
    },
    {
      "epoch": 5.743329097839898,
      "grad_norm": 2.621279239654541,
      "learning_rate": 2e-05,
      "loss": 0.1162,
      "step": 18080
    },
    {
      "epoch": 5.746505717916137,
      "grad_norm": 2.0685884952545166,
      "learning_rate": 2e-05,
      "loss": 0.1277,
      "step": 18090
    },
    {
      "epoch": 5.749682337992376,
      "grad_norm": 3.2816669940948486,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 18100
    },
    {
      "epoch": 5.752858958068615,
      "grad_norm": 1.867836594581604,
      "learning_rate": 2e-05,
      "loss": 0.113,
      "step": 18110
    },
    {
      "epoch": 5.756035578144854,
      "grad_norm": 1.747653603553772,
      "learning_rate": 2e-05,
      "loss": 0.1225,
      "step": 18120
    },
    {
      "epoch": 5.759212198221093,
      "grad_norm": 1.6058346033096313,
      "learning_rate": 2e-05,
      "loss": 0.1112,
      "step": 18130
    },
    {
      "epoch": 5.762388818297332,
      "grad_norm": 1.8341923952102661,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 18140
    },
    {
      "epoch": 5.765565438373571,
      "grad_norm": 1.4752919673919678,
      "learning_rate": 2e-05,
      "loss": 0.1252,
      "step": 18150
    },
    {
      "epoch": 5.768742058449809,
      "grad_norm": 2.3183820247650146,
      "learning_rate": 2e-05,
      "loss": 0.137,
      "step": 18160
    },
    {
      "epoch": 5.771918678526048,
      "grad_norm": 1.2265362739562988,
      "learning_rate": 2e-05,
      "loss": 0.1194,
      "step": 18170
    },
    {
      "epoch": 5.775095298602287,
      "grad_norm": 1.2902063131332397,
      "learning_rate": 2e-05,
      "loss": 0.1211,
      "step": 18180
    },
    {
      "epoch": 5.778271918678526,
      "grad_norm": 1.3320940732955933,
      "learning_rate": 2e-05,
      "loss": 0.1195,
      "step": 18190
    },
    {
      "epoch": 5.7814485387547645,
      "grad_norm": 1.2570339441299438,
      "learning_rate": 2e-05,
      "loss": 0.1255,
      "step": 18200
    },
    {
      "epoch": 5.784625158831004,
      "grad_norm": 2.0407519340515137,
      "learning_rate": 2e-05,
      "loss": 0.1118,
      "step": 18210
    },
    {
      "epoch": 5.787801778907243,
      "grad_norm": 1.638015866279602,
      "learning_rate": 2e-05,
      "loss": 0.1348,
      "step": 18220
    },
    {
      "epoch": 5.790978398983482,
      "grad_norm": 1.621567726135254,
      "learning_rate": 2e-05,
      "loss": 0.1283,
      "step": 18230
    },
    {
      "epoch": 5.794155019059721,
      "grad_norm": 1.877465009689331,
      "learning_rate": 2e-05,
      "loss": 0.131,
      "step": 18240
    },
    {
      "epoch": 5.797331639135959,
      "grad_norm": 2.4519505500793457,
      "learning_rate": 2e-05,
      "loss": 0.1278,
      "step": 18250
    },
    {
      "epoch": 5.800508259212198,
      "grad_norm": 1.5109753608703613,
      "learning_rate": 2e-05,
      "loss": 0.12,
      "step": 18260
    },
    {
      "epoch": 5.803684879288437,
      "grad_norm": 2.8131160736083984,
      "learning_rate": 2e-05,
      "loss": 0.1226,
      "step": 18270
    },
    {
      "epoch": 5.803684879288437,
      "eval_loss": 1.803818941116333,
      "eval_mse": 1.8025459723881903,
      "eval_pearson": 0.4083503352780049,
      "eval_runtime": 20.8538,
      "eval_samples_per_second": 1033.865,
      "eval_spearmanr": 0.405109280902505,
      "eval_steps_per_second": 4.076,
      "step": 18270
    },
    {
      "epoch": 5.806861499364676,
      "grad_norm": 2.448620319366455,
      "learning_rate": 2e-05,
      "loss": 0.1425,
      "step": 18280
    },
    {
      "epoch": 5.8100381194409145,
      "grad_norm": 1.7262213230133057,
      "learning_rate": 2e-05,
      "loss": 0.1219,
      "step": 18290
    },
    {
      "epoch": 5.813214739517154,
      "grad_norm": 1.7058894634246826,
      "learning_rate": 2e-05,
      "loss": 0.1208,
      "step": 18300
    },
    {
      "epoch": 5.816391359593393,
      "grad_norm": 2.011841297149658,
      "learning_rate": 2e-05,
      "loss": 0.1283,
      "step": 18310
    },
    {
      "epoch": 5.819567979669632,
      "grad_norm": 2.042433261871338,
      "learning_rate": 2e-05,
      "loss": 0.1081,
      "step": 18320
    },
    {
      "epoch": 5.822744599745871,
      "grad_norm": 2.182614803314209,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 18330
    },
    {
      "epoch": 5.825921219822109,
      "grad_norm": 1.9233239889144897,
      "learning_rate": 2e-05,
      "loss": 0.1201,
      "step": 18340
    },
    {
      "epoch": 5.829097839898348,
      "grad_norm": 1.581099033355713,
      "learning_rate": 2e-05,
      "loss": 0.1275,
      "step": 18350
    },
    {
      "epoch": 5.832274459974587,
      "grad_norm": 1.436506986618042,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 18360
    },
    {
      "epoch": 5.8354510800508255,
      "grad_norm": 1.7190066576004028,
      "learning_rate": 2e-05,
      "loss": 0.1209,
      "step": 18370
    },
    {
      "epoch": 5.838627700127065,
      "grad_norm": 1.2624387741088867,
      "learning_rate": 2e-05,
      "loss": 0.1225,
      "step": 18380
    },
    {
      "epoch": 5.841804320203304,
      "grad_norm": 1.926649808883667,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 18390
    },
    {
      "epoch": 5.844980940279543,
      "grad_norm": 1.3992571830749512,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 18400
    },
    {
      "epoch": 5.848157560355782,
      "grad_norm": 1.159440040588379,
      "learning_rate": 2e-05,
      "loss": 0.1148,
      "step": 18410
    },
    {
      "epoch": 5.85133418043202,
      "grad_norm": 1.465687870979309,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 18420
    },
    {
      "epoch": 5.854510800508259,
      "grad_norm": 2.4424571990966797,
      "learning_rate": 2e-05,
      "loss": 0.1163,
      "step": 18430
    },
    {
      "epoch": 5.857687420584498,
      "grad_norm": 1.8265727758407593,
      "learning_rate": 2e-05,
      "loss": 0.1203,
      "step": 18440
    },
    {
      "epoch": 5.860864040660737,
      "grad_norm": 2.944545030593872,
      "learning_rate": 2e-05,
      "loss": 0.1284,
      "step": 18450
    },
    {
      "epoch": 5.8640406607369755,
      "grad_norm": 2.5587809085845947,
      "learning_rate": 2e-05,
      "loss": 0.1289,
      "step": 18460
    },
    {
      "epoch": 5.867217280813215,
      "grad_norm": 1.8950060606002808,
      "learning_rate": 2e-05,
      "loss": 0.1275,
      "step": 18470
    },
    {
      "epoch": 5.870393900889454,
      "grad_norm": 2.5589981079101562,
      "learning_rate": 2e-05,
      "loss": 0.1356,
      "step": 18480
    },
    {
      "epoch": 5.873570520965693,
      "grad_norm": 1.2571661472320557,
      "learning_rate": 2e-05,
      "loss": 0.1127,
      "step": 18490
    },
    {
      "epoch": 5.876747141041932,
      "grad_norm": 2.0946414470672607,
      "learning_rate": 2e-05,
      "loss": 0.1152,
      "step": 18500
    },
    {
      "epoch": 5.87992376111817,
      "grad_norm": 2.5594730377197266,
      "learning_rate": 2e-05,
      "loss": 0.1208,
      "step": 18510
    },
    {
      "epoch": 5.883100381194409,
      "grad_norm": 3.7568440437316895,
      "learning_rate": 2e-05,
      "loss": 0.1257,
      "step": 18520
    },
    {
      "epoch": 5.886277001270648,
      "grad_norm": 1.9654127359390259,
      "learning_rate": 2e-05,
      "loss": 0.1206,
      "step": 18530
    },
    {
      "epoch": 5.889453621346886,
      "grad_norm": 2.044297456741333,
      "learning_rate": 2e-05,
      "loss": 0.1129,
      "step": 18540
    },
    {
      "epoch": 5.8926302414231255,
      "grad_norm": 1.5966042280197144,
      "learning_rate": 2e-05,
      "loss": 0.1143,
      "step": 18550
    },
    {
      "epoch": 5.895806861499365,
      "grad_norm": 2.066419839859009,
      "learning_rate": 2e-05,
      "loss": 0.1361,
      "step": 18560
    },
    {
      "epoch": 5.898983481575604,
      "grad_norm": 1.7215989828109741,
      "learning_rate": 2e-05,
      "loss": 0.1191,
      "step": 18570
    },
    {
      "epoch": 5.902160101651843,
      "grad_norm": 2.337480306625366,
      "learning_rate": 2e-05,
      "loss": 0.1179,
      "step": 18580
    },
    {
      "epoch": 5.903748411689962,
      "eval_loss": 1.7429943084716797,
      "eval_mse": 1.7417619147388081,
      "eval_pearson": 0.42710716099405577,
      "eval_runtime": 21.0107,
      "eval_samples_per_second": 1026.142,
      "eval_spearmanr": 0.4294747121557617,
      "eval_steps_per_second": 4.046,
      "step": 18585
    },
    {
      "epoch": 5.905336721728081,
      "grad_norm": 1.3889973163604736,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 18590
    },
    {
      "epoch": 5.90851334180432,
      "grad_norm": 1.812413215637207,
      "learning_rate": 2e-05,
      "loss": 0.1248,
      "step": 18600
    },
    {
      "epoch": 5.911689961880559,
      "grad_norm": 1.779540777206421,
      "learning_rate": 2e-05,
      "loss": 0.1217,
      "step": 18610
    },
    {
      "epoch": 5.914866581956798,
      "grad_norm": 1.5214784145355225,
      "learning_rate": 2e-05,
      "loss": 0.1159,
      "step": 18620
    },
    {
      "epoch": 5.9180432020330365,
      "grad_norm": 2.6629576683044434,
      "learning_rate": 2e-05,
      "loss": 0.1212,
      "step": 18630
    },
    {
      "epoch": 5.9212198221092756,
      "grad_norm": 2.8292524814605713,
      "learning_rate": 2e-05,
      "loss": 0.1231,
      "step": 18640
    },
    {
      "epoch": 5.924396442185515,
      "grad_norm": 1.53482985496521,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 18650
    },
    {
      "epoch": 5.927573062261754,
      "grad_norm": 2.776060104370117,
      "learning_rate": 2e-05,
      "loss": 0.1242,
      "step": 18660
    },
    {
      "epoch": 5.930749682337993,
      "grad_norm": 1.4836434125900269,
      "learning_rate": 2e-05,
      "loss": 0.1196,
      "step": 18670
    },
    {
      "epoch": 5.933926302414231,
      "grad_norm": 2.24822998046875,
      "learning_rate": 2e-05,
      "loss": 0.1249,
      "step": 18680
    },
    {
      "epoch": 5.93710292249047,
      "grad_norm": 1.7495391368865967,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 18690
    },
    {
      "epoch": 5.940279542566709,
      "grad_norm": 2.0301034450531006,
      "learning_rate": 2e-05,
      "loss": 0.118,
      "step": 18700
    },
    {
      "epoch": 5.943456162642947,
      "grad_norm": 1.942962408065796,
      "learning_rate": 2e-05,
      "loss": 0.1249,
      "step": 18710
    },
    {
      "epoch": 5.9466327827191865,
      "grad_norm": 1.7642382383346558,
      "learning_rate": 2e-05,
      "loss": 0.1202,
      "step": 18720
    },
    {
      "epoch": 5.949809402795426,
      "grad_norm": 1.8839111328125,
      "learning_rate": 2e-05,
      "loss": 0.1242,
      "step": 18730
    },
    {
      "epoch": 5.952986022871665,
      "grad_norm": 2.5760622024536133,
      "learning_rate": 2e-05,
      "loss": 0.1199,
      "step": 18740
    },
    {
      "epoch": 5.956162642947904,
      "grad_norm": 1.8480385541915894,
      "learning_rate": 2e-05,
      "loss": 0.1194,
      "step": 18750
    },
    {
      "epoch": 5.959339263024142,
      "grad_norm": 1.631980299949646,
      "learning_rate": 2e-05,
      "loss": 0.1175,
      "step": 18760
    },
    {
      "epoch": 5.962515883100381,
      "grad_norm": 1.2766910791397095,
      "learning_rate": 2e-05,
      "loss": 0.119,
      "step": 18770
    },
    {
      "epoch": 5.96569250317662,
      "grad_norm": 2.330087423324585,
      "learning_rate": 2e-05,
      "loss": 0.1142,
      "step": 18780
    },
    {
      "epoch": 5.968869123252859,
      "grad_norm": 1.5436179637908936,
      "learning_rate": 2e-05,
      "loss": 0.1203,
      "step": 18790
    },
    {
      "epoch": 5.972045743329097,
      "grad_norm": 2.6789562702178955,
      "learning_rate": 2e-05,
      "loss": 0.108,
      "step": 18800
    },
    {
      "epoch": 5.9752223634053365,
      "grad_norm": 1.3547204732894897,
      "learning_rate": 2e-05,
      "loss": 0.1137,
      "step": 18810
    },
    {
      "epoch": 5.978398983481576,
      "grad_norm": 4.284364700317383,
      "learning_rate": 2e-05,
      "loss": 0.118,
      "step": 18820
    },
    {
      "epoch": 5.981575603557815,
      "grad_norm": 2.732424736022949,
      "learning_rate": 2e-05,
      "loss": 0.1269,
      "step": 18830
    },
    {
      "epoch": 5.984752223634054,
      "grad_norm": 1.5921186208724976,
      "learning_rate": 2e-05,
      "loss": 0.1245,
      "step": 18840
    },
    {
      "epoch": 5.987928843710292,
      "grad_norm": 1.790649652481079,
      "learning_rate": 2e-05,
      "loss": 0.118,
      "step": 18850
    },
    {
      "epoch": 5.991105463786531,
      "grad_norm": 5.778436183929443,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 18860
    },
    {
      "epoch": 5.99428208386277,
      "grad_norm": 1.8409630060195923,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 18870
    },
    {
      "epoch": 5.997458703939009,
      "grad_norm": 1.8207449913024902,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 18880
    },
    {
      "epoch": 6.0006353240152475,
      "grad_norm": 1.8046596050262451,
      "learning_rate": 2e-05,
      "loss": 0.1151,
      "step": 18890
    },
    {
      "epoch": 6.0038119440914866,
      "grad_norm": 1.7055658102035522,
      "learning_rate": 2e-05,
      "loss": 0.1273,
      "step": 18900
    },
    {
      "epoch": 6.0038119440914866,
      "eval_loss": 1.7075682878494263,
      "eval_mse": 1.7064240507238555,
      "eval_pearson": 0.4328219439697158,
      "eval_runtime": 20.961,
      "eval_samples_per_second": 1028.576,
      "eval_spearmanr": 0.43391050527135216,
      "eval_steps_per_second": 4.055,
      "step": 18900
    },
    {
      "epoch": 6.006988564167726,
      "grad_norm": 1.5255467891693115,
      "learning_rate": 2e-05,
      "loss": 0.1085,
      "step": 18910
    },
    {
      "epoch": 6.010165184243965,
      "grad_norm": 1.2809503078460693,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 18920
    },
    {
      "epoch": 6.013341804320203,
      "grad_norm": 2.2589685916900635,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 18930
    },
    {
      "epoch": 6.016518424396442,
      "grad_norm": 0.9883061051368713,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 18940
    },
    {
      "epoch": 6.019695044472681,
      "grad_norm": 2.406825304031372,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 18950
    },
    {
      "epoch": 6.02287166454892,
      "grad_norm": 1.3915910720825195,
      "learning_rate": 2e-05,
      "loss": 0.0992,
      "step": 18960
    },
    {
      "epoch": 6.026048284625158,
      "grad_norm": 1.319558024406433,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 18970
    },
    {
      "epoch": 6.0292249047013975,
      "grad_norm": 1.6967965364456177,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 18980
    },
    {
      "epoch": 6.032401524777637,
      "grad_norm": 1.4134827852249146,
      "learning_rate": 2e-05,
      "loss": 0.1169,
      "step": 18990
    },
    {
      "epoch": 6.035578144853876,
      "grad_norm": 1.2686067819595337,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 19000
    },
    {
      "epoch": 6.038754764930115,
      "grad_norm": 1.1865774393081665,
      "learning_rate": 2e-05,
      "loss": 0.1073,
      "step": 19010
    },
    {
      "epoch": 6.041931385006353,
      "grad_norm": 1.8330379724502563,
      "learning_rate": 2e-05,
      "loss": 0.1059,
      "step": 19020
    },
    {
      "epoch": 6.045108005082592,
      "grad_norm": 1.6473454236984253,
      "learning_rate": 2e-05,
      "loss": 0.1193,
      "step": 19030
    },
    {
      "epoch": 6.048284625158831,
      "grad_norm": 2.1586201190948486,
      "learning_rate": 2e-05,
      "loss": 0.1162,
      "step": 19040
    },
    {
      "epoch": 6.05146124523507,
      "grad_norm": 8.106837272644043,
      "learning_rate": 2e-05,
      "loss": 0.111,
      "step": 19050
    },
    {
      "epoch": 6.054637865311308,
      "grad_norm": 1.255981683731079,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 19060
    },
    {
      "epoch": 6.0578144853875475,
      "grad_norm": 1.2196555137634277,
      "learning_rate": 2e-05,
      "loss": 0.106,
      "step": 19070
    },
    {
      "epoch": 6.060991105463787,
      "grad_norm": 1.6203175783157349,
      "learning_rate": 2e-05,
      "loss": 0.1143,
      "step": 19080
    },
    {
      "epoch": 6.064167725540026,
      "grad_norm": 2.3769593238830566,
      "learning_rate": 2e-05,
      "loss": 0.1125,
      "step": 19090
    },
    {
      "epoch": 6.067344345616264,
      "grad_norm": 2.407216787338257,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 19100
    },
    {
      "epoch": 6.070520965692503,
      "grad_norm": 2.60436749458313,
      "learning_rate": 2e-05,
      "loss": 0.1144,
      "step": 19110
    },
    {
      "epoch": 6.073697585768742,
      "grad_norm": 1.346052646636963,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 19120
    },
    {
      "epoch": 6.076874205844981,
      "grad_norm": 1.6869806051254272,
      "learning_rate": 2e-05,
      "loss": 0.1068,
      "step": 19130
    },
    {
      "epoch": 6.08005082592122,
      "grad_norm": 1.4374333620071411,
      "learning_rate": 2e-05,
      "loss": 0.1112,
      "step": 19140
    },
    {
      "epoch": 6.0832274459974585,
      "grad_norm": 2.895845413208008,
      "learning_rate": 2e-05,
      "loss": 0.1092,
      "step": 19150
    },
    {
      "epoch": 6.0864040660736975,
      "grad_norm": 3.318354606628418,
      "learning_rate": 2e-05,
      "loss": 0.1093,
      "step": 19160
    },
    {
      "epoch": 6.089580686149937,
      "grad_norm": 1.612258791923523,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 19170
    },
    {
      "epoch": 6.092757306226176,
      "grad_norm": 2.812831401824951,
      "learning_rate": 2e-05,
      "loss": 0.1094,
      "step": 19180
    },
    {
      "epoch": 6.095933926302414,
      "grad_norm": 3.301588296890259,
      "learning_rate": 2e-05,
      "loss": 0.1073,
      "step": 19190
    },
    {
      "epoch": 6.099110546378653,
      "grad_norm": 2.2998709678649902,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 19200
    },
    {
      "epoch": 6.102287166454892,
      "grad_norm": 1.5553158521652222,
      "learning_rate": 2e-05,
      "loss": 0.116,
      "step": 19210
    },
    {
      "epoch": 6.103875476493012,
      "eval_loss": 1.7197279930114746,
      "eval_mse": 1.7187334153389375,
      "eval_pearson": 0.43128952270820586,
      "eval_runtime": 20.8774,
      "eval_samples_per_second": 1032.695,
      "eval_spearmanr": 0.43009362062918244,
      "eval_steps_per_second": 4.071,
      "step": 19215
    },
    {
      "epoch": 6.105463786531131,
      "grad_norm": 8.697599411010742,
      "learning_rate": 2e-05,
      "loss": 0.1052,
      "step": 19220
    },
    {
      "epoch": 6.108640406607369,
      "grad_norm": 1.5332764387130737,
      "learning_rate": 2e-05,
      "loss": 0.1061,
      "step": 19230
    },
    {
      "epoch": 6.1118170266836085,
      "grad_norm": 1.5534827709197998,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 19240
    },
    {
      "epoch": 6.114993646759848,
      "grad_norm": 1.8497637510299683,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 19250
    },
    {
      "epoch": 6.118170266836087,
      "grad_norm": 1.6017295122146606,
      "learning_rate": 2e-05,
      "loss": 0.1122,
      "step": 19260
    },
    {
      "epoch": 6.121346886912325,
      "grad_norm": 2.8107287883758545,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 19270
    },
    {
      "epoch": 6.124523506988564,
      "grad_norm": 2.076289653778076,
      "learning_rate": 2e-05,
      "loss": 0.1144,
      "step": 19280
    },
    {
      "epoch": 6.127700127064803,
      "grad_norm": 2.1064162254333496,
      "learning_rate": 2e-05,
      "loss": 0.1052,
      "step": 19290
    },
    {
      "epoch": 6.130876747141042,
      "grad_norm": 1.7927234172821045,
      "learning_rate": 2e-05,
      "loss": 0.1138,
      "step": 19300
    },
    {
      "epoch": 6.134053367217281,
      "grad_norm": 3.0676217079162598,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 19310
    },
    {
      "epoch": 6.137229987293519,
      "grad_norm": 1.7066420316696167,
      "learning_rate": 2e-05,
      "loss": 0.1162,
      "step": 19320
    },
    {
      "epoch": 6.1404066073697585,
      "grad_norm": 1.4799840450286865,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 19330
    },
    {
      "epoch": 6.143583227445998,
      "grad_norm": 1.5453734397888184,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 19340
    },
    {
      "epoch": 6.146759847522237,
      "grad_norm": 2.081012010574341,
      "learning_rate": 2e-05,
      "loss": 0.1093,
      "step": 19350
    },
    {
      "epoch": 6.149936467598475,
      "grad_norm": 6.231215476989746,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 19360
    },
    {
      "epoch": 6.153113087674714,
      "grad_norm": 1.7434494495391846,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 19370
    },
    {
      "epoch": 6.156289707750953,
      "grad_norm": 8.826143264770508,
      "learning_rate": 2e-05,
      "loss": 0.1103,
      "step": 19380
    },
    {
      "epoch": 6.159466327827192,
      "grad_norm": 1.658811330795288,
      "learning_rate": 2e-05,
      "loss": 0.1273,
      "step": 19390
    },
    {
      "epoch": 6.16264294790343,
      "grad_norm": 2.9225051403045654,
      "learning_rate": 2e-05,
      "loss": 0.1242,
      "step": 19400
    },
    {
      "epoch": 6.1658195679796695,
      "grad_norm": 1.7355592250823975,
      "learning_rate": 2e-05,
      "loss": 0.1174,
      "step": 19410
    },
    {
      "epoch": 6.1689961880559085,
      "grad_norm": 1.413081169128418,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 19420
    },
    {
      "epoch": 6.172172808132148,
      "grad_norm": 2.6181623935699463,
      "learning_rate": 2e-05,
      "loss": 0.1193,
      "step": 19430
    },
    {
      "epoch": 6.175349428208387,
      "grad_norm": 3.673894166946411,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 19440
    },
    {
      "epoch": 6.178526048284625,
      "grad_norm": 3.334444999694824,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 19450
    },
    {
      "epoch": 6.181702668360864,
      "grad_norm": 2.321938991546631,
      "learning_rate": 2e-05,
      "loss": 0.102,
      "step": 19460
    },
    {
      "epoch": 6.184879288437103,
      "grad_norm": 2.9265310764312744,
      "learning_rate": 2e-05,
      "loss": 0.1178,
      "step": 19470
    },
    {
      "epoch": 6.188055908513342,
      "grad_norm": 1.716450572013855,
      "learning_rate": 2e-05,
      "loss": 0.0994,
      "step": 19480
    },
    {
      "epoch": 6.19123252858958,
      "grad_norm": 2.8954148292541504,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 19490
    },
    {
      "epoch": 6.1944091486658195,
      "grad_norm": 1.5117580890655518,
      "learning_rate": 2e-05,
      "loss": 0.1086,
      "step": 19500
    },
    {
      "epoch": 6.197585768742059,
      "grad_norm": 1.8525879383087158,
      "learning_rate": 2e-05,
      "loss": 0.1146,
      "step": 19510
    },
    {
      "epoch": 6.200762388818298,
      "grad_norm": 1.7425121068954468,
      "learning_rate": 2e-05,
      "loss": 0.1139,
      "step": 19520
    },
    {
      "epoch": 6.203939008894536,
      "grad_norm": 2.4019885063171387,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 19530
    },
    {
      "epoch": 6.203939008894536,
      "eval_loss": 1.8102402687072754,
      "eval_mse": 1.8091298542632894,
      "eval_pearson": 0.3931486240448359,
      "eval_runtime": 20.8637,
      "eval_samples_per_second": 1033.373,
      "eval_spearmanr": 0.38911024108893694,
      "eval_steps_per_second": 4.074,
      "step": 19530
    },
    {
      "epoch": 6.207115628970775,
      "grad_norm": 6.988104820251465,
      "learning_rate": 2e-05,
      "loss": 0.1171,
      "step": 19540
    },
    {
      "epoch": 6.210292249047014,
      "grad_norm": 6.831271171569824,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 19550
    },
    {
      "epoch": 6.213468869123253,
      "grad_norm": 2.3021132946014404,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 19560
    },
    {
      "epoch": 6.216645489199491,
      "grad_norm": 1.1141403913497925,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 19570
    },
    {
      "epoch": 6.21982210927573,
      "grad_norm": 1.3965829610824585,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 19580
    },
    {
      "epoch": 6.2229987293519695,
      "grad_norm": 1.3998546600341797,
      "learning_rate": 2e-05,
      "loss": 0.1,
      "step": 19590
    },
    {
      "epoch": 6.226175349428209,
      "grad_norm": 1.3748629093170166,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 19600
    },
    {
      "epoch": 6.229351969504448,
      "grad_norm": 2.9892055988311768,
      "learning_rate": 2e-05,
      "loss": 0.1123,
      "step": 19610
    },
    {
      "epoch": 6.232528589580686,
      "grad_norm": 1.7867810726165771,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 19620
    },
    {
      "epoch": 6.235705209656925,
      "grad_norm": 1.5711462497711182,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 19630
    },
    {
      "epoch": 6.238881829733164,
      "grad_norm": 3.0405385494232178,
      "learning_rate": 2e-05,
      "loss": 0.1117,
      "step": 19640
    },
    {
      "epoch": 6.242058449809403,
      "grad_norm": 1.4636170864105225,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 19650
    },
    {
      "epoch": 6.245235069885641,
      "grad_norm": 1.2519559860229492,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 19660
    },
    {
      "epoch": 6.2484116899618805,
      "grad_norm": 1.7951747179031372,
      "learning_rate": 2e-05,
      "loss": 0.1085,
      "step": 19670
    },
    {
      "epoch": 6.2515883100381195,
      "grad_norm": 1.2632453441619873,
      "learning_rate": 2e-05,
      "loss": 0.111,
      "step": 19680
    },
    {
      "epoch": 6.254764930114359,
      "grad_norm": 2.1206016540527344,
      "learning_rate": 2e-05,
      "loss": 0.1095,
      "step": 19690
    },
    {
      "epoch": 6.257941550190597,
      "grad_norm": 4.1267242431640625,
      "learning_rate": 2e-05,
      "loss": 0.1199,
      "step": 19700
    },
    {
      "epoch": 6.261118170266836,
      "grad_norm": 1.6115524768829346,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 19710
    },
    {
      "epoch": 6.264294790343075,
      "grad_norm": 1.6142126321792603,
      "learning_rate": 2e-05,
      "loss": 0.1161,
      "step": 19720
    },
    {
      "epoch": 6.267471410419314,
      "grad_norm": 1.2554205656051636,
      "learning_rate": 2e-05,
      "loss": 0.1066,
      "step": 19730
    },
    {
      "epoch": 6.270648030495552,
      "grad_norm": 2.0177392959594727,
      "learning_rate": 2e-05,
      "loss": 0.1088,
      "step": 19740
    },
    {
      "epoch": 6.273824650571791,
      "grad_norm": 1.7300059795379639,
      "learning_rate": 2e-05,
      "loss": 0.1154,
      "step": 19750
    },
    {
      "epoch": 6.2770012706480305,
      "grad_norm": 1.1200296878814697,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 19760
    },
    {
      "epoch": 6.28017789072427,
      "grad_norm": 1.2856694459915161,
      "learning_rate": 2e-05,
      "loss": 0.1143,
      "step": 19770
    },
    {
      "epoch": 6.283354510800509,
      "grad_norm": 1.190165400505066,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 19780
    },
    {
      "epoch": 6.286531130876747,
      "grad_norm": 1.3904305696487427,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 19790
    },
    {
      "epoch": 6.289707750952986,
      "grad_norm": 1.9216536283493042,
      "learning_rate": 2e-05,
      "loss": 0.1063,
      "step": 19800
    },
    {
      "epoch": 6.292884371029225,
      "grad_norm": 1.5280019044876099,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 19810
    },
    {
      "epoch": 6.296060991105464,
      "grad_norm": 1.179669976234436,
      "learning_rate": 2e-05,
      "loss": 0.1092,
      "step": 19820
    },
    {
      "epoch": 6.299237611181702,
      "grad_norm": 1.4636621475219727,
      "learning_rate": 2e-05,
      "loss": 0.106,
      "step": 19830
    },
    {
      "epoch": 6.302414231257941,
      "grad_norm": 1.6506538391113281,
      "learning_rate": 2e-05,
      "loss": 0.1174,
      "step": 19840
    },
    {
      "epoch": 6.304002541296061,
      "eval_loss": 1.6864299774169922,
      "eval_mse": 1.6848892678494578,
      "eval_pearson": 0.3851547314807561,
      "eval_runtime": 20.8667,
      "eval_samples_per_second": 1033.223,
      "eval_spearmanr": 0.38186826045618694,
      "eval_steps_per_second": 4.073,
      "step": 19845
    },
    {
      "epoch": 6.3055908513341805,
      "grad_norm": 3.142439603805542,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 19850
    },
    {
      "epoch": 6.30876747141042,
      "grad_norm": 2.9800891876220703,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 19860
    },
    {
      "epoch": 6.311944091486658,
      "grad_norm": 1.2411400079727173,
      "learning_rate": 2e-05,
      "loss": 0.1029,
      "step": 19870
    },
    {
      "epoch": 6.315120711562897,
      "grad_norm": 4.006816864013672,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 19880
    },
    {
      "epoch": 6.318297331639136,
      "grad_norm": 1.8699486255645752,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 19890
    },
    {
      "epoch": 6.321473951715375,
      "grad_norm": 2.6779625415802,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 19900
    },
    {
      "epoch": 6.324650571791613,
      "grad_norm": 2.0431759357452393,
      "learning_rate": 2e-05,
      "loss": 0.1136,
      "step": 19910
    },
    {
      "epoch": 6.327827191867852,
      "grad_norm": 1.9093801975250244,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 19920
    },
    {
      "epoch": 6.3310038119440915,
      "grad_norm": 3.006298065185547,
      "learning_rate": 2e-05,
      "loss": 0.1104,
      "step": 19930
    },
    {
      "epoch": 6.3341804320203305,
      "grad_norm": 2.048675298690796,
      "learning_rate": 2e-05,
      "loss": 0.1109,
      "step": 19940
    },
    {
      "epoch": 6.33735705209657,
      "grad_norm": 3.095771551132202,
      "learning_rate": 2e-05,
      "loss": 0.112,
      "step": 19950
    },
    {
      "epoch": 6.340533672172808,
      "grad_norm": 2.30234432220459,
      "learning_rate": 2e-05,
      "loss": 0.1173,
      "step": 19960
    },
    {
      "epoch": 6.343710292249047,
      "grad_norm": 1.4534159898757935,
      "learning_rate": 2e-05,
      "loss": 0.1132,
      "step": 19970
    },
    {
      "epoch": 6.346886912325286,
      "grad_norm": 1.683228611946106,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 19980
    },
    {
      "epoch": 6.350063532401525,
      "grad_norm": 1.8210361003875732,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 19990
    },
    {
      "epoch": 6.353240152477763,
      "grad_norm": 1.5132323503494263,
      "learning_rate": 2e-05,
      "loss": 0.0972,
      "step": 20000
    },
    {
      "epoch": 6.356416772554002,
      "grad_norm": 2.03688383102417,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 20010
    },
    {
      "epoch": 6.3595933926302415,
      "grad_norm": 1.6737884283065796,
      "learning_rate": 2e-05,
      "loss": 0.112,
      "step": 20020
    },
    {
      "epoch": 6.362770012706481,
      "grad_norm": 1.6919472217559814,
      "learning_rate": 2e-05,
      "loss": 0.1083,
      "step": 20030
    },
    {
      "epoch": 6.365946632782719,
      "grad_norm": 1.856363296508789,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 20040
    },
    {
      "epoch": 6.369123252858958,
      "grad_norm": 1.5592225790023804,
      "learning_rate": 2e-05,
      "loss": 0.1065,
      "step": 20050
    },
    {
      "epoch": 6.372299872935197,
      "grad_norm": 1.511153221130371,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 20060
    },
    {
      "epoch": 6.375476493011436,
      "grad_norm": 2.6145694255828857,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 20070
    },
    {
      "epoch": 6.378653113087674,
      "grad_norm": 1.4552087783813477,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 20080
    },
    {
      "epoch": 6.381829733163913,
      "grad_norm": 1.9433742761611938,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 20090
    },
    {
      "epoch": 6.385006353240152,
      "grad_norm": 1.8235032558441162,
      "learning_rate": 2e-05,
      "loss": 0.1247,
      "step": 20100
    },
    {
      "epoch": 6.3881829733163915,
      "grad_norm": 1.5107835531234741,
      "learning_rate": 2e-05,
      "loss": 0.1051,
      "step": 20110
    },
    {
      "epoch": 6.391359593392631,
      "grad_norm": 3.222747802734375,
      "learning_rate": 2e-05,
      "loss": 0.1236,
      "step": 20120
    },
    {
      "epoch": 6.394536213468869,
      "grad_norm": 1.6569701433181763,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 20130
    },
    {
      "epoch": 6.397712833545108,
      "grad_norm": 1.8151756525039673,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 20140
    },
    {
      "epoch": 6.400889453621347,
      "grad_norm": 1.5463547706604004,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 20150
    },
    {
      "epoch": 6.404066073697586,
      "grad_norm": 1.7195935249328613,
      "learning_rate": 2e-05,
      "loss": 0.1144,
      "step": 20160
    },
    {
      "epoch": 6.404066073697586,
      "eval_loss": 1.813679814338684,
      "eval_mse": 1.8122697610403775,
      "eval_pearson": 0.3964493271620806,
      "eval_runtime": 20.8451,
      "eval_samples_per_second": 1034.296,
      "eval_spearmanr": 0.3976588621727576,
      "eval_steps_per_second": 4.078,
      "step": 20160
    },
    {
      "epoch": 6.407242693773824,
      "grad_norm": 2.5712311267852783,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 20170
    },
    {
      "epoch": 6.410419313850063,
      "grad_norm": 1.2324771881103516,
      "learning_rate": 2e-05,
      "loss": 0.1055,
      "step": 20180
    },
    {
      "epoch": 6.4135959339263025,
      "grad_norm": 1.7428985834121704,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 20190
    },
    {
      "epoch": 6.4167725540025415,
      "grad_norm": 2.0217881202697754,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 20200
    },
    {
      "epoch": 6.419949174078781,
      "grad_norm": 1.4857739210128784,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 20210
    },
    {
      "epoch": 6.423125794155019,
      "grad_norm": 1.3873714208602905,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 20220
    },
    {
      "epoch": 6.426302414231258,
      "grad_norm": 1.6877864599227905,
      "learning_rate": 2e-05,
      "loss": 0.1161,
      "step": 20230
    },
    {
      "epoch": 6.429479034307497,
      "grad_norm": 1.3943300247192383,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 20240
    },
    {
      "epoch": 6.432655654383736,
      "grad_norm": 1.4900805950164795,
      "learning_rate": 2e-05,
      "loss": 0.1233,
      "step": 20250
    },
    {
      "epoch": 6.435832274459974,
      "grad_norm": 1.0977308750152588,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 20260
    },
    {
      "epoch": 6.439008894536213,
      "grad_norm": 1.478357195854187,
      "learning_rate": 2e-05,
      "loss": 0.1153,
      "step": 20270
    },
    {
      "epoch": 6.4421855146124525,
      "grad_norm": 2.0578982830047607,
      "learning_rate": 2e-05,
      "loss": 0.114,
      "step": 20280
    },
    {
      "epoch": 6.445362134688692,
      "grad_norm": 2.1839680671691895,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 20290
    },
    {
      "epoch": 6.44853875476493,
      "grad_norm": 1.0177226066589355,
      "learning_rate": 2e-05,
      "loss": 0.0996,
      "step": 20300
    },
    {
      "epoch": 6.451715374841169,
      "grad_norm": 2.3657126426696777,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 20310
    },
    {
      "epoch": 6.454891994917408,
      "grad_norm": 1.426586627960205,
      "learning_rate": 2e-05,
      "loss": 0.101,
      "step": 20320
    },
    {
      "epoch": 6.458068614993647,
      "grad_norm": 2.937843084335327,
      "learning_rate": 2e-05,
      "loss": 0.1052,
      "step": 20330
    },
    {
      "epoch": 6.461245235069885,
      "grad_norm": 1.7113416194915771,
      "learning_rate": 2e-05,
      "loss": 0.1213,
      "step": 20340
    },
    {
      "epoch": 6.464421855146124,
      "grad_norm": 1.8048123121261597,
      "learning_rate": 2e-05,
      "loss": 0.112,
      "step": 20350
    },
    {
      "epoch": 6.467598475222363,
      "grad_norm": 1.744349718093872,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 20360
    },
    {
      "epoch": 6.4707750952986025,
      "grad_norm": 1.9781599044799805,
      "learning_rate": 2e-05,
      "loss": 0.1066,
      "step": 20370
    },
    {
      "epoch": 6.473951715374842,
      "grad_norm": 2.62874436378479,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 20380
    },
    {
      "epoch": 6.47712833545108,
      "grad_norm": 1.4567570686340332,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 20390
    },
    {
      "epoch": 6.480304955527319,
      "grad_norm": 1.8073866367340088,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 20400
    },
    {
      "epoch": 6.483481575603558,
      "grad_norm": 2.128643035888672,
      "learning_rate": 2e-05,
      "loss": 0.1114,
      "step": 20410
    },
    {
      "epoch": 6.486658195679797,
      "grad_norm": 1.7891333103179932,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 20420
    },
    {
      "epoch": 6.489834815756035,
      "grad_norm": 2.876957416534424,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 20430
    },
    {
      "epoch": 6.493011435832274,
      "grad_norm": 3.385698080062866,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 20440
    },
    {
      "epoch": 6.4961880559085134,
      "grad_norm": 1.500077724456787,
      "learning_rate": 2e-05,
      "loss": 0.1157,
      "step": 20450
    },
    {
      "epoch": 6.4993646759847525,
      "grad_norm": 3.5199923515319824,
      "learning_rate": 2e-05,
      "loss": 0.1059,
      "step": 20460
    },
    {
      "epoch": 6.502541296060991,
      "grad_norm": 1.5862400531768799,
      "learning_rate": 2e-05,
      "loss": 0.1042,
      "step": 20470
    },
    {
      "epoch": 6.50412960609911,
      "eval_loss": 1.6642231941223145,
      "eval_mse": 1.663306079528438,
      "eval_pearson": 0.42403988237241824,
      "eval_runtime": 20.9777,
      "eval_samples_per_second": 1027.759,
      "eval_spearmanr": 0.4213995129497992,
      "eval_steps_per_second": 4.052,
      "step": 20475
    },
    {
      "epoch": 6.50571791613723,
      "grad_norm": 2.7229206562042236,
      "learning_rate": 2e-05,
      "loss": 0.1138,
      "step": 20480
    },
    {
      "epoch": 6.508894536213469,
      "grad_norm": 1.370131492614746,
      "learning_rate": 2e-05,
      "loss": 0.1084,
      "step": 20490
    },
    {
      "epoch": 6.512071156289708,
      "grad_norm": 1.4303088188171387,
      "learning_rate": 2e-05,
      "loss": 0.1093,
      "step": 20500
    },
    {
      "epoch": 6.515247776365946,
      "grad_norm": 1.9362220764160156,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 20510
    },
    {
      "epoch": 6.518424396442185,
      "grad_norm": 1.6909579038619995,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 20520
    },
    {
      "epoch": 6.521601016518424,
      "grad_norm": 1.8492448329925537,
      "learning_rate": 2e-05,
      "loss": 0.1049,
      "step": 20530
    },
    {
      "epoch": 6.5247776365946635,
      "grad_norm": 1.3547817468643188,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 20540
    },
    {
      "epoch": 6.527954256670903,
      "grad_norm": 1.6886765956878662,
      "learning_rate": 2e-05,
      "loss": 0.1155,
      "step": 20550
    },
    {
      "epoch": 6.531130876747141,
      "grad_norm": 1.2245733737945557,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 20560
    },
    {
      "epoch": 6.53430749682338,
      "grad_norm": 1.8064109086990356,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 20570
    },
    {
      "epoch": 6.537484116899619,
      "grad_norm": 1.149943232536316,
      "learning_rate": 2e-05,
      "loss": 0.1084,
      "step": 20580
    },
    {
      "epoch": 6.540660736975858,
      "grad_norm": 1.5960772037506104,
      "learning_rate": 2e-05,
      "loss": 0.1081,
      "step": 20590
    },
    {
      "epoch": 6.543837357052096,
      "grad_norm": 1.7039721012115479,
      "learning_rate": 2e-05,
      "loss": 0.0977,
      "step": 20600
    },
    {
      "epoch": 6.547013977128335,
      "grad_norm": 3.6295006275177,
      "learning_rate": 2e-05,
      "loss": 0.1118,
      "step": 20610
    },
    {
      "epoch": 6.550190597204574,
      "grad_norm": 1.8032128810882568,
      "learning_rate": 2e-05,
      "loss": 0.1122,
      "step": 20620
    },
    {
      "epoch": 6.5533672172808135,
      "grad_norm": 2.6709930896759033,
      "learning_rate": 2e-05,
      "loss": 0.1123,
      "step": 20630
    },
    {
      "epoch": 6.556543837357053,
      "grad_norm": 1.4491034746170044,
      "learning_rate": 2e-05,
      "loss": 0.1055,
      "step": 20640
    },
    {
      "epoch": 6.559720457433291,
      "grad_norm": 2.510979652404785,
      "learning_rate": 2e-05,
      "loss": 0.0994,
      "step": 20650
    },
    {
      "epoch": 6.56289707750953,
      "grad_norm": 1.7286279201507568,
      "learning_rate": 2e-05,
      "loss": 0.1111,
      "step": 20660
    },
    {
      "epoch": 6.566073697585769,
      "grad_norm": 1.6746046543121338,
      "learning_rate": 2e-05,
      "loss": 0.1095,
      "step": 20670
    },
    {
      "epoch": 6.569250317662007,
      "grad_norm": 2.4141323566436768,
      "learning_rate": 2e-05,
      "loss": 0.1086,
      "step": 20680
    },
    {
      "epoch": 6.572426937738246,
      "grad_norm": 1.7445566654205322,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 20690
    },
    {
      "epoch": 6.575603557814485,
      "grad_norm": 2.075972080230713,
      "learning_rate": 2e-05,
      "loss": 0.1069,
      "step": 20700
    },
    {
      "epoch": 6.5787801778907244,
      "grad_norm": 1.4157744646072388,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 20710
    },
    {
      "epoch": 6.5819567979669635,
      "grad_norm": 1.3932199478149414,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 20720
    },
    {
      "epoch": 6.585133418043202,
      "grad_norm": 3.0225179195404053,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 20730
    },
    {
      "epoch": 6.588310038119441,
      "grad_norm": 3.0464279651641846,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 20740
    },
    {
      "epoch": 6.59148665819568,
      "grad_norm": 1.5808321237564087,
      "learning_rate": 2e-05,
      "loss": 0.104,
      "step": 20750
    },
    {
      "epoch": 6.594663278271919,
      "grad_norm": 1.4749215841293335,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 20760
    },
    {
      "epoch": 6.597839898348157,
      "grad_norm": 3.488042116165161,
      "learning_rate": 2e-05,
      "loss": 0.1013,
      "step": 20770
    },
    {
      "epoch": 6.601016518424396,
      "grad_norm": 1.7046802043914795,
      "learning_rate": 2e-05,
      "loss": 0.1169,
      "step": 20780
    },
    {
      "epoch": 6.604193138500635,
      "grad_norm": 3.652686357498169,
      "learning_rate": 2e-05,
      "loss": 0.1138,
      "step": 20790
    },
    {
      "epoch": 6.604193138500635,
      "eval_loss": 1.7034844160079956,
      "eval_mse": 1.7021836047291026,
      "eval_pearson": 0.4094604981765695,
      "eval_runtime": 20.9542,
      "eval_samples_per_second": 1028.911,
      "eval_spearmanr": 0.4079486494389015,
      "eval_steps_per_second": 4.056,
      "step": 20790
    },
    {
      "epoch": 6.6073697585768745,
      "grad_norm": 2.169426918029785,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 20800
    },
    {
      "epoch": 6.610546378653114,
      "grad_norm": 1.631982684135437,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 20810
    },
    {
      "epoch": 6.613722998729352,
      "grad_norm": 1.1984196901321411,
      "learning_rate": 2e-05,
      "loss": 0.1127,
      "step": 20820
    },
    {
      "epoch": 6.616899618805591,
      "grad_norm": 1.3646588325500488,
      "learning_rate": 2e-05,
      "loss": 0.12,
      "step": 20830
    },
    {
      "epoch": 6.62007623888183,
      "grad_norm": 1.8116018772125244,
      "learning_rate": 2e-05,
      "loss": 0.113,
      "step": 20840
    },
    {
      "epoch": 6.623252858958068,
      "grad_norm": 2.1153998374938965,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 20850
    },
    {
      "epoch": 6.626429479034307,
      "grad_norm": 1.5234564542770386,
      "learning_rate": 2e-05,
      "loss": 0.1103,
      "step": 20860
    },
    {
      "epoch": 6.629606099110546,
      "grad_norm": 1.1490825414657593,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 20870
    },
    {
      "epoch": 6.632782719186785,
      "grad_norm": 2.124959945678711,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 20880
    },
    {
      "epoch": 6.6359593392630245,
      "grad_norm": 1.4335813522338867,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 20890
    },
    {
      "epoch": 6.639135959339263,
      "grad_norm": 2.697216272354126,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 20900
    },
    {
      "epoch": 6.642312579415502,
      "grad_norm": 1.22677743434906,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 20910
    },
    {
      "epoch": 6.645489199491741,
      "grad_norm": 1.610683560371399,
      "learning_rate": 2e-05,
      "loss": 0.1057,
      "step": 20920
    },
    {
      "epoch": 6.64866581956798,
      "grad_norm": 1.8179038763046265,
      "learning_rate": 2e-05,
      "loss": 0.1069,
      "step": 20930
    },
    {
      "epoch": 6.651842439644218,
      "grad_norm": 2.8708064556121826,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 20940
    },
    {
      "epoch": 6.655019059720457,
      "grad_norm": 1.740556001663208,
      "learning_rate": 2e-05,
      "loss": 0.0996,
      "step": 20950
    },
    {
      "epoch": 6.658195679796696,
      "grad_norm": 1.515817642211914,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 20960
    },
    {
      "epoch": 6.661372299872935,
      "grad_norm": 1.9905606508255005,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 20970
    },
    {
      "epoch": 6.6645489199491745,
      "grad_norm": 2.1880600452423096,
      "learning_rate": 2e-05,
      "loss": 0.1233,
      "step": 20980
    },
    {
      "epoch": 6.667725540025413,
      "grad_norm": 2.1936564445495605,
      "learning_rate": 2e-05,
      "loss": 0.1015,
      "step": 20990
    },
    {
      "epoch": 6.670902160101652,
      "grad_norm": 1.563494086265564,
      "learning_rate": 2e-05,
      "loss": 0.1178,
      "step": 21000
    },
    {
      "epoch": 6.674078780177891,
      "grad_norm": 1.2874045372009277,
      "learning_rate": 2e-05,
      "loss": 0.097,
      "step": 21010
    },
    {
      "epoch": 6.677255400254129,
      "grad_norm": 2.913933038711548,
      "learning_rate": 2e-05,
      "loss": 0.106,
      "step": 21020
    },
    {
      "epoch": 6.680432020330368,
      "grad_norm": 1.9950358867645264,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 21030
    },
    {
      "epoch": 6.683608640406607,
      "grad_norm": 2.059899091720581,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 21040
    },
    {
      "epoch": 6.686785260482846,
      "grad_norm": 1.3993427753448486,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 21050
    },
    {
      "epoch": 6.6899618805590855,
      "grad_norm": 1.7174561023712158,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 21060
    },
    {
      "epoch": 6.693138500635324,
      "grad_norm": 2.1263647079467773,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 21070
    },
    {
      "epoch": 6.696315120711563,
      "grad_norm": 1.6324700117111206,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 21080
    },
    {
      "epoch": 6.699491740787802,
      "grad_norm": 1.8837183713912964,
      "learning_rate": 2e-05,
      "loss": 0.1164,
      "step": 21090
    },
    {
      "epoch": 6.702668360864041,
      "grad_norm": 1.721703290939331,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 21100
    },
    {
      "epoch": 6.7042566709021605,
      "eval_loss": 1.6942769289016724,
      "eval_mse": 1.693145053014021,
      "eval_pearson": 0.41848956432753204,
      "eval_runtime": 20.8465,
      "eval_samples_per_second": 1034.226,
      "eval_spearmanr": 0.41357101769740584,
      "eval_steps_per_second": 4.077,
      "step": 21105
    },
    {
      "epoch": 6.705844980940279,
      "grad_norm": 1.3608412742614746,
      "learning_rate": 2e-05,
      "loss": 0.1013,
      "step": 21110
    },
    {
      "epoch": 6.709021601016518,
      "grad_norm": 2.146420955657959,
      "learning_rate": 2e-05,
      "loss": 0.1144,
      "step": 21120
    },
    {
      "epoch": 6.712198221092757,
      "grad_norm": 1.4827592372894287,
      "learning_rate": 2e-05,
      "loss": 0.1013,
      "step": 21130
    },
    {
      "epoch": 6.715374841168996,
      "grad_norm": 13.870546340942383,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 21140
    },
    {
      "epoch": 6.7185514612452355,
      "grad_norm": 1.529466152191162,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 21150
    },
    {
      "epoch": 6.721728081321474,
      "grad_norm": 2.2044830322265625,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 21160
    },
    {
      "epoch": 6.724904701397713,
      "grad_norm": 2.3101966381073,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 21170
    },
    {
      "epoch": 6.728081321473952,
      "grad_norm": 1.574050784111023,
      "learning_rate": 2e-05,
      "loss": 0.1105,
      "step": 21180
    },
    {
      "epoch": 6.731257941550191,
      "grad_norm": 1.3727978467941284,
      "learning_rate": 2e-05,
      "loss": 0.1151,
      "step": 21190
    },
    {
      "epoch": 6.734434561626429,
      "grad_norm": 1.357235312461853,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 21200
    },
    {
      "epoch": 6.737611181702668,
      "grad_norm": 1.3813190460205078,
      "learning_rate": 2e-05,
      "loss": 0.1014,
      "step": 21210
    },
    {
      "epoch": 6.740787801778907,
      "grad_norm": 3.0645294189453125,
      "learning_rate": 2e-05,
      "loss": 0.1064,
      "step": 21220
    },
    {
      "epoch": 6.743964421855146,
      "grad_norm": 1.437058925628662,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 21230
    },
    {
      "epoch": 6.747141041931385,
      "grad_norm": 1.8348544836044312,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 21240
    },
    {
      "epoch": 6.750317662007624,
      "grad_norm": 1.524096131324768,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 21250
    },
    {
      "epoch": 6.753494282083863,
      "grad_norm": 1.3023431301116943,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 21260
    },
    {
      "epoch": 6.756670902160102,
      "grad_norm": 1.8654749393463135,
      "learning_rate": 2e-05,
      "loss": 0.1052,
      "step": 21270
    },
    {
      "epoch": 6.75984752223634,
      "grad_norm": 1.4614683389663696,
      "learning_rate": 2e-05,
      "loss": 0.1008,
      "step": 21280
    },
    {
      "epoch": 6.763024142312579,
      "grad_norm": 1.283296823501587,
      "learning_rate": 2e-05,
      "loss": 0.1061,
      "step": 21290
    },
    {
      "epoch": 6.766200762388818,
      "grad_norm": 1.734230875968933,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 21300
    },
    {
      "epoch": 6.769377382465057,
      "grad_norm": 1.2414673566818237,
      "learning_rate": 2e-05,
      "loss": 0.1009,
      "step": 21310
    },
    {
      "epoch": 6.7725540025412965,
      "grad_norm": 1.7485631704330444,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 21320
    },
    {
      "epoch": 6.775730622617535,
      "grad_norm": 1.5260807275772095,
      "learning_rate": 2e-05,
      "loss": 0.1037,
      "step": 21330
    },
    {
      "epoch": 6.778907242693774,
      "grad_norm": 1.5895410776138306,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 21340
    },
    {
      "epoch": 6.782083862770013,
      "grad_norm": 1.4731625318527222,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 21350
    },
    {
      "epoch": 6.785260482846252,
      "grad_norm": 1.745141863822937,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 21360
    },
    {
      "epoch": 6.78843710292249,
      "grad_norm": 1.6538704633712769,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 21370
    },
    {
      "epoch": 6.791613722998729,
      "grad_norm": 3.923110246658325,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 21380
    },
    {
      "epoch": 6.794790343074968,
      "grad_norm": 3.2343263626098633,
      "learning_rate": 2e-05,
      "loss": 0.1068,
      "step": 21390
    },
    {
      "epoch": 6.797966963151207,
      "grad_norm": 1.4983257055282593,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 21400
    },
    {
      "epoch": 6.801143583227446,
      "grad_norm": 1.494675874710083,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 21410
    },
    {
      "epoch": 6.804320203303685,
      "grad_norm": 1.1669493913650513,
      "learning_rate": 2e-05,
      "loss": 0.1088,
      "step": 21420
    },
    {
      "epoch": 6.804320203303685,
      "eval_loss": 1.7144625186920166,
      "eval_mse": 1.713293248610761,
      "eval_pearson": 0.4113547654184885,
      "eval_runtime": 21.0637,
      "eval_samples_per_second": 1023.564,
      "eval_spearmanr": 0.4065727165981457,
      "eval_steps_per_second": 4.035,
      "step": 21420
    },
    {
      "epoch": 6.807496823379924,
      "grad_norm": 1.7835348844528198,
      "learning_rate": 2e-05,
      "loss": 0.1079,
      "step": 21430
    },
    {
      "epoch": 6.810673443456163,
      "grad_norm": 3.015469551086426,
      "learning_rate": 2e-05,
      "loss": 0.1065,
      "step": 21440
    },
    {
      "epoch": 6.813850063532401,
      "grad_norm": 2.863469362258911,
      "learning_rate": 2e-05,
      "loss": 0.1085,
      "step": 21450
    },
    {
      "epoch": 6.81702668360864,
      "grad_norm": 1.5275520086288452,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 21460
    },
    {
      "epoch": 6.820203303684879,
      "grad_norm": 2.2813236713409424,
      "learning_rate": 2e-05,
      "loss": 0.0925,
      "step": 21470
    },
    {
      "epoch": 6.823379923761118,
      "grad_norm": 1.697502613067627,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 21480
    },
    {
      "epoch": 6.826556543837357,
      "grad_norm": 1.4201794862747192,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 21490
    },
    {
      "epoch": 6.829733163913596,
      "grad_norm": 1.3047168254852295,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 21500
    },
    {
      "epoch": 6.832909783989835,
      "grad_norm": 2.300900936126709,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 21510
    },
    {
      "epoch": 6.836086404066074,
      "grad_norm": 1.7648518085479736,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 21520
    },
    {
      "epoch": 6.839263024142313,
      "grad_norm": 1.1914488077163696,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 21530
    },
    {
      "epoch": 6.842439644218551,
      "grad_norm": 2.161308526992798,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 21540
    },
    {
      "epoch": 6.84561626429479,
      "grad_norm": 2.8639461994171143,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 21550
    },
    {
      "epoch": 6.848792884371029,
      "grad_norm": 1.7745896577835083,
      "learning_rate": 2e-05,
      "loss": 0.1063,
      "step": 21560
    },
    {
      "epoch": 6.851969504447268,
      "grad_norm": 0.9848771691322327,
      "learning_rate": 2e-05,
      "loss": 0.0937,
      "step": 21570
    },
    {
      "epoch": 6.8551461245235075,
      "grad_norm": 1.2557635307312012,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 21580
    },
    {
      "epoch": 6.858322744599746,
      "grad_norm": 2.355480670928955,
      "learning_rate": 2e-05,
      "loss": 0.1023,
      "step": 21590
    },
    {
      "epoch": 6.861499364675985,
      "grad_norm": 1.7399359941482544,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 21600
    },
    {
      "epoch": 6.864675984752224,
      "grad_norm": 2.175503969192505,
      "learning_rate": 2e-05,
      "loss": 0.1109,
      "step": 21610
    },
    {
      "epoch": 6.867852604828462,
      "grad_norm": 1.4586255550384521,
      "learning_rate": 2e-05,
      "loss": 0.106,
      "step": 21620
    },
    {
      "epoch": 6.871029224904701,
      "grad_norm": 1.8323358297348022,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 21630
    },
    {
      "epoch": 6.87420584498094,
      "grad_norm": 1.2213047742843628,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 21640
    },
    {
      "epoch": 6.877382465057179,
      "grad_norm": 1.4692610502243042,
      "learning_rate": 2e-05,
      "loss": 0.1163,
      "step": 21650
    },
    {
      "epoch": 6.880559085133418,
      "grad_norm": 2.0565273761749268,
      "learning_rate": 2e-05,
      "loss": 0.1052,
      "step": 21660
    },
    {
      "epoch": 6.883735705209657,
      "grad_norm": 1.566906452178955,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 21670
    },
    {
      "epoch": 6.886912325285896,
      "grad_norm": 4.002223491668701,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 21680
    },
    {
      "epoch": 6.890088945362135,
      "grad_norm": 1.6532326936721802,
      "learning_rate": 2e-05,
      "loss": 0.1029,
      "step": 21690
    },
    {
      "epoch": 6.893265565438374,
      "grad_norm": 1.6152070760726929,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 21700
    },
    {
      "epoch": 6.896442185514612,
      "grad_norm": 2.0848751068115234,
      "learning_rate": 2e-05,
      "loss": 0.1095,
      "step": 21710
    },
    {
      "epoch": 6.899618805590851,
      "grad_norm": 3.457181692123413,
      "learning_rate": 2e-05,
      "loss": 0.0975,
      "step": 21720
    },
    {
      "epoch": 6.90279542566709,
      "grad_norm": 1.9164903163909912,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 21730
    },
    {
      "epoch": 6.90438373570521,
      "eval_loss": 1.7932180166244507,
      "eval_mse": 1.7922703801529873,
      "eval_pearson": 0.41557079228689325,
      "eval_runtime": 20.9433,
      "eval_samples_per_second": 1029.446,
      "eval_spearmanr": 0.4156510015571789,
      "eval_steps_per_second": 4.059,
      "step": 21735
    },
    {
      "epoch": 6.905972045743329,
      "grad_norm": 1.0649203062057495,
      "learning_rate": 2e-05,
      "loss": 0.0995,
      "step": 21740
    },
    {
      "epoch": 6.909148665819568,
      "grad_norm": 1.5333080291748047,
      "learning_rate": 2e-05,
      "loss": 0.1038,
      "step": 21750
    },
    {
      "epoch": 6.912325285895807,
      "grad_norm": 1.0333209037780762,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 21760
    },
    {
      "epoch": 6.915501905972046,
      "grad_norm": 2.359759569168091,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 21770
    },
    {
      "epoch": 6.918678526048285,
      "grad_norm": 2.539125680923462,
      "learning_rate": 2e-05,
      "loss": 0.1046,
      "step": 21780
    },
    {
      "epoch": 6.921855146124523,
      "grad_norm": 1.97250235080719,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 21790
    },
    {
      "epoch": 6.925031766200762,
      "grad_norm": 1.4807733297348022,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 21800
    },
    {
      "epoch": 6.928208386277001,
      "grad_norm": 1.0949156284332275,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 21810
    },
    {
      "epoch": 6.93138500635324,
      "grad_norm": 1.1238983869552612,
      "learning_rate": 2e-05,
      "loss": 0.1102,
      "step": 21820
    },
    {
      "epoch": 6.934561626429479,
      "grad_norm": 1.3855291604995728,
      "learning_rate": 2e-05,
      "loss": 0.0994,
      "step": 21830
    },
    {
      "epoch": 6.937738246505718,
      "grad_norm": 1.2584142684936523,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 21840
    },
    {
      "epoch": 6.940914866581957,
      "grad_norm": 2.0056040287017822,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 21850
    },
    {
      "epoch": 6.944091486658196,
      "grad_norm": 1.6675496101379395,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 21860
    },
    {
      "epoch": 6.947268106734435,
      "grad_norm": 1.7348763942718506,
      "learning_rate": 2e-05,
      "loss": 0.1061,
      "step": 21870
    },
    {
      "epoch": 6.950444726810673,
      "grad_norm": 1.388229250907898,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 21880
    },
    {
      "epoch": 6.953621346886912,
      "grad_norm": 1.5680102109909058,
      "learning_rate": 2e-05,
      "loss": 0.111,
      "step": 21890
    },
    {
      "epoch": 6.956797966963151,
      "grad_norm": 1.427611231803894,
      "learning_rate": 2e-05,
      "loss": 0.1048,
      "step": 21900
    },
    {
      "epoch": 6.95997458703939,
      "grad_norm": 2.0267813205718994,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 21910
    },
    {
      "epoch": 6.963151207115629,
      "grad_norm": 1.452621579170227,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 21920
    },
    {
      "epoch": 6.966327827191868,
      "grad_norm": 1.7906631231307983,
      "learning_rate": 2e-05,
      "loss": 0.1099,
      "step": 21930
    },
    {
      "epoch": 6.969504447268107,
      "grad_norm": 1.3713979721069336,
      "learning_rate": 2e-05,
      "loss": 0.0939,
      "step": 21940
    },
    {
      "epoch": 6.972681067344346,
      "grad_norm": 1.594448208808899,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 21950
    },
    {
      "epoch": 6.975857687420585,
      "grad_norm": 2.6630024909973145,
      "learning_rate": 2e-05,
      "loss": 0.1069,
      "step": 21960
    },
    {
      "epoch": 6.979034307496823,
      "grad_norm": 1.8303874731063843,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 21970
    },
    {
      "epoch": 6.982210927573062,
      "grad_norm": 1.9062966108322144,
      "learning_rate": 2e-05,
      "loss": 0.1008,
      "step": 21980
    },
    {
      "epoch": 6.985387547649301,
      "grad_norm": 2.1930243968963623,
      "learning_rate": 2e-05,
      "loss": 0.0976,
      "step": 21990
    },
    {
      "epoch": 6.98856416772554,
      "grad_norm": 1.6204007863998413,
      "learning_rate": 2e-05,
      "loss": 0.0999,
      "step": 22000
    },
    {
      "epoch": 6.9917407878017785,
      "grad_norm": 1.9346165657043457,
      "learning_rate": 2e-05,
      "loss": 0.1043,
      "step": 22010
    },
    {
      "epoch": 6.994917407878018,
      "grad_norm": 1.6559604406356812,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 22020
    },
    {
      "epoch": 6.998094027954257,
      "grad_norm": 2.0743408203125,
      "learning_rate": 2e-05,
      "loss": 0.1063,
      "step": 22030
    },
    {
      "epoch": 7.001270648030496,
      "grad_norm": 1.7605957984924316,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 22040
    },
    {
      "epoch": 7.004447268106734,
      "grad_norm": 1.3393090963363647,
      "learning_rate": 2e-05,
      "loss": 0.0849,
      "step": 22050
    },
    {
      "epoch": 7.004447268106734,
      "eval_loss": 1.7122702598571777,
      "eval_mse": 1.711058687995863,
      "eval_pearson": 0.42227572622253307,
      "eval_runtime": 20.9601,
      "eval_samples_per_second": 1028.62,
      "eval_spearmanr": 0.42182912533964534,
      "eval_steps_per_second": 4.055,
      "step": 22050
    },
    {
      "epoch": 7.007623888182973,
      "grad_norm": 1.6138604879379272,
      "learning_rate": 2e-05,
      "loss": 0.0922,
      "step": 22060
    },
    {
      "epoch": 7.010800508259212,
      "grad_norm": 1.6484332084655762,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 22070
    },
    {
      "epoch": 7.013977128335451,
      "grad_norm": 1.3511656522750854,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 22080
    },
    {
      "epoch": 7.01715374841169,
      "grad_norm": 2.1471593379974365,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 22090
    },
    {
      "epoch": 7.020330368487929,
      "grad_norm": 1.0053950548171997,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 22100
    },
    {
      "epoch": 7.023506988564168,
      "grad_norm": 1.1135942935943604,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 22110
    },
    {
      "epoch": 7.026683608640407,
      "grad_norm": 2.020855188369751,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 22120
    },
    {
      "epoch": 7.029860228716646,
      "grad_norm": 3.091869354248047,
      "learning_rate": 2e-05,
      "loss": 0.0888,
      "step": 22130
    },
    {
      "epoch": 7.033036848792884,
      "grad_norm": 2.915936231613159,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 22140
    },
    {
      "epoch": 7.036213468869123,
      "grad_norm": 1.1007936000823975,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 22150
    },
    {
      "epoch": 7.039390088945362,
      "grad_norm": 1.1437937021255493,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 22160
    },
    {
      "epoch": 7.042566709021601,
      "grad_norm": 1.2956761121749878,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 22170
    },
    {
      "epoch": 7.0457433290978395,
      "grad_norm": 2.4379897117614746,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 22180
    },
    {
      "epoch": 7.048919949174079,
      "grad_norm": 3.0182833671569824,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 22190
    },
    {
      "epoch": 7.052096569250318,
      "grad_norm": 1.9090712070465088,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 22200
    },
    {
      "epoch": 7.055273189326557,
      "grad_norm": 1.351197361946106,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 22210
    },
    {
      "epoch": 7.058449809402795,
      "grad_norm": 2.178910732269287,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 22220
    },
    {
      "epoch": 7.061626429479034,
      "grad_norm": 1.677242398262024,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 22230
    },
    {
      "epoch": 7.064803049555273,
      "grad_norm": 1.4569236040115356,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 22240
    },
    {
      "epoch": 7.067979669631512,
      "grad_norm": 1.1704379320144653,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 22250
    },
    {
      "epoch": 7.071156289707751,
      "grad_norm": 2.669422149658203,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 22260
    },
    {
      "epoch": 7.0743329097839895,
      "grad_norm": 1.4459037780761719,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 22270
    },
    {
      "epoch": 7.077509529860229,
      "grad_norm": 1.6820670366287231,
      "learning_rate": 2e-05,
      "loss": 0.0955,
      "step": 22280
    },
    {
      "epoch": 7.080686149936468,
      "grad_norm": 1.458147406578064,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 22290
    },
    {
      "epoch": 7.083862770012707,
      "grad_norm": 1.9516290426254272,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 22300
    },
    {
      "epoch": 7.087039390088945,
      "grad_norm": 2.6644623279571533,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 22310
    },
    {
      "epoch": 7.090216010165184,
      "grad_norm": 1.9869530200958252,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 22320
    },
    {
      "epoch": 7.093392630241423,
      "grad_norm": 1.6685630083084106,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 22330
    },
    {
      "epoch": 7.096569250317662,
      "grad_norm": 1.4883321523666382,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 22340
    },
    {
      "epoch": 7.0997458703939005,
      "grad_norm": 1.5499634742736816,
      "learning_rate": 2e-05,
      "loss": 0.0907,
      "step": 22350
    },
    {
      "epoch": 7.10292249047014,
      "grad_norm": 1.9803416728973389,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 22360
    },
    {
      "epoch": 7.104510800508259,
      "eval_loss": 1.7292454242706299,
      "eval_mse": 1.7280450000417835,
      "eval_pearson": 0.41293040549544596,
      "eval_runtime": 20.8793,
      "eval_samples_per_second": 1032.602,
      "eval_spearmanr": 0.41418306590681336,
      "eval_steps_per_second": 4.071,
      "step": 22365
    },
    {
      "epoch": 7.106099110546379,
      "grad_norm": 1.934112548828125,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 22370
    },
    {
      "epoch": 7.109275730622618,
      "grad_norm": 3.119049310684204,
      "learning_rate": 2e-05,
      "loss": 0.0979,
      "step": 22380
    },
    {
      "epoch": 7.112452350698857,
      "grad_norm": 1.391419529914856,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 22390
    },
    {
      "epoch": 7.115628970775095,
      "grad_norm": 1.9944292306900024,
      "learning_rate": 2e-05,
      "loss": 0.0918,
      "step": 22400
    },
    {
      "epoch": 7.118805590851334,
      "grad_norm": 1.7934688329696655,
      "learning_rate": 2e-05,
      "loss": 0.0922,
      "step": 22410
    },
    {
      "epoch": 7.121982210927573,
      "grad_norm": 1.596833348274231,
      "learning_rate": 2e-05,
      "loss": 0.0979,
      "step": 22420
    },
    {
      "epoch": 7.125158831003812,
      "grad_norm": 1.659340262413025,
      "learning_rate": 2e-05,
      "loss": 0.0963,
      "step": 22430
    },
    {
      "epoch": 7.1283354510800505,
      "grad_norm": 1.9205394983291626,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 22440
    },
    {
      "epoch": 7.13151207115629,
      "grad_norm": 1.6989777088165283,
      "learning_rate": 2e-05,
      "loss": 0.0942,
      "step": 22450
    },
    {
      "epoch": 7.134688691232529,
      "grad_norm": 1.2972122430801392,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 22460
    },
    {
      "epoch": 7.137865311308768,
      "grad_norm": 1.637347936630249,
      "learning_rate": 2e-05,
      "loss": 0.0925,
      "step": 22470
    },
    {
      "epoch": 7.141041931385006,
      "grad_norm": 1.770506739616394,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 22480
    },
    {
      "epoch": 7.144218551461245,
      "grad_norm": 1.5225685834884644,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 22490
    },
    {
      "epoch": 7.147395171537484,
      "grad_norm": 1.907904028892517,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 22500
    },
    {
      "epoch": 7.150571791613723,
      "grad_norm": 0.936630368232727,
      "learning_rate": 2e-05,
      "loss": 0.0905,
      "step": 22510
    },
    {
      "epoch": 7.1537484116899615,
      "grad_norm": 2.8406741619110107,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 22520
    },
    {
      "epoch": 7.1569250317662005,
      "grad_norm": 2.3276796340942383,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 22530
    },
    {
      "epoch": 7.16010165184244,
      "grad_norm": 1.8379615545272827,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 22540
    },
    {
      "epoch": 7.163278271918679,
      "grad_norm": 1.31550133228302,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 22550
    },
    {
      "epoch": 7.166454891994918,
      "grad_norm": 1.6119863986968994,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 22560
    },
    {
      "epoch": 7.169631512071156,
      "grad_norm": 1.4800193309783936,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 22570
    },
    {
      "epoch": 7.172808132147395,
      "grad_norm": 1.245947003364563,
      "learning_rate": 2e-05,
      "loss": 0.096,
      "step": 22580
    },
    {
      "epoch": 7.175984752223634,
      "grad_norm": 14.547119140625,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 22590
    },
    {
      "epoch": 7.179161372299873,
      "grad_norm": 1.0976084470748901,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 22600
    },
    {
      "epoch": 7.1823379923761115,
      "grad_norm": 1.3806575536727905,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 22610
    },
    {
      "epoch": 7.185514612452351,
      "grad_norm": 1.8475977182388306,
      "learning_rate": 2e-05,
      "loss": 0.0955,
      "step": 22620
    },
    {
      "epoch": 7.18869123252859,
      "grad_norm": 3.0966784954071045,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 22630
    },
    {
      "epoch": 7.191867852604829,
      "grad_norm": 3.8471972942352295,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 22640
    },
    {
      "epoch": 7.195044472681067,
      "grad_norm": 3.2530736923217773,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 22650
    },
    {
      "epoch": 7.198221092757306,
      "grad_norm": 1.6760149002075195,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 22660
    },
    {
      "epoch": 7.201397712833545,
      "grad_norm": 1.328852653503418,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 22670
    },
    {
      "epoch": 7.204574332909784,
      "grad_norm": 1.7737482786178589,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 22680
    },
    {
      "epoch": 7.204574332909784,
      "eval_loss": 1.7666161060333252,
      "eval_mse": 1.7653083259851459,
      "eval_pearson": 0.3777377633786619,
      "eval_runtime": 20.9868,
      "eval_samples_per_second": 1027.311,
      "eval_spearmanr": 0.37596528234147014,
      "eval_steps_per_second": 4.05,
      "step": 22680
    },
    {
      "epoch": 7.207750952986023,
      "grad_norm": 1.497045636177063,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 22690
    },
    {
      "epoch": 7.2109275730622615,
      "grad_norm": 1.8776369094848633,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 22700
    },
    {
      "epoch": 7.214104193138501,
      "grad_norm": 1.1634612083435059,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 22710
    },
    {
      "epoch": 7.21728081321474,
      "grad_norm": 1.676384687423706,
      "learning_rate": 2e-05,
      "loss": 0.0908,
      "step": 22720
    },
    {
      "epoch": 7.220457433290979,
      "grad_norm": 1.7191087007522583,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 22730
    },
    {
      "epoch": 7.223634053367217,
      "grad_norm": 1.6276755332946777,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 22740
    },
    {
      "epoch": 7.226810673443456,
      "grad_norm": 0.9659706354141235,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 22750
    },
    {
      "epoch": 7.229987293519695,
      "grad_norm": 1.0170023441314697,
      "learning_rate": 2e-05,
      "loss": 0.1007,
      "step": 22760
    },
    {
      "epoch": 7.233163913595934,
      "grad_norm": 1.4596225023269653,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 22770
    },
    {
      "epoch": 7.2363405336721724,
      "grad_norm": 1.7090859413146973,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 22780
    },
    {
      "epoch": 7.2395171537484115,
      "grad_norm": 1.5656893253326416,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 22790
    },
    {
      "epoch": 7.242693773824651,
      "grad_norm": 1.5228331089019775,
      "learning_rate": 2e-05,
      "loss": 0.1009,
      "step": 22800
    },
    {
      "epoch": 7.24587039390089,
      "grad_norm": 1.4121432304382324,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 22810
    },
    {
      "epoch": 7.249047013977128,
      "grad_norm": 1.0847673416137695,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 22820
    },
    {
      "epoch": 7.252223634053367,
      "grad_norm": 1.0904440879821777,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 22830
    },
    {
      "epoch": 7.255400254129606,
      "grad_norm": 4.440629959106445,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 22840
    },
    {
      "epoch": 7.258576874205845,
      "grad_norm": 2.496721029281616,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 22850
    },
    {
      "epoch": 7.261753494282084,
      "grad_norm": 1.2614736557006836,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 22860
    },
    {
      "epoch": 7.2649301143583225,
      "grad_norm": 1.6513878107070923,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 22870
    },
    {
      "epoch": 7.268106734434562,
      "grad_norm": 1.2033357620239258,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 22880
    },
    {
      "epoch": 7.271283354510801,
      "grad_norm": 1.3915654420852661,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 22890
    },
    {
      "epoch": 7.27445997458704,
      "grad_norm": 1.4025574922561646,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 22900
    },
    {
      "epoch": 7.277636594663278,
      "grad_norm": 1.2454077005386353,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 22910
    },
    {
      "epoch": 7.280813214739517,
      "grad_norm": 1.5748451948165894,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 22920
    },
    {
      "epoch": 7.283989834815756,
      "grad_norm": 1.3906188011169434,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 22930
    },
    {
      "epoch": 7.287166454891995,
      "grad_norm": 1.265191674232483,
      "learning_rate": 2e-05,
      "loss": 0.0928,
      "step": 22940
    },
    {
      "epoch": 7.290343074968233,
      "grad_norm": 1.8699241876602173,
      "learning_rate": 2e-05,
      "loss": 0.0908,
      "step": 22950
    },
    {
      "epoch": 7.2935196950444725,
      "grad_norm": 1.8830441236495972,
      "learning_rate": 2e-05,
      "loss": 0.1073,
      "step": 22960
    },
    {
      "epoch": 7.296696315120712,
      "grad_norm": 1.8732367753982544,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 22970
    },
    {
      "epoch": 7.299872935196951,
      "grad_norm": 1.4611786603927612,
      "learning_rate": 2e-05,
      "loss": 0.091,
      "step": 22980
    },
    {
      "epoch": 7.303049555273189,
      "grad_norm": 1.2091163396835327,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 22990
    },
    {
      "epoch": 7.304637865311308,
      "eval_loss": 1.675549030303955,
      "eval_mse": 1.6747787023612548,
      "eval_pearson": 0.41838944867681227,
      "eval_runtime": 20.8529,
      "eval_samples_per_second": 1033.908,
      "eval_spearmanr": 0.4156913029746729,
      "eval_steps_per_second": 4.076,
      "step": 22995
    },
    {
      "epoch": 7.306226175349428,
      "grad_norm": 1.3177043199539185,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 23000
    },
    {
      "epoch": 7.309402795425667,
      "grad_norm": 1.3375300168991089,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 23010
    },
    {
      "epoch": 7.312579415501906,
      "grad_norm": 4.171545028686523,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 23020
    },
    {
      "epoch": 7.315756035578145,
      "grad_norm": 1.6745246648788452,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 23030
    },
    {
      "epoch": 7.3189326556543834,
      "grad_norm": 2.417729616165161,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 23040
    },
    {
      "epoch": 7.3221092757306225,
      "grad_norm": 1.4849278926849365,
      "learning_rate": 2e-05,
      "loss": 0.0849,
      "step": 23050
    },
    {
      "epoch": 7.325285895806862,
      "grad_norm": 2.7779109477996826,
      "learning_rate": 2e-05,
      "loss": 0.0912,
      "step": 23060
    },
    {
      "epoch": 7.328462515883101,
      "grad_norm": 1.2493692636489868,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 23070
    },
    {
      "epoch": 7.331639135959339,
      "grad_norm": 1.5274306535720825,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 23080
    },
    {
      "epoch": 7.334815756035578,
      "grad_norm": 1.4319182634353638,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 23090
    },
    {
      "epoch": 7.337992376111817,
      "grad_norm": 1.8932909965515137,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 23100
    },
    {
      "epoch": 7.341168996188056,
      "grad_norm": 1.4156720638275146,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 23110
    },
    {
      "epoch": 7.344345616264294,
      "grad_norm": 1.524694800376892,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 23120
    },
    {
      "epoch": 7.3475222363405335,
      "grad_norm": 2.6799464225769043,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 23130
    },
    {
      "epoch": 7.350698856416773,
      "grad_norm": 1.1576086282730103,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 23140
    },
    {
      "epoch": 7.353875476493012,
      "grad_norm": 1.413769245147705,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 23150
    },
    {
      "epoch": 7.35705209656925,
      "grad_norm": 1.2154542207717896,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 23160
    },
    {
      "epoch": 7.360228716645489,
      "grad_norm": 1.7035671472549438,
      "learning_rate": 2e-05,
      "loss": 0.0922,
      "step": 23170
    },
    {
      "epoch": 7.363405336721728,
      "grad_norm": 3.2393107414245605,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 23180
    },
    {
      "epoch": 7.366581956797967,
      "grad_norm": 1.478215217590332,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 23190
    },
    {
      "epoch": 7.369758576874206,
      "grad_norm": 1.400894045829773,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 23200
    },
    {
      "epoch": 7.372935196950444,
      "grad_norm": 2.7854225635528564,
      "learning_rate": 2e-05,
      "loss": 0.0928,
      "step": 23210
    },
    {
      "epoch": 7.3761118170266835,
      "grad_norm": 2.655693531036377,
      "learning_rate": 2e-05,
      "loss": 0.0918,
      "step": 23220
    },
    {
      "epoch": 7.379288437102923,
      "grad_norm": 1.1394195556640625,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 23230
    },
    {
      "epoch": 7.382465057179162,
      "grad_norm": 1.6421936750411987,
      "learning_rate": 2e-05,
      "loss": 0.091,
      "step": 23240
    },
    {
      "epoch": 7.3856416772554,
      "grad_norm": 1.6154611110687256,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 23250
    },
    {
      "epoch": 7.388818297331639,
      "grad_norm": 1.9931038618087769,
      "learning_rate": 2e-05,
      "loss": 0.0871,
      "step": 23260
    },
    {
      "epoch": 7.391994917407878,
      "grad_norm": 1.328284502029419,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 23270
    },
    {
      "epoch": 7.395171537484117,
      "grad_norm": 3.0980703830718994,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 23280
    },
    {
      "epoch": 7.398348157560356,
      "grad_norm": 2.05893874168396,
      "learning_rate": 2e-05,
      "loss": 0.0954,
      "step": 23290
    },
    {
      "epoch": 7.401524777636594,
      "grad_norm": 2.4149248600006104,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 23300
    },
    {
      "epoch": 7.4047013977128335,
      "grad_norm": 1.7781974077224731,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 23310
    },
    {
      "epoch": 7.4047013977128335,
      "eval_loss": 1.6997510194778442,
      "eval_mse": 1.698593621463712,
      "eval_pearson": 0.4154372462327261,
      "eval_runtime": 20.9705,
      "eval_samples_per_second": 1028.11,
      "eval_spearmanr": 0.4088607977483531,
      "eval_steps_per_second": 4.053,
      "step": 23310
    },
    {
      "epoch": 7.407878017789073,
      "grad_norm": 2.4393997192382812,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 23320
    },
    {
      "epoch": 7.411054637865312,
      "grad_norm": 1.3463040590286255,
      "learning_rate": 2e-05,
      "loss": 0.0903,
      "step": 23330
    },
    {
      "epoch": 7.41423125794155,
      "grad_norm": 2.028823137283325,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 23340
    },
    {
      "epoch": 7.417407878017789,
      "grad_norm": 2.474724769592285,
      "learning_rate": 2e-05,
      "loss": 0.0979,
      "step": 23350
    },
    {
      "epoch": 7.420584498094028,
      "grad_norm": 1.7652888298034668,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 23360
    },
    {
      "epoch": 7.423761118170267,
      "grad_norm": 1.5012845993041992,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 23370
    },
    {
      "epoch": 7.426937738246505,
      "grad_norm": 1.7803068161010742,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 23380
    },
    {
      "epoch": 7.4301143583227445,
      "grad_norm": 1.1031047105789185,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 23390
    },
    {
      "epoch": 7.433290978398984,
      "grad_norm": 1.31680428981781,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 23400
    },
    {
      "epoch": 7.436467598475223,
      "grad_norm": 1.2914985418319702,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 23410
    },
    {
      "epoch": 7.439644218551461,
      "grad_norm": 1.6270676851272583,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 23420
    },
    {
      "epoch": 7.4428208386277,
      "grad_norm": 1.3376977443695068,
      "learning_rate": 2e-05,
      "loss": 0.0831,
      "step": 23430
    },
    {
      "epoch": 7.445997458703939,
      "grad_norm": 1.566319227218628,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 23440
    },
    {
      "epoch": 7.449174078780178,
      "grad_norm": 2.056668281555176,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 23450
    },
    {
      "epoch": 7.452350698856417,
      "grad_norm": 1.099363088607788,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 23460
    },
    {
      "epoch": 7.455527318932655,
      "grad_norm": 1.2724729776382446,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 23470
    },
    {
      "epoch": 7.4587039390088945,
      "grad_norm": 1.1659455299377441,
      "learning_rate": 2e-05,
      "loss": 0.0975,
      "step": 23480
    },
    {
      "epoch": 7.461880559085134,
      "grad_norm": 2.3499579429626465,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 23490
    },
    {
      "epoch": 7.465057179161373,
      "grad_norm": 1.3546804189682007,
      "learning_rate": 2e-05,
      "loss": 0.0847,
      "step": 23500
    },
    {
      "epoch": 7.468233799237611,
      "grad_norm": 1.7560585737228394,
      "learning_rate": 2e-05,
      "loss": 0.0861,
      "step": 23510
    },
    {
      "epoch": 7.47141041931385,
      "grad_norm": 1.2889903783798218,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 23520
    },
    {
      "epoch": 7.474587039390089,
      "grad_norm": 3.23460054397583,
      "learning_rate": 2e-05,
      "loss": 0.0992,
      "step": 23530
    },
    {
      "epoch": 7.477763659466328,
      "grad_norm": 1.5512206554412842,
      "learning_rate": 2e-05,
      "loss": 0.0955,
      "step": 23540
    },
    {
      "epoch": 7.480940279542566,
      "grad_norm": 1.98725163936615,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 23550
    },
    {
      "epoch": 7.484116899618805,
      "grad_norm": 1.0371090173721313,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 23560
    },
    {
      "epoch": 7.4872935196950445,
      "grad_norm": 1.8389066457748413,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 23570
    },
    {
      "epoch": 7.490470139771284,
      "grad_norm": 1.8173567056655884,
      "learning_rate": 2e-05,
      "loss": 0.0944,
      "step": 23580
    },
    {
      "epoch": 7.493646759847522,
      "grad_norm": 1.4356627464294434,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 23590
    },
    {
      "epoch": 7.496823379923761,
      "grad_norm": 1.1979864835739136,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 23600
    },
    {
      "epoch": 7.5,
      "grad_norm": 1.3031375408172607,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 23610
    },
    {
      "epoch": 7.503176620076239,
      "grad_norm": 2.6469056606292725,
      "learning_rate": 2e-05,
      "loss": 0.0967,
      "step": 23620
    },
    {
      "epoch": 7.504764930114359,
      "eval_loss": 1.5925962924957275,
      "eval_mse": 1.5915436036518475,
      "eval_pearson": 0.42190069997947394,
      "eval_runtime": 20.8655,
      "eval_samples_per_second": 1033.284,
      "eval_spearmanr": 0.42613436754669026,
      "eval_steps_per_second": 4.074,
      "step": 23625
    },
    {
      "epoch": 7.506353240152478,
      "grad_norm": 1.792726993560791,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 23630
    },
    {
      "epoch": 7.509529860228716,
      "grad_norm": 1.2597053050994873,
      "learning_rate": 2e-05,
      "loss": 0.0994,
      "step": 23640
    },
    {
      "epoch": 7.5127064803049555,
      "grad_norm": 1.7940218448638916,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 23650
    },
    {
      "epoch": 7.515883100381195,
      "grad_norm": 2.160088062286377,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 23660
    },
    {
      "epoch": 7.519059720457434,
      "grad_norm": 1.2958890199661255,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 23670
    },
    {
      "epoch": 7.522236340533672,
      "grad_norm": 1.285988450050354,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 23680
    },
    {
      "epoch": 7.525412960609911,
      "grad_norm": 1.9366294145584106,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 23690
    },
    {
      "epoch": 7.52858958068615,
      "grad_norm": 4.963865756988525,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 23700
    },
    {
      "epoch": 7.531766200762389,
      "grad_norm": 1.8522696495056152,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 23710
    },
    {
      "epoch": 7.534942820838627,
      "grad_norm": 3.456470012664795,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 23720
    },
    {
      "epoch": 7.538119440914866,
      "grad_norm": 1.2968467473983765,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 23730
    },
    {
      "epoch": 7.5412960609911055,
      "grad_norm": 1.2414191961288452,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 23740
    },
    {
      "epoch": 7.544472681067345,
      "grad_norm": 1.1043436527252197,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 23750
    },
    {
      "epoch": 7.547649301143583,
      "grad_norm": 1.205299735069275,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 23760
    },
    {
      "epoch": 7.550825921219822,
      "grad_norm": 2.223973035812378,
      "learning_rate": 2e-05,
      "loss": 0.0932,
      "step": 23770
    },
    {
      "epoch": 7.554002541296061,
      "grad_norm": 1.3885083198547363,
      "learning_rate": 2e-05,
      "loss": 0.0972,
      "step": 23780
    },
    {
      "epoch": 7.5571791613723,
      "grad_norm": 1.7196927070617676,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 23790
    },
    {
      "epoch": 7.560355781448539,
      "grad_norm": 2.7828783988952637,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 23800
    },
    {
      "epoch": 7.563532401524777,
      "grad_norm": 2.3035993576049805,
      "learning_rate": 2e-05,
      "loss": 0.0861,
      "step": 23810
    },
    {
      "epoch": 7.566709021601016,
      "grad_norm": 1.422553300857544,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 23820
    },
    {
      "epoch": 7.5698856416772555,
      "grad_norm": 0.9088431000709534,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 23830
    },
    {
      "epoch": 7.573062261753495,
      "grad_norm": 1.9627033472061157,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 23840
    },
    {
      "epoch": 7.576238881829733,
      "grad_norm": 1.695603609085083,
      "learning_rate": 2e-05,
      "loss": 0.0998,
      "step": 23850
    },
    {
      "epoch": 7.579415501905972,
      "grad_norm": 1.7546387910842896,
      "learning_rate": 2e-05,
      "loss": 0.0873,
      "step": 23860
    },
    {
      "epoch": 7.582592121982211,
      "grad_norm": 2.229581356048584,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 23870
    },
    {
      "epoch": 7.58576874205845,
      "grad_norm": 1.995159387588501,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 23880
    },
    {
      "epoch": 7.588945362134689,
      "grad_norm": 1.3529702425003052,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 23890
    },
    {
      "epoch": 7.592121982210927,
      "grad_norm": 1.4563062191009521,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 23900
    },
    {
      "epoch": 7.5952986022871665,
      "grad_norm": 2.334733486175537,
      "learning_rate": 2e-05,
      "loss": 0.1007,
      "step": 23910
    },
    {
      "epoch": 7.598475222363406,
      "grad_norm": 1.309254765510559,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 23920
    },
    {
      "epoch": 7.601651842439644,
      "grad_norm": 1.5034784078598022,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 23930
    },
    {
      "epoch": 7.604828462515883,
      "grad_norm": 2.161656141281128,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 23940
    },
    {
      "epoch": 7.604828462515883,
      "eval_loss": 1.756010890007019,
      "eval_mse": 1.7550720538657152,
      "eval_pearson": 0.3940393505681288,
      "eval_runtime": 20.8606,
      "eval_samples_per_second": 1033.529,
      "eval_spearmanr": 0.39288292206165343,
      "eval_steps_per_second": 4.075,
      "step": 23940
    },
    {
      "epoch": 7.608005082592122,
      "grad_norm": 1.3488712310791016,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 23950
    },
    {
      "epoch": 7.611181702668361,
      "grad_norm": 1.8588370084762573,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 23960
    },
    {
      "epoch": 7.6143583227446,
      "grad_norm": 6.655149459838867,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 23970
    },
    {
      "epoch": 7.617534942820838,
      "grad_norm": 4.322673320770264,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 23980
    },
    {
      "epoch": 7.620711562897077,
      "grad_norm": 2.9003570079803467,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 23990
    },
    {
      "epoch": 7.6238881829733165,
      "grad_norm": 1.2966197729110718,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 24000
    },
    {
      "epoch": 7.627064803049556,
      "grad_norm": 1.5827244520187378,
      "learning_rate": 2e-05,
      "loss": 0.0918,
      "step": 24010
    },
    {
      "epoch": 7.630241423125794,
      "grad_norm": 1.1590256690979004,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 24020
    },
    {
      "epoch": 7.633418043202033,
      "grad_norm": 0.9687777757644653,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 24030
    },
    {
      "epoch": 7.636594663278272,
      "grad_norm": 1.7786415815353394,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 24040
    },
    {
      "epoch": 7.639771283354511,
      "grad_norm": 1.7166904211044312,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 24050
    },
    {
      "epoch": 7.64294790343075,
      "grad_norm": 1.268729329109192,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 24060
    },
    {
      "epoch": 7.646124523506988,
      "grad_norm": 1.297592282295227,
      "learning_rate": 2e-05,
      "loss": 0.0845,
      "step": 24070
    },
    {
      "epoch": 7.649301143583227,
      "grad_norm": 1.9514553546905518,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 24080
    },
    {
      "epoch": 7.6524777636594665,
      "grad_norm": 1.8500677347183228,
      "learning_rate": 2e-05,
      "loss": 0.0912,
      "step": 24090
    },
    {
      "epoch": 7.655654383735705,
      "grad_norm": 1.7626680135726929,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 24100
    },
    {
      "epoch": 7.658831003811944,
      "grad_norm": 1.1341782808303833,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 24110
    },
    {
      "epoch": 7.662007623888183,
      "grad_norm": 2.3085379600524902,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 24120
    },
    {
      "epoch": 7.665184243964422,
      "grad_norm": 2.070533514022827,
      "learning_rate": 2e-05,
      "loss": 0.0837,
      "step": 24130
    },
    {
      "epoch": 7.668360864040661,
      "grad_norm": 1.394047737121582,
      "learning_rate": 2e-05,
      "loss": 0.0873,
      "step": 24140
    },
    {
      "epoch": 7.671537484116899,
      "grad_norm": 1.3037021160125732,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 24150
    },
    {
      "epoch": 7.674714104193138,
      "grad_norm": 2.0705759525299072,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 24160
    },
    {
      "epoch": 7.6778907242693775,
      "grad_norm": 2.0591022968292236,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 24170
    },
    {
      "epoch": 7.6810673443456166,
      "grad_norm": 3.824453115463257,
      "learning_rate": 2e-05,
      "loss": 0.0852,
      "step": 24180
    },
    {
      "epoch": 7.684243964421855,
      "grad_norm": 2.600234031677246,
      "learning_rate": 2e-05,
      "loss": 0.0912,
      "step": 24190
    },
    {
      "epoch": 7.687420584498094,
      "grad_norm": 1.3877853155136108,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 24200
    },
    {
      "epoch": 7.690597204574333,
      "grad_norm": 1.555870771408081,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 24210
    },
    {
      "epoch": 7.693773824650572,
      "grad_norm": 1.5693532228469849,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 24220
    },
    {
      "epoch": 7.696950444726811,
      "grad_norm": 1.6712597608566284,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 24230
    },
    {
      "epoch": 7.700127064803049,
      "grad_norm": 2.8639276027679443,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 24240
    },
    {
      "epoch": 7.703303684879288,
      "grad_norm": 0.9010177254676819,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 24250
    },
    {
      "epoch": 7.704891994917408,
      "eval_loss": 1.6660199165344238,
      "eval_mse": 1.6647020230109153,
      "eval_pearson": 0.4411762321317326,
      "eval_runtime": 21.1491,
      "eval_samples_per_second": 1019.427,
      "eval_spearmanr": 0.43969288779093285,
      "eval_steps_per_second": 4.019,
      "step": 24255
    },
    {
      "epoch": 7.7064803049555275,
      "grad_norm": 1.1289187669754028,
      "learning_rate": 2e-05,
      "loss": 0.0905,
      "step": 24260
    },
    {
      "epoch": 7.709656925031767,
      "grad_norm": 1.7330384254455566,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 24270
    },
    {
      "epoch": 7.712833545108005,
      "grad_norm": 1.7715121507644653,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 24280
    },
    {
      "epoch": 7.716010165184244,
      "grad_norm": 5.58577299118042,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 24290
    },
    {
      "epoch": 7.719186785260483,
      "grad_norm": 2.0631191730499268,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 24300
    },
    {
      "epoch": 7.722363405336722,
      "grad_norm": 0.9737908244132996,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 24310
    },
    {
      "epoch": 7.72554002541296,
      "grad_norm": 2.0475611686706543,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 24320
    },
    {
      "epoch": 7.728716645489199,
      "grad_norm": 1.0669996738433838,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 24330
    },
    {
      "epoch": 7.731893265565438,
      "grad_norm": 1.6263006925582886,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 24340
    },
    {
      "epoch": 7.7350698856416775,
      "grad_norm": 1.1370117664337158,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 24350
    },
    {
      "epoch": 7.738246505717916,
      "grad_norm": 2.145345449447632,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 24360
    },
    {
      "epoch": 7.741423125794155,
      "grad_norm": 1.5278761386871338,
      "learning_rate": 2e-05,
      "loss": 0.0937,
      "step": 24370
    },
    {
      "epoch": 7.744599745870394,
      "grad_norm": 1.4236477613449097,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 24380
    },
    {
      "epoch": 7.747776365946633,
      "grad_norm": 1.2516334056854248,
      "learning_rate": 2e-05,
      "loss": 0.0853,
      "step": 24390
    },
    {
      "epoch": 7.750952986022872,
      "grad_norm": 2.3588032722473145,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 24400
    },
    {
      "epoch": 7.75412960609911,
      "grad_norm": 1.2952994108200073,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 24410
    },
    {
      "epoch": 7.757306226175349,
      "grad_norm": 1.223405122756958,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 24420
    },
    {
      "epoch": 7.7604828462515885,
      "grad_norm": 1.0387327671051025,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 24430
    },
    {
      "epoch": 7.7636594663278276,
      "grad_norm": 1.8815444707870483,
      "learning_rate": 2e-05,
      "loss": 0.0888,
      "step": 24440
    },
    {
      "epoch": 7.766836086404066,
      "grad_norm": 2.311079502105713,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 24450
    },
    {
      "epoch": 7.770012706480305,
      "grad_norm": 2.190985918045044,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 24460
    },
    {
      "epoch": 7.773189326556544,
      "grad_norm": 1.3280982971191406,
      "learning_rate": 2e-05,
      "loss": 0.0849,
      "step": 24470
    },
    {
      "epoch": 7.776365946632783,
      "grad_norm": 1.6285606622695923,
      "learning_rate": 2e-05,
      "loss": 0.0855,
      "step": 24480
    },
    {
      "epoch": 7.779542566709021,
      "grad_norm": 9.8023099899292,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 24490
    },
    {
      "epoch": 7.78271918678526,
      "grad_norm": 10.552816390991211,
      "learning_rate": 2e-05,
      "loss": 0.0857,
      "step": 24500
    },
    {
      "epoch": 7.785895806861499,
      "grad_norm": 1.117180347442627,
      "learning_rate": 2e-05,
      "loss": 0.1032,
      "step": 24510
    },
    {
      "epoch": 7.7890724269377385,
      "grad_norm": 1.846274733543396,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 24520
    },
    {
      "epoch": 7.792249047013977,
      "grad_norm": 2.1240522861480713,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 24530
    },
    {
      "epoch": 7.795425667090216,
      "grad_norm": 1.1796951293945312,
      "learning_rate": 2e-05,
      "loss": 0.0915,
      "step": 24540
    },
    {
      "epoch": 7.798602287166455,
      "grad_norm": 1.2437204122543335,
      "learning_rate": 2e-05,
      "loss": 0.0866,
      "step": 24550
    },
    {
      "epoch": 7.801778907242694,
      "grad_norm": 1.819288969039917,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 24560
    },
    {
      "epoch": 7.804955527318933,
      "grad_norm": 2.049409866333008,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 24570
    },
    {
      "epoch": 7.804955527318933,
      "eval_loss": 1.65628182888031,
      "eval_mse": 1.6552207881615164,
      "eval_pearson": 0.4472273536841733,
      "eval_runtime": 20.9752,
      "eval_samples_per_second": 1027.879,
      "eval_spearmanr": 0.446076801743231,
      "eval_steps_per_second": 4.052,
      "step": 24570
    },
    {
      "epoch": 7.808132147395171,
      "grad_norm": 1.7818846702575684,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 24580
    },
    {
      "epoch": 7.81130876747141,
      "grad_norm": 2.8588430881500244,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 24590
    },
    {
      "epoch": 7.814485387547649,
      "grad_norm": 0.9936164021492004,
      "learning_rate": 2e-05,
      "loss": 0.0873,
      "step": 24600
    },
    {
      "epoch": 7.8176620076238885,
      "grad_norm": 1.4585227966308594,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 24610
    },
    {
      "epoch": 7.820838627700127,
      "grad_norm": 1.257344365119934,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 24620
    },
    {
      "epoch": 7.824015247776366,
      "grad_norm": 1.1590129137039185,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 24630
    },
    {
      "epoch": 7.827191867852605,
      "grad_norm": 1.767608642578125,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 24640
    },
    {
      "epoch": 7.830368487928844,
      "grad_norm": 1.3908857107162476,
      "learning_rate": 2e-05,
      "loss": 0.0977,
      "step": 24650
    },
    {
      "epoch": 7.833545108005083,
      "grad_norm": 1.5295169353485107,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 24660
    },
    {
      "epoch": 7.836721728081321,
      "grad_norm": 1.7508792877197266,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 24670
    },
    {
      "epoch": 7.83989834815756,
      "grad_norm": 2.9239091873168945,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 24680
    },
    {
      "epoch": 7.8430749682337995,
      "grad_norm": 1.2337431907653809,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 24690
    },
    {
      "epoch": 7.846251588310038,
      "grad_norm": 1.3082079887390137,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 24700
    },
    {
      "epoch": 7.849428208386277,
      "grad_norm": 1.4159791469573975,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 24710
    },
    {
      "epoch": 7.852604828462516,
      "grad_norm": 1.9282881021499634,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 24720
    },
    {
      "epoch": 7.855781448538755,
      "grad_norm": 1.5993417501449585,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 24730
    },
    {
      "epoch": 7.858958068614994,
      "grad_norm": 2.5753800868988037,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 24740
    },
    {
      "epoch": 7.862134688691232,
      "grad_norm": 1.965395212173462,
      "learning_rate": 2e-05,
      "loss": 0.0793,
      "step": 24750
    },
    {
      "epoch": 7.865311308767471,
      "grad_norm": 1.5397727489471436,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 24760
    },
    {
      "epoch": 7.86848792884371,
      "grad_norm": 1.272091269493103,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 24770
    },
    {
      "epoch": 7.8716645489199495,
      "grad_norm": 2.1315364837646484,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 24780
    },
    {
      "epoch": 7.874841168996188,
      "grad_norm": 3.075890302658081,
      "learning_rate": 2e-05,
      "loss": 0.0888,
      "step": 24790
    },
    {
      "epoch": 7.878017789072427,
      "grad_norm": 1.5151833295822144,
      "learning_rate": 2e-05,
      "loss": 0.0942,
      "step": 24800
    },
    {
      "epoch": 7.881194409148666,
      "grad_norm": 0.9305610656738281,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 24810
    },
    {
      "epoch": 7.884371029224905,
      "grad_norm": 0.8529732823371887,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 24820
    },
    {
      "epoch": 7.887547649301144,
      "grad_norm": 1.1505815982818604,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 24830
    },
    {
      "epoch": 7.890724269377382,
      "grad_norm": 1.3788235187530518,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 24840
    },
    {
      "epoch": 7.893900889453621,
      "grad_norm": 1.7765108346939087,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 24850
    },
    {
      "epoch": 7.89707750952986,
      "grad_norm": 1.8248885869979858,
      "learning_rate": 2e-05,
      "loss": 0.1088,
      "step": 24860
    },
    {
      "epoch": 7.900254129606099,
      "grad_norm": 2.0175089836120605,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 24870
    },
    {
      "epoch": 7.903430749682338,
      "grad_norm": 1.399836778640747,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 24880
    },
    {
      "epoch": 7.905019059720457,
      "eval_loss": 1.8872989416122437,
      "eval_mse": 1.8859992995706247,
      "eval_pearson": 0.3599629769037313,
      "eval_runtime": 20.8786,
      "eval_samples_per_second": 1032.636,
      "eval_spearmanr": 0.3580740233535982,
      "eval_steps_per_second": 4.071,
      "step": 24885
    },
    {
      "epoch": 7.906607369758577,
      "grad_norm": 6.528039455413818,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 24890
    },
    {
      "epoch": 7.909783989834816,
      "grad_norm": 1.2699849605560303,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 24900
    },
    {
      "epoch": 7.912960609911055,
      "grad_norm": 1.7657437324523926,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 24910
    },
    {
      "epoch": 7.916137229987293,
      "grad_norm": 1.0740102529525757,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 24920
    },
    {
      "epoch": 7.919313850063532,
      "grad_norm": 1.5171757936477661,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 24930
    },
    {
      "epoch": 7.922490470139771,
      "grad_norm": 1.4531289339065552,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 24940
    },
    {
      "epoch": 7.9256670902160105,
      "grad_norm": 1.2448315620422363,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 24950
    },
    {
      "epoch": 7.928843710292249,
      "grad_norm": 1.3961617946624756,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 24960
    },
    {
      "epoch": 7.932020330368488,
      "grad_norm": 1.5466609001159668,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 24970
    },
    {
      "epoch": 7.935196950444727,
      "grad_norm": 1.4759286642074585,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 24980
    },
    {
      "epoch": 7.938373570520966,
      "grad_norm": 1.725257396697998,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 24990
    },
    {
      "epoch": 7.941550190597205,
      "grad_norm": 1.2803984880447388,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 25000
    },
    {
      "epoch": 7.944726810673443,
      "grad_norm": 1.6834443807601929,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 25010
    },
    {
      "epoch": 7.947903430749682,
      "grad_norm": 1.051077127456665,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 25020
    },
    {
      "epoch": 7.951080050825921,
      "grad_norm": 1.055335283279419,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 25030
    },
    {
      "epoch": 7.9542566709021605,
      "grad_norm": 1.8760623931884766,
      "learning_rate": 2e-05,
      "loss": 0.0867,
      "step": 25040
    },
    {
      "epoch": 7.957433290978399,
      "grad_norm": 0.9189737439155579,
      "learning_rate": 2e-05,
      "loss": 0.0874,
      "step": 25050
    },
    {
      "epoch": 7.960609911054638,
      "grad_norm": 1.3451024293899536,
      "learning_rate": 2e-05,
      "loss": 0.101,
      "step": 25060
    },
    {
      "epoch": 7.963786531130877,
      "grad_norm": 1.1176656484603882,
      "learning_rate": 2e-05,
      "loss": 0.0888,
      "step": 25070
    },
    {
      "epoch": 7.966963151207116,
      "grad_norm": 2.0329337120056152,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 25080
    },
    {
      "epoch": 7.970139771283354,
      "grad_norm": 0.9695687294006348,
      "learning_rate": 2e-05,
      "loss": 0.0852,
      "step": 25090
    },
    {
      "epoch": 7.973316391359593,
      "grad_norm": 1.0569716691970825,
      "learning_rate": 2e-05,
      "loss": 0.0905,
      "step": 25100
    },
    {
      "epoch": 7.976493011435832,
      "grad_norm": 1.1715823411941528,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 25110
    },
    {
      "epoch": 7.979669631512071,
      "grad_norm": 1.2354978322982788,
      "learning_rate": 2e-05,
      "loss": 0.097,
      "step": 25120
    },
    {
      "epoch": 7.98284625158831,
      "grad_norm": 2.4488821029663086,
      "learning_rate": 2e-05,
      "loss": 0.0922,
      "step": 25130
    },
    {
      "epoch": 7.986022871664549,
      "grad_norm": 1.4417049884796143,
      "learning_rate": 2e-05,
      "loss": 0.0922,
      "step": 25140
    },
    {
      "epoch": 7.989199491740788,
      "grad_norm": 0.9903172254562378,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 25150
    },
    {
      "epoch": 7.992376111817027,
      "grad_norm": 1.7149869203567505,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 25160
    },
    {
      "epoch": 7.995552731893266,
      "grad_norm": 1.213975191116333,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 25170
    },
    {
      "epoch": 7.998729351969504,
      "grad_norm": 1.44263756275177,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 25180
    },
    {
      "epoch": 8.001905972045744,
      "grad_norm": 1.9127789735794067,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 25190
    },
    {
      "epoch": 8.005082592121981,
      "grad_norm": 1.2822016477584839,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 25200
    },
    {
      "epoch": 8.005082592121981,
      "eval_loss": 1.941476583480835,
      "eval_mse": 1.9406764243080974,
      "eval_pearson": 0.37493461912290205,
      "eval_runtime": 21.0725,
      "eval_samples_per_second": 1023.133,
      "eval_spearmanr": 0.37186173853644333,
      "eval_steps_per_second": 4.034,
      "step": 25200
    },
    {
      "epoch": 8.00825921219822,
      "grad_norm": 1.3194419145584106,
      "learning_rate": 2e-05,
      "loss": 0.0796,
      "step": 25210
    },
    {
      "epoch": 8.01143583227446,
      "grad_norm": 1.757535696029663,
      "learning_rate": 2e-05,
      "loss": 0.0744,
      "step": 25220
    },
    {
      "epoch": 8.014612452350699,
      "grad_norm": 1.8102811574935913,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 25230
    },
    {
      "epoch": 8.017789072426938,
      "grad_norm": 1.3545573949813843,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 25240
    },
    {
      "epoch": 8.020965692503177,
      "grad_norm": 1.3311920166015625,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 25250
    },
    {
      "epoch": 8.024142312579416,
      "grad_norm": 1.2420365810394287,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 25260
    },
    {
      "epoch": 8.027318932655655,
      "grad_norm": 1.3860703706741333,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 25270
    },
    {
      "epoch": 8.030495552731892,
      "grad_norm": 1.023598313331604,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 25280
    },
    {
      "epoch": 8.033672172808132,
      "grad_norm": 1.6861763000488281,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 25290
    },
    {
      "epoch": 8.03684879288437,
      "grad_norm": 1.5378155708312988,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 25300
    },
    {
      "epoch": 8.04002541296061,
      "grad_norm": 1.1604480743408203,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 25310
    },
    {
      "epoch": 8.043202033036849,
      "grad_norm": 1.555751085281372,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 25320
    },
    {
      "epoch": 8.046378653113088,
      "grad_norm": 0.9961222410202026,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 25330
    },
    {
      "epoch": 8.049555273189327,
      "grad_norm": 1.342892050743103,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 25340
    },
    {
      "epoch": 8.052731893265566,
      "grad_norm": 1.293960690498352,
      "learning_rate": 2e-05,
      "loss": 0.0832,
      "step": 25350
    },
    {
      "epoch": 8.055908513341805,
      "grad_norm": 1.4206202030181885,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 25360
    },
    {
      "epoch": 8.059085133418042,
      "grad_norm": 1.113770604133606,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 25370
    },
    {
      "epoch": 8.062261753494282,
      "grad_norm": 1.1405184268951416,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 25380
    },
    {
      "epoch": 8.06543837357052,
      "grad_norm": 1.814577341079712,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 25390
    },
    {
      "epoch": 8.06861499364676,
      "grad_norm": 0.759833574295044,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 25400
    },
    {
      "epoch": 8.071791613722999,
      "grad_norm": 1.0362452268600464,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 25410
    },
    {
      "epoch": 8.074968233799238,
      "grad_norm": 1.6423661708831787,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 25420
    },
    {
      "epoch": 8.078144853875477,
      "grad_norm": 0.7870394587516785,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 25430
    },
    {
      "epoch": 8.081321473951716,
      "grad_norm": 1.7076106071472168,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 25440
    },
    {
      "epoch": 8.084498094027953,
      "grad_norm": 1.3658440113067627,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 25450
    },
    {
      "epoch": 8.087674714104192,
      "grad_norm": 1.0649187564849854,
      "learning_rate": 2e-05,
      "loss": 0.0799,
      "step": 25460
    },
    {
      "epoch": 8.090851334180432,
      "grad_norm": 1.0098263025283813,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 25470
    },
    {
      "epoch": 8.09402795425667,
      "grad_norm": 1.0876812934875488,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 25480
    },
    {
      "epoch": 8.09720457433291,
      "grad_norm": 4.593423843383789,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 25490
    },
    {
      "epoch": 8.100381194409149,
      "grad_norm": 2.0727479457855225,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 25500
    },
    {
      "epoch": 8.103557814485388,
      "grad_norm": 1.4765592813491821,
      "learning_rate": 2e-05,
      "loss": 0.0834,
      "step": 25510
    },
    {
      "epoch": 8.105146124523507,
      "eval_loss": 1.7826610803604126,
      "eval_mse": 1.7818795462569987,
      "eval_pearson": 0.40123430015832817,
      "eval_runtime": 20.9188,
      "eval_samples_per_second": 1030.654,
      "eval_spearmanr": 0.39794568330958474,
      "eval_steps_per_second": 4.063,
      "step": 25515
    }
  ],
  "logging_steps": 10,
  "max_steps": 62960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 315,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.718614686255022e+18,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
