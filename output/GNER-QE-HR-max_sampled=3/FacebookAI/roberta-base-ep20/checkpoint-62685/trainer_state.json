{
  "best_metric": 0.44949190345807716,
  "best_model_checkpoint": "output-lfs/GNER-QE-HR-2/FacebookAI/roberta-base-max_sampled=3/checkpoint-1890",
  "epoch": 19.912642947903432,
  "eval_steps": 315,
  "global_step": 62685,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0031766200762388818,
      "grad_norm": 42.53852844238281,
      "learning_rate": 2e-05,
      "loss": 6.3574,
      "step": 10
    },
    {
      "epoch": 0.0063532401524777635,
      "grad_norm": 19.485984802246094,
      "learning_rate": 2e-05,
      "loss": 1.3588,
      "step": 20
    },
    {
      "epoch": 0.009529860228716646,
      "grad_norm": 13.047877311706543,
      "learning_rate": 2e-05,
      "loss": 1.3124,
      "step": 30
    },
    {
      "epoch": 0.012706480304955527,
      "grad_norm": 13.044604301452637,
      "learning_rate": 2e-05,
      "loss": 1.2342,
      "step": 40
    },
    {
      "epoch": 0.01588310038119441,
      "grad_norm": 4.088164329528809,
      "learning_rate": 2e-05,
      "loss": 1.1453,
      "step": 50
    },
    {
      "epoch": 0.01905972045743329,
      "grad_norm": 12.082822799682617,
      "learning_rate": 2e-05,
      "loss": 1.27,
      "step": 60
    },
    {
      "epoch": 0.022236340533672173,
      "grad_norm": 18.536048889160156,
      "learning_rate": 2e-05,
      "loss": 1.148,
      "step": 70
    },
    {
      "epoch": 0.025412960609911054,
      "grad_norm": 15.674713134765625,
      "learning_rate": 2e-05,
      "loss": 1.1694,
      "step": 80
    },
    {
      "epoch": 0.028589580686149935,
      "grad_norm": 4.794887065887451,
      "learning_rate": 2e-05,
      "loss": 1.1426,
      "step": 90
    },
    {
      "epoch": 0.03176620076238882,
      "grad_norm": 8.29433822631836,
      "learning_rate": 2e-05,
      "loss": 1.0921,
      "step": 100
    },
    {
      "epoch": 0.0349428208386277,
      "grad_norm": 13.569055557250977,
      "learning_rate": 2e-05,
      "loss": 1.0665,
      "step": 110
    },
    {
      "epoch": 0.03811944091486658,
      "grad_norm": 5.662858009338379,
      "learning_rate": 2e-05,
      "loss": 1.1252,
      "step": 120
    },
    {
      "epoch": 0.041296060991105464,
      "grad_norm": 9.695500373840332,
      "learning_rate": 2e-05,
      "loss": 1.1672,
      "step": 130
    },
    {
      "epoch": 0.044472681067344345,
      "grad_norm": 3.4943511486053467,
      "learning_rate": 2e-05,
      "loss": 1.1541,
      "step": 140
    },
    {
      "epoch": 0.04764930114358323,
      "grad_norm": 2.9279448986053467,
      "learning_rate": 2e-05,
      "loss": 1.1229,
      "step": 150
    },
    {
      "epoch": 0.05082592121982211,
      "grad_norm": 8.464885711669922,
      "learning_rate": 2e-05,
      "loss": 1.0495,
      "step": 160
    },
    {
      "epoch": 0.05400254129606099,
      "grad_norm": 7.904899597167969,
      "learning_rate": 2e-05,
      "loss": 1.2026,
      "step": 170
    },
    {
      "epoch": 0.05717916137229987,
      "grad_norm": 7.821130275726318,
      "learning_rate": 2e-05,
      "loss": 1.0906,
      "step": 180
    },
    {
      "epoch": 0.06035578144853875,
      "grad_norm": 4.030617713928223,
      "learning_rate": 2e-05,
      "loss": 1.0434,
      "step": 190
    },
    {
      "epoch": 0.06353240152477764,
      "grad_norm": 3.1077730655670166,
      "learning_rate": 2e-05,
      "loss": 1.1531,
      "step": 200
    },
    {
      "epoch": 0.06670902160101652,
      "grad_norm": 3.6123456954956055,
      "learning_rate": 2e-05,
      "loss": 1.1222,
      "step": 210
    },
    {
      "epoch": 0.0698856416772554,
      "grad_norm": 2.5921452045440674,
      "learning_rate": 2e-05,
      "loss": 1.1335,
      "step": 220
    },
    {
      "epoch": 0.07306226175349428,
      "grad_norm": 3.1890413761138916,
      "learning_rate": 2e-05,
      "loss": 1.1256,
      "step": 230
    },
    {
      "epoch": 0.07623888182973317,
      "grad_norm": 5.475101470947266,
      "learning_rate": 2e-05,
      "loss": 1.0933,
      "step": 240
    },
    {
      "epoch": 0.07941550190597205,
      "grad_norm": 7.749471187591553,
      "learning_rate": 2e-05,
      "loss": 1.1237,
      "step": 250
    },
    {
      "epoch": 0.08259212198221093,
      "grad_norm": 3.6897902488708496,
      "learning_rate": 2e-05,
      "loss": 1.1114,
      "step": 260
    },
    {
      "epoch": 0.08576874205844981,
      "grad_norm": 10.741327285766602,
      "learning_rate": 2e-05,
      "loss": 1.0481,
      "step": 270
    },
    {
      "epoch": 0.08894536213468869,
      "grad_norm": 5.57321310043335,
      "learning_rate": 2e-05,
      "loss": 1.09,
      "step": 280
    },
    {
      "epoch": 0.09212198221092757,
      "grad_norm": 12.088486671447754,
      "learning_rate": 2e-05,
      "loss": 1.1258,
      "step": 290
    },
    {
      "epoch": 0.09529860228716645,
      "grad_norm": 3.2766354084014893,
      "learning_rate": 2e-05,
      "loss": 1.0338,
      "step": 300
    },
    {
      "epoch": 0.09847522236340533,
      "grad_norm": 5.1415324211120605,
      "learning_rate": 2e-05,
      "loss": 1.0419,
      "step": 310
    },
    {
      "epoch": 0.10006353240152478,
      "eval_loss": 1.5232889652252197,
      "eval_mse": 1.5219143941583793,
      "eval_pearson": 0.439252880243838,
      "eval_runtime": 7.3733,
      "eval_samples_per_second": 2924.056,
      "eval_spearmanr": 0.4225737354561571,
      "eval_steps_per_second": 11.528,
      "step": 315
    },
    {
      "epoch": 0.10165184243964422,
      "grad_norm": 3.629209041595459,
      "learning_rate": 2e-05,
      "loss": 1.0867,
      "step": 320
    },
    {
      "epoch": 0.1048284625158831,
      "grad_norm": 5.53952169418335,
      "learning_rate": 2e-05,
      "loss": 1.0124,
      "step": 330
    },
    {
      "epoch": 0.10800508259212198,
      "grad_norm": 3.78043532371521,
      "learning_rate": 2e-05,
      "loss": 1.0729,
      "step": 340
    },
    {
      "epoch": 0.11118170266836086,
      "grad_norm": 14.417452812194824,
      "learning_rate": 2e-05,
      "loss": 1.0724,
      "step": 350
    },
    {
      "epoch": 0.11435832274459974,
      "grad_norm": 4.284093856811523,
      "learning_rate": 2e-05,
      "loss": 1.0107,
      "step": 360
    },
    {
      "epoch": 0.11753494282083862,
      "grad_norm": 7.045243740081787,
      "learning_rate": 2e-05,
      "loss": 1.0604,
      "step": 370
    },
    {
      "epoch": 0.1207115628970775,
      "grad_norm": 8.126378059387207,
      "learning_rate": 2e-05,
      "loss": 1.067,
      "step": 380
    },
    {
      "epoch": 0.12388818297331639,
      "grad_norm": 11.11319637298584,
      "learning_rate": 2e-05,
      "loss": 1.0816,
      "step": 390
    },
    {
      "epoch": 0.12706480304955528,
      "grad_norm": 10.49304485321045,
      "learning_rate": 2e-05,
      "loss": 1.0676,
      "step": 400
    },
    {
      "epoch": 0.13024142312579415,
      "grad_norm": 10.379779815673828,
      "learning_rate": 2e-05,
      "loss": 1.0938,
      "step": 410
    },
    {
      "epoch": 0.13341804320203304,
      "grad_norm": 8.145538330078125,
      "learning_rate": 2e-05,
      "loss": 1.0573,
      "step": 420
    },
    {
      "epoch": 0.1365946632782719,
      "grad_norm": 3.124001979827881,
      "learning_rate": 2e-05,
      "loss": 1.0175,
      "step": 430
    },
    {
      "epoch": 0.1397712833545108,
      "grad_norm": 9.80490493774414,
      "learning_rate": 2e-05,
      "loss": 1.1002,
      "step": 440
    },
    {
      "epoch": 0.14294790343074967,
      "grad_norm": 6.4963226318359375,
      "learning_rate": 2e-05,
      "loss": 1.0809,
      "step": 450
    },
    {
      "epoch": 0.14612452350698857,
      "grad_norm": 10.397116661071777,
      "learning_rate": 2e-05,
      "loss": 1.0299,
      "step": 460
    },
    {
      "epoch": 0.14930114358322744,
      "grad_norm": 28.03232765197754,
      "learning_rate": 2e-05,
      "loss": 0.9973,
      "step": 470
    },
    {
      "epoch": 0.15247776365946633,
      "grad_norm": 4.6494293212890625,
      "learning_rate": 2e-05,
      "loss": 0.9805,
      "step": 480
    },
    {
      "epoch": 0.1556543837357052,
      "grad_norm": 7.220850944519043,
      "learning_rate": 2e-05,
      "loss": 1.0009,
      "step": 490
    },
    {
      "epoch": 0.1588310038119441,
      "grad_norm": 9.263932228088379,
      "learning_rate": 2e-05,
      "loss": 1.0996,
      "step": 500
    },
    {
      "epoch": 0.16200762388818296,
      "grad_norm": 6.03701639175415,
      "learning_rate": 2e-05,
      "loss": 1.0178,
      "step": 510
    },
    {
      "epoch": 0.16518424396442186,
      "grad_norm": 6.948718547821045,
      "learning_rate": 2e-05,
      "loss": 1.0122,
      "step": 520
    },
    {
      "epoch": 0.16836086404066072,
      "grad_norm": 6.38271951675415,
      "learning_rate": 2e-05,
      "loss": 1.0014,
      "step": 530
    },
    {
      "epoch": 0.17153748411689962,
      "grad_norm": 4.033596992492676,
      "learning_rate": 2e-05,
      "loss": 0.9576,
      "step": 540
    },
    {
      "epoch": 0.17471410419313851,
      "grad_norm": 5.990135192871094,
      "learning_rate": 2e-05,
      "loss": 1.0455,
      "step": 550
    },
    {
      "epoch": 0.17789072426937738,
      "grad_norm": 3.403928518295288,
      "learning_rate": 2e-05,
      "loss": 1.0386,
      "step": 560
    },
    {
      "epoch": 0.18106734434561628,
      "grad_norm": 7.42679500579834,
      "learning_rate": 2e-05,
      "loss": 1.0002,
      "step": 570
    },
    {
      "epoch": 0.18424396442185514,
      "grad_norm": 3.2370314598083496,
      "learning_rate": 2e-05,
      "loss": 1.0026,
      "step": 580
    },
    {
      "epoch": 0.18742058449809404,
      "grad_norm": 6.2319488525390625,
      "learning_rate": 2e-05,
      "loss": 1.0136,
      "step": 590
    },
    {
      "epoch": 0.1905972045743329,
      "grad_norm": 8.101423263549805,
      "learning_rate": 2e-05,
      "loss": 1.0136,
      "step": 600
    },
    {
      "epoch": 0.1937738246505718,
      "grad_norm": 2.8690130710601807,
      "learning_rate": 2e-05,
      "loss": 1.0094,
      "step": 610
    },
    {
      "epoch": 0.19695044472681067,
      "grad_norm": 6.098987579345703,
      "learning_rate": 2e-05,
      "loss": 1.0115,
      "step": 620
    },
    {
      "epoch": 0.20012706480304956,
      "grad_norm": 9.842846870422363,
      "learning_rate": 2e-05,
      "loss": 1.0296,
      "step": 630
    },
    {
      "epoch": 0.20012706480304956,
      "eval_loss": 1.6036779880523682,
      "eval_mse": 1.6022396188277703,
      "eval_pearson": 0.4210194130679172,
      "eval_runtime": 7.5021,
      "eval_samples_per_second": 2873.854,
      "eval_spearmanr": 0.39835424065743985,
      "eval_steps_per_second": 11.33,
      "step": 630
    },
    {
      "epoch": 0.20330368487928843,
      "grad_norm": 10.922093391418457,
      "learning_rate": 2e-05,
      "loss": 1.0521,
      "step": 640
    },
    {
      "epoch": 0.20648030495552733,
      "grad_norm": 5.586153507232666,
      "learning_rate": 2e-05,
      "loss": 0.987,
      "step": 650
    },
    {
      "epoch": 0.2096569250317662,
      "grad_norm": 3.2207322120666504,
      "learning_rate": 2e-05,
      "loss": 1.0051,
      "step": 660
    },
    {
      "epoch": 0.2128335451080051,
      "grad_norm": 5.473106384277344,
      "learning_rate": 2e-05,
      "loss": 0.9771,
      "step": 670
    },
    {
      "epoch": 0.21601016518424396,
      "grad_norm": 6.441494464874268,
      "learning_rate": 2e-05,
      "loss": 0.9812,
      "step": 680
    },
    {
      "epoch": 0.21918678526048285,
      "grad_norm": 11.374065399169922,
      "learning_rate": 2e-05,
      "loss": 1.0377,
      "step": 690
    },
    {
      "epoch": 0.22236340533672172,
      "grad_norm": 7.578051567077637,
      "learning_rate": 2e-05,
      "loss": 0.9746,
      "step": 700
    },
    {
      "epoch": 0.22554002541296062,
      "grad_norm": 5.4013848304748535,
      "learning_rate": 2e-05,
      "loss": 1.0277,
      "step": 710
    },
    {
      "epoch": 0.22871664548919948,
      "grad_norm": 8.262565612792969,
      "learning_rate": 2e-05,
      "loss": 0.9938,
      "step": 720
    },
    {
      "epoch": 0.23189326556543838,
      "grad_norm": 8.2123441696167,
      "learning_rate": 2e-05,
      "loss": 0.9973,
      "step": 730
    },
    {
      "epoch": 0.23506988564167725,
      "grad_norm": 4.631077766418457,
      "learning_rate": 2e-05,
      "loss": 0.9617,
      "step": 740
    },
    {
      "epoch": 0.23824650571791614,
      "grad_norm": 3.578380823135376,
      "learning_rate": 2e-05,
      "loss": 0.9734,
      "step": 750
    },
    {
      "epoch": 0.241423125794155,
      "grad_norm": 4.767801761627197,
      "learning_rate": 2e-05,
      "loss": 0.9891,
      "step": 760
    },
    {
      "epoch": 0.2445997458703939,
      "grad_norm": 8.783123016357422,
      "learning_rate": 2e-05,
      "loss": 0.9696,
      "step": 770
    },
    {
      "epoch": 0.24777636594663277,
      "grad_norm": 3.8611748218536377,
      "learning_rate": 2e-05,
      "loss": 0.9426,
      "step": 780
    },
    {
      "epoch": 0.25095298602287164,
      "grad_norm": 3.9327878952026367,
      "learning_rate": 2e-05,
      "loss": 0.9245,
      "step": 790
    },
    {
      "epoch": 0.25412960609911056,
      "grad_norm": 10.22214412689209,
      "learning_rate": 2e-05,
      "loss": 1.0578,
      "step": 800
    },
    {
      "epoch": 0.25730622617534943,
      "grad_norm": 10.100834846496582,
      "learning_rate": 2e-05,
      "loss": 0.9731,
      "step": 810
    },
    {
      "epoch": 0.2604828462515883,
      "grad_norm": 16.657670974731445,
      "learning_rate": 2e-05,
      "loss": 0.9658,
      "step": 820
    },
    {
      "epoch": 0.2636594663278272,
      "grad_norm": 10.112929344177246,
      "learning_rate": 2e-05,
      "loss": 0.9898,
      "step": 830
    },
    {
      "epoch": 0.2668360864040661,
      "grad_norm": 4.280663967132568,
      "learning_rate": 2e-05,
      "loss": 0.9893,
      "step": 840
    },
    {
      "epoch": 0.27001270648030495,
      "grad_norm": 5.326986312866211,
      "learning_rate": 2e-05,
      "loss": 0.9705,
      "step": 850
    },
    {
      "epoch": 0.2731893265565438,
      "grad_norm": 3.3062093257904053,
      "learning_rate": 2e-05,
      "loss": 0.9612,
      "step": 860
    },
    {
      "epoch": 0.27636594663278274,
      "grad_norm": 6.447878360748291,
      "learning_rate": 2e-05,
      "loss": 1.0147,
      "step": 870
    },
    {
      "epoch": 0.2795425667090216,
      "grad_norm": 7.457481861114502,
      "learning_rate": 2e-05,
      "loss": 0.9588,
      "step": 880
    },
    {
      "epoch": 0.2827191867852605,
      "grad_norm": 4.0466389656066895,
      "learning_rate": 2e-05,
      "loss": 0.9229,
      "step": 890
    },
    {
      "epoch": 0.28589580686149935,
      "grad_norm": 7.740208625793457,
      "learning_rate": 2e-05,
      "loss": 0.9658,
      "step": 900
    },
    {
      "epoch": 0.28907242693773827,
      "grad_norm": 10.817301750183105,
      "learning_rate": 2e-05,
      "loss": 0.9701,
      "step": 910
    },
    {
      "epoch": 0.29224904701397714,
      "grad_norm": 5.236296653747559,
      "learning_rate": 2e-05,
      "loss": 0.9556,
      "step": 920
    },
    {
      "epoch": 0.295425667090216,
      "grad_norm": 5.81690788269043,
      "learning_rate": 2e-05,
      "loss": 0.9491,
      "step": 930
    },
    {
      "epoch": 0.29860228716645487,
      "grad_norm": 16.156831741333008,
      "learning_rate": 2e-05,
      "loss": 0.9682,
      "step": 940
    },
    {
      "epoch": 0.3001905972045743,
      "eval_loss": 1.9620078802108765,
      "eval_mse": 1.9614732971217947,
      "eval_pearson": 0.43791838710891456,
      "eval_runtime": 7.6055,
      "eval_samples_per_second": 2834.8,
      "eval_spearmanr": 0.4176236421587771,
      "eval_steps_per_second": 11.176,
      "step": 945
    },
    {
      "epoch": 0.3017789072426938,
      "grad_norm": 9.39229965209961,
      "learning_rate": 2e-05,
      "loss": 0.9458,
      "step": 950
    },
    {
      "epoch": 0.30495552731893266,
      "grad_norm": 3.3538949489593506,
      "learning_rate": 2e-05,
      "loss": 0.9869,
      "step": 960
    },
    {
      "epoch": 0.30813214739517153,
      "grad_norm": 4.66791296005249,
      "learning_rate": 2e-05,
      "loss": 0.9864,
      "step": 970
    },
    {
      "epoch": 0.3113087674714104,
      "grad_norm": 4.4199700355529785,
      "learning_rate": 2e-05,
      "loss": 0.9415,
      "step": 980
    },
    {
      "epoch": 0.3144853875476493,
      "grad_norm": 11.043015480041504,
      "learning_rate": 2e-05,
      "loss": 0.9685,
      "step": 990
    },
    {
      "epoch": 0.3176620076238882,
      "grad_norm": 6.493607044219971,
      "learning_rate": 2e-05,
      "loss": 0.957,
      "step": 1000
    },
    {
      "epoch": 0.32083862770012705,
      "grad_norm": 6.981820106506348,
      "learning_rate": 2e-05,
      "loss": 0.9247,
      "step": 1010
    },
    {
      "epoch": 0.3240152477763659,
      "grad_norm": 7.373409748077393,
      "learning_rate": 2e-05,
      "loss": 0.9657,
      "step": 1020
    },
    {
      "epoch": 0.32719186785260485,
      "grad_norm": 7.605625152587891,
      "learning_rate": 2e-05,
      "loss": 0.9545,
      "step": 1030
    },
    {
      "epoch": 0.3303684879288437,
      "grad_norm": 4.865631580352783,
      "learning_rate": 2e-05,
      "loss": 0.9515,
      "step": 1040
    },
    {
      "epoch": 0.3335451080050826,
      "grad_norm": 4.3182220458984375,
      "learning_rate": 2e-05,
      "loss": 0.9214,
      "step": 1050
    },
    {
      "epoch": 0.33672172808132145,
      "grad_norm": 6.588730335235596,
      "learning_rate": 2e-05,
      "loss": 0.9506,
      "step": 1060
    },
    {
      "epoch": 0.33989834815756037,
      "grad_norm": 4.558032989501953,
      "learning_rate": 2e-05,
      "loss": 0.931,
      "step": 1070
    },
    {
      "epoch": 0.34307496823379924,
      "grad_norm": 4.704413890838623,
      "learning_rate": 2e-05,
      "loss": 0.9241,
      "step": 1080
    },
    {
      "epoch": 0.3462515883100381,
      "grad_norm": 5.300989151000977,
      "learning_rate": 2e-05,
      "loss": 0.9197,
      "step": 1090
    },
    {
      "epoch": 0.34942820838627703,
      "grad_norm": 3.9676918983459473,
      "learning_rate": 2e-05,
      "loss": 0.9532,
      "step": 1100
    },
    {
      "epoch": 0.3526048284625159,
      "grad_norm": 5.566225528717041,
      "learning_rate": 2e-05,
      "loss": 0.9197,
      "step": 1110
    },
    {
      "epoch": 0.35578144853875476,
      "grad_norm": 10.327537536621094,
      "learning_rate": 2e-05,
      "loss": 0.9657,
      "step": 1120
    },
    {
      "epoch": 0.35895806861499363,
      "grad_norm": 9.862358093261719,
      "learning_rate": 2e-05,
      "loss": 0.9578,
      "step": 1130
    },
    {
      "epoch": 0.36213468869123255,
      "grad_norm": 4.764556884765625,
      "learning_rate": 2e-05,
      "loss": 0.955,
      "step": 1140
    },
    {
      "epoch": 0.3653113087674714,
      "grad_norm": 6.2169976234436035,
      "learning_rate": 2e-05,
      "loss": 0.9078,
      "step": 1150
    },
    {
      "epoch": 0.3684879288437103,
      "grad_norm": 7.430384159088135,
      "learning_rate": 2e-05,
      "loss": 0.9128,
      "step": 1160
    },
    {
      "epoch": 0.37166454891994916,
      "grad_norm": 3.825080633163452,
      "learning_rate": 2e-05,
      "loss": 0.9396,
      "step": 1170
    },
    {
      "epoch": 0.3748411689961881,
      "grad_norm": 3.867755174636841,
      "learning_rate": 2e-05,
      "loss": 0.9201,
      "step": 1180
    },
    {
      "epoch": 0.37801778907242695,
      "grad_norm": 8.5967435836792,
      "learning_rate": 2e-05,
      "loss": 0.9232,
      "step": 1190
    },
    {
      "epoch": 0.3811944091486658,
      "grad_norm": 7.858545780181885,
      "learning_rate": 2e-05,
      "loss": 0.9158,
      "step": 1200
    },
    {
      "epoch": 0.3843710292249047,
      "grad_norm": 9.796663284301758,
      "learning_rate": 2e-05,
      "loss": 0.9134,
      "step": 1210
    },
    {
      "epoch": 0.3875476493011436,
      "grad_norm": 4.869119644165039,
      "learning_rate": 2e-05,
      "loss": 0.9694,
      "step": 1220
    },
    {
      "epoch": 0.39072426937738247,
      "grad_norm": 10.429972648620605,
      "learning_rate": 2e-05,
      "loss": 0.9521,
      "step": 1230
    },
    {
      "epoch": 0.39390088945362134,
      "grad_norm": 12.964526176452637,
      "learning_rate": 2e-05,
      "loss": 0.8723,
      "step": 1240
    },
    {
      "epoch": 0.3970775095298602,
      "grad_norm": 10.243279457092285,
      "learning_rate": 2e-05,
      "loss": 0.888,
      "step": 1250
    },
    {
      "epoch": 0.40025412960609913,
      "grad_norm": 5.782315731048584,
      "learning_rate": 2e-05,
      "loss": 0.9032,
      "step": 1260
    },
    {
      "epoch": 0.40025412960609913,
      "eval_loss": 1.7860548496246338,
      "eval_mse": 1.7855228727292927,
      "eval_pearson": 0.4054312243125583,
      "eval_runtime": 7.4793,
      "eval_samples_per_second": 2882.624,
      "eval_spearmanr": 0.3802361676520045,
      "eval_steps_per_second": 11.365,
      "step": 1260
    },
    {
      "epoch": 0.403430749682338,
      "grad_norm": 6.9834675788879395,
      "learning_rate": 2e-05,
      "loss": 0.8879,
      "step": 1270
    },
    {
      "epoch": 0.40660736975857686,
      "grad_norm": 24.677587509155273,
      "learning_rate": 2e-05,
      "loss": 0.92,
      "step": 1280
    },
    {
      "epoch": 0.40978398983481573,
      "grad_norm": 9.250638008117676,
      "learning_rate": 2e-05,
      "loss": 0.9089,
      "step": 1290
    },
    {
      "epoch": 0.41296060991105465,
      "grad_norm": 16.08355712890625,
      "learning_rate": 2e-05,
      "loss": 0.8933,
      "step": 1300
    },
    {
      "epoch": 0.4161372299872935,
      "grad_norm": 4.891936302185059,
      "learning_rate": 2e-05,
      "loss": 0.9032,
      "step": 1310
    },
    {
      "epoch": 0.4193138500635324,
      "grad_norm": 6.12583065032959,
      "learning_rate": 2e-05,
      "loss": 0.9149,
      "step": 1320
    },
    {
      "epoch": 0.42249047013977126,
      "grad_norm": 8.486957550048828,
      "learning_rate": 2e-05,
      "loss": 0.9308,
      "step": 1330
    },
    {
      "epoch": 0.4256670902160102,
      "grad_norm": 11.810437202453613,
      "learning_rate": 2e-05,
      "loss": 0.9258,
      "step": 1340
    },
    {
      "epoch": 0.42884371029224905,
      "grad_norm": 5.3630547523498535,
      "learning_rate": 2e-05,
      "loss": 0.9434,
      "step": 1350
    },
    {
      "epoch": 0.4320203303684879,
      "grad_norm": 5.544841766357422,
      "learning_rate": 2e-05,
      "loss": 0.9001,
      "step": 1360
    },
    {
      "epoch": 0.43519695044472684,
      "grad_norm": 10.718059539794922,
      "learning_rate": 2e-05,
      "loss": 0.9127,
      "step": 1370
    },
    {
      "epoch": 0.4383735705209657,
      "grad_norm": 4.513680458068848,
      "learning_rate": 2e-05,
      "loss": 0.9372,
      "step": 1380
    },
    {
      "epoch": 0.4415501905972046,
      "grad_norm": 7.925620079040527,
      "learning_rate": 2e-05,
      "loss": 0.911,
      "step": 1390
    },
    {
      "epoch": 0.44472681067344344,
      "grad_norm": 7.119612216949463,
      "learning_rate": 2e-05,
      "loss": 0.8954,
      "step": 1400
    },
    {
      "epoch": 0.44790343074968236,
      "grad_norm": 14.261873245239258,
      "learning_rate": 2e-05,
      "loss": 0.8528,
      "step": 1410
    },
    {
      "epoch": 0.45108005082592123,
      "grad_norm": 12.15605354309082,
      "learning_rate": 2e-05,
      "loss": 0.8937,
      "step": 1420
    },
    {
      "epoch": 0.4542566709021601,
      "grad_norm": 8.426251411437988,
      "learning_rate": 2e-05,
      "loss": 0.8713,
      "step": 1430
    },
    {
      "epoch": 0.45743329097839897,
      "grad_norm": 4.653706073760986,
      "learning_rate": 2e-05,
      "loss": 0.9034,
      "step": 1440
    },
    {
      "epoch": 0.4606099110546379,
      "grad_norm": 10.494522094726562,
      "learning_rate": 2e-05,
      "loss": 0.8947,
      "step": 1450
    },
    {
      "epoch": 0.46378653113087676,
      "grad_norm": 10.48471736907959,
      "learning_rate": 2e-05,
      "loss": 0.9007,
      "step": 1460
    },
    {
      "epoch": 0.4669631512071156,
      "grad_norm": 10.725944519042969,
      "learning_rate": 2e-05,
      "loss": 0.8598,
      "step": 1470
    },
    {
      "epoch": 0.4701397712833545,
      "grad_norm": 6.394395351409912,
      "learning_rate": 2e-05,
      "loss": 0.9203,
      "step": 1480
    },
    {
      "epoch": 0.4733163913595934,
      "grad_norm": 15.586531639099121,
      "learning_rate": 2e-05,
      "loss": 0.8511,
      "step": 1490
    },
    {
      "epoch": 0.4764930114358323,
      "grad_norm": 9.299652099609375,
      "learning_rate": 2e-05,
      "loss": 0.9114,
      "step": 1500
    },
    {
      "epoch": 0.47966963151207115,
      "grad_norm": 5.3885178565979,
      "learning_rate": 2e-05,
      "loss": 0.9091,
      "step": 1510
    },
    {
      "epoch": 0.48284625158831,
      "grad_norm": 15.513581275939941,
      "learning_rate": 2e-05,
      "loss": 0.8729,
      "step": 1520
    },
    {
      "epoch": 0.48602287166454894,
      "grad_norm": 8.674871444702148,
      "learning_rate": 2e-05,
      "loss": 0.8555,
      "step": 1530
    },
    {
      "epoch": 0.4891994917407878,
      "grad_norm": 9.415547370910645,
      "learning_rate": 2e-05,
      "loss": 0.8779,
      "step": 1540
    },
    {
      "epoch": 0.4923761118170267,
      "grad_norm": 5.002098560333252,
      "learning_rate": 2e-05,
      "loss": 0.8977,
      "step": 1550
    },
    {
      "epoch": 0.49555273189326554,
      "grad_norm": 7.906076908111572,
      "learning_rate": 2e-05,
      "loss": 0.8776,
      "step": 1560
    },
    {
      "epoch": 0.49872935196950446,
      "grad_norm": 12.83834171295166,
      "learning_rate": 2e-05,
      "loss": 0.8935,
      "step": 1570
    },
    {
      "epoch": 0.5003176620076238,
      "eval_loss": 1.674828290939331,
      "eval_mse": 1.6736223753617734,
      "eval_pearson": 0.433113631264073,
      "eval_runtime": 7.5753,
      "eval_samples_per_second": 2846.109,
      "eval_spearmanr": 0.4167420736908496,
      "eval_steps_per_second": 11.221,
      "step": 1575
    },
    {
      "epoch": 0.5019059720457433,
      "grad_norm": 6.77203369140625,
      "learning_rate": 2e-05,
      "loss": 0.8722,
      "step": 1580
    },
    {
      "epoch": 0.5050825921219823,
      "grad_norm": 15.53852653503418,
      "learning_rate": 2e-05,
      "loss": 0.8593,
      "step": 1590
    },
    {
      "epoch": 0.5082592121982211,
      "grad_norm": 5.68862771987915,
      "learning_rate": 2e-05,
      "loss": 0.8777,
      "step": 1600
    },
    {
      "epoch": 0.51143583227446,
      "grad_norm": 9.109695434570312,
      "learning_rate": 2e-05,
      "loss": 0.8527,
      "step": 1610
    },
    {
      "epoch": 0.5146124523506989,
      "grad_norm": 12.036908149719238,
      "learning_rate": 2e-05,
      "loss": 0.8329,
      "step": 1620
    },
    {
      "epoch": 0.5177890724269377,
      "grad_norm": 11.85720157623291,
      "learning_rate": 2e-05,
      "loss": 0.8668,
      "step": 1630
    },
    {
      "epoch": 0.5209656925031766,
      "grad_norm": 15.993840217590332,
      "learning_rate": 2e-05,
      "loss": 0.8688,
      "step": 1640
    },
    {
      "epoch": 0.5241423125794155,
      "grad_norm": 5.68026876449585,
      "learning_rate": 2e-05,
      "loss": 0.8559,
      "step": 1650
    },
    {
      "epoch": 0.5273189326556544,
      "grad_norm": 14.447815895080566,
      "learning_rate": 2e-05,
      "loss": 0.8464,
      "step": 1660
    },
    {
      "epoch": 0.5304955527318933,
      "grad_norm": 11.273104667663574,
      "learning_rate": 2e-05,
      "loss": 0.8469,
      "step": 1670
    },
    {
      "epoch": 0.5336721728081322,
      "grad_norm": 6.707162857055664,
      "learning_rate": 2e-05,
      "loss": 0.8669,
      "step": 1680
    },
    {
      "epoch": 0.536848792884371,
      "grad_norm": 4.678793430328369,
      "learning_rate": 2e-05,
      "loss": 0.8521,
      "step": 1690
    },
    {
      "epoch": 0.5400254129606099,
      "grad_norm": 10.19075870513916,
      "learning_rate": 2e-05,
      "loss": 0.8498,
      "step": 1700
    },
    {
      "epoch": 0.5432020330368488,
      "grad_norm": 11.442326545715332,
      "learning_rate": 2e-05,
      "loss": 0.837,
      "step": 1710
    },
    {
      "epoch": 0.5463786531130876,
      "grad_norm": 11.241987228393555,
      "learning_rate": 2e-05,
      "loss": 0.8383,
      "step": 1720
    },
    {
      "epoch": 0.5495552731893265,
      "grad_norm": 5.470778942108154,
      "learning_rate": 2e-05,
      "loss": 0.8435,
      "step": 1730
    },
    {
      "epoch": 0.5527318932655655,
      "grad_norm": 10.631587982177734,
      "learning_rate": 2e-05,
      "loss": 0.8125,
      "step": 1740
    },
    {
      "epoch": 0.5559085133418044,
      "grad_norm": 21.91876220703125,
      "learning_rate": 2e-05,
      "loss": 0.839,
      "step": 1750
    },
    {
      "epoch": 0.5590851334180432,
      "grad_norm": 10.208935737609863,
      "learning_rate": 2e-05,
      "loss": 0.8775,
      "step": 1760
    },
    {
      "epoch": 0.5622617534942821,
      "grad_norm": 6.271909713745117,
      "learning_rate": 2e-05,
      "loss": 0.8457,
      "step": 1770
    },
    {
      "epoch": 0.565438373570521,
      "grad_norm": 5.791699409484863,
      "learning_rate": 2e-05,
      "loss": 0.8959,
      "step": 1780
    },
    {
      "epoch": 0.5686149936467598,
      "grad_norm": 7.91825008392334,
      "learning_rate": 2e-05,
      "loss": 0.8382,
      "step": 1790
    },
    {
      "epoch": 0.5717916137229987,
      "grad_norm": 7.127465724945068,
      "learning_rate": 2e-05,
      "loss": 0.8288,
      "step": 1800
    },
    {
      "epoch": 0.5749682337992376,
      "grad_norm": 8.4805908203125,
      "learning_rate": 2e-05,
      "loss": 0.8296,
      "step": 1810
    },
    {
      "epoch": 0.5781448538754765,
      "grad_norm": 12.221505165100098,
      "learning_rate": 2e-05,
      "loss": 0.8201,
      "step": 1820
    },
    {
      "epoch": 0.5813214739517154,
      "grad_norm": 9.737865447998047,
      "learning_rate": 2e-05,
      "loss": 0.8257,
      "step": 1830
    },
    {
      "epoch": 0.5844980940279543,
      "grad_norm": 6.068592548370361,
      "learning_rate": 2e-05,
      "loss": 0.8639,
      "step": 1840
    },
    {
      "epoch": 0.5876747141041931,
      "grad_norm": 7.744661808013916,
      "learning_rate": 2e-05,
      "loss": 0.8042,
      "step": 1850
    },
    {
      "epoch": 0.590851334180432,
      "grad_norm": 4.466408729553223,
      "learning_rate": 2e-05,
      "loss": 0.8071,
      "step": 1860
    },
    {
      "epoch": 0.5940279542566709,
      "grad_norm": 6.357308387756348,
      "learning_rate": 2e-05,
      "loss": 0.84,
      "step": 1870
    },
    {
      "epoch": 0.5972045743329097,
      "grad_norm": 13.464912414550781,
      "learning_rate": 2e-05,
      "loss": 0.8293,
      "step": 1880
    },
    {
      "epoch": 0.6003811944091486,
      "grad_norm": 9.808059692382812,
      "learning_rate": 2e-05,
      "loss": 0.8397,
      "step": 1890
    },
    {
      "epoch": 0.6003811944091486,
      "eval_loss": 1.9662243127822876,
      "eval_mse": 1.9656583454922976,
      "eval_pearson": 0.44949190345807716,
      "eval_runtime": 7.4973,
      "eval_samples_per_second": 2875.689,
      "eval_spearmanr": 0.4295801576316513,
      "eval_steps_per_second": 11.337,
      "step": 1890
    },
    {
      "epoch": 0.6035578144853876,
      "grad_norm": 7.833551406860352,
      "learning_rate": 2e-05,
      "loss": 0.8022,
      "step": 1900
    },
    {
      "epoch": 0.6067344345616265,
      "grad_norm": 13.136836051940918,
      "learning_rate": 2e-05,
      "loss": 0.8192,
      "step": 1910
    },
    {
      "epoch": 0.6099110546378653,
      "grad_norm": 5.2769341468811035,
      "learning_rate": 2e-05,
      "loss": 0.7906,
      "step": 1920
    },
    {
      "epoch": 0.6130876747141042,
      "grad_norm": 7.619232654571533,
      "learning_rate": 2e-05,
      "loss": 0.8292,
      "step": 1930
    },
    {
      "epoch": 0.6162642947903431,
      "grad_norm": 10.283294677734375,
      "learning_rate": 2e-05,
      "loss": 0.8427,
      "step": 1940
    },
    {
      "epoch": 0.6194409148665819,
      "grad_norm": 18.165285110473633,
      "learning_rate": 2e-05,
      "loss": 0.8414,
      "step": 1950
    },
    {
      "epoch": 0.6226175349428208,
      "grad_norm": 5.836966514587402,
      "learning_rate": 2e-05,
      "loss": 0.8247,
      "step": 1960
    },
    {
      "epoch": 0.6257941550190598,
      "grad_norm": 10.882250785827637,
      "learning_rate": 2e-05,
      "loss": 0.7939,
      "step": 1970
    },
    {
      "epoch": 0.6289707750952986,
      "grad_norm": 7.207993507385254,
      "learning_rate": 2e-05,
      "loss": 0.8202,
      "step": 1980
    },
    {
      "epoch": 0.6321473951715375,
      "grad_norm": 9.710821151733398,
      "learning_rate": 2e-05,
      "loss": 0.8678,
      "step": 1990
    },
    {
      "epoch": 0.6353240152477764,
      "grad_norm": 19.950918197631836,
      "learning_rate": 2e-05,
      "loss": 0.8146,
      "step": 2000
    },
    {
      "epoch": 0.6385006353240152,
      "grad_norm": 7.3346943855285645,
      "learning_rate": 2e-05,
      "loss": 0.8214,
      "step": 2010
    },
    {
      "epoch": 0.6416772554002541,
      "grad_norm": 9.924559593200684,
      "learning_rate": 2e-05,
      "loss": 0.7694,
      "step": 2020
    },
    {
      "epoch": 0.644853875476493,
      "grad_norm": 10.370646476745605,
      "learning_rate": 2e-05,
      "loss": 0.7971,
      "step": 2030
    },
    {
      "epoch": 0.6480304955527318,
      "grad_norm": 9.305110931396484,
      "learning_rate": 2e-05,
      "loss": 0.8313,
      "step": 2040
    },
    {
      "epoch": 0.6512071156289708,
      "grad_norm": 17.297208786010742,
      "learning_rate": 2e-05,
      "loss": 0.791,
      "step": 2050
    },
    {
      "epoch": 0.6543837357052097,
      "grad_norm": 13.522163391113281,
      "learning_rate": 2e-05,
      "loss": 0.785,
      "step": 2060
    },
    {
      "epoch": 0.6575603557814486,
      "grad_norm": 5.087051868438721,
      "learning_rate": 2e-05,
      "loss": 0.8051,
      "step": 2070
    },
    {
      "epoch": 0.6607369758576874,
      "grad_norm": 9.312372207641602,
      "learning_rate": 2e-05,
      "loss": 0.7534,
      "step": 2080
    },
    {
      "epoch": 0.6639135959339263,
      "grad_norm": 11.019914627075195,
      "learning_rate": 2e-05,
      "loss": 0.8022,
      "step": 2090
    },
    {
      "epoch": 0.6670902160101652,
      "grad_norm": 13.933417320251465,
      "learning_rate": 2e-05,
      "loss": 0.7981,
      "step": 2100
    },
    {
      "epoch": 0.670266836086404,
      "grad_norm": 12.143195152282715,
      "learning_rate": 2e-05,
      "loss": 0.783,
      "step": 2110
    },
    {
      "epoch": 0.6734434561626429,
      "grad_norm": 10.34657096862793,
      "learning_rate": 2e-05,
      "loss": 0.7816,
      "step": 2120
    },
    {
      "epoch": 0.6766200762388819,
      "grad_norm": 5.0284857749938965,
      "learning_rate": 2e-05,
      "loss": 0.8187,
      "step": 2130
    },
    {
      "epoch": 0.6797966963151207,
      "grad_norm": 8.207871437072754,
      "learning_rate": 2e-05,
      "loss": 0.7584,
      "step": 2140
    },
    {
      "epoch": 0.6829733163913596,
      "grad_norm": 8.130475997924805,
      "learning_rate": 2e-05,
      "loss": 0.8238,
      "step": 2150
    },
    {
      "epoch": 0.6861499364675985,
      "grad_norm": 7.461042881011963,
      "learning_rate": 2e-05,
      "loss": 0.7829,
      "step": 2160
    },
    {
      "epoch": 0.6893265565438373,
      "grad_norm": 6.6663312911987305,
      "learning_rate": 2e-05,
      "loss": 0.8034,
      "step": 2170
    },
    {
      "epoch": 0.6925031766200762,
      "grad_norm": 6.806113243103027,
      "learning_rate": 2e-05,
      "loss": 0.7726,
      "step": 2180
    },
    {
      "epoch": 0.6956797966963151,
      "grad_norm": 5.35689640045166,
      "learning_rate": 2e-05,
      "loss": 0.8073,
      "step": 2190
    },
    {
      "epoch": 0.6988564167725541,
      "grad_norm": 8.037109375,
      "learning_rate": 2e-05,
      "loss": 0.8052,
      "step": 2200
    },
    {
      "epoch": 0.7004447268106735,
      "eval_loss": 1.9820631742477417,
      "eval_mse": 1.9816287795997507,
      "eval_pearson": 0.41501492134492063,
      "eval_runtime": 7.4811,
      "eval_samples_per_second": 2881.913,
      "eval_spearmanr": 0.39322145899648875,
      "eval_steps_per_second": 11.362,
      "step": 2205
    },
    {
      "epoch": 0.7020330368487929,
      "grad_norm": 9.961763381958008,
      "learning_rate": 2e-05,
      "loss": 0.7995,
      "step": 2210
    },
    {
      "epoch": 0.7052096569250318,
      "grad_norm": 11.01928997039795,
      "learning_rate": 2e-05,
      "loss": 0.7881,
      "step": 2220
    },
    {
      "epoch": 0.7083862770012707,
      "grad_norm": 8.783029556274414,
      "learning_rate": 2e-05,
      "loss": 0.7576,
      "step": 2230
    },
    {
      "epoch": 0.7115628970775095,
      "grad_norm": 6.311171054840088,
      "learning_rate": 2e-05,
      "loss": 0.7692,
      "step": 2240
    },
    {
      "epoch": 0.7147395171537484,
      "grad_norm": 10.80319595336914,
      "learning_rate": 2e-05,
      "loss": 0.8038,
      "step": 2250
    },
    {
      "epoch": 0.7179161372299873,
      "grad_norm": 12.519754409790039,
      "learning_rate": 2e-05,
      "loss": 0.8007,
      "step": 2260
    },
    {
      "epoch": 0.7210927573062261,
      "grad_norm": 12.523812294006348,
      "learning_rate": 2e-05,
      "loss": 0.7703,
      "step": 2270
    },
    {
      "epoch": 0.7242693773824651,
      "grad_norm": 6.052608966827393,
      "learning_rate": 2e-05,
      "loss": 0.8106,
      "step": 2280
    },
    {
      "epoch": 0.727445997458704,
      "grad_norm": 15.602230072021484,
      "learning_rate": 2e-05,
      "loss": 0.7886,
      "step": 2290
    },
    {
      "epoch": 0.7306226175349428,
      "grad_norm": 7.152783393859863,
      "learning_rate": 2e-05,
      "loss": 0.8016,
      "step": 2300
    },
    {
      "epoch": 0.7337992376111817,
      "grad_norm": 5.344683647155762,
      "learning_rate": 2e-05,
      "loss": 0.8119,
      "step": 2310
    },
    {
      "epoch": 0.7369758576874206,
      "grad_norm": 8.364535331726074,
      "learning_rate": 2e-05,
      "loss": 0.8337,
      "step": 2320
    },
    {
      "epoch": 0.7401524777636594,
      "grad_norm": 9.170923233032227,
      "learning_rate": 2e-05,
      "loss": 0.7408,
      "step": 2330
    },
    {
      "epoch": 0.7433290978398983,
      "grad_norm": 7.200644016265869,
      "learning_rate": 2e-05,
      "loss": 0.7549,
      "step": 2340
    },
    {
      "epoch": 0.7465057179161372,
      "grad_norm": 12.266861915588379,
      "learning_rate": 2e-05,
      "loss": 0.7617,
      "step": 2350
    },
    {
      "epoch": 0.7496823379923762,
      "grad_norm": 13.620789527893066,
      "learning_rate": 2e-05,
      "loss": 0.7659,
      "step": 2360
    },
    {
      "epoch": 0.752858958068615,
      "grad_norm": 12.51858901977539,
      "learning_rate": 2e-05,
      "loss": 0.7897,
      "step": 2370
    },
    {
      "epoch": 0.7560355781448539,
      "grad_norm": 6.880588054656982,
      "learning_rate": 2e-05,
      "loss": 0.7872,
      "step": 2380
    },
    {
      "epoch": 0.7592121982210928,
      "grad_norm": 6.314327239990234,
      "learning_rate": 2e-05,
      "loss": 0.7322,
      "step": 2390
    },
    {
      "epoch": 0.7623888182973316,
      "grad_norm": 8.566459655761719,
      "learning_rate": 2e-05,
      "loss": 0.7679,
      "step": 2400
    },
    {
      "epoch": 0.7655654383735705,
      "grad_norm": 7.3217902183532715,
      "learning_rate": 2e-05,
      "loss": 0.8135,
      "step": 2410
    },
    {
      "epoch": 0.7687420584498094,
      "grad_norm": 7.560632705688477,
      "learning_rate": 2e-05,
      "loss": 0.7742,
      "step": 2420
    },
    {
      "epoch": 0.7719186785260482,
      "grad_norm": 12.873980522155762,
      "learning_rate": 2e-05,
      "loss": 0.8033,
      "step": 2430
    },
    {
      "epoch": 0.7750952986022872,
      "grad_norm": 13.883133888244629,
      "learning_rate": 2e-05,
      "loss": 0.8224,
      "step": 2440
    },
    {
      "epoch": 0.7782719186785261,
      "grad_norm": 7.048093318939209,
      "learning_rate": 2e-05,
      "loss": 0.7641,
      "step": 2450
    },
    {
      "epoch": 0.7814485387547649,
      "grad_norm": 6.12138032913208,
      "learning_rate": 2e-05,
      "loss": 0.7347,
      "step": 2460
    },
    {
      "epoch": 0.7846251588310038,
      "grad_norm": 6.0334014892578125,
      "learning_rate": 2e-05,
      "loss": 0.7455,
      "step": 2470
    },
    {
      "epoch": 0.7878017789072427,
      "grad_norm": 9.876029014587402,
      "learning_rate": 2e-05,
      "loss": 0.7271,
      "step": 2480
    },
    {
      "epoch": 0.7909783989834815,
      "grad_norm": 12.678186416625977,
      "learning_rate": 2e-05,
      "loss": 0.769,
      "step": 2490
    },
    {
      "epoch": 0.7941550190597204,
      "grad_norm": 8.23136043548584,
      "learning_rate": 2e-05,
      "loss": 0.7533,
      "step": 2500
    },
    {
      "epoch": 0.7973316391359594,
      "grad_norm": 4.966917514801025,
      "learning_rate": 2e-05,
      "loss": 0.7241,
      "step": 2510
    },
    {
      "epoch": 0.8005082592121983,
      "grad_norm": 7.798238277435303,
      "learning_rate": 2e-05,
      "loss": 0.773,
      "step": 2520
    },
    {
      "epoch": 0.8005082592121983,
      "eval_loss": 1.815932273864746,
      "eval_mse": 1.814583565905718,
      "eval_pearson": 0.43622118947325095,
      "eval_runtime": 7.4773,
      "eval_samples_per_second": 2883.405,
      "eval_spearmanr": 0.41597852305839494,
      "eval_steps_per_second": 11.368,
      "step": 2520
    },
    {
      "epoch": 0.8036848792884371,
      "grad_norm": 12.609728813171387,
      "learning_rate": 2e-05,
      "loss": 0.7444,
      "step": 2530
    },
    {
      "epoch": 0.806861499364676,
      "grad_norm": 7.591594696044922,
      "learning_rate": 2e-05,
      "loss": 0.7892,
      "step": 2540
    },
    {
      "epoch": 0.8100381194409149,
      "grad_norm": 7.855697154998779,
      "learning_rate": 2e-05,
      "loss": 0.7753,
      "step": 2550
    },
    {
      "epoch": 0.8132147395171537,
      "grad_norm": 5.798956394195557,
      "learning_rate": 2e-05,
      "loss": 0.7687,
      "step": 2560
    },
    {
      "epoch": 0.8163913595933926,
      "grad_norm": 15.347102165222168,
      "learning_rate": 2e-05,
      "loss": 0.7939,
      "step": 2570
    },
    {
      "epoch": 0.8195679796696315,
      "grad_norm": 10.471433639526367,
      "learning_rate": 2e-05,
      "loss": 0.7137,
      "step": 2580
    },
    {
      "epoch": 0.8227445997458704,
      "grad_norm": 9.867934226989746,
      "learning_rate": 2e-05,
      "loss": 0.7726,
      "step": 2590
    },
    {
      "epoch": 0.8259212198221093,
      "grad_norm": 8.051739692687988,
      "learning_rate": 2e-05,
      "loss": 0.7766,
      "step": 2600
    },
    {
      "epoch": 0.8290978398983482,
      "grad_norm": 5.908209323883057,
      "learning_rate": 2e-05,
      "loss": 0.7623,
      "step": 2610
    },
    {
      "epoch": 0.832274459974587,
      "grad_norm": 19.71361541748047,
      "learning_rate": 2e-05,
      "loss": 0.7472,
      "step": 2620
    },
    {
      "epoch": 0.8354510800508259,
      "grad_norm": 5.572208404541016,
      "learning_rate": 2e-05,
      "loss": 0.7529,
      "step": 2630
    },
    {
      "epoch": 0.8386277001270648,
      "grad_norm": 12.715657234191895,
      "learning_rate": 2e-05,
      "loss": 0.7374,
      "step": 2640
    },
    {
      "epoch": 0.8418043202033036,
      "grad_norm": 7.5858049392700195,
      "learning_rate": 2e-05,
      "loss": 0.6928,
      "step": 2650
    },
    {
      "epoch": 0.8449809402795425,
      "grad_norm": 10.786730766296387,
      "learning_rate": 2e-05,
      "loss": 0.7533,
      "step": 2660
    },
    {
      "epoch": 0.8481575603557815,
      "grad_norm": 6.632851600646973,
      "learning_rate": 2e-05,
      "loss": 0.7329,
      "step": 2670
    },
    {
      "epoch": 0.8513341804320204,
      "grad_norm": 8.508359909057617,
      "learning_rate": 2e-05,
      "loss": 0.7486,
      "step": 2680
    },
    {
      "epoch": 0.8545108005082592,
      "grad_norm": 5.731115341186523,
      "learning_rate": 2e-05,
      "loss": 0.7449,
      "step": 2690
    },
    {
      "epoch": 0.8576874205844981,
      "grad_norm": 12.145011901855469,
      "learning_rate": 2e-05,
      "loss": 0.7371,
      "step": 2700
    },
    {
      "epoch": 0.860864040660737,
      "grad_norm": 5.63694429397583,
      "learning_rate": 2e-05,
      "loss": 0.7248,
      "step": 2710
    },
    {
      "epoch": 0.8640406607369758,
      "grad_norm": 6.555166244506836,
      "learning_rate": 2e-05,
      "loss": 0.7426,
      "step": 2720
    },
    {
      "epoch": 0.8672172808132147,
      "grad_norm": 7.171817779541016,
      "learning_rate": 2e-05,
      "loss": 0.7224,
      "step": 2730
    },
    {
      "epoch": 0.8703939008894537,
      "grad_norm": 5.950831890106201,
      "learning_rate": 2e-05,
      "loss": 0.7723,
      "step": 2740
    },
    {
      "epoch": 0.8735705209656925,
      "grad_norm": 6.710800647735596,
      "learning_rate": 2e-05,
      "loss": 0.736,
      "step": 2750
    },
    {
      "epoch": 0.8767471410419314,
      "grad_norm": 5.931962013244629,
      "learning_rate": 2e-05,
      "loss": 0.6985,
      "step": 2760
    },
    {
      "epoch": 0.8799237611181703,
      "grad_norm": 8.00861644744873,
      "learning_rate": 2e-05,
      "loss": 0.7237,
      "step": 2770
    },
    {
      "epoch": 0.8831003811944091,
      "grad_norm": 8.723923683166504,
      "learning_rate": 2e-05,
      "loss": 0.7518,
      "step": 2780
    },
    {
      "epoch": 0.886277001270648,
      "grad_norm": 14.277718544006348,
      "learning_rate": 2e-05,
      "loss": 0.7659,
      "step": 2790
    },
    {
      "epoch": 0.8894536213468869,
      "grad_norm": 15.963705062866211,
      "learning_rate": 2e-05,
      "loss": 0.7748,
      "step": 2800
    },
    {
      "epoch": 0.8926302414231257,
      "grad_norm": 14.361404418945312,
      "learning_rate": 2e-05,
      "loss": 0.7268,
      "step": 2810
    },
    {
      "epoch": 0.8958068614993647,
      "grad_norm": 6.884066104888916,
      "learning_rate": 2e-05,
      "loss": 0.7178,
      "step": 2820
    },
    {
      "epoch": 0.8989834815756036,
      "grad_norm": 7.143640995025635,
      "learning_rate": 2e-05,
      "loss": 0.7596,
      "step": 2830
    },
    {
      "epoch": 0.900571791613723,
      "eval_loss": 1.6441702842712402,
      "eval_mse": 1.6433927742802368,
      "eval_pearson": 0.4242477915849317,
      "eval_runtime": 7.5785,
      "eval_samples_per_second": 2844.872,
      "eval_spearmanr": 0.4087077748378668,
      "eval_steps_per_second": 11.216,
      "step": 2835
    },
    {
      "epoch": 0.9021601016518425,
      "grad_norm": 9.03129768371582,
      "learning_rate": 2e-05,
      "loss": 0.7372,
      "step": 2840
    },
    {
      "epoch": 0.9053367217280813,
      "grad_norm": 6.726619243621826,
      "learning_rate": 2e-05,
      "loss": 0.6896,
      "step": 2850
    },
    {
      "epoch": 0.9085133418043202,
      "grad_norm": 8.530685424804688,
      "learning_rate": 2e-05,
      "loss": 0.7346,
      "step": 2860
    },
    {
      "epoch": 0.9116899618805591,
      "grad_norm": 12.840006828308105,
      "learning_rate": 2e-05,
      "loss": 0.7073,
      "step": 2870
    },
    {
      "epoch": 0.9148665819567979,
      "grad_norm": 6.234256267547607,
      "learning_rate": 2e-05,
      "loss": 0.7429,
      "step": 2880
    },
    {
      "epoch": 0.9180432020330368,
      "grad_norm": 12.289382934570312,
      "learning_rate": 2e-05,
      "loss": 0.7164,
      "step": 2890
    },
    {
      "epoch": 0.9212198221092758,
      "grad_norm": 5.548684597015381,
      "learning_rate": 2e-05,
      "loss": 0.7012,
      "step": 2900
    },
    {
      "epoch": 0.9243964421855146,
      "grad_norm": 8.37503719329834,
      "learning_rate": 2e-05,
      "loss": 0.693,
      "step": 2910
    },
    {
      "epoch": 0.9275730622617535,
      "grad_norm": 8.326530456542969,
      "learning_rate": 2e-05,
      "loss": 0.759,
      "step": 2920
    },
    {
      "epoch": 0.9307496823379924,
      "grad_norm": 7.895495414733887,
      "learning_rate": 2e-05,
      "loss": 0.7405,
      "step": 2930
    },
    {
      "epoch": 0.9339263024142312,
      "grad_norm": 6.084003925323486,
      "learning_rate": 2e-05,
      "loss": 0.7173,
      "step": 2940
    },
    {
      "epoch": 0.9371029224904701,
      "grad_norm": 7.6589155197143555,
      "learning_rate": 2e-05,
      "loss": 0.7276,
      "step": 2950
    },
    {
      "epoch": 0.940279542566709,
      "grad_norm": 8.496661186218262,
      "learning_rate": 2e-05,
      "loss": 0.7359,
      "step": 2960
    },
    {
      "epoch": 0.9434561626429478,
      "grad_norm": 5.712784290313721,
      "learning_rate": 2e-05,
      "loss": 0.7351,
      "step": 2970
    },
    {
      "epoch": 0.9466327827191868,
      "grad_norm": 5.774264335632324,
      "learning_rate": 2e-05,
      "loss": 0.7242,
      "step": 2980
    },
    {
      "epoch": 0.9498094027954257,
      "grad_norm": 14.616183280944824,
      "learning_rate": 2e-05,
      "loss": 0.7113,
      "step": 2990
    },
    {
      "epoch": 0.9529860228716646,
      "grad_norm": 6.369576930999756,
      "learning_rate": 2e-05,
      "loss": 0.7287,
      "step": 3000
    },
    {
      "epoch": 0.9561626429479034,
      "grad_norm": 10.553384780883789,
      "learning_rate": 2e-05,
      "loss": 0.7075,
      "step": 3010
    },
    {
      "epoch": 0.9593392630241423,
      "grad_norm": 9.443268775939941,
      "learning_rate": 2e-05,
      "loss": 0.7246,
      "step": 3020
    },
    {
      "epoch": 0.9625158831003812,
      "grad_norm": 9.474709510803223,
      "learning_rate": 2e-05,
      "loss": 0.7376,
      "step": 3030
    },
    {
      "epoch": 0.96569250317662,
      "grad_norm": 8.056540489196777,
      "learning_rate": 2e-05,
      "loss": 0.7232,
      "step": 3040
    },
    {
      "epoch": 0.968869123252859,
      "grad_norm": 9.638696670532227,
      "learning_rate": 2e-05,
      "loss": 0.7309,
      "step": 3050
    },
    {
      "epoch": 0.9720457433290979,
      "grad_norm": 6.712467670440674,
      "learning_rate": 2e-05,
      "loss": 0.7288,
      "step": 3060
    },
    {
      "epoch": 0.9752223634053367,
      "grad_norm": 12.276859283447266,
      "learning_rate": 2e-05,
      "loss": 0.7346,
      "step": 3070
    },
    {
      "epoch": 0.9783989834815756,
      "grad_norm": 7.312282562255859,
      "learning_rate": 2e-05,
      "loss": 0.7129,
      "step": 3080
    },
    {
      "epoch": 0.9815756035578145,
      "grad_norm": 10.140604972839355,
      "learning_rate": 2e-05,
      "loss": 0.7193,
      "step": 3090
    },
    {
      "epoch": 0.9847522236340533,
      "grad_norm": 9.194476127624512,
      "learning_rate": 2e-05,
      "loss": 0.705,
      "step": 3100
    },
    {
      "epoch": 0.9879288437102922,
      "grad_norm": 6.194860458374023,
      "learning_rate": 2e-05,
      "loss": 0.7013,
      "step": 3110
    },
    {
      "epoch": 0.9911054637865311,
      "grad_norm": 8.98914909362793,
      "learning_rate": 2e-05,
      "loss": 0.7055,
      "step": 3120
    },
    {
      "epoch": 0.9942820838627701,
      "grad_norm": 15.71261215209961,
      "learning_rate": 2e-05,
      "loss": 0.7582,
      "step": 3130
    },
    {
      "epoch": 0.9974587039390089,
      "grad_norm": 7.067221164703369,
      "learning_rate": 2e-05,
      "loss": 0.7034,
      "step": 3140
    },
    {
      "epoch": 1.0006353240152477,
      "grad_norm": 10.297992706298828,
      "learning_rate": 2e-05,
      "loss": 0.6883,
      "step": 3150
    },
    {
      "epoch": 1.0006353240152477,
      "eval_loss": 1.8543388843536377,
      "eval_mse": 1.8539340770532116,
      "eval_pearson": 0.4238405224170214,
      "eval_runtime": 7.496,
      "eval_samples_per_second": 2876.191,
      "eval_spearmanr": 0.40785304151024754,
      "eval_steps_per_second": 11.339,
      "step": 3150
    },
    {
      "epoch": 1.0038119440914866,
      "grad_norm": 9.431381225585938,
      "learning_rate": 2e-05,
      "loss": 0.6597,
      "step": 3160
    },
    {
      "epoch": 1.0069885641677256,
      "grad_norm": 5.599091529846191,
      "learning_rate": 2e-05,
      "loss": 0.6936,
      "step": 3170
    },
    {
      "epoch": 1.0101651842439645,
      "grad_norm": 6.62102746963501,
      "learning_rate": 2e-05,
      "loss": 0.6569,
      "step": 3180
    },
    {
      "epoch": 1.0133418043202034,
      "grad_norm": 8.239564895629883,
      "learning_rate": 2e-05,
      "loss": 0.6352,
      "step": 3190
    },
    {
      "epoch": 1.0165184243964422,
      "grad_norm": 8.957038879394531,
      "learning_rate": 2e-05,
      "loss": 0.6254,
      "step": 3200
    },
    {
      "epoch": 1.0196950444726811,
      "grad_norm": 7.84431266784668,
      "learning_rate": 2e-05,
      "loss": 0.6597,
      "step": 3210
    },
    {
      "epoch": 1.02287166454892,
      "grad_norm": 8.871665000915527,
      "learning_rate": 2e-05,
      "loss": 0.6814,
      "step": 3220
    },
    {
      "epoch": 1.0260482846251588,
      "grad_norm": 8.722651481628418,
      "learning_rate": 2e-05,
      "loss": 0.6533,
      "step": 3230
    },
    {
      "epoch": 1.0292249047013977,
      "grad_norm": 7.386929035186768,
      "learning_rate": 2e-05,
      "loss": 0.6447,
      "step": 3240
    },
    {
      "epoch": 1.0324015247776366,
      "grad_norm": 6.673479080200195,
      "learning_rate": 2e-05,
      "loss": 0.6825,
      "step": 3250
    },
    {
      "epoch": 1.0355781448538754,
      "grad_norm": 10.395864486694336,
      "learning_rate": 2e-05,
      "loss": 0.6566,
      "step": 3260
    },
    {
      "epoch": 1.0387547649301143,
      "grad_norm": 16.15125274658203,
      "learning_rate": 2e-05,
      "loss": 0.6789,
      "step": 3270
    },
    {
      "epoch": 1.0419313850063532,
      "grad_norm": 5.809149742126465,
      "learning_rate": 2e-05,
      "loss": 0.668,
      "step": 3280
    },
    {
      "epoch": 1.045108005082592,
      "grad_norm": 8.58527660369873,
      "learning_rate": 2e-05,
      "loss": 0.6746,
      "step": 3290
    },
    {
      "epoch": 1.048284625158831,
      "grad_norm": 7.055189609527588,
      "learning_rate": 2e-05,
      "loss": 0.6642,
      "step": 3300
    },
    {
      "epoch": 1.0514612452350698,
      "grad_norm": 11.494665145874023,
      "learning_rate": 2e-05,
      "loss": 0.6749,
      "step": 3310
    },
    {
      "epoch": 1.0546378653113089,
      "grad_norm": 12.30816650390625,
      "learning_rate": 2e-05,
      "loss": 0.6742,
      "step": 3320
    },
    {
      "epoch": 1.0578144853875477,
      "grad_norm": 9.951556205749512,
      "learning_rate": 2e-05,
      "loss": 0.6566,
      "step": 3330
    },
    {
      "epoch": 1.0609911054637866,
      "grad_norm": 11.076092720031738,
      "learning_rate": 2e-05,
      "loss": 0.6157,
      "step": 3340
    },
    {
      "epoch": 1.0641677255400255,
      "grad_norm": 6.517472267150879,
      "learning_rate": 2e-05,
      "loss": 0.6634,
      "step": 3350
    },
    {
      "epoch": 1.0673443456162643,
      "grad_norm": 9.997252464294434,
      "learning_rate": 2e-05,
      "loss": 0.6674,
      "step": 3360
    },
    {
      "epoch": 1.0705209656925032,
      "grad_norm": 10.652308464050293,
      "learning_rate": 2e-05,
      "loss": 0.6453,
      "step": 3370
    },
    {
      "epoch": 1.073697585768742,
      "grad_norm": 6.51257848739624,
      "learning_rate": 2e-05,
      "loss": 0.6173,
      "step": 3380
    },
    {
      "epoch": 1.076874205844981,
      "grad_norm": 17.60382652282715,
      "learning_rate": 2e-05,
      "loss": 0.626,
      "step": 3390
    },
    {
      "epoch": 1.0800508259212198,
      "grad_norm": 32.8763313293457,
      "learning_rate": 2e-05,
      "loss": 0.7036,
      "step": 3400
    },
    {
      "epoch": 1.0832274459974587,
      "grad_norm": 10.371403694152832,
      "learning_rate": 2e-05,
      "loss": 0.6971,
      "step": 3410
    },
    {
      "epoch": 1.0864040660736975,
      "grad_norm": 13.068199157714844,
      "learning_rate": 2e-05,
      "loss": 0.6523,
      "step": 3420
    },
    {
      "epoch": 1.0895806861499364,
      "grad_norm": 12.506413459777832,
      "learning_rate": 2e-05,
      "loss": 0.661,
      "step": 3430
    },
    {
      "epoch": 1.0927573062261753,
      "grad_norm": 6.494304180145264,
      "learning_rate": 2e-05,
      "loss": 0.6315,
      "step": 3440
    },
    {
      "epoch": 1.0959339263024142,
      "grad_norm": 8.685287475585938,
      "learning_rate": 2e-05,
      "loss": 0.6348,
      "step": 3450
    },
    {
      "epoch": 1.099110546378653,
      "grad_norm": 11.531715393066406,
      "learning_rate": 2e-05,
      "loss": 0.6597,
      "step": 3460
    },
    {
      "epoch": 1.1006988564167726,
      "eval_loss": 1.8364999294281006,
      "eval_mse": 1.8359364656658916,
      "eval_pearson": 0.4131925152894725,
      "eval_runtime": 7.4827,
      "eval_samples_per_second": 2881.303,
      "eval_spearmanr": 0.3936584088203092,
      "eval_steps_per_second": 11.359,
      "step": 3465
    },
    {
      "epoch": 1.102287166454892,
      "grad_norm": 11.768857955932617,
      "learning_rate": 2e-05,
      "loss": 0.6401,
      "step": 3470
    },
    {
      "epoch": 1.105463786531131,
      "grad_norm": 8.663150787353516,
      "learning_rate": 2e-05,
      "loss": 0.6475,
      "step": 3480
    },
    {
      "epoch": 1.1086404066073698,
      "grad_norm": 8.028526306152344,
      "learning_rate": 2e-05,
      "loss": 0.6273,
      "step": 3490
    },
    {
      "epoch": 1.1118170266836087,
      "grad_norm": 13.689892768859863,
      "learning_rate": 2e-05,
      "loss": 0.64,
      "step": 3500
    },
    {
      "epoch": 1.1149936467598476,
      "grad_norm": 9.335601806640625,
      "learning_rate": 2e-05,
      "loss": 0.6463,
      "step": 3510
    },
    {
      "epoch": 1.1181702668360864,
      "grad_norm": 6.199362277984619,
      "learning_rate": 2e-05,
      "loss": 0.6093,
      "step": 3520
    },
    {
      "epoch": 1.1213468869123253,
      "grad_norm": 7.634125709533691,
      "learning_rate": 2e-05,
      "loss": 0.6647,
      "step": 3530
    },
    {
      "epoch": 1.1245235069885642,
      "grad_norm": 9.501873016357422,
      "learning_rate": 2e-05,
      "loss": 0.655,
      "step": 3540
    },
    {
      "epoch": 1.127700127064803,
      "grad_norm": 11.171480178833008,
      "learning_rate": 2e-05,
      "loss": 0.5891,
      "step": 3550
    },
    {
      "epoch": 1.130876747141042,
      "grad_norm": 8.141892433166504,
      "learning_rate": 2e-05,
      "loss": 0.6909,
      "step": 3560
    },
    {
      "epoch": 1.1340533672172808,
      "grad_norm": 9.325850486755371,
      "learning_rate": 2e-05,
      "loss": 0.6225,
      "step": 3570
    },
    {
      "epoch": 1.1372299872935197,
      "grad_norm": 11.111506462097168,
      "learning_rate": 2e-05,
      "loss": 0.6837,
      "step": 3580
    },
    {
      "epoch": 1.1404066073697585,
      "grad_norm": 10.50063419342041,
      "learning_rate": 2e-05,
      "loss": 0.6407,
      "step": 3590
    },
    {
      "epoch": 1.1435832274459974,
      "grad_norm": 6.605621814727783,
      "learning_rate": 2e-05,
      "loss": 0.6567,
      "step": 3600
    },
    {
      "epoch": 1.1467598475222363,
      "grad_norm": 8.577044486999512,
      "learning_rate": 2e-05,
      "loss": 0.6312,
      "step": 3610
    },
    {
      "epoch": 1.1499364675984753,
      "grad_norm": 7.271310806274414,
      "learning_rate": 2e-05,
      "loss": 0.6517,
      "step": 3620
    },
    {
      "epoch": 1.153113087674714,
      "grad_norm": 12.496528625488281,
      "learning_rate": 2e-05,
      "loss": 0.6197,
      "step": 3630
    },
    {
      "epoch": 1.156289707750953,
      "grad_norm": 6.779688358306885,
      "learning_rate": 2e-05,
      "loss": 0.631,
      "step": 3640
    },
    {
      "epoch": 1.159466327827192,
      "grad_norm": 6.68905782699585,
      "learning_rate": 2e-05,
      "loss": 0.6331,
      "step": 3650
    },
    {
      "epoch": 1.1626429479034308,
      "grad_norm": 23.91899871826172,
      "learning_rate": 2e-05,
      "loss": 0.6738,
      "step": 3660
    },
    {
      "epoch": 1.1658195679796697,
      "grad_norm": 7.56466817855835,
      "learning_rate": 2e-05,
      "loss": 0.6735,
      "step": 3670
    },
    {
      "epoch": 1.1689961880559085,
      "grad_norm": 14.746411323547363,
      "learning_rate": 2e-05,
      "loss": 0.6908,
      "step": 3680
    },
    {
      "epoch": 1.1721728081321474,
      "grad_norm": 6.47336483001709,
      "learning_rate": 2e-05,
      "loss": 0.6042,
      "step": 3690
    },
    {
      "epoch": 1.1753494282083863,
      "grad_norm": 7.657367706298828,
      "learning_rate": 2e-05,
      "loss": 0.6248,
      "step": 3700
    },
    {
      "epoch": 1.1785260482846251,
      "grad_norm": 9.205927848815918,
      "learning_rate": 2e-05,
      "loss": 0.627,
      "step": 3710
    },
    {
      "epoch": 1.181702668360864,
      "grad_norm": 7.057096481323242,
      "learning_rate": 2e-05,
      "loss": 0.6453,
      "step": 3720
    },
    {
      "epoch": 1.1848792884371029,
      "grad_norm": 6.465487480163574,
      "learning_rate": 2e-05,
      "loss": 0.6383,
      "step": 3730
    },
    {
      "epoch": 1.1880559085133418,
      "grad_norm": 12.609660148620605,
      "learning_rate": 2e-05,
      "loss": 0.6111,
      "step": 3740
    },
    {
      "epoch": 1.1912325285895806,
      "grad_norm": 9.495497703552246,
      "learning_rate": 2e-05,
      "loss": 0.6429,
      "step": 3750
    },
    {
      "epoch": 1.1944091486658195,
      "grad_norm": 10.15272331237793,
      "learning_rate": 2e-05,
      "loss": 0.6253,
      "step": 3760
    },
    {
      "epoch": 1.1975857687420584,
      "grad_norm": 8.231531143188477,
      "learning_rate": 2e-05,
      "loss": 0.6294,
      "step": 3770
    },
    {
      "epoch": 1.2007623888182972,
      "grad_norm": 10.294867515563965,
      "learning_rate": 2e-05,
      "loss": 0.7017,
      "step": 3780
    },
    {
      "epoch": 1.2007623888182972,
      "eval_loss": 2.1341655254364014,
      "eval_mse": 2.1337922082761223,
      "eval_pearson": 0.40340642256768033,
      "eval_runtime": 7.3925,
      "eval_samples_per_second": 2916.456,
      "eval_spearmanr": 0.37811599525271067,
      "eval_steps_per_second": 11.498,
      "step": 3780
    },
    {
      "epoch": 1.2039390088945363,
      "grad_norm": 6.434593677520752,
      "learning_rate": 2e-05,
      "loss": 0.6469,
      "step": 3790
    },
    {
      "epoch": 1.2071156289707752,
      "grad_norm": 6.094249725341797,
      "learning_rate": 2e-05,
      "loss": 0.5859,
      "step": 3800
    },
    {
      "epoch": 1.210292249047014,
      "grad_norm": 8.399943351745605,
      "learning_rate": 2e-05,
      "loss": 0.6287,
      "step": 3810
    },
    {
      "epoch": 1.213468869123253,
      "grad_norm": 7.532402038574219,
      "learning_rate": 2e-05,
      "loss": 0.6321,
      "step": 3820
    },
    {
      "epoch": 1.2166454891994918,
      "grad_norm": 5.800781726837158,
      "learning_rate": 2e-05,
      "loss": 0.6573,
      "step": 3830
    },
    {
      "epoch": 1.2198221092757306,
      "grad_norm": 10.471839904785156,
      "learning_rate": 2e-05,
      "loss": 0.6526,
      "step": 3840
    },
    {
      "epoch": 1.2229987293519695,
      "grad_norm": 9.180879592895508,
      "learning_rate": 2e-05,
      "loss": 0.6175,
      "step": 3850
    },
    {
      "epoch": 1.2261753494282084,
      "grad_norm": 10.10012435913086,
      "learning_rate": 2e-05,
      "loss": 0.6441,
      "step": 3860
    },
    {
      "epoch": 1.2293519695044473,
      "grad_norm": 5.545263290405273,
      "learning_rate": 2e-05,
      "loss": 0.5941,
      "step": 3870
    },
    {
      "epoch": 1.2325285895806861,
      "grad_norm": 7.275649070739746,
      "learning_rate": 2e-05,
      "loss": 0.6306,
      "step": 3880
    },
    {
      "epoch": 1.235705209656925,
      "grad_norm": 7.184678077697754,
      "learning_rate": 2e-05,
      "loss": 0.632,
      "step": 3890
    },
    {
      "epoch": 1.2388818297331639,
      "grad_norm": 6.869322299957275,
      "learning_rate": 2e-05,
      "loss": 0.6161,
      "step": 3900
    },
    {
      "epoch": 1.2420584498094027,
      "grad_norm": 12.433398246765137,
      "learning_rate": 2e-05,
      "loss": 0.6604,
      "step": 3910
    },
    {
      "epoch": 1.2452350698856416,
      "grad_norm": 6.212101936340332,
      "learning_rate": 2e-05,
      "loss": 0.611,
      "step": 3920
    },
    {
      "epoch": 1.2484116899618805,
      "grad_norm": 6.016934871673584,
      "learning_rate": 2e-05,
      "loss": 0.6134,
      "step": 3930
    },
    {
      "epoch": 1.2515883100381195,
      "grad_norm": 6.373297214508057,
      "learning_rate": 2e-05,
      "loss": 0.6205,
      "step": 3940
    },
    {
      "epoch": 1.2547649301143582,
      "grad_norm": 15.44961166381836,
      "learning_rate": 2e-05,
      "loss": 0.611,
      "step": 3950
    },
    {
      "epoch": 1.2579415501905973,
      "grad_norm": 7.234003067016602,
      "learning_rate": 2e-05,
      "loss": 0.6213,
      "step": 3960
    },
    {
      "epoch": 1.2611181702668361,
      "grad_norm": 7.300190448760986,
      "learning_rate": 2e-05,
      "loss": 0.6624,
      "step": 3970
    },
    {
      "epoch": 1.264294790343075,
      "grad_norm": 11.504998207092285,
      "learning_rate": 2e-05,
      "loss": 0.6177,
      "step": 3980
    },
    {
      "epoch": 1.2674714104193139,
      "grad_norm": 6.731441974639893,
      "learning_rate": 2e-05,
      "loss": 0.6722,
      "step": 3990
    },
    {
      "epoch": 1.2706480304955527,
      "grad_norm": 10.120185852050781,
      "learning_rate": 2e-05,
      "loss": 0.6174,
      "step": 4000
    },
    {
      "epoch": 1.2738246505717916,
      "grad_norm": 11.06923770904541,
      "learning_rate": 2e-05,
      "loss": 0.5927,
      "step": 4010
    },
    {
      "epoch": 1.2770012706480305,
      "grad_norm": 13.625094413757324,
      "learning_rate": 2e-05,
      "loss": 0.6782,
      "step": 4020
    },
    {
      "epoch": 1.2801778907242694,
      "grad_norm": 8.856471061706543,
      "learning_rate": 2e-05,
      "loss": 0.6117,
      "step": 4030
    },
    {
      "epoch": 1.2833545108005082,
      "grad_norm": 9.962976455688477,
      "learning_rate": 2e-05,
      "loss": 0.6372,
      "step": 4040
    },
    {
      "epoch": 1.286531130876747,
      "grad_norm": 8.514540672302246,
      "learning_rate": 2e-05,
      "loss": 0.6292,
      "step": 4050
    },
    {
      "epoch": 1.289707750952986,
      "grad_norm": 12.910623550415039,
      "learning_rate": 2e-05,
      "loss": 0.6014,
      "step": 4060
    },
    {
      "epoch": 1.2928843710292248,
      "grad_norm": 7.267306804656982,
      "learning_rate": 2e-05,
      "loss": 0.6388,
      "step": 4070
    },
    {
      "epoch": 1.2960609911054637,
      "grad_norm": 7.058420658111572,
      "learning_rate": 2e-05,
      "loss": 0.6062,
      "step": 4080
    },
    {
      "epoch": 1.2992376111817028,
      "grad_norm": 13.560319900512695,
      "learning_rate": 2e-05,
      "loss": 0.6051,
      "step": 4090
    },
    {
      "epoch": 1.300825921219822,
      "eval_loss": 2.096027135848999,
      "eval_mse": 2.0952238182521707,
      "eval_pearson": 0.41555954789814165,
      "eval_runtime": 7.478,
      "eval_samples_per_second": 2883.133,
      "eval_spearmanr": 0.3977483179338249,
      "eval_steps_per_second": 11.367,
      "step": 4095
    },
    {
      "epoch": 1.3024142312579414,
      "grad_norm": 6.326121807098389,
      "learning_rate": 2e-05,
      "loss": 0.598,
      "step": 4100
    },
    {
      "epoch": 1.3055908513341805,
      "grad_norm": 8.632485389709473,
      "learning_rate": 2e-05,
      "loss": 0.5969,
      "step": 4110
    },
    {
      "epoch": 1.3087674714104194,
      "grad_norm": 7.757164001464844,
      "learning_rate": 2e-05,
      "loss": 0.5753,
      "step": 4120
    },
    {
      "epoch": 1.3119440914866582,
      "grad_norm": 10.872457504272461,
      "learning_rate": 2e-05,
      "loss": 0.6366,
      "step": 4130
    },
    {
      "epoch": 1.3151207115628971,
      "grad_norm": 8.75146198272705,
      "learning_rate": 2e-05,
      "loss": 0.6395,
      "step": 4140
    },
    {
      "epoch": 1.318297331639136,
      "grad_norm": 8.349898338317871,
      "learning_rate": 2e-05,
      "loss": 0.6131,
      "step": 4150
    },
    {
      "epoch": 1.3214739517153749,
      "grad_norm": 7.584353446960449,
      "learning_rate": 2e-05,
      "loss": 0.6144,
      "step": 4160
    },
    {
      "epoch": 1.3246505717916137,
      "grad_norm": 10.035866737365723,
      "learning_rate": 2e-05,
      "loss": 0.5892,
      "step": 4170
    },
    {
      "epoch": 1.3278271918678526,
      "grad_norm": 13.561723709106445,
      "learning_rate": 2e-05,
      "loss": 0.6464,
      "step": 4180
    },
    {
      "epoch": 1.3310038119440915,
      "grad_norm": 10.679742813110352,
      "learning_rate": 2e-05,
      "loss": 0.6078,
      "step": 4190
    },
    {
      "epoch": 1.3341804320203303,
      "grad_norm": 12.1284761428833,
      "learning_rate": 2e-05,
      "loss": 0.6059,
      "step": 4200
    },
    {
      "epoch": 1.3373570520965692,
      "grad_norm": 6.28842830657959,
      "learning_rate": 2e-05,
      "loss": 0.5853,
      "step": 4210
    },
    {
      "epoch": 1.340533672172808,
      "grad_norm": 10.406563758850098,
      "learning_rate": 2e-05,
      "loss": 0.5867,
      "step": 4220
    },
    {
      "epoch": 1.343710292249047,
      "grad_norm": 5.627748012542725,
      "learning_rate": 2e-05,
      "loss": 0.6052,
      "step": 4230
    },
    {
      "epoch": 1.346886912325286,
      "grad_norm": 5.590696334838867,
      "learning_rate": 2e-05,
      "loss": 0.6,
      "step": 4240
    },
    {
      "epoch": 1.3500635324015247,
      "grad_norm": 5.9497857093811035,
      "learning_rate": 2e-05,
      "loss": 0.5581,
      "step": 4250
    },
    {
      "epoch": 1.3532401524777637,
      "grad_norm": 7.798040390014648,
      "learning_rate": 2e-05,
      "loss": 0.5908,
      "step": 4260
    },
    {
      "epoch": 1.3564167725540026,
      "grad_norm": 8.137452125549316,
      "learning_rate": 2e-05,
      "loss": 0.6304,
      "step": 4270
    },
    {
      "epoch": 1.3595933926302415,
      "grad_norm": 9.878058433532715,
      "learning_rate": 2e-05,
      "loss": 0.5776,
      "step": 4280
    },
    {
      "epoch": 1.3627700127064803,
      "grad_norm": 6.232355117797852,
      "learning_rate": 2e-05,
      "loss": 0.5662,
      "step": 4290
    },
    {
      "epoch": 1.3659466327827192,
      "grad_norm": 7.433079719543457,
      "learning_rate": 2e-05,
      "loss": 0.6516,
      "step": 4300
    },
    {
      "epoch": 1.369123252858958,
      "grad_norm": 7.461129188537598,
      "learning_rate": 2e-05,
      "loss": 0.6042,
      "step": 4310
    },
    {
      "epoch": 1.372299872935197,
      "grad_norm": 11.059162139892578,
      "learning_rate": 2e-05,
      "loss": 0.5753,
      "step": 4320
    },
    {
      "epoch": 1.3754764930114358,
      "grad_norm": 7.575908184051514,
      "learning_rate": 2e-05,
      "loss": 0.5799,
      "step": 4330
    },
    {
      "epoch": 1.3786531130876747,
      "grad_norm": 8.385992050170898,
      "learning_rate": 2e-05,
      "loss": 0.594,
      "step": 4340
    },
    {
      "epoch": 1.3818297331639136,
      "grad_norm": 6.968587398529053,
      "learning_rate": 2e-05,
      "loss": 0.6064,
      "step": 4350
    },
    {
      "epoch": 1.3850063532401524,
      "grad_norm": 11.052976608276367,
      "learning_rate": 2e-05,
      "loss": 0.6118,
      "step": 4360
    },
    {
      "epoch": 1.3881829733163913,
      "grad_norm": 6.155497074127197,
      "learning_rate": 2e-05,
      "loss": 0.6104,
      "step": 4370
    },
    {
      "epoch": 1.3913595933926302,
      "grad_norm": 6.358121871948242,
      "learning_rate": 2e-05,
      "loss": 0.5885,
      "step": 4380
    },
    {
      "epoch": 1.3945362134688692,
      "grad_norm": 7.280232906341553,
      "learning_rate": 2e-05,
      "loss": 0.6321,
      "step": 4390
    },
    {
      "epoch": 1.397712833545108,
      "grad_norm": 6.828925609588623,
      "learning_rate": 2e-05,
      "loss": 0.5673,
      "step": 4400
    },
    {
      "epoch": 1.400889453621347,
      "grad_norm": 9.612995147705078,
      "learning_rate": 2e-05,
      "loss": 0.6059,
      "step": 4410
    },
    {
      "epoch": 1.400889453621347,
      "eval_loss": 1.8421475887298584,
      "eval_mse": 1.8417688382357524,
      "eval_pearson": 0.4232235088189084,
      "eval_runtime": 7.3851,
      "eval_samples_per_second": 2919.379,
      "eval_spearmanr": 0.40118686275113213,
      "eval_steps_per_second": 11.51,
      "step": 4410
    },
    {
      "epoch": 1.4040660736975858,
      "grad_norm": 7.266571521759033,
      "learning_rate": 2e-05,
      "loss": 0.612,
      "step": 4420
    },
    {
      "epoch": 1.4072426937738247,
      "grad_norm": 7.263364315032959,
      "learning_rate": 2e-05,
      "loss": 0.5629,
      "step": 4430
    },
    {
      "epoch": 1.4104193138500636,
      "grad_norm": 9.350534439086914,
      "learning_rate": 2e-05,
      "loss": 0.626,
      "step": 4440
    },
    {
      "epoch": 1.4135959339263025,
      "grad_norm": 16.57775115966797,
      "learning_rate": 2e-05,
      "loss": 0.6632,
      "step": 4450
    },
    {
      "epoch": 1.4167725540025413,
      "grad_norm": 7.760377883911133,
      "learning_rate": 2e-05,
      "loss": 0.6081,
      "step": 4460
    },
    {
      "epoch": 1.4199491740787802,
      "grad_norm": 8.199722290039062,
      "learning_rate": 2e-05,
      "loss": 0.5583,
      "step": 4470
    },
    {
      "epoch": 1.423125794155019,
      "grad_norm": 13.35922622680664,
      "learning_rate": 2e-05,
      "loss": 0.6046,
      "step": 4480
    },
    {
      "epoch": 1.426302414231258,
      "grad_norm": 6.129631996154785,
      "learning_rate": 2e-05,
      "loss": 0.5881,
      "step": 4490
    },
    {
      "epoch": 1.4294790343074968,
      "grad_norm": 8.668325424194336,
      "learning_rate": 2e-05,
      "loss": 0.6082,
      "step": 4500
    },
    {
      "epoch": 1.4326556543837357,
      "grad_norm": 11.210810661315918,
      "learning_rate": 2e-05,
      "loss": 0.6036,
      "step": 4510
    },
    {
      "epoch": 1.4358322744599745,
      "grad_norm": 7.438877105712891,
      "learning_rate": 2e-05,
      "loss": 0.5897,
      "step": 4520
    },
    {
      "epoch": 1.4390088945362134,
      "grad_norm": 6.268460750579834,
      "learning_rate": 2e-05,
      "loss": 0.591,
      "step": 4530
    },
    {
      "epoch": 1.4421855146124525,
      "grad_norm": 6.2258830070495605,
      "learning_rate": 2e-05,
      "loss": 0.5823,
      "step": 4540
    },
    {
      "epoch": 1.4453621346886911,
      "grad_norm": 7.674869060516357,
      "learning_rate": 2e-05,
      "loss": 0.6193,
      "step": 4550
    },
    {
      "epoch": 1.4485387547649302,
      "grad_norm": 5.865850448608398,
      "learning_rate": 2e-05,
      "loss": 0.585,
      "step": 4560
    },
    {
      "epoch": 1.4517153748411689,
      "grad_norm": 5.856314659118652,
      "learning_rate": 2e-05,
      "loss": 0.6062,
      "step": 4570
    },
    {
      "epoch": 1.454891994917408,
      "grad_norm": 7.377039909362793,
      "learning_rate": 2e-05,
      "loss": 0.6058,
      "step": 4580
    },
    {
      "epoch": 1.4580686149936468,
      "grad_norm": 7.700839042663574,
      "learning_rate": 2e-05,
      "loss": 0.5733,
      "step": 4590
    },
    {
      "epoch": 1.4612452350698857,
      "grad_norm": 5.761405944824219,
      "learning_rate": 2e-05,
      "loss": 0.5997,
      "step": 4600
    },
    {
      "epoch": 1.4644218551461246,
      "grad_norm": 7.959899425506592,
      "learning_rate": 2e-05,
      "loss": 0.6003,
      "step": 4610
    },
    {
      "epoch": 1.4675984752223634,
      "grad_norm": 6.070359706878662,
      "learning_rate": 2e-05,
      "loss": 0.5747,
      "step": 4620
    },
    {
      "epoch": 1.4707750952986023,
      "grad_norm": 6.422358989715576,
      "learning_rate": 2e-05,
      "loss": 0.6059,
      "step": 4630
    },
    {
      "epoch": 1.4739517153748412,
      "grad_norm": 7.39280891418457,
      "learning_rate": 2e-05,
      "loss": 0.5858,
      "step": 4640
    },
    {
      "epoch": 1.47712833545108,
      "grad_norm": 10.735636711120605,
      "learning_rate": 2e-05,
      "loss": 0.5328,
      "step": 4650
    },
    {
      "epoch": 1.4803049555273189,
      "grad_norm": 7.554252624511719,
      "learning_rate": 2e-05,
      "loss": 0.5571,
      "step": 4660
    },
    {
      "epoch": 1.4834815756035578,
      "grad_norm": 8.19340991973877,
      "learning_rate": 2e-05,
      "loss": 0.6097,
      "step": 4670
    },
    {
      "epoch": 1.4866581956797966,
      "grad_norm": 10.684687614440918,
      "learning_rate": 2e-05,
      "loss": 0.5711,
      "step": 4680
    },
    {
      "epoch": 1.4898348157560357,
      "grad_norm": 8.934853553771973,
      "learning_rate": 2e-05,
      "loss": 0.5866,
      "step": 4690
    },
    {
      "epoch": 1.4930114358322744,
      "grad_norm": 9.620524406433105,
      "learning_rate": 2e-05,
      "loss": 0.5539,
      "step": 4700
    },
    {
      "epoch": 1.4961880559085134,
      "grad_norm": 8.43221378326416,
      "learning_rate": 2e-05,
      "loss": 0.5627,
      "step": 4710
    },
    {
      "epoch": 1.499364675984752,
      "grad_norm": 8.58193302154541,
      "learning_rate": 2e-05,
      "loss": 0.5926,
      "step": 4720
    },
    {
      "epoch": 1.5009529860228716,
      "eval_loss": 1.7934651374816895,
      "eval_mse": 1.7918474520071097,
      "eval_pearson": 0.4130457455466661,
      "eval_runtime": 7.4687,
      "eval_samples_per_second": 2886.731,
      "eval_spearmanr": 0.394872903912612,
      "eval_steps_per_second": 11.381,
      "step": 4725
    },
    {
      "epoch": 1.5025412960609912,
      "grad_norm": 9.7354736328125,
      "learning_rate": 2e-05,
      "loss": 0.5965,
      "step": 4730
    },
    {
      "epoch": 1.5057179161372298,
      "grad_norm": 7.63712739944458,
      "learning_rate": 2e-05,
      "loss": 0.5729,
      "step": 4740
    },
    {
      "epoch": 1.508894536213469,
      "grad_norm": 6.226789951324463,
      "learning_rate": 2e-05,
      "loss": 0.5882,
      "step": 4750
    },
    {
      "epoch": 1.5120711562897078,
      "grad_norm": 11.885536193847656,
      "learning_rate": 2e-05,
      "loss": 0.6094,
      "step": 4760
    },
    {
      "epoch": 1.5152477763659467,
      "grad_norm": 8.395115852355957,
      "learning_rate": 2e-05,
      "loss": 0.5851,
      "step": 4770
    },
    {
      "epoch": 1.5184243964421855,
      "grad_norm": 12.515556335449219,
      "learning_rate": 2e-05,
      "loss": 0.5687,
      "step": 4780
    },
    {
      "epoch": 1.5216010165184244,
      "grad_norm": 7.357561111450195,
      "learning_rate": 2e-05,
      "loss": 0.5972,
      "step": 4790
    },
    {
      "epoch": 1.5247776365946633,
      "grad_norm": 6.232713222503662,
      "learning_rate": 2e-05,
      "loss": 0.5708,
      "step": 4800
    },
    {
      "epoch": 1.5279542566709021,
      "grad_norm": 10.474063873291016,
      "learning_rate": 2e-05,
      "loss": 0.6045,
      "step": 4810
    },
    {
      "epoch": 1.531130876747141,
      "grad_norm": 11.39164924621582,
      "learning_rate": 2e-05,
      "loss": 0.5761,
      "step": 4820
    },
    {
      "epoch": 1.5343074968233799,
      "grad_norm": 8.75641918182373,
      "learning_rate": 2e-05,
      "loss": 0.6063,
      "step": 4830
    },
    {
      "epoch": 1.537484116899619,
      "grad_norm": 6.200427055358887,
      "learning_rate": 2e-05,
      "loss": 0.5974,
      "step": 4840
    },
    {
      "epoch": 1.5406607369758576,
      "grad_norm": 6.712892055511475,
      "learning_rate": 2e-05,
      "loss": 0.5525,
      "step": 4850
    },
    {
      "epoch": 1.5438373570520967,
      "grad_norm": 6.382983684539795,
      "learning_rate": 2e-05,
      "loss": 0.5636,
      "step": 4860
    },
    {
      "epoch": 1.5470139771283353,
      "grad_norm": 6.790600776672363,
      "learning_rate": 2e-05,
      "loss": 0.5606,
      "step": 4870
    },
    {
      "epoch": 1.5501905972045744,
      "grad_norm": 5.589390277862549,
      "learning_rate": 2e-05,
      "loss": 0.5799,
      "step": 4880
    },
    {
      "epoch": 1.553367217280813,
      "grad_norm": 8.729907035827637,
      "learning_rate": 2e-05,
      "loss": 0.5544,
      "step": 4890
    },
    {
      "epoch": 1.5565438373570522,
      "grad_norm": 6.191897869110107,
      "learning_rate": 2e-05,
      "loss": 0.5877,
      "step": 4900
    },
    {
      "epoch": 1.559720457433291,
      "grad_norm": 8.650344848632812,
      "learning_rate": 2e-05,
      "loss": 0.5739,
      "step": 4910
    },
    {
      "epoch": 1.5628970775095299,
      "grad_norm": 6.974544525146484,
      "learning_rate": 2e-05,
      "loss": 0.5504,
      "step": 4920
    },
    {
      "epoch": 1.5660736975857688,
      "grad_norm": 9.769577980041504,
      "learning_rate": 2e-05,
      "loss": 0.5847,
      "step": 4930
    },
    {
      "epoch": 1.5692503176620076,
      "grad_norm": 11.846597671508789,
      "learning_rate": 2e-05,
      "loss": 0.5878,
      "step": 4940
    },
    {
      "epoch": 1.5724269377382465,
      "grad_norm": 6.382549285888672,
      "learning_rate": 2e-05,
      "loss": 0.6073,
      "step": 4950
    },
    {
      "epoch": 1.5756035578144854,
      "grad_norm": 6.585967540740967,
      "learning_rate": 2e-05,
      "loss": 0.5541,
      "step": 4960
    },
    {
      "epoch": 1.5787801778907242,
      "grad_norm": 10.547816276550293,
      "learning_rate": 2e-05,
      "loss": 0.5598,
      "step": 4970
    },
    {
      "epoch": 1.581956797966963,
      "grad_norm": 13.53447437286377,
      "learning_rate": 2e-05,
      "loss": 0.562,
      "step": 4980
    },
    {
      "epoch": 1.5851334180432022,
      "grad_norm": 6.665318012237549,
      "learning_rate": 2e-05,
      "loss": 0.5473,
      "step": 4990
    },
    {
      "epoch": 1.5883100381194408,
      "grad_norm": 15.917991638183594,
      "learning_rate": 2e-05,
      "loss": 0.5869,
      "step": 5000
    },
    {
      "epoch": 1.59148665819568,
      "grad_norm": 12.181025505065918,
      "learning_rate": 2e-05,
      "loss": 0.5685,
      "step": 5010
    },
    {
      "epoch": 1.5946632782719186,
      "grad_norm": 7.529914855957031,
      "learning_rate": 2e-05,
      "loss": 0.5803,
      "step": 5020
    },
    {
      "epoch": 1.5978398983481577,
      "grad_norm": 11.15804672241211,
      "learning_rate": 2e-05,
      "loss": 0.5507,
      "step": 5030
    },
    {
      "epoch": 1.6010165184243963,
      "grad_norm": 12.700719833374023,
      "learning_rate": 2e-05,
      "loss": 0.5959,
      "step": 5040
    },
    {
      "epoch": 1.6010165184243963,
      "eval_loss": 1.7648669481277466,
      "eval_mse": 1.7629589201346842,
      "eval_pearson": 0.3959564310178492,
      "eval_runtime": 7.4727,
      "eval_samples_per_second": 2885.179,
      "eval_spearmanr": 0.37901500067481925,
      "eval_steps_per_second": 11.375,
      "step": 5040
    },
    {
      "epoch": 1.6041931385006354,
      "grad_norm": 11.612563133239746,
      "learning_rate": 2e-05,
      "loss": 0.5512,
      "step": 5050
    },
    {
      "epoch": 1.6073697585768743,
      "grad_norm": 7.078606605529785,
      "learning_rate": 2e-05,
      "loss": 0.5772,
      "step": 5060
    },
    {
      "epoch": 1.6105463786531131,
      "grad_norm": 15.627187728881836,
      "learning_rate": 2e-05,
      "loss": 0.5792,
      "step": 5070
    },
    {
      "epoch": 1.613722998729352,
      "grad_norm": 6.1971306800842285,
      "learning_rate": 2e-05,
      "loss": 0.5535,
      "step": 5080
    },
    {
      "epoch": 1.6168996188055909,
      "grad_norm": 7.0813164710998535,
      "learning_rate": 2e-05,
      "loss": 0.5179,
      "step": 5090
    },
    {
      "epoch": 1.6200762388818297,
      "grad_norm": 6.249373435974121,
      "learning_rate": 2e-05,
      "loss": 0.5687,
      "step": 5100
    },
    {
      "epoch": 1.6232528589580686,
      "grad_norm": 7.340102672576904,
      "learning_rate": 2e-05,
      "loss": 0.5927,
      "step": 5110
    },
    {
      "epoch": 1.6264294790343075,
      "grad_norm": 5.872334003448486,
      "learning_rate": 2e-05,
      "loss": 0.5858,
      "step": 5120
    },
    {
      "epoch": 1.6296060991105463,
      "grad_norm": 8.755084037780762,
      "learning_rate": 2e-05,
      "loss": 0.5906,
      "step": 5130
    },
    {
      "epoch": 1.6327827191867854,
      "grad_norm": 13.495930671691895,
      "learning_rate": 2e-05,
      "loss": 0.596,
      "step": 5140
    },
    {
      "epoch": 1.635959339263024,
      "grad_norm": 5.691752910614014,
      "learning_rate": 2e-05,
      "loss": 0.554,
      "step": 5150
    },
    {
      "epoch": 1.6391359593392631,
      "grad_norm": 12.15339183807373,
      "learning_rate": 2e-05,
      "loss": 0.5467,
      "step": 5160
    },
    {
      "epoch": 1.6423125794155018,
      "grad_norm": 6.397144317626953,
      "learning_rate": 2e-05,
      "loss": 0.5652,
      "step": 5170
    },
    {
      "epoch": 1.6454891994917409,
      "grad_norm": 13.347758293151855,
      "learning_rate": 2e-05,
      "loss": 0.5639,
      "step": 5180
    },
    {
      "epoch": 1.6486658195679795,
      "grad_norm": 6.022191047668457,
      "learning_rate": 2e-05,
      "loss": 0.5153,
      "step": 5190
    },
    {
      "epoch": 1.6518424396442186,
      "grad_norm": 10.580526351928711,
      "learning_rate": 2e-05,
      "loss": 0.5928,
      "step": 5200
    },
    {
      "epoch": 1.6550190597204575,
      "grad_norm": 4.927528381347656,
      "learning_rate": 2e-05,
      "loss": 0.5606,
      "step": 5210
    },
    {
      "epoch": 1.6581956797966964,
      "grad_norm": 9.859396934509277,
      "learning_rate": 2e-05,
      "loss": 0.5347,
      "step": 5220
    },
    {
      "epoch": 1.6613722998729352,
      "grad_norm": 8.11610221862793,
      "learning_rate": 2e-05,
      "loss": 0.5688,
      "step": 5230
    },
    {
      "epoch": 1.664548919949174,
      "grad_norm": 7.273239612579346,
      "learning_rate": 2e-05,
      "loss": 0.5698,
      "step": 5240
    },
    {
      "epoch": 1.667725540025413,
      "grad_norm": 10.26085376739502,
      "learning_rate": 2e-05,
      "loss": 0.5878,
      "step": 5250
    },
    {
      "epoch": 1.6709021601016518,
      "grad_norm": 5.7255659103393555,
      "learning_rate": 2e-05,
      "loss": 0.5366,
      "step": 5260
    },
    {
      "epoch": 1.6740787801778907,
      "grad_norm": 8.494301795959473,
      "learning_rate": 2e-05,
      "loss": 0.5663,
      "step": 5270
    },
    {
      "epoch": 1.6772554002541296,
      "grad_norm": 7.563917636871338,
      "learning_rate": 2e-05,
      "loss": 0.5729,
      "step": 5280
    },
    {
      "epoch": 1.6804320203303686,
      "grad_norm": 9.361614227294922,
      "learning_rate": 2e-05,
      "loss": 0.5776,
      "step": 5290
    },
    {
      "epoch": 1.6836086404066073,
      "grad_norm": 9.888408660888672,
      "learning_rate": 2e-05,
      "loss": 0.5629,
      "step": 5300
    },
    {
      "epoch": 1.6867852604828464,
      "grad_norm": 7.120459079742432,
      "learning_rate": 2e-05,
      "loss": 0.5881,
      "step": 5310
    },
    {
      "epoch": 1.689961880559085,
      "grad_norm": 8.471427917480469,
      "learning_rate": 2e-05,
      "loss": 0.5177,
      "step": 5320
    },
    {
      "epoch": 1.6931385006353241,
      "grad_norm": 10.169387817382812,
      "learning_rate": 2e-05,
      "loss": 0.5349,
      "step": 5330
    },
    {
      "epoch": 1.6963151207115628,
      "grad_norm": 16.2408390045166,
      "learning_rate": 2e-05,
      "loss": 0.5906,
      "step": 5340
    },
    {
      "epoch": 1.6994917407878019,
      "grad_norm": 7.531872749328613,
      "learning_rate": 2e-05,
      "loss": 0.5623,
      "step": 5350
    },
    {
      "epoch": 1.7010800508259212,
      "eval_loss": 1.8339502811431885,
      "eval_mse": 1.8323252764393978,
      "eval_pearson": 0.3881891657388794,
      "eval_runtime": 7.468,
      "eval_samples_per_second": 2886.991,
      "eval_spearmanr": 0.37480294870171654,
      "eval_steps_per_second": 11.382,
      "step": 5355
    },
    {
      "epoch": 1.7026683608640405,
      "grad_norm": 13.51515007019043,
      "learning_rate": 2e-05,
      "loss": 0.5341,
      "step": 5360
    },
    {
      "epoch": 1.7058449809402796,
      "grad_norm": 14.676652908325195,
      "learning_rate": 2e-05,
      "loss": 0.5761,
      "step": 5370
    },
    {
      "epoch": 1.7090216010165185,
      "grad_norm": 7.051819801330566,
      "learning_rate": 2e-05,
      "loss": 0.5617,
      "step": 5380
    },
    {
      "epoch": 1.7121982210927573,
      "grad_norm": 7.130765438079834,
      "learning_rate": 2e-05,
      "loss": 0.559,
      "step": 5390
    },
    {
      "epoch": 1.7153748411689962,
      "grad_norm": 8.613693237304688,
      "learning_rate": 2e-05,
      "loss": 0.5556,
      "step": 5400
    },
    {
      "epoch": 1.718551461245235,
      "grad_norm": 7.056950569152832,
      "learning_rate": 2e-05,
      "loss": 0.5572,
      "step": 5410
    },
    {
      "epoch": 1.721728081321474,
      "grad_norm": 9.517352104187012,
      "learning_rate": 2e-05,
      "loss": 0.5567,
      "step": 5420
    },
    {
      "epoch": 1.7249047013977128,
      "grad_norm": 5.916719913482666,
      "learning_rate": 2e-05,
      "loss": 0.5165,
      "step": 5430
    },
    {
      "epoch": 1.7280813214739519,
      "grad_norm": 8.966700553894043,
      "learning_rate": 2e-05,
      "loss": 0.5676,
      "step": 5440
    },
    {
      "epoch": 1.7312579415501905,
      "grad_norm": 11.761602401733398,
      "learning_rate": 2e-05,
      "loss": 0.5437,
      "step": 5450
    },
    {
      "epoch": 1.7344345616264296,
      "grad_norm": 7.703560829162598,
      "learning_rate": 2e-05,
      "loss": 0.545,
      "step": 5460
    },
    {
      "epoch": 1.7376111817026683,
      "grad_norm": 8.775735855102539,
      "learning_rate": 2e-05,
      "loss": 0.5363,
      "step": 5470
    },
    {
      "epoch": 1.7407878017789074,
      "grad_norm": 7.146817684173584,
      "learning_rate": 2e-05,
      "loss": 0.5488,
      "step": 5480
    },
    {
      "epoch": 1.743964421855146,
      "grad_norm": 7.066385269165039,
      "learning_rate": 2e-05,
      "loss": 0.5417,
      "step": 5490
    },
    {
      "epoch": 1.747141041931385,
      "grad_norm": 7.57414436340332,
      "learning_rate": 2e-05,
      "loss": 0.5821,
      "step": 5500
    },
    {
      "epoch": 1.7503176620076237,
      "grad_norm": 7.291613578796387,
      "learning_rate": 2e-05,
      "loss": 0.5761,
      "step": 5510
    },
    {
      "epoch": 1.7534942820838628,
      "grad_norm": 15.34997272491455,
      "learning_rate": 2e-05,
      "loss": 0.5708,
      "step": 5520
    },
    {
      "epoch": 1.7566709021601017,
      "grad_norm": 11.457698822021484,
      "learning_rate": 2e-05,
      "loss": 0.5477,
      "step": 5530
    },
    {
      "epoch": 1.7598475222363406,
      "grad_norm": 7.5460076332092285,
      "learning_rate": 2e-05,
      "loss": 0.5687,
      "step": 5540
    },
    {
      "epoch": 1.7630241423125794,
      "grad_norm": 15.845281600952148,
      "learning_rate": 2e-05,
      "loss": 0.5486,
      "step": 5550
    },
    {
      "epoch": 1.7662007623888183,
      "grad_norm": 11.541104316711426,
      "learning_rate": 2e-05,
      "loss": 0.5372,
      "step": 5560
    },
    {
      "epoch": 1.7693773824650572,
      "grad_norm": 20.17609977722168,
      "learning_rate": 2e-05,
      "loss": 0.5639,
      "step": 5570
    },
    {
      "epoch": 1.772554002541296,
      "grad_norm": 8.503460884094238,
      "learning_rate": 2e-05,
      "loss": 0.5453,
      "step": 5580
    },
    {
      "epoch": 1.775730622617535,
      "grad_norm": 8.832805633544922,
      "learning_rate": 2e-05,
      "loss": 0.5581,
      "step": 5590
    },
    {
      "epoch": 1.7789072426937738,
      "grad_norm": 7.120851516723633,
      "learning_rate": 2e-05,
      "loss": 0.5451,
      "step": 5600
    },
    {
      "epoch": 1.7820838627700128,
      "grad_norm": 10.612730979919434,
      "learning_rate": 2e-05,
      "loss": 0.5358,
      "step": 5610
    },
    {
      "epoch": 1.7852604828462515,
      "grad_norm": 10.485228538513184,
      "learning_rate": 2e-05,
      "loss": 0.5738,
      "step": 5620
    },
    {
      "epoch": 1.7884371029224906,
      "grad_norm": 9.757412910461426,
      "learning_rate": 2e-05,
      "loss": 0.5572,
      "step": 5630
    },
    {
      "epoch": 1.7916137229987292,
      "grad_norm": 9.065579414367676,
      "learning_rate": 2e-05,
      "loss": 0.5476,
      "step": 5640
    },
    {
      "epoch": 1.7947903430749683,
      "grad_norm": 6.986004829406738,
      "learning_rate": 2e-05,
      "loss": 0.5409,
      "step": 5650
    },
    {
      "epoch": 1.797966963151207,
      "grad_norm": 12.460947036743164,
      "learning_rate": 2e-05,
      "loss": 0.559,
      "step": 5660
    },
    {
      "epoch": 1.801143583227446,
      "grad_norm": 10.354944229125977,
      "learning_rate": 2e-05,
      "loss": 0.5453,
      "step": 5670
    },
    {
      "epoch": 1.801143583227446,
      "eval_loss": 1.8903590440750122,
      "eval_mse": 1.8890059823007885,
      "eval_pearson": 0.3951286904074159,
      "eval_runtime": 7.4634,
      "eval_samples_per_second": 2888.754,
      "eval_spearmanr": 0.3803322037653197,
      "eval_steps_per_second": 11.389,
      "step": 5670
    },
    {
      "epoch": 1.804320203303685,
      "grad_norm": 5.344051361083984,
      "learning_rate": 2e-05,
      "loss": 0.5153,
      "step": 5680
    },
    {
      "epoch": 1.8074968233799238,
      "grad_norm": 8.857312202453613,
      "learning_rate": 2e-05,
      "loss": 0.5357,
      "step": 5690
    },
    {
      "epoch": 1.8106734434561627,
      "grad_norm": 17.727188110351562,
      "learning_rate": 2e-05,
      "loss": 0.5804,
      "step": 5700
    },
    {
      "epoch": 1.8138500635324015,
      "grad_norm": 13.071670532226562,
      "learning_rate": 2e-05,
      "loss": 0.5066,
      "step": 5710
    },
    {
      "epoch": 1.8170266836086404,
      "grad_norm": 7.2278151512146,
      "learning_rate": 2e-05,
      "loss": 0.5849,
      "step": 5720
    },
    {
      "epoch": 1.8202033036848793,
      "grad_norm": 7.81569242477417,
      "learning_rate": 2e-05,
      "loss": 0.5143,
      "step": 5730
    },
    {
      "epoch": 1.8233799237611181,
      "grad_norm": 8.841909408569336,
      "learning_rate": 2e-05,
      "loss": 0.5543,
      "step": 5740
    },
    {
      "epoch": 1.826556543837357,
      "grad_norm": 6.248218536376953,
      "learning_rate": 2e-05,
      "loss": 0.5545,
      "step": 5750
    },
    {
      "epoch": 1.829733163913596,
      "grad_norm": 10.093232154846191,
      "learning_rate": 2e-05,
      "loss": 0.522,
      "step": 5760
    },
    {
      "epoch": 1.8329097839898347,
      "grad_norm": 8.90698528289795,
      "learning_rate": 2e-05,
      "loss": 0.5313,
      "step": 5770
    },
    {
      "epoch": 1.8360864040660738,
      "grad_norm": 8.87448501586914,
      "learning_rate": 2e-05,
      "loss": 0.5272,
      "step": 5780
    },
    {
      "epoch": 1.8392630241423125,
      "grad_norm": 6.694533824920654,
      "learning_rate": 2e-05,
      "loss": 0.5313,
      "step": 5790
    },
    {
      "epoch": 1.8424396442185516,
      "grad_norm": 6.641631603240967,
      "learning_rate": 2e-05,
      "loss": 0.5161,
      "step": 5800
    },
    {
      "epoch": 1.8456162642947902,
      "grad_norm": 7.132815361022949,
      "learning_rate": 2e-05,
      "loss": 0.5369,
      "step": 5810
    },
    {
      "epoch": 1.8487928843710293,
      "grad_norm": 13.750299453735352,
      "learning_rate": 2e-05,
      "loss": 0.5135,
      "step": 5820
    },
    {
      "epoch": 1.8519695044472682,
      "grad_norm": 7.2496867179870605,
      "learning_rate": 2e-05,
      "loss": 0.5663,
      "step": 5830
    },
    {
      "epoch": 1.855146124523507,
      "grad_norm": 7.721468448638916,
      "learning_rate": 2e-05,
      "loss": 0.5512,
      "step": 5840
    },
    {
      "epoch": 1.858322744599746,
      "grad_norm": 7.246976852416992,
      "learning_rate": 2e-05,
      "loss": 0.5619,
      "step": 5850
    },
    {
      "epoch": 1.8614993646759848,
      "grad_norm": 10.593361854553223,
      "learning_rate": 2e-05,
      "loss": 0.5511,
      "step": 5860
    },
    {
      "epoch": 1.8646759847522236,
      "grad_norm": 9.800806045532227,
      "learning_rate": 2e-05,
      "loss": 0.4991,
      "step": 5870
    },
    {
      "epoch": 1.8678526048284625,
      "grad_norm": 4.919579982757568,
      "learning_rate": 2e-05,
      "loss": 0.517,
      "step": 5880
    },
    {
      "epoch": 1.8710292249047014,
      "grad_norm": 5.958682060241699,
      "learning_rate": 2e-05,
      "loss": 0.5511,
      "step": 5890
    },
    {
      "epoch": 1.8742058449809402,
      "grad_norm": 6.323265075683594,
      "learning_rate": 2e-05,
      "loss": 0.558,
      "step": 5900
    },
    {
      "epoch": 1.8773824650571793,
      "grad_norm": 7.315802097320557,
      "learning_rate": 2e-05,
      "loss": 0.542,
      "step": 5910
    },
    {
      "epoch": 1.880559085133418,
      "grad_norm": 5.72703742980957,
      "learning_rate": 2e-05,
      "loss": 0.5323,
      "step": 5920
    },
    {
      "epoch": 1.883735705209657,
      "grad_norm": 7.441452980041504,
      "learning_rate": 2e-05,
      "loss": 0.5389,
      "step": 5930
    },
    {
      "epoch": 1.8869123252858957,
      "grad_norm": 7.766057968139648,
      "learning_rate": 2e-05,
      "loss": 0.4807,
      "step": 5940
    },
    {
      "epoch": 1.8900889453621348,
      "grad_norm": 7.695096969604492,
      "learning_rate": 2e-05,
      "loss": 0.5356,
      "step": 5950
    },
    {
      "epoch": 1.8932655654383734,
      "grad_norm": 5.291534900665283,
      "learning_rate": 2e-05,
      "loss": 0.5384,
      "step": 5960
    },
    {
      "epoch": 1.8964421855146125,
      "grad_norm": 5.468392848968506,
      "learning_rate": 2e-05,
      "loss": 0.5162,
      "step": 5970
    },
    {
      "epoch": 1.8996188055908514,
      "grad_norm": 7.17441987991333,
      "learning_rate": 2e-05,
      "loss": 0.5034,
      "step": 5980
    },
    {
      "epoch": 1.9012071156289707,
      "eval_loss": 2.0443644523620605,
      "eval_mse": 2.042278458562322,
      "eval_pearson": 0.41682278633700937,
      "eval_runtime": 7.3638,
      "eval_samples_per_second": 2927.837,
      "eval_spearmanr": 0.40080329176830815,
      "eval_steps_per_second": 11.543,
      "step": 5985
    },
    {
      "epoch": 1.9027954256670903,
      "grad_norm": 6.8955488204956055,
      "learning_rate": 2e-05,
      "loss": 0.5183,
      "step": 5990
    },
    {
      "epoch": 1.9059720457433291,
      "grad_norm": 13.881420135498047,
      "learning_rate": 2e-05,
      "loss": 0.5561,
      "step": 6000
    },
    {
      "epoch": 1.909148665819568,
      "grad_norm": 9.766778945922852,
      "learning_rate": 2e-05,
      "loss": 0.5681,
      "step": 6010
    },
    {
      "epoch": 1.9123252858958069,
      "grad_norm": 7.114117622375488,
      "learning_rate": 2e-05,
      "loss": 0.5291,
      "step": 6020
    },
    {
      "epoch": 1.9155019059720457,
      "grad_norm": 6.451526165008545,
      "learning_rate": 2e-05,
      "loss": 0.5298,
      "step": 6030
    },
    {
      "epoch": 1.9186785260482846,
      "grad_norm": 7.360313415527344,
      "learning_rate": 2e-05,
      "loss": 0.5225,
      "step": 6040
    },
    {
      "epoch": 1.9218551461245235,
      "grad_norm": 9.579190254211426,
      "learning_rate": 2e-05,
      "loss": 0.5278,
      "step": 6050
    },
    {
      "epoch": 1.9250317662007626,
      "grad_norm": 7.349203109741211,
      "learning_rate": 2e-05,
      "loss": 0.5295,
      "step": 6060
    },
    {
      "epoch": 1.9282083862770012,
      "grad_norm": 7.080692291259766,
      "learning_rate": 2e-05,
      "loss": 0.5494,
      "step": 6070
    },
    {
      "epoch": 1.9313850063532403,
      "grad_norm": 5.194149017333984,
      "learning_rate": 2e-05,
      "loss": 0.5148,
      "step": 6080
    },
    {
      "epoch": 1.934561626429479,
      "grad_norm": 7.88374662399292,
      "learning_rate": 2e-05,
      "loss": 0.5369,
      "step": 6090
    },
    {
      "epoch": 1.937738246505718,
      "grad_norm": 16.268280029296875,
      "learning_rate": 2e-05,
      "loss": 0.56,
      "step": 6100
    },
    {
      "epoch": 1.9409148665819567,
      "grad_norm": 15.598672866821289,
      "learning_rate": 2e-05,
      "loss": 0.533,
      "step": 6110
    },
    {
      "epoch": 1.9440914866581958,
      "grad_norm": 9.013372421264648,
      "learning_rate": 2e-05,
      "loss": 0.5474,
      "step": 6120
    },
    {
      "epoch": 1.9472681067344344,
      "grad_norm": 11.255301475524902,
      "learning_rate": 2e-05,
      "loss": 0.511,
      "step": 6130
    },
    {
      "epoch": 1.9504447268106735,
      "grad_norm": 8.488515853881836,
      "learning_rate": 2e-05,
      "loss": 0.4962,
      "step": 6140
    },
    {
      "epoch": 1.9536213468869124,
      "grad_norm": 8.918256759643555,
      "learning_rate": 2e-05,
      "loss": 0.5377,
      "step": 6150
    },
    {
      "epoch": 1.9567979669631512,
      "grad_norm": 6.800439834594727,
      "learning_rate": 2e-05,
      "loss": 0.5345,
      "step": 6160
    },
    {
      "epoch": 1.95997458703939,
      "grad_norm": 9.613008499145508,
      "learning_rate": 2e-05,
      "loss": 0.5118,
      "step": 6170
    },
    {
      "epoch": 1.963151207115629,
      "grad_norm": 6.02481746673584,
      "learning_rate": 2e-05,
      "loss": 0.5595,
      "step": 6180
    },
    {
      "epoch": 1.9663278271918678,
      "grad_norm": 15.973050117492676,
      "learning_rate": 2e-05,
      "loss": 0.5373,
      "step": 6190
    },
    {
      "epoch": 1.9695044472681067,
      "grad_norm": 13.297379493713379,
      "learning_rate": 2e-05,
      "loss": 0.552,
      "step": 6200
    },
    {
      "epoch": 1.9726810673443456,
      "grad_norm": 7.444815635681152,
      "learning_rate": 2e-05,
      "loss": 0.5177,
      "step": 6210
    },
    {
      "epoch": 1.9758576874205844,
      "grad_norm": 10.264196395874023,
      "learning_rate": 2e-05,
      "loss": 0.5479,
      "step": 6220
    },
    {
      "epoch": 1.9790343074968235,
      "grad_norm": 11.393414497375488,
      "learning_rate": 2e-05,
      "loss": 0.5457,
      "step": 6230
    },
    {
      "epoch": 1.9822109275730622,
      "grad_norm": 6.931863307952881,
      "learning_rate": 2e-05,
      "loss": 0.5114,
      "step": 6240
    },
    {
      "epoch": 1.9853875476493013,
      "grad_norm": 10.63680648803711,
      "learning_rate": 2e-05,
      "loss": 0.5203,
      "step": 6250
    },
    {
      "epoch": 1.98856416772554,
      "grad_norm": 9.332232475280762,
      "learning_rate": 2e-05,
      "loss": 0.5646,
      "step": 6260
    },
    {
      "epoch": 1.991740787801779,
      "grad_norm": 6.074685096740723,
      "learning_rate": 2e-05,
      "loss": 0.5468,
      "step": 6270
    },
    {
      "epoch": 1.9949174078780176,
      "grad_norm": 8.797356605529785,
      "learning_rate": 2e-05,
      "loss": 0.5387,
      "step": 6280
    },
    {
      "epoch": 1.9980940279542567,
      "grad_norm": 6.551932334899902,
      "learning_rate": 2e-05,
      "loss": 0.4947,
      "step": 6290
    },
    {
      "epoch": 2.0012706480304954,
      "grad_norm": 12.562931060791016,
      "learning_rate": 2e-05,
      "loss": 0.5,
      "step": 6300
    },
    {
      "epoch": 2.0012706480304954,
      "eval_loss": 1.6679919958114624,
      "eval_mse": 1.6658406588938328,
      "eval_pearson": 0.3932594536032028,
      "eval_runtime": 7.2933,
      "eval_samples_per_second": 2956.118,
      "eval_spearmanr": 0.3832413784268687,
      "eval_steps_per_second": 11.654,
      "step": 6300
    },
    {
      "epoch": 2.0044472681067345,
      "grad_norm": 7.085913181304932,
      "learning_rate": 2e-05,
      "loss": 0.4735,
      "step": 6310
    },
    {
      "epoch": 2.007623888182973,
      "grad_norm": 6.526975154876709,
      "learning_rate": 2e-05,
      "loss": 0.4744,
      "step": 6320
    },
    {
      "epoch": 2.010800508259212,
      "grad_norm": 9.440799713134766,
      "learning_rate": 2e-05,
      "loss": 0.4797,
      "step": 6330
    },
    {
      "epoch": 2.0139771283354513,
      "grad_norm": 7.324129581451416,
      "learning_rate": 2e-05,
      "loss": 0.4877,
      "step": 6340
    },
    {
      "epoch": 2.01715374841169,
      "grad_norm": 8.797134399414062,
      "learning_rate": 2e-05,
      "loss": 0.4647,
      "step": 6350
    },
    {
      "epoch": 2.020330368487929,
      "grad_norm": 8.185826301574707,
      "learning_rate": 2e-05,
      "loss": 0.426,
      "step": 6360
    },
    {
      "epoch": 2.0235069885641677,
      "grad_norm": 5.929063320159912,
      "learning_rate": 2e-05,
      "loss": 0.4911,
      "step": 6370
    },
    {
      "epoch": 2.0266836086404068,
      "grad_norm": 9.03991985321045,
      "learning_rate": 2e-05,
      "loss": 0.4952,
      "step": 6380
    },
    {
      "epoch": 2.0298602287166454,
      "grad_norm": 8.46635913848877,
      "learning_rate": 2e-05,
      "loss": 0.4603,
      "step": 6390
    },
    {
      "epoch": 2.0330368487928845,
      "grad_norm": 6.230850696563721,
      "learning_rate": 2e-05,
      "loss": 0.4899,
      "step": 6400
    },
    {
      "epoch": 2.036213468869123,
      "grad_norm": 6.468533515930176,
      "learning_rate": 2e-05,
      "loss": 0.4935,
      "step": 6410
    },
    {
      "epoch": 2.0393900889453622,
      "grad_norm": 7.903836727142334,
      "learning_rate": 2e-05,
      "loss": 0.4825,
      "step": 6420
    },
    {
      "epoch": 2.042566709021601,
      "grad_norm": 7.352954864501953,
      "learning_rate": 2e-05,
      "loss": 0.4886,
      "step": 6430
    },
    {
      "epoch": 2.04574332909784,
      "grad_norm": 9.837128639221191,
      "learning_rate": 2e-05,
      "loss": 0.4872,
      "step": 6440
    },
    {
      "epoch": 2.0489199491740786,
      "grad_norm": 8.590714454650879,
      "learning_rate": 2e-05,
      "loss": 0.4675,
      "step": 6450
    },
    {
      "epoch": 2.0520965692503177,
      "grad_norm": 5.673531532287598,
      "learning_rate": 2e-05,
      "loss": 0.4958,
      "step": 6460
    },
    {
      "epoch": 2.0552731893265563,
      "grad_norm": 9.139788627624512,
      "learning_rate": 2e-05,
      "loss": 0.4612,
      "step": 6470
    },
    {
      "epoch": 2.0584498094027954,
      "grad_norm": 9.315732955932617,
      "learning_rate": 2e-05,
      "loss": 0.4638,
      "step": 6480
    },
    {
      "epoch": 2.0616264294790345,
      "grad_norm": 7.331208229064941,
      "learning_rate": 2e-05,
      "loss": 0.4991,
      "step": 6490
    },
    {
      "epoch": 2.064803049555273,
      "grad_norm": 4.891284465789795,
      "learning_rate": 2e-05,
      "loss": 0.4383,
      "step": 6500
    },
    {
      "epoch": 2.0679796696315123,
      "grad_norm": 10.401172637939453,
      "learning_rate": 2e-05,
      "loss": 0.4675,
      "step": 6510
    },
    {
      "epoch": 2.071156289707751,
      "grad_norm": 7.847555160522461,
      "learning_rate": 2e-05,
      "loss": 0.4485,
      "step": 6520
    },
    {
      "epoch": 2.07433290978399,
      "grad_norm": 8.759344100952148,
      "learning_rate": 2e-05,
      "loss": 0.4427,
      "step": 6530
    },
    {
      "epoch": 2.0775095298602286,
      "grad_norm": 8.947698593139648,
      "learning_rate": 2e-05,
      "loss": 0.5013,
      "step": 6540
    },
    {
      "epoch": 2.0806861499364677,
      "grad_norm": 6.138969421386719,
      "learning_rate": 2e-05,
      "loss": 0.4899,
      "step": 6550
    },
    {
      "epoch": 2.0838627700127064,
      "grad_norm": 11.08069133758545,
      "learning_rate": 2e-05,
      "loss": 0.4678,
      "step": 6560
    },
    {
      "epoch": 2.0870393900889455,
      "grad_norm": 8.60502815246582,
      "learning_rate": 2e-05,
      "loss": 0.4721,
      "step": 6570
    },
    {
      "epoch": 2.090216010165184,
      "grad_norm": 10.70083999633789,
      "learning_rate": 2e-05,
      "loss": 0.4709,
      "step": 6580
    },
    {
      "epoch": 2.093392630241423,
      "grad_norm": 10.04909610748291,
      "learning_rate": 2e-05,
      "loss": 0.4908,
      "step": 6590
    },
    {
      "epoch": 2.096569250317662,
      "grad_norm": 7.438442230224609,
      "learning_rate": 2e-05,
      "loss": 0.4938,
      "step": 6600
    },
    {
      "epoch": 2.099745870393901,
      "grad_norm": 6.14838981628418,
      "learning_rate": 2e-05,
      "loss": 0.4539,
      "step": 6610
    },
    {
      "epoch": 2.1013341804320205,
      "eval_loss": 1.985471487045288,
      "eval_mse": 1.983944036536513,
      "eval_pearson": 0.41042557905053173,
      "eval_runtime": 7.5917,
      "eval_samples_per_second": 2839.952,
      "eval_spearmanr": 0.3949329896437434,
      "eval_steps_per_second": 11.196,
      "step": 6615
    },
    {
      "epoch": 2.1029224904701396,
      "grad_norm": 8.732705116271973,
      "learning_rate": 2e-05,
      "loss": 0.4944,
      "step": 6620
    },
    {
      "epoch": 2.1060991105463787,
      "grad_norm": 6.407029628753662,
      "learning_rate": 2e-05,
      "loss": 0.5017,
      "step": 6630
    },
    {
      "epoch": 2.1092757306226178,
      "grad_norm": 6.165624618530273,
      "learning_rate": 2e-05,
      "loss": 0.4436,
      "step": 6640
    },
    {
      "epoch": 2.1124523506988564,
      "grad_norm": 6.448183059692383,
      "learning_rate": 2e-05,
      "loss": 0.4739,
      "step": 6650
    },
    {
      "epoch": 2.1156289707750955,
      "grad_norm": 7.463535308837891,
      "learning_rate": 2e-05,
      "loss": 0.4719,
      "step": 6660
    },
    {
      "epoch": 2.118805590851334,
      "grad_norm": 6.705504894256592,
      "learning_rate": 2e-05,
      "loss": 0.4871,
      "step": 6670
    },
    {
      "epoch": 2.121982210927573,
      "grad_norm": 11.52484130859375,
      "learning_rate": 2e-05,
      "loss": 0.5055,
      "step": 6680
    },
    {
      "epoch": 2.125158831003812,
      "grad_norm": 8.813976287841797,
      "learning_rate": 2e-05,
      "loss": 0.452,
      "step": 6690
    },
    {
      "epoch": 2.128335451080051,
      "grad_norm": 6.937641143798828,
      "learning_rate": 2e-05,
      "loss": 0.4363,
      "step": 6700
    },
    {
      "epoch": 2.1315120711562896,
      "grad_norm": 7.179991245269775,
      "learning_rate": 2e-05,
      "loss": 0.4466,
      "step": 6710
    },
    {
      "epoch": 2.1346886912325287,
      "grad_norm": 9.562252044677734,
      "learning_rate": 2e-05,
      "loss": 0.4761,
      "step": 6720
    },
    {
      "epoch": 2.1378653113087673,
      "grad_norm": 7.007988929748535,
      "learning_rate": 2e-05,
      "loss": 0.4704,
      "step": 6730
    },
    {
      "epoch": 2.1410419313850064,
      "grad_norm": 7.208205223083496,
      "learning_rate": 2e-05,
      "loss": 0.4608,
      "step": 6740
    },
    {
      "epoch": 2.144218551461245,
      "grad_norm": 5.501314640045166,
      "learning_rate": 2e-05,
      "loss": 0.4319,
      "step": 6750
    },
    {
      "epoch": 2.147395171537484,
      "grad_norm": 8.10190200805664,
      "learning_rate": 2e-05,
      "loss": 0.4956,
      "step": 6760
    },
    {
      "epoch": 2.150571791613723,
      "grad_norm": 12.410962104797363,
      "learning_rate": 2e-05,
      "loss": 0.4967,
      "step": 6770
    },
    {
      "epoch": 2.153748411689962,
      "grad_norm": 12.472238540649414,
      "learning_rate": 2e-05,
      "loss": 0.5043,
      "step": 6780
    },
    {
      "epoch": 2.1569250317662005,
      "grad_norm": 9.62721061706543,
      "learning_rate": 2e-05,
      "loss": 0.4663,
      "step": 6790
    },
    {
      "epoch": 2.1601016518424396,
      "grad_norm": 10.075599670410156,
      "learning_rate": 2e-05,
      "loss": 0.4659,
      "step": 6800
    },
    {
      "epoch": 2.1632782719186787,
      "grad_norm": 6.025472164154053,
      "learning_rate": 2e-05,
      "loss": 0.4774,
      "step": 6810
    },
    {
      "epoch": 2.1664548919949174,
      "grad_norm": 5.686647415161133,
      "learning_rate": 2e-05,
      "loss": 0.4731,
      "step": 6820
    },
    {
      "epoch": 2.1696315120711565,
      "grad_norm": 7.933743476867676,
      "learning_rate": 2e-05,
      "loss": 0.4604,
      "step": 6830
    },
    {
      "epoch": 2.172808132147395,
      "grad_norm": 7.238635063171387,
      "learning_rate": 2e-05,
      "loss": 0.436,
      "step": 6840
    },
    {
      "epoch": 2.175984752223634,
      "grad_norm": 9.220810890197754,
      "learning_rate": 2e-05,
      "loss": 0.4561,
      "step": 6850
    },
    {
      "epoch": 2.179161372299873,
      "grad_norm": 5.67222261428833,
      "learning_rate": 2e-05,
      "loss": 0.4652,
      "step": 6860
    },
    {
      "epoch": 2.182337992376112,
      "grad_norm": 6.455075263977051,
      "learning_rate": 2e-05,
      "loss": 0.4812,
      "step": 6870
    },
    {
      "epoch": 2.1855146124523506,
      "grad_norm": 5.708413124084473,
      "learning_rate": 2e-05,
      "loss": 0.4708,
      "step": 6880
    },
    {
      "epoch": 2.1886912325285897,
      "grad_norm": 5.652093887329102,
      "learning_rate": 2e-05,
      "loss": 0.4828,
      "step": 6890
    },
    {
      "epoch": 2.1918678526048283,
      "grad_norm": 9.098099708557129,
      "learning_rate": 2e-05,
      "loss": 0.4799,
      "step": 6900
    },
    {
      "epoch": 2.1950444726810674,
      "grad_norm": 8.125938415527344,
      "learning_rate": 2e-05,
      "loss": 0.4579,
      "step": 6910
    },
    {
      "epoch": 2.198221092757306,
      "grad_norm": 5.779496192932129,
      "learning_rate": 2e-05,
      "loss": 0.4474,
      "step": 6920
    },
    {
      "epoch": 2.201397712833545,
      "grad_norm": 7.043169021606445,
      "learning_rate": 2e-05,
      "loss": 0.4816,
      "step": 6930
    },
    {
      "epoch": 2.201397712833545,
      "eval_loss": 1.6417309045791626,
      "eval_mse": 1.6397405126873328,
      "eval_pearson": 0.38821815938577503,
      "eval_runtime": 7.4823,
      "eval_samples_per_second": 2881.454,
      "eval_spearmanr": 0.3768180260225675,
      "eval_steps_per_second": 11.36,
      "step": 6930
    },
    {
      "epoch": 2.204574332909784,
      "grad_norm": 9.777323722839355,
      "learning_rate": 2e-05,
      "loss": 0.4515,
      "step": 6940
    },
    {
      "epoch": 2.207750952986023,
      "grad_norm": 7.093116283416748,
      "learning_rate": 2e-05,
      "loss": 0.4749,
      "step": 6950
    },
    {
      "epoch": 2.210927573062262,
      "grad_norm": 11.898585319519043,
      "learning_rate": 2e-05,
      "loss": 0.4788,
      "step": 6960
    },
    {
      "epoch": 2.2141041931385006,
      "grad_norm": 6.025348663330078,
      "learning_rate": 2e-05,
      "loss": 0.4787,
      "step": 6970
    },
    {
      "epoch": 2.2172808132147397,
      "grad_norm": 9.718277931213379,
      "learning_rate": 2e-05,
      "loss": 0.4812,
      "step": 6980
    },
    {
      "epoch": 2.2204574332909783,
      "grad_norm": 6.091098785400391,
      "learning_rate": 2e-05,
      "loss": 0.4578,
      "step": 6990
    },
    {
      "epoch": 2.2236340533672174,
      "grad_norm": 9.791378021240234,
      "learning_rate": 2e-05,
      "loss": 0.4535,
      "step": 7000
    },
    {
      "epoch": 2.226810673443456,
      "grad_norm": 12.606955528259277,
      "learning_rate": 2e-05,
      "loss": 0.4716,
      "step": 7010
    },
    {
      "epoch": 2.229987293519695,
      "grad_norm": 6.780576229095459,
      "learning_rate": 2e-05,
      "loss": 0.4566,
      "step": 7020
    },
    {
      "epoch": 2.233163913595934,
      "grad_norm": 5.965950965881348,
      "learning_rate": 2e-05,
      "loss": 0.4352,
      "step": 7030
    },
    {
      "epoch": 2.236340533672173,
      "grad_norm": 5.348851203918457,
      "learning_rate": 2e-05,
      "loss": 0.4252,
      "step": 7040
    },
    {
      "epoch": 2.2395171537484115,
      "grad_norm": 6.913572311401367,
      "learning_rate": 2e-05,
      "loss": 0.4483,
      "step": 7050
    },
    {
      "epoch": 2.2426937738246506,
      "grad_norm": 7.437412261962891,
      "learning_rate": 2e-05,
      "loss": 0.4309,
      "step": 7060
    },
    {
      "epoch": 2.2458703939008893,
      "grad_norm": 9.890776634216309,
      "learning_rate": 2e-05,
      "loss": 0.4894,
      "step": 7070
    },
    {
      "epoch": 2.2490470139771284,
      "grad_norm": 10.159137725830078,
      "learning_rate": 2e-05,
      "loss": 0.4988,
      "step": 7080
    },
    {
      "epoch": 2.252223634053367,
      "grad_norm": 11.750113487243652,
      "learning_rate": 2e-05,
      "loss": 0.4832,
      "step": 7090
    },
    {
      "epoch": 2.255400254129606,
      "grad_norm": 7.349648952484131,
      "learning_rate": 2e-05,
      "loss": 0.4407,
      "step": 7100
    },
    {
      "epoch": 2.258576874205845,
      "grad_norm": 6.311980247497559,
      "learning_rate": 2e-05,
      "loss": 0.4657,
      "step": 7110
    },
    {
      "epoch": 2.261753494282084,
      "grad_norm": 5.187108039855957,
      "learning_rate": 2e-05,
      "loss": 0.4587,
      "step": 7120
    },
    {
      "epoch": 2.264930114358323,
      "grad_norm": 9.651742935180664,
      "learning_rate": 2e-05,
      "loss": 0.4477,
      "step": 7130
    },
    {
      "epoch": 2.2681067344345616,
      "grad_norm": 10.588754653930664,
      "learning_rate": 2e-05,
      "loss": 0.4504,
      "step": 7140
    },
    {
      "epoch": 2.2712833545108007,
      "grad_norm": 9.912654876708984,
      "learning_rate": 2e-05,
      "loss": 0.4788,
      "step": 7150
    },
    {
      "epoch": 2.2744599745870393,
      "grad_norm": 6.712432384490967,
      "learning_rate": 2e-05,
      "loss": 0.4823,
      "step": 7160
    },
    {
      "epoch": 2.2776365946632784,
      "grad_norm": 10.411233901977539,
      "learning_rate": 2e-05,
      "loss": 0.4587,
      "step": 7170
    },
    {
      "epoch": 2.280813214739517,
      "grad_norm": 5.733196258544922,
      "learning_rate": 2e-05,
      "loss": 0.4478,
      "step": 7180
    },
    {
      "epoch": 2.283989834815756,
      "grad_norm": 6.6156487464904785,
      "learning_rate": 2e-05,
      "loss": 0.4853,
      "step": 7190
    },
    {
      "epoch": 2.2871664548919948,
      "grad_norm": 7.177816390991211,
      "learning_rate": 2e-05,
      "loss": 0.4471,
      "step": 7200
    },
    {
      "epoch": 2.290343074968234,
      "grad_norm": 7.196031093597412,
      "learning_rate": 2e-05,
      "loss": 0.4399,
      "step": 7210
    },
    {
      "epoch": 2.2935196950444725,
      "grad_norm": 6.541018962860107,
      "learning_rate": 2e-05,
      "loss": 0.4388,
      "step": 7220
    },
    {
      "epoch": 2.2966963151207116,
      "grad_norm": 8.765022277832031,
      "learning_rate": 2e-05,
      "loss": 0.4544,
      "step": 7230
    },
    {
      "epoch": 2.2998729351969507,
      "grad_norm": 7.651186943054199,
      "learning_rate": 2e-05,
      "loss": 0.492,
      "step": 7240
    },
    {
      "epoch": 2.30146124523507,
      "eval_loss": 1.7164642810821533,
      "eval_mse": 1.7147944319310748,
      "eval_pearson": 0.40203494791103356,
      "eval_runtime": 7.5838,
      "eval_samples_per_second": 2842.917,
      "eval_spearmanr": 0.38933033856792154,
      "eval_steps_per_second": 11.208,
      "step": 7245
    },
    {
      "epoch": 2.3030495552731893,
      "grad_norm": 7.952330589294434,
      "learning_rate": 2e-05,
      "loss": 0.444,
      "step": 7250
    },
    {
      "epoch": 2.306226175349428,
      "grad_norm": 11.68187141418457,
      "learning_rate": 2e-05,
      "loss": 0.4587,
      "step": 7260
    },
    {
      "epoch": 2.309402795425667,
      "grad_norm": 7.238870143890381,
      "learning_rate": 2e-05,
      "loss": 0.4606,
      "step": 7270
    },
    {
      "epoch": 2.312579415501906,
      "grad_norm": 10.716309547424316,
      "learning_rate": 2e-05,
      "loss": 0.4516,
      "step": 7280
    },
    {
      "epoch": 2.315756035578145,
      "grad_norm": 14.93757152557373,
      "learning_rate": 2e-05,
      "loss": 0.4848,
      "step": 7290
    },
    {
      "epoch": 2.318932655654384,
      "grad_norm": 6.11715841293335,
      "learning_rate": 2e-05,
      "loss": 0.4625,
      "step": 7300
    },
    {
      "epoch": 2.3221092757306225,
      "grad_norm": 7.50869607925415,
      "learning_rate": 2e-05,
      "loss": 0.4472,
      "step": 7310
    },
    {
      "epoch": 2.3252858958068616,
      "grad_norm": 10.236553192138672,
      "learning_rate": 2e-05,
      "loss": 0.4664,
      "step": 7320
    },
    {
      "epoch": 2.3284625158831003,
      "grad_norm": 5.279016971588135,
      "learning_rate": 2e-05,
      "loss": 0.4388,
      "step": 7330
    },
    {
      "epoch": 2.3316391359593394,
      "grad_norm": 17.058290481567383,
      "learning_rate": 2e-05,
      "loss": 0.4467,
      "step": 7340
    },
    {
      "epoch": 2.334815756035578,
      "grad_norm": 6.682929992675781,
      "learning_rate": 2e-05,
      "loss": 0.4708,
      "step": 7350
    },
    {
      "epoch": 2.337992376111817,
      "grad_norm": 8.44645881652832,
      "learning_rate": 2e-05,
      "loss": 0.4763,
      "step": 7360
    },
    {
      "epoch": 2.3411689961880557,
      "grad_norm": 5.873553276062012,
      "learning_rate": 2e-05,
      "loss": 0.4516,
      "step": 7370
    },
    {
      "epoch": 2.344345616264295,
      "grad_norm": 5.408382415771484,
      "learning_rate": 2e-05,
      "loss": 0.4289,
      "step": 7380
    },
    {
      "epoch": 2.3475222363405335,
      "grad_norm": 12.46180534362793,
      "learning_rate": 2e-05,
      "loss": 0.4535,
      "step": 7390
    },
    {
      "epoch": 2.3506988564167726,
      "grad_norm": 6.188430309295654,
      "learning_rate": 2e-05,
      "loss": 0.4454,
      "step": 7400
    },
    {
      "epoch": 2.3538754764930117,
      "grad_norm": 7.336051940917969,
      "learning_rate": 2e-05,
      "loss": 0.4575,
      "step": 7410
    },
    {
      "epoch": 2.3570520965692503,
      "grad_norm": 5.270523548126221,
      "learning_rate": 2e-05,
      "loss": 0.4564,
      "step": 7420
    },
    {
      "epoch": 2.3602287166454894,
      "grad_norm": 5.50368070602417,
      "learning_rate": 2e-05,
      "loss": 0.4494,
      "step": 7430
    },
    {
      "epoch": 2.363405336721728,
      "grad_norm": 7.677380084991455,
      "learning_rate": 2e-05,
      "loss": 0.4805,
      "step": 7440
    },
    {
      "epoch": 2.366581956797967,
      "grad_norm": 16.364337921142578,
      "learning_rate": 2e-05,
      "loss": 0.4937,
      "step": 7450
    },
    {
      "epoch": 2.3697585768742058,
      "grad_norm": 8.452272415161133,
      "learning_rate": 2e-05,
      "loss": 0.4627,
      "step": 7460
    },
    {
      "epoch": 2.372935196950445,
      "grad_norm": 8.04479694366455,
      "learning_rate": 2e-05,
      "loss": 0.4488,
      "step": 7470
    },
    {
      "epoch": 2.3761118170266835,
      "grad_norm": 7.9217305183410645,
      "learning_rate": 2e-05,
      "loss": 0.4414,
      "step": 7480
    },
    {
      "epoch": 2.3792884371029226,
      "grad_norm": 13.330118179321289,
      "learning_rate": 2e-05,
      "loss": 0.4709,
      "step": 7490
    },
    {
      "epoch": 2.3824650571791612,
      "grad_norm": 6.699594974517822,
      "learning_rate": 2e-05,
      "loss": 0.4327,
      "step": 7500
    },
    {
      "epoch": 2.3856416772554003,
      "grad_norm": 6.71045446395874,
      "learning_rate": 2e-05,
      "loss": 0.4425,
      "step": 7510
    },
    {
      "epoch": 2.388818297331639,
      "grad_norm": 5.59855842590332,
      "learning_rate": 2e-05,
      "loss": 0.4668,
      "step": 7520
    },
    {
      "epoch": 2.391994917407878,
      "grad_norm": 6.322535037994385,
      "learning_rate": 2e-05,
      "loss": 0.4499,
      "step": 7530
    },
    {
      "epoch": 2.3951715374841167,
      "grad_norm": 6.781406402587891,
      "learning_rate": 2e-05,
      "loss": 0.4423,
      "step": 7540
    },
    {
      "epoch": 2.398348157560356,
      "grad_norm": 6.3846917152404785,
      "learning_rate": 2e-05,
      "loss": 0.4458,
      "step": 7550
    },
    {
      "epoch": 2.4015247776365944,
      "grad_norm": 7.704438209533691,
      "learning_rate": 2e-05,
      "loss": 0.4418,
      "step": 7560
    },
    {
      "epoch": 2.4015247776365944,
      "eval_loss": 1.7978084087371826,
      "eval_mse": 1.7963321173097075,
      "eval_pearson": 0.41838069992397253,
      "eval_runtime": 7.306,
      "eval_samples_per_second": 2951.0,
      "eval_spearmanr": 0.4033547096723465,
      "eval_steps_per_second": 11.634,
      "step": 7560
    },
    {
      "epoch": 2.4047013977128335,
      "grad_norm": 8.190739631652832,
      "learning_rate": 2e-05,
      "loss": 0.4543,
      "step": 7570
    },
    {
      "epoch": 2.4078780177890726,
      "grad_norm": 10.391599655151367,
      "learning_rate": 2e-05,
      "loss": 0.4971,
      "step": 7580
    },
    {
      "epoch": 2.4110546378653113,
      "grad_norm": 6.04283332824707,
      "learning_rate": 2e-05,
      "loss": 0.4434,
      "step": 7590
    },
    {
      "epoch": 2.4142312579415504,
      "grad_norm": 8.016902923583984,
      "learning_rate": 2e-05,
      "loss": 0.464,
      "step": 7600
    },
    {
      "epoch": 2.417407878017789,
      "grad_norm": 6.178648948669434,
      "learning_rate": 2e-05,
      "loss": 0.4585,
      "step": 7610
    },
    {
      "epoch": 2.420584498094028,
      "grad_norm": 6.6454901695251465,
      "learning_rate": 2e-05,
      "loss": 0.4357,
      "step": 7620
    },
    {
      "epoch": 2.4237611181702667,
      "grad_norm": 7.482873439788818,
      "learning_rate": 2e-05,
      "loss": 0.4304,
      "step": 7630
    },
    {
      "epoch": 2.426937738246506,
      "grad_norm": 6.00634765625,
      "learning_rate": 2e-05,
      "loss": 0.4199,
      "step": 7640
    },
    {
      "epoch": 2.4301143583227445,
      "grad_norm": 7.655211925506592,
      "learning_rate": 2e-05,
      "loss": 0.4548,
      "step": 7650
    },
    {
      "epoch": 2.4332909783989836,
      "grad_norm": 7.683920383453369,
      "learning_rate": 2e-05,
      "loss": 0.4539,
      "step": 7660
    },
    {
      "epoch": 2.436467598475222,
      "grad_norm": 11.372706413269043,
      "learning_rate": 2e-05,
      "loss": 0.4333,
      "step": 7670
    },
    {
      "epoch": 2.4396442185514613,
      "grad_norm": 6.849515914916992,
      "learning_rate": 2e-05,
      "loss": 0.458,
      "step": 7680
    },
    {
      "epoch": 2.4428208386277,
      "grad_norm": 5.526314735412598,
      "learning_rate": 2e-05,
      "loss": 0.448,
      "step": 7690
    },
    {
      "epoch": 2.445997458703939,
      "grad_norm": 5.30673885345459,
      "learning_rate": 2e-05,
      "loss": 0.4562,
      "step": 7700
    },
    {
      "epoch": 2.449174078780178,
      "grad_norm": 5.6552557945251465,
      "learning_rate": 2e-05,
      "loss": 0.4518,
      "step": 7710
    },
    {
      "epoch": 2.4523506988564168,
      "grad_norm": 9.810567855834961,
      "learning_rate": 2e-05,
      "loss": 0.4255,
      "step": 7720
    },
    {
      "epoch": 2.4555273189326554,
      "grad_norm": 5.012530326843262,
      "learning_rate": 2e-05,
      "loss": 0.4393,
      "step": 7730
    },
    {
      "epoch": 2.4587039390088945,
      "grad_norm": 6.3843560218811035,
      "learning_rate": 2e-05,
      "loss": 0.4541,
      "step": 7740
    },
    {
      "epoch": 2.4618805590851336,
      "grad_norm": 5.649569034576416,
      "learning_rate": 2e-05,
      "loss": 0.4309,
      "step": 7750
    },
    {
      "epoch": 2.4650571791613722,
      "grad_norm": 9.286011695861816,
      "learning_rate": 2e-05,
      "loss": 0.4617,
      "step": 7760
    },
    {
      "epoch": 2.4682337992376113,
      "grad_norm": 14.522283554077148,
      "learning_rate": 2e-05,
      "loss": 0.4739,
      "step": 7770
    },
    {
      "epoch": 2.47141041931385,
      "grad_norm": 7.228809356689453,
      "learning_rate": 2e-05,
      "loss": 0.4448,
      "step": 7780
    },
    {
      "epoch": 2.474587039390089,
      "grad_norm": 8.303199768066406,
      "learning_rate": 2e-05,
      "loss": 0.4254,
      "step": 7790
    },
    {
      "epoch": 2.4777636594663277,
      "grad_norm": 6.667778968811035,
      "learning_rate": 2e-05,
      "loss": 0.4403,
      "step": 7800
    },
    {
      "epoch": 2.480940279542567,
      "grad_norm": 6.036048889160156,
      "learning_rate": 2e-05,
      "loss": 0.429,
      "step": 7810
    },
    {
      "epoch": 2.4841168996188054,
      "grad_norm": 9.444075584411621,
      "learning_rate": 2e-05,
      "loss": 0.4611,
      "step": 7820
    },
    {
      "epoch": 2.4872935196950445,
      "grad_norm": 5.524951457977295,
      "learning_rate": 2e-05,
      "loss": 0.4219,
      "step": 7830
    },
    {
      "epoch": 2.490470139771283,
      "grad_norm": 8.08026123046875,
      "learning_rate": 2e-05,
      "loss": 0.4514,
      "step": 7840
    },
    {
      "epoch": 2.4936467598475223,
      "grad_norm": 12.595657348632812,
      "learning_rate": 2e-05,
      "loss": 0.4499,
      "step": 7850
    },
    {
      "epoch": 2.496823379923761,
      "grad_norm": 6.455231189727783,
      "learning_rate": 2e-05,
      "loss": 0.4362,
      "step": 7860
    },
    {
      "epoch": 2.5,
      "grad_norm": 7.669276237487793,
      "learning_rate": 2e-05,
      "loss": 0.4472,
      "step": 7870
    },
    {
      "epoch": 2.5015883100381195,
      "eval_loss": 1.7905844449996948,
      "eval_mse": 1.7884227770330567,
      "eval_pearson": 0.4229255642615928,
      "eval_runtime": 7.3873,
      "eval_samples_per_second": 2918.518,
      "eval_spearmanr": 0.4083281263004161,
      "eval_steps_per_second": 11.506,
      "step": 7875
    },
    {
      "epoch": 2.503176620076239,
      "grad_norm": 6.4424519538879395,
      "learning_rate": 2e-05,
      "loss": 0.4266,
      "step": 7880
    },
    {
      "epoch": 2.5063532401524777,
      "grad_norm": 6.004136085510254,
      "learning_rate": 2e-05,
      "loss": 0.4564,
      "step": 7890
    },
    {
      "epoch": 2.5095298602287164,
      "grad_norm": 5.456489562988281,
      "learning_rate": 2e-05,
      "loss": 0.4286,
      "step": 7900
    },
    {
      "epoch": 2.5127064803049555,
      "grad_norm": 8.861396789550781,
      "learning_rate": 2e-05,
      "loss": 0.4363,
      "step": 7910
    },
    {
      "epoch": 2.5158831003811946,
      "grad_norm": 4.954570770263672,
      "learning_rate": 2e-05,
      "loss": 0.4267,
      "step": 7920
    },
    {
      "epoch": 2.519059720457433,
      "grad_norm": 9.23414134979248,
      "learning_rate": 2e-05,
      "loss": 0.4432,
      "step": 7930
    },
    {
      "epoch": 2.5222363405336723,
      "grad_norm": 14.276975631713867,
      "learning_rate": 2e-05,
      "loss": 0.456,
      "step": 7940
    },
    {
      "epoch": 2.525412960609911,
      "grad_norm": 10.398425102233887,
      "learning_rate": 2e-05,
      "loss": 0.4343,
      "step": 7950
    },
    {
      "epoch": 2.52858958068615,
      "grad_norm": 7.042468070983887,
      "learning_rate": 2e-05,
      "loss": 0.4289,
      "step": 7960
    },
    {
      "epoch": 2.5317662007623887,
      "grad_norm": 6.679754734039307,
      "learning_rate": 2e-05,
      "loss": 0.4338,
      "step": 7970
    },
    {
      "epoch": 2.5349428208386278,
      "grad_norm": 8.93626594543457,
      "learning_rate": 2e-05,
      "loss": 0.4583,
      "step": 7980
    },
    {
      "epoch": 2.5381194409148664,
      "grad_norm": 7.705271244049072,
      "learning_rate": 2e-05,
      "loss": 0.4505,
      "step": 7990
    },
    {
      "epoch": 2.5412960609911055,
      "grad_norm": 6.29357385635376,
      "learning_rate": 2e-05,
      "loss": 0.4156,
      "step": 8000
    },
    {
      "epoch": 2.5444726810673446,
      "grad_norm": 6.75706672668457,
      "learning_rate": 2e-05,
      "loss": 0.4262,
      "step": 8010
    },
    {
      "epoch": 2.5476493011435832,
      "grad_norm": 7.365876197814941,
      "learning_rate": 2e-05,
      "loss": 0.4177,
      "step": 8020
    },
    {
      "epoch": 2.550825921219822,
      "grad_norm": 8.643160820007324,
      "learning_rate": 2e-05,
      "loss": 0.4518,
      "step": 8030
    },
    {
      "epoch": 2.554002541296061,
      "grad_norm": 7.557302474975586,
      "learning_rate": 2e-05,
      "loss": 0.4238,
      "step": 8040
    },
    {
      "epoch": 2.5571791613723,
      "grad_norm": 5.812878608703613,
      "learning_rate": 2e-05,
      "loss": 0.4518,
      "step": 8050
    },
    {
      "epoch": 2.5603557814485387,
      "grad_norm": 6.668148517608643,
      "learning_rate": 2e-05,
      "loss": 0.4277,
      "step": 8060
    },
    {
      "epoch": 2.563532401524778,
      "grad_norm": 7.292388439178467,
      "learning_rate": 2e-05,
      "loss": 0.4276,
      "step": 8070
    },
    {
      "epoch": 2.5667090216010164,
      "grad_norm": 9.260751724243164,
      "learning_rate": 2e-05,
      "loss": 0.4201,
      "step": 8080
    },
    {
      "epoch": 2.5698856416772555,
      "grad_norm": 6.091033935546875,
      "learning_rate": 2e-05,
      "loss": 0.436,
      "step": 8090
    },
    {
      "epoch": 2.573062261753494,
      "grad_norm": 7.961021423339844,
      "learning_rate": 2e-05,
      "loss": 0.442,
      "step": 8100
    },
    {
      "epoch": 2.5762388818297333,
      "grad_norm": 6.193151950836182,
      "learning_rate": 2e-05,
      "loss": 0.4419,
      "step": 8110
    },
    {
      "epoch": 2.579415501905972,
      "grad_norm": 6.600782871246338,
      "learning_rate": 2e-05,
      "loss": 0.4348,
      "step": 8120
    },
    {
      "epoch": 2.582592121982211,
      "grad_norm": 7.808441162109375,
      "learning_rate": 2e-05,
      "loss": 0.4214,
      "step": 8130
    },
    {
      "epoch": 2.5857687420584496,
      "grad_norm": 7.5038743019104,
      "learning_rate": 2e-05,
      "loss": 0.4579,
      "step": 8140
    },
    {
      "epoch": 2.5889453621346887,
      "grad_norm": 8.117643356323242,
      "learning_rate": 2e-05,
      "loss": 0.4157,
      "step": 8150
    },
    {
      "epoch": 2.5921219822109274,
      "grad_norm": 7.257579326629639,
      "learning_rate": 2e-05,
      "loss": 0.4161,
      "step": 8160
    },
    {
      "epoch": 2.5952986022871665,
      "grad_norm": 5.299810886383057,
      "learning_rate": 2e-05,
      "loss": 0.4027,
      "step": 8170
    },
    {
      "epoch": 2.5984752223634056,
      "grad_norm": 10.061535835266113,
      "learning_rate": 2e-05,
      "loss": 0.4179,
      "step": 8180
    },
    {
      "epoch": 2.601651842439644,
      "grad_norm": 7.045886039733887,
      "learning_rate": 2e-05,
      "loss": 0.4298,
      "step": 8190
    },
    {
      "epoch": 2.601651842439644,
      "eval_loss": 1.9730831384658813,
      "eval_mse": 1.9711005842226776,
      "eval_pearson": 0.4288265885310906,
      "eval_runtime": 7.5787,
      "eval_samples_per_second": 2844.826,
      "eval_spearmanr": 0.4164049179791536,
      "eval_steps_per_second": 11.216,
      "step": 8190
    },
    {
      "epoch": 2.604828462515883,
      "grad_norm": 6.390346527099609,
      "learning_rate": 2e-05,
      "loss": 0.4435,
      "step": 8200
    },
    {
      "epoch": 2.608005082592122,
      "grad_norm": 7.49127197265625,
      "learning_rate": 2e-05,
      "loss": 0.4498,
      "step": 8210
    },
    {
      "epoch": 2.611181702668361,
      "grad_norm": 6.457354545593262,
      "learning_rate": 2e-05,
      "loss": 0.417,
      "step": 8220
    },
    {
      "epoch": 2.6143583227445997,
      "grad_norm": 7.588809490203857,
      "learning_rate": 2e-05,
      "loss": 0.4444,
      "step": 8230
    },
    {
      "epoch": 2.6175349428208388,
      "grad_norm": 10.64474105834961,
      "learning_rate": 2e-05,
      "loss": 0.4212,
      "step": 8240
    },
    {
      "epoch": 2.6207115628970774,
      "grad_norm": 6.377129077911377,
      "learning_rate": 2e-05,
      "loss": 0.4344,
      "step": 8250
    },
    {
      "epoch": 2.6238881829733165,
      "grad_norm": 5.4808125495910645,
      "learning_rate": 2e-05,
      "loss": 0.4354,
      "step": 8260
    },
    {
      "epoch": 2.627064803049555,
      "grad_norm": 8.8563232421875,
      "learning_rate": 2e-05,
      "loss": 0.4292,
      "step": 8270
    },
    {
      "epoch": 2.6302414231257942,
      "grad_norm": 9.912458419799805,
      "learning_rate": 2e-05,
      "loss": 0.4275,
      "step": 8280
    },
    {
      "epoch": 2.633418043202033,
      "grad_norm": 9.180883407592773,
      "learning_rate": 2e-05,
      "loss": 0.4253,
      "step": 8290
    },
    {
      "epoch": 2.636594663278272,
      "grad_norm": 7.865004062652588,
      "learning_rate": 2e-05,
      "loss": 0.4337,
      "step": 8300
    },
    {
      "epoch": 2.639771283354511,
      "grad_norm": 7.5495147705078125,
      "learning_rate": 2e-05,
      "loss": 0.4561,
      "step": 8310
    },
    {
      "epoch": 2.6429479034307497,
      "grad_norm": 7.921594619750977,
      "learning_rate": 2e-05,
      "loss": 0.4363,
      "step": 8320
    },
    {
      "epoch": 2.6461245235069883,
      "grad_norm": 9.116625785827637,
      "learning_rate": 2e-05,
      "loss": 0.4486,
      "step": 8330
    },
    {
      "epoch": 2.6493011435832274,
      "grad_norm": 9.915578842163086,
      "learning_rate": 2e-05,
      "loss": 0.4329,
      "step": 8340
    },
    {
      "epoch": 2.6524777636594665,
      "grad_norm": 7.410239219665527,
      "learning_rate": 2e-05,
      "loss": 0.3952,
      "step": 8350
    },
    {
      "epoch": 2.655654383735705,
      "grad_norm": 6.167774677276611,
      "learning_rate": 2e-05,
      "loss": 0.4173,
      "step": 8360
    },
    {
      "epoch": 2.658831003811944,
      "grad_norm": 9.886702537536621,
      "learning_rate": 2e-05,
      "loss": 0.4099,
      "step": 8370
    },
    {
      "epoch": 2.662007623888183,
      "grad_norm": 7.238011360168457,
      "learning_rate": 2e-05,
      "loss": 0.4328,
      "step": 8380
    },
    {
      "epoch": 2.665184243964422,
      "grad_norm": 8.12236499786377,
      "learning_rate": 2e-05,
      "loss": 0.4221,
      "step": 8390
    },
    {
      "epoch": 2.6683608640406606,
      "grad_norm": 7.268558979034424,
      "learning_rate": 2e-05,
      "loss": 0.4096,
      "step": 8400
    },
    {
      "epoch": 2.6715374841168997,
      "grad_norm": 10.803645133972168,
      "learning_rate": 2e-05,
      "loss": 0.3894,
      "step": 8410
    },
    {
      "epoch": 2.6747141041931384,
      "grad_norm": 16.932865142822266,
      "learning_rate": 2e-05,
      "loss": 0.4485,
      "step": 8420
    },
    {
      "epoch": 2.6778907242693775,
      "grad_norm": 5.870325565338135,
      "learning_rate": 2e-05,
      "loss": 0.4633,
      "step": 8430
    },
    {
      "epoch": 2.681067344345616,
      "grad_norm": 13.847900390625,
      "learning_rate": 2e-05,
      "loss": 0.415,
      "step": 8440
    },
    {
      "epoch": 2.684243964421855,
      "grad_norm": 7.066134929656982,
      "learning_rate": 2e-05,
      "loss": 0.412,
      "step": 8450
    },
    {
      "epoch": 2.687420584498094,
      "grad_norm": 13.191446304321289,
      "learning_rate": 2e-05,
      "loss": 0.4417,
      "step": 8460
    },
    {
      "epoch": 2.690597204574333,
      "grad_norm": 12.975982666015625,
      "learning_rate": 2e-05,
      "loss": 0.4142,
      "step": 8470
    },
    {
      "epoch": 2.693773824650572,
      "grad_norm": 8.528519630432129,
      "learning_rate": 2e-05,
      "loss": 0.4264,
      "step": 8480
    },
    {
      "epoch": 2.6969504447268107,
      "grad_norm": 11.142606735229492,
      "learning_rate": 2e-05,
      "loss": 0.4221,
      "step": 8490
    },
    {
      "epoch": 2.7001270648030493,
      "grad_norm": 8.429957389831543,
      "learning_rate": 2e-05,
      "loss": 0.4114,
      "step": 8500
    },
    {
      "epoch": 2.701715374841169,
      "eval_loss": 1.7455604076385498,
      "eval_mse": 1.7442172273412928,
      "eval_pearson": 0.3933431494708963,
      "eval_runtime": 7.4691,
      "eval_samples_per_second": 2886.569,
      "eval_spearmanr": 0.3830712982835591,
      "eval_steps_per_second": 11.38,
      "step": 8505
    },
    {
      "epoch": 2.7033036848792884,
      "grad_norm": 6.674666404724121,
      "learning_rate": 2e-05,
      "loss": 0.4055,
      "step": 8510
    },
    {
      "epoch": 2.7064803049555275,
      "grad_norm": 7.671877384185791,
      "learning_rate": 2e-05,
      "loss": 0.4091,
      "step": 8520
    },
    {
      "epoch": 2.709656925031766,
      "grad_norm": 10.271172523498535,
      "learning_rate": 2e-05,
      "loss": 0.4239,
      "step": 8530
    },
    {
      "epoch": 2.7128335451080052,
      "grad_norm": 8.380353927612305,
      "learning_rate": 2e-05,
      "loss": 0.4416,
      "step": 8540
    },
    {
      "epoch": 2.716010165184244,
      "grad_norm": 11.699953079223633,
      "learning_rate": 2e-05,
      "loss": 0.421,
      "step": 8550
    },
    {
      "epoch": 2.719186785260483,
      "grad_norm": 7.994764804840088,
      "learning_rate": 2e-05,
      "loss": 0.4281,
      "step": 8560
    },
    {
      "epoch": 2.7223634053367216,
      "grad_norm": 8.014800071716309,
      "learning_rate": 2e-05,
      "loss": 0.4111,
      "step": 8570
    },
    {
      "epoch": 2.7255400254129607,
      "grad_norm": 6.510680675506592,
      "learning_rate": 2e-05,
      "loss": 0.3989,
      "step": 8580
    },
    {
      "epoch": 2.7287166454891993,
      "grad_norm": 12.143143653869629,
      "learning_rate": 2e-05,
      "loss": 0.4252,
      "step": 8590
    },
    {
      "epoch": 2.7318932655654384,
      "grad_norm": 8.036783218383789,
      "learning_rate": 2e-05,
      "loss": 0.4214,
      "step": 8600
    },
    {
      "epoch": 2.7350698856416775,
      "grad_norm": 4.729624271392822,
      "learning_rate": 2e-05,
      "loss": 0.4093,
      "step": 8610
    },
    {
      "epoch": 2.738246505717916,
      "grad_norm": 9.970498085021973,
      "learning_rate": 2e-05,
      "loss": 0.4046,
      "step": 8620
    },
    {
      "epoch": 2.741423125794155,
      "grad_norm": 9.063440322875977,
      "learning_rate": 2e-05,
      "loss": 0.4454,
      "step": 8630
    },
    {
      "epoch": 2.744599745870394,
      "grad_norm": 9.506174087524414,
      "learning_rate": 2e-05,
      "loss": 0.3729,
      "step": 8640
    },
    {
      "epoch": 2.747776365946633,
      "grad_norm": 6.767229080200195,
      "learning_rate": 2e-05,
      "loss": 0.411,
      "step": 8650
    },
    {
      "epoch": 2.7509529860228716,
      "grad_norm": 8.687923431396484,
      "learning_rate": 2e-05,
      "loss": 0.3852,
      "step": 8660
    },
    {
      "epoch": 2.7541296060991103,
      "grad_norm": 9.436406135559082,
      "learning_rate": 2e-05,
      "loss": 0.4143,
      "step": 8670
    },
    {
      "epoch": 2.7573062261753494,
      "grad_norm": 6.2429704666137695,
      "learning_rate": 2e-05,
      "loss": 0.4248,
      "step": 8680
    },
    {
      "epoch": 2.7604828462515885,
      "grad_norm": 6.382792949676514,
      "learning_rate": 2e-05,
      "loss": 0.3986,
      "step": 8690
    },
    {
      "epoch": 2.763659466327827,
      "grad_norm": 6.159056186676025,
      "learning_rate": 2e-05,
      "loss": 0.4231,
      "step": 8700
    },
    {
      "epoch": 2.766836086404066,
      "grad_norm": 6.458274841308594,
      "learning_rate": 2e-05,
      "loss": 0.4134,
      "step": 8710
    },
    {
      "epoch": 2.770012706480305,
      "grad_norm": 6.839884281158447,
      "learning_rate": 2e-05,
      "loss": 0.4127,
      "step": 8720
    },
    {
      "epoch": 2.773189326556544,
      "grad_norm": 7.323084354400635,
      "learning_rate": 2e-05,
      "loss": 0.3976,
      "step": 8730
    },
    {
      "epoch": 2.7763659466327826,
      "grad_norm": 5.6859450340271,
      "learning_rate": 2e-05,
      "loss": 0.4225,
      "step": 8740
    },
    {
      "epoch": 2.7795425667090217,
      "grad_norm": 7.718360424041748,
      "learning_rate": 2e-05,
      "loss": 0.4142,
      "step": 8750
    },
    {
      "epoch": 2.7827191867852603,
      "grad_norm": 13.596260070800781,
      "learning_rate": 2e-05,
      "loss": 0.4244,
      "step": 8760
    },
    {
      "epoch": 2.7858958068614994,
      "grad_norm": 5.573570251464844,
      "learning_rate": 2e-05,
      "loss": 0.4123,
      "step": 8770
    },
    {
      "epoch": 2.7890724269377385,
      "grad_norm": 7.114290714263916,
      "learning_rate": 2e-05,
      "loss": 0.4224,
      "step": 8780
    },
    {
      "epoch": 2.792249047013977,
      "grad_norm": 4.603968620300293,
      "learning_rate": 2e-05,
      "loss": 0.3912,
      "step": 8790
    },
    {
      "epoch": 2.795425667090216,
      "grad_norm": 6.934799671173096,
      "learning_rate": 2e-05,
      "loss": 0.4191,
      "step": 8800
    },
    {
      "epoch": 2.798602287166455,
      "grad_norm": 9.553873062133789,
      "learning_rate": 2e-05,
      "loss": 0.3962,
      "step": 8810
    },
    {
      "epoch": 2.801778907242694,
      "grad_norm": 7.6343584060668945,
      "learning_rate": 2e-05,
      "loss": 0.4124,
      "step": 8820
    },
    {
      "epoch": 2.801778907242694,
      "eval_loss": 1.9143030643463135,
      "eval_mse": 1.9123510684175262,
      "eval_pearson": 0.3938889713995661,
      "eval_runtime": 7.3829,
      "eval_samples_per_second": 2920.274,
      "eval_spearmanr": 0.3850376197440244,
      "eval_steps_per_second": 11.513,
      "step": 8820
    },
    {
      "epoch": 2.8049555273189326,
      "grad_norm": 6.8554301261901855,
      "learning_rate": 2e-05,
      "loss": 0.4094,
      "step": 8830
    },
    {
      "epoch": 2.8081321473951717,
      "grad_norm": 6.406148910522461,
      "learning_rate": 2e-05,
      "loss": 0.4287,
      "step": 8840
    },
    {
      "epoch": 2.8113087674714103,
      "grad_norm": 7.15477180480957,
      "learning_rate": 2e-05,
      "loss": 0.4183,
      "step": 8850
    },
    {
      "epoch": 2.8144853875476494,
      "grad_norm": 7.39546537399292,
      "learning_rate": 2e-05,
      "loss": 0.3995,
      "step": 8860
    },
    {
      "epoch": 2.817662007623888,
      "grad_norm": 5.8056817054748535,
      "learning_rate": 2e-05,
      "loss": 0.4112,
      "step": 8870
    },
    {
      "epoch": 2.820838627700127,
      "grad_norm": 8.849764823913574,
      "learning_rate": 2e-05,
      "loss": 0.4195,
      "step": 8880
    },
    {
      "epoch": 2.824015247776366,
      "grad_norm": 8.13556957244873,
      "learning_rate": 2e-05,
      "loss": 0.45,
      "step": 8890
    },
    {
      "epoch": 2.827191867852605,
      "grad_norm": 6.585310935974121,
      "learning_rate": 2e-05,
      "loss": 0.4219,
      "step": 8900
    },
    {
      "epoch": 2.8303684879288435,
      "grad_norm": 4.692183971405029,
      "learning_rate": 2e-05,
      "loss": 0.3828,
      "step": 8910
    },
    {
      "epoch": 2.8335451080050826,
      "grad_norm": 5.558744430541992,
      "learning_rate": 2e-05,
      "loss": 0.3974,
      "step": 8920
    },
    {
      "epoch": 2.8367217280813213,
      "grad_norm": 7.508631229400635,
      "learning_rate": 2e-05,
      "loss": 0.3904,
      "step": 8930
    },
    {
      "epoch": 2.8398983481575604,
      "grad_norm": 5.671886444091797,
      "learning_rate": 2e-05,
      "loss": 0.3873,
      "step": 8940
    },
    {
      "epoch": 2.8430749682337995,
      "grad_norm": 7.9682135581970215,
      "learning_rate": 2e-05,
      "loss": 0.4431,
      "step": 8950
    },
    {
      "epoch": 2.846251588310038,
      "grad_norm": 10.274255752563477,
      "learning_rate": 2e-05,
      "loss": 0.4253,
      "step": 8960
    },
    {
      "epoch": 2.8494282083862768,
      "grad_norm": 12.167844772338867,
      "learning_rate": 2e-05,
      "loss": 0.4399,
      "step": 8970
    },
    {
      "epoch": 2.852604828462516,
      "grad_norm": 13.058037757873535,
      "learning_rate": 2e-05,
      "loss": 0.4423,
      "step": 8980
    },
    {
      "epoch": 2.855781448538755,
      "grad_norm": 5.425501346588135,
      "learning_rate": 2e-05,
      "loss": 0.4034,
      "step": 8990
    },
    {
      "epoch": 2.8589580686149936,
      "grad_norm": 7.908729553222656,
      "learning_rate": 2e-05,
      "loss": 0.4422,
      "step": 9000
    },
    {
      "epoch": 2.8621346886912327,
      "grad_norm": 5.30088472366333,
      "learning_rate": 2e-05,
      "loss": 0.4206,
      "step": 9010
    },
    {
      "epoch": 2.8653113087674713,
      "grad_norm": 8.631181716918945,
      "learning_rate": 2e-05,
      "loss": 0.4208,
      "step": 9020
    },
    {
      "epoch": 2.8684879288437104,
      "grad_norm": 8.95977783203125,
      "learning_rate": 2e-05,
      "loss": 0.3903,
      "step": 9030
    },
    {
      "epoch": 2.871664548919949,
      "grad_norm": 6.198479175567627,
      "learning_rate": 2e-05,
      "loss": 0.3895,
      "step": 9040
    },
    {
      "epoch": 2.874841168996188,
      "grad_norm": 5.804186820983887,
      "learning_rate": 2e-05,
      "loss": 0.4023,
      "step": 9050
    },
    {
      "epoch": 2.878017789072427,
      "grad_norm": 7.759317398071289,
      "learning_rate": 2e-05,
      "loss": 0.4162,
      "step": 9060
    },
    {
      "epoch": 2.881194409148666,
      "grad_norm": 5.741779327392578,
      "learning_rate": 2e-05,
      "loss": 0.4117,
      "step": 9070
    },
    {
      "epoch": 2.884371029224905,
      "grad_norm": 10.500414848327637,
      "learning_rate": 2e-05,
      "loss": 0.3905,
      "step": 9080
    },
    {
      "epoch": 2.8875476493011436,
      "grad_norm": 5.96759557723999,
      "learning_rate": 2e-05,
      "loss": 0.4146,
      "step": 9090
    },
    {
      "epoch": 2.8907242693773822,
      "grad_norm": 7.860116004943848,
      "learning_rate": 2e-05,
      "loss": 0.4033,
      "step": 9100
    },
    {
      "epoch": 2.8939008894536213,
      "grad_norm": 8.330412864685059,
      "learning_rate": 2e-05,
      "loss": 0.4012,
      "step": 9110
    },
    {
      "epoch": 2.8970775095298604,
      "grad_norm": 10.71135425567627,
      "learning_rate": 2e-05,
      "loss": 0.4081,
      "step": 9120
    },
    {
      "epoch": 2.900254129606099,
      "grad_norm": 6.442282199859619,
      "learning_rate": 2e-05,
      "loss": 0.3916,
      "step": 9130
    },
    {
      "epoch": 2.9018424396442186,
      "eval_loss": 1.7741738557815552,
      "eval_mse": 1.7729434506680837,
      "eval_pearson": 0.38446522107123154,
      "eval_runtime": 7.2885,
      "eval_samples_per_second": 2958.085,
      "eval_spearmanr": 0.3770884642318358,
      "eval_steps_per_second": 11.662,
      "step": 9135
    },
    {
      "epoch": 2.9034307496823377,
      "grad_norm": 6.716885089874268,
      "learning_rate": 2e-05,
      "loss": 0.4449,
      "step": 9140
    },
    {
      "epoch": 2.906607369758577,
      "grad_norm": 6.015932083129883,
      "learning_rate": 2e-05,
      "loss": 0.3901,
      "step": 9150
    },
    {
      "epoch": 2.909783989834816,
      "grad_norm": 6.903626441955566,
      "learning_rate": 2e-05,
      "loss": 0.3948,
      "step": 9160
    },
    {
      "epoch": 2.9129606099110545,
      "grad_norm": 6.4226789474487305,
      "learning_rate": 2e-05,
      "loss": 0.4024,
      "step": 9170
    },
    {
      "epoch": 2.9161372299872936,
      "grad_norm": 7.094135761260986,
      "learning_rate": 2e-05,
      "loss": 0.4164,
      "step": 9180
    },
    {
      "epoch": 2.9193138500635323,
      "grad_norm": 5.447540283203125,
      "learning_rate": 2e-05,
      "loss": 0.4126,
      "step": 9190
    },
    {
      "epoch": 2.9224904701397714,
      "grad_norm": 5.869362831115723,
      "learning_rate": 2e-05,
      "loss": 0.4045,
      "step": 9200
    },
    {
      "epoch": 2.92566709021601,
      "grad_norm": 7.059160232543945,
      "learning_rate": 2e-05,
      "loss": 0.3941,
      "step": 9210
    },
    {
      "epoch": 2.928843710292249,
      "grad_norm": 6.839916706085205,
      "learning_rate": 2e-05,
      "loss": 0.4225,
      "step": 9220
    },
    {
      "epoch": 2.9320203303684877,
      "grad_norm": 6.519367694854736,
      "learning_rate": 2e-05,
      "loss": 0.3883,
      "step": 9230
    },
    {
      "epoch": 2.935196950444727,
      "grad_norm": 6.108266353607178,
      "learning_rate": 2e-05,
      "loss": 0.3987,
      "step": 9240
    },
    {
      "epoch": 2.938373570520966,
      "grad_norm": 10.695695877075195,
      "learning_rate": 2e-05,
      "loss": 0.3974,
      "step": 9250
    },
    {
      "epoch": 2.9415501905972046,
      "grad_norm": 7.526893615722656,
      "learning_rate": 2e-05,
      "loss": 0.4021,
      "step": 9260
    },
    {
      "epoch": 2.944726810673443,
      "grad_norm": 5.384827136993408,
      "learning_rate": 2e-05,
      "loss": 0.3926,
      "step": 9270
    },
    {
      "epoch": 2.9479034307496823,
      "grad_norm": 6.35385799407959,
      "learning_rate": 2e-05,
      "loss": 0.4087,
      "step": 9280
    },
    {
      "epoch": 2.9510800508259214,
      "grad_norm": 11.815314292907715,
      "learning_rate": 2e-05,
      "loss": 0.4002,
      "step": 9290
    },
    {
      "epoch": 2.95425667090216,
      "grad_norm": 12.535730361938477,
      "learning_rate": 2e-05,
      "loss": 0.4067,
      "step": 9300
    },
    {
      "epoch": 2.957433290978399,
      "grad_norm": 11.471912384033203,
      "learning_rate": 2e-05,
      "loss": 0.3915,
      "step": 9310
    },
    {
      "epoch": 2.9606099110546378,
      "grad_norm": 6.313663005828857,
      "learning_rate": 2e-05,
      "loss": 0.3851,
      "step": 9320
    },
    {
      "epoch": 2.963786531130877,
      "grad_norm": 7.067392349243164,
      "learning_rate": 2e-05,
      "loss": 0.4274,
      "step": 9330
    },
    {
      "epoch": 2.9669631512071155,
      "grad_norm": 5.6882781982421875,
      "learning_rate": 2e-05,
      "loss": 0.3859,
      "step": 9340
    },
    {
      "epoch": 2.9701397712833546,
      "grad_norm": 7.278969764709473,
      "learning_rate": 2e-05,
      "loss": 0.4027,
      "step": 9350
    },
    {
      "epoch": 2.9733163913595932,
      "grad_norm": 6.345293045043945,
      "learning_rate": 2e-05,
      "loss": 0.4074,
      "step": 9360
    },
    {
      "epoch": 2.9764930114358323,
      "grad_norm": 5.568417072296143,
      "learning_rate": 2e-05,
      "loss": 0.381,
      "step": 9370
    },
    {
      "epoch": 2.9796696315120714,
      "grad_norm": 6.3632283210754395,
      "learning_rate": 2e-05,
      "loss": 0.3945,
      "step": 9380
    },
    {
      "epoch": 2.98284625158831,
      "grad_norm": 5.7278923988342285,
      "learning_rate": 2e-05,
      "loss": 0.4099,
      "step": 9390
    },
    {
      "epoch": 2.9860228716645487,
      "grad_norm": 5.284567832946777,
      "learning_rate": 2e-05,
      "loss": 0.3792,
      "step": 9400
    },
    {
      "epoch": 2.989199491740788,
      "grad_norm": 5.059900760650635,
      "learning_rate": 2e-05,
      "loss": 0.3964,
      "step": 9410
    },
    {
      "epoch": 2.992376111817027,
      "grad_norm": 5.652832508087158,
      "learning_rate": 2e-05,
      "loss": 0.4011,
      "step": 9420
    },
    {
      "epoch": 2.9955527318932655,
      "grad_norm": 6.056999206542969,
      "learning_rate": 2e-05,
      "loss": 0.4055,
      "step": 9430
    },
    {
      "epoch": 2.998729351969504,
      "grad_norm": 9.35781192779541,
      "learning_rate": 2e-05,
      "loss": 0.4057,
      "step": 9440
    },
    {
      "epoch": 3.0019059720457433,
      "grad_norm": 5.110934734344482,
      "learning_rate": 2e-05,
      "loss": 0.4007,
      "step": 9450
    },
    {
      "epoch": 3.0019059720457433,
      "eval_loss": 1.7537062168121338,
      "eval_mse": 1.7517115649354258,
      "eval_pearson": 0.40678062411122623,
      "eval_runtime": 7.3892,
      "eval_samples_per_second": 2917.772,
      "eval_spearmanr": 0.3997734452265875,
      "eval_steps_per_second": 11.503,
      "step": 9450
    },
    {
      "epoch": 3.0050825921219824,
      "grad_norm": 10.440732955932617,
      "learning_rate": 2e-05,
      "loss": 0.3694,
      "step": 9460
    },
    {
      "epoch": 3.008259212198221,
      "grad_norm": 6.789734840393066,
      "learning_rate": 2e-05,
      "loss": 0.3743,
      "step": 9470
    },
    {
      "epoch": 3.01143583227446,
      "grad_norm": 5.349228382110596,
      "learning_rate": 2e-05,
      "loss": 0.3593,
      "step": 9480
    },
    {
      "epoch": 3.0146124523506987,
      "grad_norm": 6.691859245300293,
      "learning_rate": 2e-05,
      "loss": 0.344,
      "step": 9490
    },
    {
      "epoch": 3.017789072426938,
      "grad_norm": 5.783539295196533,
      "learning_rate": 2e-05,
      "loss": 0.3733,
      "step": 9500
    },
    {
      "epoch": 3.0209656925031765,
      "grad_norm": 6.6228766441345215,
      "learning_rate": 2e-05,
      "loss": 0.3753,
      "step": 9510
    },
    {
      "epoch": 3.0241423125794156,
      "grad_norm": 8.514884948730469,
      "learning_rate": 2e-05,
      "loss": 0.3518,
      "step": 9520
    },
    {
      "epoch": 3.027318932655654,
      "grad_norm": 5.356919765472412,
      "learning_rate": 2e-05,
      "loss": 0.3729,
      "step": 9530
    },
    {
      "epoch": 3.0304955527318933,
      "grad_norm": 5.700867652893066,
      "learning_rate": 2e-05,
      "loss": 0.3687,
      "step": 9540
    },
    {
      "epoch": 3.033672172808132,
      "grad_norm": 6.905707359313965,
      "learning_rate": 2e-05,
      "loss": 0.3497,
      "step": 9550
    },
    {
      "epoch": 3.036848792884371,
      "grad_norm": 9.057767868041992,
      "learning_rate": 2e-05,
      "loss": 0.3591,
      "step": 9560
    },
    {
      "epoch": 3.04002541296061,
      "grad_norm": 8.622365951538086,
      "learning_rate": 2e-05,
      "loss": 0.3544,
      "step": 9570
    },
    {
      "epoch": 3.0432020330368488,
      "grad_norm": 10.125658988952637,
      "learning_rate": 2e-05,
      "loss": 0.3627,
      "step": 9580
    },
    {
      "epoch": 3.046378653113088,
      "grad_norm": 7.993267059326172,
      "learning_rate": 2e-05,
      "loss": 0.3349,
      "step": 9590
    },
    {
      "epoch": 3.0495552731893265,
      "grad_norm": 5.503838062286377,
      "learning_rate": 2e-05,
      "loss": 0.3801,
      "step": 9600
    },
    {
      "epoch": 3.0527318932655656,
      "grad_norm": 5.819117069244385,
      "learning_rate": 2e-05,
      "loss": 0.3714,
      "step": 9610
    },
    {
      "epoch": 3.0559085133418042,
      "grad_norm": 8.414080619812012,
      "learning_rate": 2e-05,
      "loss": 0.3537,
      "step": 9620
    },
    {
      "epoch": 3.0590851334180433,
      "grad_norm": 21.880924224853516,
      "learning_rate": 2e-05,
      "loss": 0.3552,
      "step": 9630
    },
    {
      "epoch": 3.062261753494282,
      "grad_norm": 8.247028350830078,
      "learning_rate": 2e-05,
      "loss": 0.3702,
      "step": 9640
    },
    {
      "epoch": 3.065438373570521,
      "grad_norm": 6.468778133392334,
      "learning_rate": 2e-05,
      "loss": 0.3793,
      "step": 9650
    },
    {
      "epoch": 3.0686149936467597,
      "grad_norm": 6.0318193435668945,
      "learning_rate": 2e-05,
      "loss": 0.4102,
      "step": 9660
    },
    {
      "epoch": 3.071791613722999,
      "grad_norm": 8.808958053588867,
      "learning_rate": 2e-05,
      "loss": 0.3862,
      "step": 9670
    },
    {
      "epoch": 3.0749682337992374,
      "grad_norm": 6.568852424621582,
      "learning_rate": 2e-05,
      "loss": 0.3771,
      "step": 9680
    },
    {
      "epoch": 3.0781448538754765,
      "grad_norm": 6.756014347076416,
      "learning_rate": 2e-05,
      "loss": 0.3298,
      "step": 9690
    },
    {
      "epoch": 3.081321473951715,
      "grad_norm": 5.091325759887695,
      "learning_rate": 2e-05,
      "loss": 0.3486,
      "step": 9700
    },
    {
      "epoch": 3.0844980940279543,
      "grad_norm": 5.426685333251953,
      "learning_rate": 2e-05,
      "loss": 0.3466,
      "step": 9710
    },
    {
      "epoch": 3.0876747141041934,
      "grad_norm": 7.418276309967041,
      "learning_rate": 2e-05,
      "loss": 0.3598,
      "step": 9720
    },
    {
      "epoch": 3.090851334180432,
      "grad_norm": 5.522915840148926,
      "learning_rate": 2e-05,
      "loss": 0.3487,
      "step": 9730
    },
    {
      "epoch": 3.094027954256671,
      "grad_norm": 7.6534271240234375,
      "learning_rate": 2e-05,
      "loss": 0.3632,
      "step": 9740
    },
    {
      "epoch": 3.0972045743329097,
      "grad_norm": 7.395597457885742,
      "learning_rate": 2e-05,
      "loss": 0.3585,
      "step": 9750
    },
    {
      "epoch": 3.100381194409149,
      "grad_norm": 6.383460998535156,
      "learning_rate": 2e-05,
      "loss": 0.35,
      "step": 9760
    },
    {
      "epoch": 3.101969504447268,
      "eval_loss": 2.1153554916381836,
      "eval_mse": 2.1144364395837174,
      "eval_pearson": 0.395413820568084,
      "eval_runtime": 7.4876,
      "eval_samples_per_second": 2879.432,
      "eval_spearmanr": 0.3834436292064352,
      "eval_steps_per_second": 11.352,
      "step": 9765
    },
    {
      "epoch": 3.1035578144853875,
      "grad_norm": 8.857939720153809,
      "learning_rate": 2e-05,
      "loss": 0.379,
      "step": 9770
    },
    {
      "epoch": 3.1067344345616266,
      "grad_norm": 6.814366817474365,
      "learning_rate": 2e-05,
      "loss": 0.3598,
      "step": 9780
    },
    {
      "epoch": 3.109911054637865,
      "grad_norm": 6.458741188049316,
      "learning_rate": 2e-05,
      "loss": 0.3598,
      "step": 9790
    },
    {
      "epoch": 3.1130876747141043,
      "grad_norm": 7.770566940307617,
      "learning_rate": 2e-05,
      "loss": 0.3522,
      "step": 9800
    },
    {
      "epoch": 3.116264294790343,
      "grad_norm": 6.726543426513672,
      "learning_rate": 2e-05,
      "loss": 0.3567,
      "step": 9810
    },
    {
      "epoch": 3.119440914866582,
      "grad_norm": 7.939614772796631,
      "learning_rate": 2e-05,
      "loss": 0.3799,
      "step": 9820
    },
    {
      "epoch": 3.1226175349428207,
      "grad_norm": 11.984393119812012,
      "learning_rate": 2e-05,
      "loss": 0.3647,
      "step": 9830
    },
    {
      "epoch": 3.1257941550190598,
      "grad_norm": 5.477398872375488,
      "learning_rate": 2e-05,
      "loss": 0.3566,
      "step": 9840
    },
    {
      "epoch": 3.1289707750952984,
      "grad_norm": 6.2109808921813965,
      "learning_rate": 2e-05,
      "loss": 0.3608,
      "step": 9850
    },
    {
      "epoch": 3.1321473951715375,
      "grad_norm": 7.774623394012451,
      "learning_rate": 2e-05,
      "loss": 0.3616,
      "step": 9860
    },
    {
      "epoch": 3.135324015247776,
      "grad_norm": 6.659529209136963,
      "learning_rate": 2e-05,
      "loss": 0.3532,
      "step": 9870
    },
    {
      "epoch": 3.1385006353240152,
      "grad_norm": 8.421786308288574,
      "learning_rate": 2e-05,
      "loss": 0.3487,
      "step": 9880
    },
    {
      "epoch": 3.1416772554002543,
      "grad_norm": 7.069745063781738,
      "learning_rate": 2e-05,
      "loss": 0.3687,
      "step": 9890
    },
    {
      "epoch": 3.144853875476493,
      "grad_norm": 8.27641773223877,
      "learning_rate": 2e-05,
      "loss": 0.3552,
      "step": 9900
    },
    {
      "epoch": 3.148030495552732,
      "grad_norm": 5.509864330291748,
      "learning_rate": 2e-05,
      "loss": 0.3321,
      "step": 9910
    },
    {
      "epoch": 3.1512071156289707,
      "grad_norm": 8.0120849609375,
      "learning_rate": 2e-05,
      "loss": 0.3687,
      "step": 9920
    },
    {
      "epoch": 3.15438373570521,
      "grad_norm": 8.512821197509766,
      "learning_rate": 2e-05,
      "loss": 0.3661,
      "step": 9930
    },
    {
      "epoch": 3.1575603557814484,
      "grad_norm": 7.299633026123047,
      "learning_rate": 2e-05,
      "loss": 0.3489,
      "step": 9940
    },
    {
      "epoch": 3.1607369758576875,
      "grad_norm": 6.5847086906433105,
      "learning_rate": 2e-05,
      "loss": 0.3696,
      "step": 9950
    },
    {
      "epoch": 3.163913595933926,
      "grad_norm": 10.185541152954102,
      "learning_rate": 2e-05,
      "loss": 0.356,
      "step": 9960
    },
    {
      "epoch": 3.1670902160101653,
      "grad_norm": 7.111883163452148,
      "learning_rate": 2e-05,
      "loss": 0.3697,
      "step": 9970
    },
    {
      "epoch": 3.170266836086404,
      "grad_norm": 9.402384757995605,
      "learning_rate": 2e-05,
      "loss": 0.3747,
      "step": 9980
    },
    {
      "epoch": 3.173443456162643,
      "grad_norm": 5.798315048217773,
      "learning_rate": 2e-05,
      "loss": 0.3617,
      "step": 9990
    },
    {
      "epoch": 3.1766200762388817,
      "grad_norm": 9.78255844116211,
      "learning_rate": 2e-05,
      "loss": 0.3718,
      "step": 10000
    },
    {
      "epoch": 3.1797966963151207,
      "grad_norm": 6.188634872436523,
      "learning_rate": 2e-05,
      "loss": 0.3716,
      "step": 10010
    },
    {
      "epoch": 3.1829733163913594,
      "grad_norm": 8.006095886230469,
      "learning_rate": 2e-05,
      "loss": 0.3454,
      "step": 10020
    },
    {
      "epoch": 3.1861499364675985,
      "grad_norm": 6.268058776855469,
      "learning_rate": 2e-05,
      "loss": 0.3385,
      "step": 10030
    },
    {
      "epoch": 3.189326556543837,
      "grad_norm": 8.390192031860352,
      "learning_rate": 2e-05,
      "loss": 0.3486,
      "step": 10040
    },
    {
      "epoch": 3.192503176620076,
      "grad_norm": 13.7420072555542,
      "learning_rate": 2e-05,
      "loss": 0.3768,
      "step": 10050
    },
    {
      "epoch": 3.1956797966963153,
      "grad_norm": 8.306937217712402,
      "learning_rate": 2e-05,
      "loss": 0.3613,
      "step": 10060
    },
    {
      "epoch": 3.198856416772554,
      "grad_norm": 8.092628479003906,
      "learning_rate": 2e-05,
      "loss": 0.3511,
      "step": 10070
    },
    {
      "epoch": 3.202033036848793,
      "grad_norm": 6.75223970413208,
      "learning_rate": 2e-05,
      "loss": 0.3564,
      "step": 10080
    },
    {
      "epoch": 3.202033036848793,
      "eval_loss": 1.8294092416763306,
      "eval_mse": 1.8285225778099337,
      "eval_pearson": 0.3979859533124895,
      "eval_runtime": 7.4928,
      "eval_samples_per_second": 2877.447,
      "eval_spearmanr": 0.38989006927895953,
      "eval_steps_per_second": 11.344,
      "step": 10080
    },
    {
      "epoch": 3.2052096569250317,
      "grad_norm": 6.723917484283447,
      "learning_rate": 2e-05,
      "loss": 0.35,
      "step": 10090
    },
    {
      "epoch": 3.2083862770012708,
      "grad_norm": 5.3366498947143555,
      "learning_rate": 2e-05,
      "loss": 0.3508,
      "step": 10100
    },
    {
      "epoch": 3.2115628970775094,
      "grad_norm": 6.000649929046631,
      "learning_rate": 2e-05,
      "loss": 0.3687,
      "step": 10110
    },
    {
      "epoch": 3.2147395171537485,
      "grad_norm": 8.956836700439453,
      "learning_rate": 2e-05,
      "loss": 0.3513,
      "step": 10120
    },
    {
      "epoch": 3.217916137229987,
      "grad_norm": 12.610921859741211,
      "learning_rate": 2e-05,
      "loss": 0.3814,
      "step": 10130
    },
    {
      "epoch": 3.2210927573062262,
      "grad_norm": 9.1718168258667,
      "learning_rate": 2e-05,
      "loss": 0.3674,
      "step": 10140
    },
    {
      "epoch": 3.224269377382465,
      "grad_norm": 6.66562557220459,
      "learning_rate": 2e-05,
      "loss": 0.3573,
      "step": 10150
    },
    {
      "epoch": 3.227445997458704,
      "grad_norm": 11.268718719482422,
      "learning_rate": 2e-05,
      "loss": 0.3629,
      "step": 10160
    },
    {
      "epoch": 3.2306226175349426,
      "grad_norm": 5.594298839569092,
      "learning_rate": 2e-05,
      "loss": 0.3623,
      "step": 10170
    },
    {
      "epoch": 3.2337992376111817,
      "grad_norm": 6.58158016204834,
      "learning_rate": 2e-05,
      "loss": 0.3699,
      "step": 10180
    },
    {
      "epoch": 3.236975857687421,
      "grad_norm": 7.163756847381592,
      "learning_rate": 2e-05,
      "loss": 0.345,
      "step": 10190
    },
    {
      "epoch": 3.2401524777636594,
      "grad_norm": 5.3387298583984375,
      "learning_rate": 2e-05,
      "loss": 0.3573,
      "step": 10200
    },
    {
      "epoch": 3.2433290978398985,
      "grad_norm": 9.048044204711914,
      "learning_rate": 2e-05,
      "loss": 0.334,
      "step": 10210
    },
    {
      "epoch": 3.246505717916137,
      "grad_norm": 6.05501127243042,
      "learning_rate": 2e-05,
      "loss": 0.3579,
      "step": 10220
    },
    {
      "epoch": 3.2496823379923763,
      "grad_norm": 6.9640116691589355,
      "learning_rate": 2e-05,
      "loss": 0.371,
      "step": 10230
    },
    {
      "epoch": 3.252858958068615,
      "grad_norm": 9.074628829956055,
      "learning_rate": 2e-05,
      "loss": 0.3444,
      "step": 10240
    },
    {
      "epoch": 3.256035578144854,
      "grad_norm": 5.981867790222168,
      "learning_rate": 2e-05,
      "loss": 0.361,
      "step": 10250
    },
    {
      "epoch": 3.2592121982210926,
      "grad_norm": 9.122356414794922,
      "learning_rate": 2e-05,
      "loss": 0.3517,
      "step": 10260
    },
    {
      "epoch": 3.2623888182973317,
      "grad_norm": 6.764631748199463,
      "learning_rate": 2e-05,
      "loss": 0.3435,
      "step": 10270
    },
    {
      "epoch": 3.2655654383735704,
      "grad_norm": 8.565485954284668,
      "learning_rate": 2e-05,
      "loss": 0.3584,
      "step": 10280
    },
    {
      "epoch": 3.2687420584498095,
      "grad_norm": 6.1443963050842285,
      "learning_rate": 2e-05,
      "loss": 0.3424,
      "step": 10290
    },
    {
      "epoch": 3.271918678526048,
      "grad_norm": 5.958495616912842,
      "learning_rate": 2e-05,
      "loss": 0.3419,
      "step": 10300
    },
    {
      "epoch": 3.275095298602287,
      "grad_norm": 7.0725483894348145,
      "learning_rate": 2e-05,
      "loss": 0.3657,
      "step": 10310
    },
    {
      "epoch": 3.2782719186785263,
      "grad_norm": 8.49245834350586,
      "learning_rate": 2e-05,
      "loss": 0.3542,
      "step": 10320
    },
    {
      "epoch": 3.281448538754765,
      "grad_norm": 5.62831974029541,
      "learning_rate": 2e-05,
      "loss": 0.3346,
      "step": 10330
    },
    {
      "epoch": 3.2846251588310036,
      "grad_norm": 5.238554954528809,
      "learning_rate": 2e-05,
      "loss": 0.3473,
      "step": 10340
    },
    {
      "epoch": 3.2878017789072427,
      "grad_norm": 10.12295150756836,
      "learning_rate": 2e-05,
      "loss": 0.3445,
      "step": 10350
    },
    {
      "epoch": 3.2909783989834818,
      "grad_norm": 12.551606178283691,
      "learning_rate": 2e-05,
      "loss": 0.3544,
      "step": 10360
    },
    {
      "epoch": 3.2941550190597204,
      "grad_norm": 5.02885103225708,
      "learning_rate": 2e-05,
      "loss": 0.3544,
      "step": 10370
    },
    {
      "epoch": 3.2973316391359595,
      "grad_norm": 10.859153747558594,
      "learning_rate": 2e-05,
      "loss": 0.3527,
      "step": 10380
    },
    {
      "epoch": 3.300508259212198,
      "grad_norm": 6.3351569175720215,
      "learning_rate": 2e-05,
      "loss": 0.3428,
      "step": 10390
    },
    {
      "epoch": 3.3020965692503177,
      "eval_loss": 1.8533120155334473,
      "eval_mse": 1.8521204529446333,
      "eval_pearson": 0.37238448614606723,
      "eval_runtime": 7.4542,
      "eval_samples_per_second": 2892.314,
      "eval_spearmanr": 0.36760870036182786,
      "eval_steps_per_second": 11.403,
      "step": 10395
    },
    {
      "epoch": 3.3036848792884372,
      "grad_norm": 4.957098960876465,
      "learning_rate": 2e-05,
      "loss": 0.3369,
      "step": 10400
    },
    {
      "epoch": 3.306861499364676,
      "grad_norm": 7.321340084075928,
      "learning_rate": 2e-05,
      "loss": 0.3605,
      "step": 10410
    },
    {
      "epoch": 3.310038119440915,
      "grad_norm": 6.524716377258301,
      "learning_rate": 2e-05,
      "loss": 0.366,
      "step": 10420
    },
    {
      "epoch": 3.3132147395171536,
      "grad_norm": 5.728745460510254,
      "learning_rate": 2e-05,
      "loss": 0.3745,
      "step": 10430
    },
    {
      "epoch": 3.3163913595933927,
      "grad_norm": 5.628979206085205,
      "learning_rate": 2e-05,
      "loss": 0.3495,
      "step": 10440
    },
    {
      "epoch": 3.3195679796696314,
      "grad_norm": 6.2809929847717285,
      "learning_rate": 2e-05,
      "loss": 0.3534,
      "step": 10450
    },
    {
      "epoch": 3.3227445997458704,
      "grad_norm": 5.0823845863342285,
      "learning_rate": 2e-05,
      "loss": 0.3609,
      "step": 10460
    },
    {
      "epoch": 3.325921219822109,
      "grad_norm": 6.6883111000061035,
      "learning_rate": 2e-05,
      "loss": 0.3349,
      "step": 10470
    },
    {
      "epoch": 3.329097839898348,
      "grad_norm": 8.83803939819336,
      "learning_rate": 2e-05,
      "loss": 0.3677,
      "step": 10480
    },
    {
      "epoch": 3.3322744599745873,
      "grad_norm": 9.312841415405273,
      "learning_rate": 2e-05,
      "loss": 0.3676,
      "step": 10490
    },
    {
      "epoch": 3.335451080050826,
      "grad_norm": 17.291366577148438,
      "learning_rate": 2e-05,
      "loss": 0.3308,
      "step": 10500
    },
    {
      "epoch": 3.3386277001270646,
      "grad_norm": 5.67772912979126,
      "learning_rate": 2e-05,
      "loss": 0.3362,
      "step": 10510
    },
    {
      "epoch": 3.3418043202033036,
      "grad_norm": 7.262624740600586,
      "learning_rate": 2e-05,
      "loss": 0.3329,
      "step": 10520
    },
    {
      "epoch": 3.3449809402795427,
      "grad_norm": 6.2007832527160645,
      "learning_rate": 2e-05,
      "loss": 0.3364,
      "step": 10530
    },
    {
      "epoch": 3.3481575603557814,
      "grad_norm": 10.39635944366455,
      "learning_rate": 2e-05,
      "loss": 0.3431,
      "step": 10540
    },
    {
      "epoch": 3.3513341804320205,
      "grad_norm": 5.108249664306641,
      "learning_rate": 2e-05,
      "loss": 0.3225,
      "step": 10550
    },
    {
      "epoch": 3.354510800508259,
      "grad_norm": 7.012030124664307,
      "learning_rate": 2e-05,
      "loss": 0.3498,
      "step": 10560
    },
    {
      "epoch": 3.357687420584498,
      "grad_norm": 9.951348304748535,
      "learning_rate": 2e-05,
      "loss": 0.3689,
      "step": 10570
    },
    {
      "epoch": 3.360864040660737,
      "grad_norm": 6.279865264892578,
      "learning_rate": 2e-05,
      "loss": 0.3451,
      "step": 10580
    },
    {
      "epoch": 3.364040660736976,
      "grad_norm": 5.777081489562988,
      "learning_rate": 2e-05,
      "loss": 0.3472,
      "step": 10590
    },
    {
      "epoch": 3.3672172808132146,
      "grad_norm": 5.953026294708252,
      "learning_rate": 2e-05,
      "loss": 0.3306,
      "step": 10600
    },
    {
      "epoch": 3.3703939008894537,
      "grad_norm": 8.192676544189453,
      "learning_rate": 2e-05,
      "loss": 0.3456,
      "step": 10610
    },
    {
      "epoch": 3.3735705209656923,
      "grad_norm": 7.26738977432251,
      "learning_rate": 2e-05,
      "loss": 0.3492,
      "step": 10620
    },
    {
      "epoch": 3.3767471410419314,
      "grad_norm": 4.503255844116211,
      "learning_rate": 2e-05,
      "loss": 0.3399,
      "step": 10630
    },
    {
      "epoch": 3.37992376111817,
      "grad_norm": 6.8730268478393555,
      "learning_rate": 2e-05,
      "loss": 0.3373,
      "step": 10640
    },
    {
      "epoch": 3.383100381194409,
      "grad_norm": 5.432163238525391,
      "learning_rate": 2e-05,
      "loss": 0.3324,
      "step": 10650
    },
    {
      "epoch": 3.3862770012706482,
      "grad_norm": 7.948975086212158,
      "learning_rate": 2e-05,
      "loss": 0.3488,
      "step": 10660
    },
    {
      "epoch": 3.389453621346887,
      "grad_norm": 10.131075859069824,
      "learning_rate": 2e-05,
      "loss": 0.3566,
      "step": 10670
    },
    {
      "epoch": 3.392630241423126,
      "grad_norm": 6.545769691467285,
      "learning_rate": 2e-05,
      "loss": 0.3696,
      "step": 10680
    },
    {
      "epoch": 3.3958068614993646,
      "grad_norm": 7.199118614196777,
      "learning_rate": 2e-05,
      "loss": 0.3383,
      "step": 10690
    },
    {
      "epoch": 3.3989834815756037,
      "grad_norm": 3.752291202545166,
      "learning_rate": 2e-05,
      "loss": 0.3259,
      "step": 10700
    },
    {
      "epoch": 3.4021601016518423,
      "grad_norm": 5.545583248138428,
      "learning_rate": 2e-05,
      "loss": 0.3705,
      "step": 10710
    },
    {
      "epoch": 3.4021601016518423,
      "eval_loss": 1.8364686965942383,
      "eval_mse": 1.8347748939410986,
      "eval_pearson": 0.4054964356835924,
      "eval_runtime": 7.3806,
      "eval_samples_per_second": 2921.156,
      "eval_spearmanr": 0.3996507430478974,
      "eval_steps_per_second": 11.517,
      "step": 10710
    },
    {
      "epoch": 3.4053367217280814,
      "grad_norm": 4.917753219604492,
      "learning_rate": 2e-05,
      "loss": 0.3364,
      "step": 10720
    },
    {
      "epoch": 3.40851334180432,
      "grad_norm": 4.620743751525879,
      "learning_rate": 2e-05,
      "loss": 0.3422,
      "step": 10730
    },
    {
      "epoch": 3.411689961880559,
      "grad_norm": 7.629533290863037,
      "learning_rate": 2e-05,
      "loss": 0.3395,
      "step": 10740
    },
    {
      "epoch": 3.414866581956798,
      "grad_norm": 4.979445934295654,
      "learning_rate": 2e-05,
      "loss": 0.3562,
      "step": 10750
    },
    {
      "epoch": 3.418043202033037,
      "grad_norm": 5.845890522003174,
      "learning_rate": 2e-05,
      "loss": 0.3458,
      "step": 10760
    },
    {
      "epoch": 3.4212198221092756,
      "grad_norm": 7.439898490905762,
      "learning_rate": 2e-05,
      "loss": 0.3293,
      "step": 10770
    },
    {
      "epoch": 3.4243964421855146,
      "grad_norm": 6.372344493865967,
      "learning_rate": 2e-05,
      "loss": 0.3394,
      "step": 10780
    },
    {
      "epoch": 3.4275730622617537,
      "grad_norm": 7.900597095489502,
      "learning_rate": 2e-05,
      "loss": 0.3381,
      "step": 10790
    },
    {
      "epoch": 3.4307496823379924,
      "grad_norm": 6.271590709686279,
      "learning_rate": 2e-05,
      "loss": 0.3542,
      "step": 10800
    },
    {
      "epoch": 3.433926302414231,
      "grad_norm": 5.920870304107666,
      "learning_rate": 2e-05,
      "loss": 0.3533,
      "step": 10810
    },
    {
      "epoch": 3.43710292249047,
      "grad_norm": 4.887311935424805,
      "learning_rate": 2e-05,
      "loss": 0.3208,
      "step": 10820
    },
    {
      "epoch": 3.440279542566709,
      "grad_norm": 5.478696823120117,
      "learning_rate": 2e-05,
      "loss": 0.3339,
      "step": 10830
    },
    {
      "epoch": 3.443456162642948,
      "grad_norm": 11.833868980407715,
      "learning_rate": 2e-05,
      "loss": 0.3497,
      "step": 10840
    },
    {
      "epoch": 3.446632782719187,
      "grad_norm": 6.648456573486328,
      "learning_rate": 2e-05,
      "loss": 0.3224,
      "step": 10850
    },
    {
      "epoch": 3.4498094027954256,
      "grad_norm": 6.7943525314331055,
      "learning_rate": 2e-05,
      "loss": 0.3258,
      "step": 10860
    },
    {
      "epoch": 3.4529860228716647,
      "grad_norm": 7.021305561065674,
      "learning_rate": 2e-05,
      "loss": 0.3402,
      "step": 10870
    },
    {
      "epoch": 3.4561626429479033,
      "grad_norm": 4.978427410125732,
      "learning_rate": 2e-05,
      "loss": 0.3329,
      "step": 10880
    },
    {
      "epoch": 3.4593392630241424,
      "grad_norm": 10.461420059204102,
      "learning_rate": 2e-05,
      "loss": 0.3366,
      "step": 10890
    },
    {
      "epoch": 3.462515883100381,
      "grad_norm": 7.474348545074463,
      "learning_rate": 2e-05,
      "loss": 0.3249,
      "step": 10900
    },
    {
      "epoch": 3.46569250317662,
      "grad_norm": 6.9096856117248535,
      "learning_rate": 2e-05,
      "loss": 0.3498,
      "step": 10910
    },
    {
      "epoch": 3.468869123252859,
      "grad_norm": 5.802359104156494,
      "learning_rate": 2e-05,
      "loss": 0.3408,
      "step": 10920
    },
    {
      "epoch": 3.472045743329098,
      "grad_norm": 4.317800521850586,
      "learning_rate": 2e-05,
      "loss": 0.3433,
      "step": 10930
    },
    {
      "epoch": 3.4752223634053365,
      "grad_norm": 5.245253562927246,
      "learning_rate": 2e-05,
      "loss": 0.3356,
      "step": 10940
    },
    {
      "epoch": 3.4783989834815756,
      "grad_norm": 5.092769622802734,
      "learning_rate": 2e-05,
      "loss": 0.3606,
      "step": 10950
    },
    {
      "epoch": 3.4815756035578147,
      "grad_norm": 5.763636112213135,
      "learning_rate": 2e-05,
      "loss": 0.3256,
      "step": 10960
    },
    {
      "epoch": 3.4847522236340533,
      "grad_norm": 6.971682548522949,
      "learning_rate": 2e-05,
      "loss": 0.3384,
      "step": 10970
    },
    {
      "epoch": 3.4879288437102924,
      "grad_norm": 7.773190498352051,
      "learning_rate": 2e-05,
      "loss": 0.3269,
      "step": 10980
    },
    {
      "epoch": 3.491105463786531,
      "grad_norm": 6.5670485496521,
      "learning_rate": 2e-05,
      "loss": 0.3245,
      "step": 10990
    },
    {
      "epoch": 3.49428208386277,
      "grad_norm": 6.919008731842041,
      "learning_rate": 2e-05,
      "loss": 0.3291,
      "step": 11000
    },
    {
      "epoch": 3.497458703939009,
      "grad_norm": 10.108831405639648,
      "learning_rate": 2e-05,
      "loss": 0.324,
      "step": 11010
    },
    {
      "epoch": 3.500635324015248,
      "grad_norm": 6.436855316162109,
      "learning_rate": 2e-05,
      "loss": 0.3414,
      "step": 11020
    },
    {
      "epoch": 3.502223634053367,
      "eval_loss": 1.9019473791122437,
      "eval_mse": 1.9009843451193844,
      "eval_pearson": 0.41130780794047683,
      "eval_runtime": 7.5704,
      "eval_samples_per_second": 2847.931,
      "eval_spearmanr": 0.40507562488599286,
      "eval_steps_per_second": 11.228,
      "step": 11025
    },
    {
      "epoch": 3.5038119440914866,
      "grad_norm": 5.822469711303711,
      "learning_rate": 2e-05,
      "loss": 0.3428,
      "step": 11030
    },
    {
      "epoch": 3.5069885641677256,
      "grad_norm": 4.9946794509887695,
      "learning_rate": 2e-05,
      "loss": 0.3368,
      "step": 11040
    },
    {
      "epoch": 3.5101651842439643,
      "grad_norm": 8.888306617736816,
      "learning_rate": 2e-05,
      "loss": 0.3287,
      "step": 11050
    },
    {
      "epoch": 3.5133418043202034,
      "grad_norm": 7.432765007019043,
      "learning_rate": 2e-05,
      "loss": 0.3574,
      "step": 11060
    },
    {
      "epoch": 3.516518424396442,
      "grad_norm": 6.7498250007629395,
      "learning_rate": 2e-05,
      "loss": 0.3222,
      "step": 11070
    },
    {
      "epoch": 3.519695044472681,
      "grad_norm": 9.015247344970703,
      "learning_rate": 2e-05,
      "loss": 0.343,
      "step": 11080
    },
    {
      "epoch": 3.52287166454892,
      "grad_norm": 9.017461776733398,
      "learning_rate": 2e-05,
      "loss": 0.3566,
      "step": 11090
    },
    {
      "epoch": 3.526048284625159,
      "grad_norm": 6.020216464996338,
      "learning_rate": 2e-05,
      "loss": 0.3535,
      "step": 11100
    },
    {
      "epoch": 3.5292249047013975,
      "grad_norm": 8.239970207214355,
      "learning_rate": 2e-05,
      "loss": 0.3618,
      "step": 11110
    },
    {
      "epoch": 3.5324015247776366,
      "grad_norm": 5.0103583335876465,
      "learning_rate": 2e-05,
      "loss": 0.3428,
      "step": 11120
    },
    {
      "epoch": 3.5355781448538757,
      "grad_norm": 5.742815971374512,
      "learning_rate": 2e-05,
      "loss": 0.3169,
      "step": 11130
    },
    {
      "epoch": 3.5387547649301143,
      "grad_norm": 5.973605155944824,
      "learning_rate": 2e-05,
      "loss": 0.3223,
      "step": 11140
    },
    {
      "epoch": 3.5419313850063534,
      "grad_norm": 5.458488464355469,
      "learning_rate": 2e-05,
      "loss": 0.325,
      "step": 11150
    },
    {
      "epoch": 3.545108005082592,
      "grad_norm": 6.991095542907715,
      "learning_rate": 2e-05,
      "loss": 0.322,
      "step": 11160
    },
    {
      "epoch": 3.548284625158831,
      "grad_norm": 5.233395099639893,
      "learning_rate": 2e-05,
      "loss": 0.323,
      "step": 11170
    },
    {
      "epoch": 3.55146124523507,
      "grad_norm": 6.236850738525391,
      "learning_rate": 2e-05,
      "loss": 0.3452,
      "step": 11180
    },
    {
      "epoch": 3.554637865311309,
      "grad_norm": 5.793039798736572,
      "learning_rate": 2e-05,
      "loss": 0.3199,
      "step": 11190
    },
    {
      "epoch": 3.5578144853875475,
      "grad_norm": 8.643445014953613,
      "learning_rate": 2e-05,
      "loss": 0.335,
      "step": 11200
    },
    {
      "epoch": 3.5609911054637866,
      "grad_norm": 7.485466003417969,
      "learning_rate": 2e-05,
      "loss": 0.3437,
      "step": 11210
    },
    {
      "epoch": 3.5641677255400253,
      "grad_norm": 6.577239036560059,
      "learning_rate": 2e-05,
      "loss": 0.331,
      "step": 11220
    },
    {
      "epoch": 3.5673443456162643,
      "grad_norm": 10.945422172546387,
      "learning_rate": 2e-05,
      "loss": 0.3531,
      "step": 11230
    },
    {
      "epoch": 3.570520965692503,
      "grad_norm": 10.244205474853516,
      "learning_rate": 2e-05,
      "loss": 0.3369,
      "step": 11240
    },
    {
      "epoch": 3.573697585768742,
      "grad_norm": 6.8257222175598145,
      "learning_rate": 2e-05,
      "loss": 0.3465,
      "step": 11250
    },
    {
      "epoch": 3.576874205844981,
      "grad_norm": 6.609820365905762,
      "learning_rate": 2e-05,
      "loss": 0.3472,
      "step": 11260
    },
    {
      "epoch": 3.58005082592122,
      "grad_norm": 6.234280586242676,
      "learning_rate": 2e-05,
      "loss": 0.3346,
      "step": 11270
    },
    {
      "epoch": 3.5832274459974585,
      "grad_norm": 7.251525402069092,
      "learning_rate": 2e-05,
      "loss": 0.3459,
      "step": 11280
    },
    {
      "epoch": 3.5864040660736975,
      "grad_norm": 6.493782043457031,
      "learning_rate": 2e-05,
      "loss": 0.3426,
      "step": 11290
    },
    {
      "epoch": 3.5895806861499366,
      "grad_norm": 11.269369125366211,
      "learning_rate": 2e-05,
      "loss": 0.3445,
      "step": 11300
    },
    {
      "epoch": 3.5927573062261753,
      "grad_norm": 9.720425605773926,
      "learning_rate": 2e-05,
      "loss": 0.346,
      "step": 11310
    },
    {
      "epoch": 3.5959339263024144,
      "grad_norm": 5.200738906860352,
      "learning_rate": 2e-05,
      "loss": 0.3292,
      "step": 11320
    },
    {
      "epoch": 3.599110546378653,
      "grad_norm": 9.691923141479492,
      "learning_rate": 2e-05,
      "loss": 0.3334,
      "step": 11330
    },
    {
      "epoch": 3.602287166454892,
      "grad_norm": 5.237699508666992,
      "learning_rate": 2e-05,
      "loss": 0.3378,
      "step": 11340
    },
    {
      "epoch": 3.602287166454892,
      "eval_loss": 1.804502010345459,
      "eval_mse": 1.8024731082686245,
      "eval_pearson": 0.3912611378593656,
      "eval_runtime": 7.4853,
      "eval_samples_per_second": 2880.303,
      "eval_spearmanr": 0.38375040080164646,
      "eval_steps_per_second": 11.356,
      "step": 11340
    },
    {
      "epoch": 3.6054637865311308,
      "grad_norm": 6.671877384185791,
      "learning_rate": 2e-05,
      "loss": 0.3507,
      "step": 11350
    },
    {
      "epoch": 3.60864040660737,
      "grad_norm": 8.594380378723145,
      "learning_rate": 2e-05,
      "loss": 0.3327,
      "step": 11360
    },
    {
      "epoch": 3.6118170266836085,
      "grad_norm": 6.497907638549805,
      "learning_rate": 2e-05,
      "loss": 0.3129,
      "step": 11370
    },
    {
      "epoch": 3.6149936467598476,
      "grad_norm": 7.393501281738281,
      "learning_rate": 2e-05,
      "loss": 0.3603,
      "step": 11380
    },
    {
      "epoch": 3.6181702668360867,
      "grad_norm": 4.397348880767822,
      "learning_rate": 2e-05,
      "loss": 0.3329,
      "step": 11390
    },
    {
      "epoch": 3.6213468869123253,
      "grad_norm": 7.192322254180908,
      "learning_rate": 2e-05,
      "loss": 0.3267,
      "step": 11400
    },
    {
      "epoch": 3.624523506988564,
      "grad_norm": 5.499237537384033,
      "learning_rate": 2e-05,
      "loss": 0.3127,
      "step": 11410
    },
    {
      "epoch": 3.627700127064803,
      "grad_norm": 6.8480610847473145,
      "learning_rate": 2e-05,
      "loss": 0.3288,
      "step": 11420
    },
    {
      "epoch": 3.630876747141042,
      "grad_norm": 7.222301006317139,
      "learning_rate": 2e-05,
      "loss": 0.329,
      "step": 11430
    },
    {
      "epoch": 3.634053367217281,
      "grad_norm": 5.772631645202637,
      "learning_rate": 2e-05,
      "loss": 0.344,
      "step": 11440
    },
    {
      "epoch": 3.6372299872935194,
      "grad_norm": 6.812313556671143,
      "learning_rate": 2e-05,
      "loss": 0.3311,
      "step": 11450
    },
    {
      "epoch": 3.6404066073697585,
      "grad_norm": 7.564378261566162,
      "learning_rate": 2e-05,
      "loss": 0.3612,
      "step": 11460
    },
    {
      "epoch": 3.6435832274459976,
      "grad_norm": 6.176558017730713,
      "learning_rate": 2e-05,
      "loss": 0.3078,
      "step": 11470
    },
    {
      "epoch": 3.6467598475222363,
      "grad_norm": 6.913457870483398,
      "learning_rate": 2e-05,
      "loss": 0.3295,
      "step": 11480
    },
    {
      "epoch": 3.6499364675984753,
      "grad_norm": 6.221114158630371,
      "learning_rate": 2e-05,
      "loss": 0.3264,
      "step": 11490
    },
    {
      "epoch": 3.653113087674714,
      "grad_norm": 5.592415809631348,
      "learning_rate": 2e-05,
      "loss": 0.3182,
      "step": 11500
    },
    {
      "epoch": 3.656289707750953,
      "grad_norm": 5.681270122528076,
      "learning_rate": 2e-05,
      "loss": 0.33,
      "step": 11510
    },
    {
      "epoch": 3.6594663278271917,
      "grad_norm": 6.32842493057251,
      "learning_rate": 2e-05,
      "loss": 0.3458,
      "step": 11520
    },
    {
      "epoch": 3.662642947903431,
      "grad_norm": 11.194734573364258,
      "learning_rate": 2e-05,
      "loss": 0.3249,
      "step": 11530
    },
    {
      "epoch": 3.6658195679796695,
      "grad_norm": 9.519719123840332,
      "learning_rate": 2e-05,
      "loss": 0.3285,
      "step": 11540
    },
    {
      "epoch": 3.6689961880559085,
      "grad_norm": 9.264303207397461,
      "learning_rate": 2e-05,
      "loss": 0.3528,
      "step": 11550
    },
    {
      "epoch": 3.6721728081321476,
      "grad_norm": 5.566423416137695,
      "learning_rate": 2e-05,
      "loss": 0.3331,
      "step": 11560
    },
    {
      "epoch": 3.6753494282083863,
      "grad_norm": 8.915494918823242,
      "learning_rate": 2e-05,
      "loss": 0.3293,
      "step": 11570
    },
    {
      "epoch": 3.678526048284625,
      "grad_norm": 4.874773979187012,
      "learning_rate": 2e-05,
      "loss": 0.3171,
      "step": 11580
    },
    {
      "epoch": 3.681702668360864,
      "grad_norm": 8.377230644226074,
      "learning_rate": 2e-05,
      "loss": 0.3281,
      "step": 11590
    },
    {
      "epoch": 3.684879288437103,
      "grad_norm": 4.682575225830078,
      "learning_rate": 2e-05,
      "loss": 0.304,
      "step": 11600
    },
    {
      "epoch": 3.6880559085133418,
      "grad_norm": 5.140285015106201,
      "learning_rate": 2e-05,
      "loss": 0.3195,
      "step": 11610
    },
    {
      "epoch": 3.691232528589581,
      "grad_norm": 6.099205493927002,
      "learning_rate": 2e-05,
      "loss": 0.3229,
      "step": 11620
    },
    {
      "epoch": 3.6944091486658195,
      "grad_norm": 8.569205284118652,
      "learning_rate": 2e-05,
      "loss": 0.3351,
      "step": 11630
    },
    {
      "epoch": 3.6975857687420586,
      "grad_norm": 6.719531059265137,
      "learning_rate": 2e-05,
      "loss": 0.3329,
      "step": 11640
    },
    {
      "epoch": 3.700762388818297,
      "grad_norm": 5.995066165924072,
      "learning_rate": 2e-05,
      "loss": 0.3418,
      "step": 11650
    },
    {
      "epoch": 3.7023506988564168,
      "eval_loss": 1.7518422603607178,
      "eval_mse": 1.7501263634282715,
      "eval_pearson": 0.38248066996061425,
      "eval_runtime": 7.5768,
      "eval_samples_per_second": 2845.539,
      "eval_spearmanr": 0.37666454627726337,
      "eval_steps_per_second": 11.218,
      "step": 11655
    },
    {
      "epoch": 3.7039390088945363,
      "grad_norm": 6.617833614349365,
      "learning_rate": 2e-05,
      "loss": 0.3379,
      "step": 11660
    },
    {
      "epoch": 3.707115628970775,
      "grad_norm": 7.840745449066162,
      "learning_rate": 2e-05,
      "loss": 0.3034,
      "step": 11670
    },
    {
      "epoch": 3.710292249047014,
      "grad_norm": 5.110176086425781,
      "learning_rate": 2e-05,
      "loss": 0.3317,
      "step": 11680
    },
    {
      "epoch": 3.713468869123253,
      "grad_norm": 5.976383686065674,
      "learning_rate": 2e-05,
      "loss": 0.3292,
      "step": 11690
    },
    {
      "epoch": 3.716645489199492,
      "grad_norm": 6.230624198913574,
      "learning_rate": 2e-05,
      "loss": 0.3326,
      "step": 11700
    },
    {
      "epoch": 3.7198221092757304,
      "grad_norm": 8.587445259094238,
      "learning_rate": 2e-05,
      "loss": 0.3248,
      "step": 11710
    },
    {
      "epoch": 3.7229987293519695,
      "grad_norm": 4.381974220275879,
      "learning_rate": 2e-05,
      "loss": 0.3063,
      "step": 11720
    },
    {
      "epoch": 3.7261753494282086,
      "grad_norm": 6.587836265563965,
      "learning_rate": 2e-05,
      "loss": 0.3538,
      "step": 11730
    },
    {
      "epoch": 3.7293519695044473,
      "grad_norm": 9.316452980041504,
      "learning_rate": 2e-05,
      "loss": 0.3076,
      "step": 11740
    },
    {
      "epoch": 3.732528589580686,
      "grad_norm": 5.312280654907227,
      "learning_rate": 2e-05,
      "loss": 0.3184,
      "step": 11750
    },
    {
      "epoch": 3.735705209656925,
      "grad_norm": 5.760844707489014,
      "learning_rate": 2e-05,
      "loss": 0.3253,
      "step": 11760
    },
    {
      "epoch": 3.738881829733164,
      "grad_norm": 6.098721981048584,
      "learning_rate": 2e-05,
      "loss": 0.3088,
      "step": 11770
    },
    {
      "epoch": 3.7420584498094027,
      "grad_norm": 8.740072250366211,
      "learning_rate": 2e-05,
      "loss": 0.311,
      "step": 11780
    },
    {
      "epoch": 3.745235069885642,
      "grad_norm": 5.322945594787598,
      "learning_rate": 2e-05,
      "loss": 0.3327,
      "step": 11790
    },
    {
      "epoch": 3.7484116899618805,
      "grad_norm": 7.040465831756592,
      "learning_rate": 2e-05,
      "loss": 0.3301,
      "step": 11800
    },
    {
      "epoch": 3.7515883100381195,
      "grad_norm": 6.362988471984863,
      "learning_rate": 2e-05,
      "loss": 0.3333,
      "step": 11810
    },
    {
      "epoch": 3.754764930114358,
      "grad_norm": 5.935192584991455,
      "learning_rate": 2e-05,
      "loss": 0.3361,
      "step": 11820
    },
    {
      "epoch": 3.7579415501905973,
      "grad_norm": 7.466903209686279,
      "learning_rate": 2e-05,
      "loss": 0.3069,
      "step": 11830
    },
    {
      "epoch": 3.761118170266836,
      "grad_norm": 4.363603115081787,
      "learning_rate": 2e-05,
      "loss": 0.318,
      "step": 11840
    },
    {
      "epoch": 3.764294790343075,
      "grad_norm": 13.073240280151367,
      "learning_rate": 2e-05,
      "loss": 0.3242,
      "step": 11850
    },
    {
      "epoch": 3.767471410419314,
      "grad_norm": 6.241647720336914,
      "learning_rate": 2e-05,
      "loss": 0.3258,
      "step": 11860
    },
    {
      "epoch": 3.7706480304955527,
      "grad_norm": 6.085158348083496,
      "learning_rate": 2e-05,
      "loss": 0.3014,
      "step": 11870
    },
    {
      "epoch": 3.7738246505717914,
      "grad_norm": 7.209122180938721,
      "learning_rate": 2e-05,
      "loss": 0.3398,
      "step": 11880
    },
    {
      "epoch": 3.7770012706480305,
      "grad_norm": 7.024434566497803,
      "learning_rate": 2e-05,
      "loss": 0.3118,
      "step": 11890
    },
    {
      "epoch": 3.7801778907242696,
      "grad_norm": 4.43220329284668,
      "learning_rate": 2e-05,
      "loss": 0.312,
      "step": 11900
    },
    {
      "epoch": 3.783354510800508,
      "grad_norm": 4.967680931091309,
      "learning_rate": 2e-05,
      "loss": 0.3023,
      "step": 11910
    },
    {
      "epoch": 3.786531130876747,
      "grad_norm": 7.8042988777160645,
      "learning_rate": 2e-05,
      "loss": 0.3165,
      "step": 11920
    },
    {
      "epoch": 3.789707750952986,
      "grad_norm": 5.911795616149902,
      "learning_rate": 2e-05,
      "loss": 0.3045,
      "step": 11930
    },
    {
      "epoch": 3.792884371029225,
      "grad_norm": 6.158440589904785,
      "learning_rate": 2e-05,
      "loss": 0.3334,
      "step": 11940
    },
    {
      "epoch": 3.7960609911054637,
      "grad_norm": 6.743885040283203,
      "learning_rate": 2e-05,
      "loss": 0.3176,
      "step": 11950
    },
    {
      "epoch": 3.799237611181703,
      "grad_norm": 5.933493137359619,
      "learning_rate": 2e-05,
      "loss": 0.3181,
      "step": 11960
    },
    {
      "epoch": 3.8024142312579414,
      "grad_norm": 7.152449131011963,
      "learning_rate": 2e-05,
      "loss": 0.3248,
      "step": 11970
    },
    {
      "epoch": 3.8024142312579414,
      "eval_loss": 1.8655394315719604,
      "eval_mse": 1.8642984081730107,
      "eval_pearson": 0.32664108276483655,
      "eval_runtime": 7.5987,
      "eval_samples_per_second": 2837.34,
      "eval_spearmanr": 0.32763717866259545,
      "eval_steps_per_second": 11.186,
      "step": 11970
    },
    {
      "epoch": 3.8055908513341805,
      "grad_norm": 6.418464183807373,
      "learning_rate": 2e-05,
      "loss": 0.3102,
      "step": 11980
    },
    {
      "epoch": 3.808767471410419,
      "grad_norm": 5.539224147796631,
      "learning_rate": 2e-05,
      "loss": 0.2894,
      "step": 11990
    },
    {
      "epoch": 3.8119440914866582,
      "grad_norm": 4.215779781341553,
      "learning_rate": 2e-05,
      "loss": 0.3009,
      "step": 12000
    },
    {
      "epoch": 3.815120711562897,
      "grad_norm": 5.188717842102051,
      "learning_rate": 2e-05,
      "loss": 0.3314,
      "step": 12010
    },
    {
      "epoch": 3.818297331639136,
      "grad_norm": 6.089399814605713,
      "learning_rate": 2e-05,
      "loss": 0.3196,
      "step": 12020
    },
    {
      "epoch": 3.821473951715375,
      "grad_norm": 8.91506290435791,
      "learning_rate": 2e-05,
      "loss": 0.3315,
      "step": 12030
    },
    {
      "epoch": 3.8246505717916137,
      "grad_norm": 7.66724967956543,
      "learning_rate": 2e-05,
      "loss": 0.3135,
      "step": 12040
    },
    {
      "epoch": 3.8278271918678524,
      "grad_norm": 5.532282829284668,
      "learning_rate": 2e-05,
      "loss": 0.327,
      "step": 12050
    },
    {
      "epoch": 3.8310038119440915,
      "grad_norm": 6.5177764892578125,
      "learning_rate": 2e-05,
      "loss": 0.33,
      "step": 12060
    },
    {
      "epoch": 3.8341804320203305,
      "grad_norm": 4.230216026306152,
      "learning_rate": 2e-05,
      "loss": 0.3187,
      "step": 12070
    },
    {
      "epoch": 3.837357052096569,
      "grad_norm": 6.698598384857178,
      "learning_rate": 2e-05,
      "loss": 0.3273,
      "step": 12080
    },
    {
      "epoch": 3.8405336721728083,
      "grad_norm": 5.5019707679748535,
      "learning_rate": 2e-05,
      "loss": 0.3074,
      "step": 12090
    },
    {
      "epoch": 3.843710292249047,
      "grad_norm": 4.882965564727783,
      "learning_rate": 2e-05,
      "loss": 0.3345,
      "step": 12100
    },
    {
      "epoch": 3.846886912325286,
      "grad_norm": 4.823078155517578,
      "learning_rate": 2e-05,
      "loss": 0.2982,
      "step": 12110
    },
    {
      "epoch": 3.8500635324015247,
      "grad_norm": 9.137151718139648,
      "learning_rate": 2e-05,
      "loss": 0.3317,
      "step": 12120
    },
    {
      "epoch": 3.8532401524777637,
      "grad_norm": 9.097660064697266,
      "learning_rate": 2e-05,
      "loss": 0.3056,
      "step": 12130
    },
    {
      "epoch": 3.8564167725540024,
      "grad_norm": 10.707263946533203,
      "learning_rate": 2e-05,
      "loss": 0.3455,
      "step": 12140
    },
    {
      "epoch": 3.8595933926302415,
      "grad_norm": 6.213700294494629,
      "learning_rate": 2e-05,
      "loss": 0.3377,
      "step": 12150
    },
    {
      "epoch": 3.8627700127064806,
      "grad_norm": 8.697686195373535,
      "learning_rate": 2e-05,
      "loss": 0.3291,
      "step": 12160
    },
    {
      "epoch": 3.865946632782719,
      "grad_norm": 5.5810394287109375,
      "learning_rate": 2e-05,
      "loss": 0.3304,
      "step": 12170
    },
    {
      "epoch": 3.869123252858958,
      "grad_norm": 5.897428512573242,
      "learning_rate": 2e-05,
      "loss": 0.3342,
      "step": 12180
    },
    {
      "epoch": 3.872299872935197,
      "grad_norm": 6.175180435180664,
      "learning_rate": 2e-05,
      "loss": 0.3253,
      "step": 12190
    },
    {
      "epoch": 3.875476493011436,
      "grad_norm": 4.885848522186279,
      "learning_rate": 2e-05,
      "loss": 0.3197,
      "step": 12200
    },
    {
      "epoch": 3.8786531130876747,
      "grad_norm": 5.285909175872803,
      "learning_rate": 2e-05,
      "loss": 0.3106,
      "step": 12210
    },
    {
      "epoch": 3.8818297331639133,
      "grad_norm": 5.855268955230713,
      "learning_rate": 2e-05,
      "loss": 0.2885,
      "step": 12220
    },
    {
      "epoch": 3.8850063532401524,
      "grad_norm": 5.951265811920166,
      "learning_rate": 2e-05,
      "loss": 0.3219,
      "step": 12230
    },
    {
      "epoch": 3.8881829733163915,
      "grad_norm": 5.06274938583374,
      "learning_rate": 2e-05,
      "loss": 0.3204,
      "step": 12240
    },
    {
      "epoch": 3.89135959339263,
      "grad_norm": 8.359131813049316,
      "learning_rate": 2e-05,
      "loss": 0.3344,
      "step": 12250
    },
    {
      "epoch": 3.8945362134688692,
      "grad_norm": 6.349368095397949,
      "learning_rate": 2e-05,
      "loss": 0.3115,
      "step": 12260
    },
    {
      "epoch": 3.897712833545108,
      "grad_norm": 5.1590752601623535,
      "learning_rate": 2e-05,
      "loss": 0.3042,
      "step": 12270
    },
    {
      "epoch": 3.900889453621347,
      "grad_norm": 8.614713668823242,
      "learning_rate": 2e-05,
      "loss": 0.3066,
      "step": 12280
    },
    {
      "epoch": 3.9024777636594665,
      "eval_loss": 1.945786476135254,
      "eval_mse": 1.9445859515600612,
      "eval_pearson": 0.37430567862511704,
      "eval_runtime": 7.5959,
      "eval_samples_per_second": 2838.378,
      "eval_spearmanr": 0.3743541378047381,
      "eval_steps_per_second": 11.19,
      "step": 12285
    },
    {
      "epoch": 3.9040660736975856,
      "grad_norm": 5.049482822418213,
      "learning_rate": 2e-05,
      "loss": 0.3247,
      "step": 12290
    },
    {
      "epoch": 3.9072426937738247,
      "grad_norm": 5.2946648597717285,
      "learning_rate": 2e-05,
      "loss": 0.31,
      "step": 12300
    },
    {
      "epoch": 3.9104193138500634,
      "grad_norm": 8.188413619995117,
      "learning_rate": 2e-05,
      "loss": 0.3083,
      "step": 12310
    },
    {
      "epoch": 3.9135959339263025,
      "grad_norm": 6.8550944328308105,
      "learning_rate": 2e-05,
      "loss": 0.3107,
      "step": 12320
    },
    {
      "epoch": 3.9167725540025415,
      "grad_norm": 6.549495697021484,
      "learning_rate": 2e-05,
      "loss": 0.3127,
      "step": 12330
    },
    {
      "epoch": 3.91994917407878,
      "grad_norm": 6.47048282623291,
      "learning_rate": 2e-05,
      "loss": 0.3134,
      "step": 12340
    },
    {
      "epoch": 3.923125794155019,
      "grad_norm": 5.0462327003479,
      "learning_rate": 2e-05,
      "loss": 0.3077,
      "step": 12350
    },
    {
      "epoch": 3.926302414231258,
      "grad_norm": 5.876649856567383,
      "learning_rate": 2e-05,
      "loss": 0.3361,
      "step": 12360
    },
    {
      "epoch": 3.929479034307497,
      "grad_norm": 5.6496992111206055,
      "learning_rate": 2e-05,
      "loss": 0.3227,
      "step": 12370
    },
    {
      "epoch": 3.9326556543837357,
      "grad_norm": 4.694831371307373,
      "learning_rate": 2e-05,
      "loss": 0.3202,
      "step": 12380
    },
    {
      "epoch": 3.9358322744599747,
      "grad_norm": 6.83478307723999,
      "learning_rate": 2e-05,
      "loss": 0.3385,
      "step": 12390
    },
    {
      "epoch": 3.9390088945362134,
      "grad_norm": 6.656188011169434,
      "learning_rate": 2e-05,
      "loss": 0.306,
      "step": 12400
    },
    {
      "epoch": 3.9421855146124525,
      "grad_norm": 9.756580352783203,
      "learning_rate": 2e-05,
      "loss": 0.3136,
      "step": 12410
    },
    {
      "epoch": 3.945362134688691,
      "grad_norm": 6.767573356628418,
      "learning_rate": 2e-05,
      "loss": 0.3232,
      "step": 12420
    },
    {
      "epoch": 3.94853875476493,
      "grad_norm": 6.858188629150391,
      "learning_rate": 2e-05,
      "loss": 0.3277,
      "step": 12430
    },
    {
      "epoch": 3.951715374841169,
      "grad_norm": 8.443196296691895,
      "learning_rate": 2e-05,
      "loss": 0.3045,
      "step": 12440
    },
    {
      "epoch": 3.954891994917408,
      "grad_norm": 4.398453712463379,
      "learning_rate": 2e-05,
      "loss": 0.3117,
      "step": 12450
    },
    {
      "epoch": 3.9580686149936466,
      "grad_norm": 5.822903156280518,
      "learning_rate": 2e-05,
      "loss": 0.2879,
      "step": 12460
    },
    {
      "epoch": 3.9612452350698857,
      "grad_norm": 6.183759689331055,
      "learning_rate": 2e-05,
      "loss": 0.3121,
      "step": 12470
    },
    {
      "epoch": 3.9644218551461243,
      "grad_norm": 6.7936601638793945,
      "learning_rate": 2e-05,
      "loss": 0.3285,
      "step": 12480
    },
    {
      "epoch": 3.9675984752223634,
      "grad_norm": 5.4215617179870605,
      "learning_rate": 2e-05,
      "loss": 0.3137,
      "step": 12490
    },
    {
      "epoch": 3.9707750952986025,
      "grad_norm": 7.66989803314209,
      "learning_rate": 2e-05,
      "loss": 0.3166,
      "step": 12500
    },
    {
      "epoch": 3.973951715374841,
      "grad_norm": 4.682333469390869,
      "learning_rate": 2e-05,
      "loss": 0.3114,
      "step": 12510
    },
    {
      "epoch": 3.97712833545108,
      "grad_norm": 6.462408065795898,
      "learning_rate": 2e-05,
      "loss": 0.3009,
      "step": 12520
    },
    {
      "epoch": 3.980304955527319,
      "grad_norm": 10.784626960754395,
      "learning_rate": 2e-05,
      "loss": 0.3011,
      "step": 12530
    },
    {
      "epoch": 3.983481575603558,
      "grad_norm": 6.898252010345459,
      "learning_rate": 2e-05,
      "loss": 0.3268,
      "step": 12540
    },
    {
      "epoch": 3.9866581956797966,
      "grad_norm": 4.431435585021973,
      "learning_rate": 2e-05,
      "loss": 0.3205,
      "step": 12550
    },
    {
      "epoch": 3.9898348157560357,
      "grad_norm": 5.831711292266846,
      "learning_rate": 2e-05,
      "loss": 0.3426,
      "step": 12560
    },
    {
      "epoch": 3.9930114358322744,
      "grad_norm": 9.784432411193848,
      "learning_rate": 2e-05,
      "loss": 0.3158,
      "step": 12570
    },
    {
      "epoch": 3.9961880559085134,
      "grad_norm": 8.492918968200684,
      "learning_rate": 2e-05,
      "loss": 0.3047,
      "step": 12580
    },
    {
      "epoch": 3.999364675984752,
      "grad_norm": 5.694738864898682,
      "learning_rate": 2e-05,
      "loss": 0.314,
      "step": 12590
    },
    {
      "epoch": 4.002541296060991,
      "grad_norm": 10.314467430114746,
      "learning_rate": 2e-05,
      "loss": 0.294,
      "step": 12600
    },
    {
      "epoch": 4.002541296060991,
      "eval_loss": 1.8213169574737549,
      "eval_mse": 1.819030143016455,
      "eval_pearson": 0.3783326715014188,
      "eval_runtime": 7.505,
      "eval_samples_per_second": 2872.753,
      "eval_spearmanr": 0.3769794544521372,
      "eval_steps_per_second": 11.326,
      "step": 12600
    },
    {
      "epoch": 4.00571791613723,
      "grad_norm": 8.228075981140137,
      "learning_rate": 2e-05,
      "loss": 0.2822,
      "step": 12610
    },
    {
      "epoch": 4.008894536213469,
      "grad_norm": 8.528634071350098,
      "learning_rate": 2e-05,
      "loss": 0.2871,
      "step": 12620
    },
    {
      "epoch": 4.012071156289708,
      "grad_norm": 5.900599002838135,
      "learning_rate": 2e-05,
      "loss": 0.2941,
      "step": 12630
    },
    {
      "epoch": 4.015247776365946,
      "grad_norm": 12.34656810760498,
      "learning_rate": 2e-05,
      "loss": 0.2896,
      "step": 12640
    },
    {
      "epoch": 4.018424396442185,
      "grad_norm": 5.775909900665283,
      "learning_rate": 2e-05,
      "loss": 0.2737,
      "step": 12650
    },
    {
      "epoch": 4.021601016518424,
      "grad_norm": 6.813509464263916,
      "learning_rate": 2e-05,
      "loss": 0.2794,
      "step": 12660
    },
    {
      "epoch": 4.0247776365946635,
      "grad_norm": 5.78201961517334,
      "learning_rate": 2e-05,
      "loss": 0.2726,
      "step": 12670
    },
    {
      "epoch": 4.027954256670903,
      "grad_norm": 6.744632244110107,
      "learning_rate": 2e-05,
      "loss": 0.2845,
      "step": 12680
    },
    {
      "epoch": 4.031130876747141,
      "grad_norm": 6.283113956451416,
      "learning_rate": 2e-05,
      "loss": 0.2829,
      "step": 12690
    },
    {
      "epoch": 4.03430749682338,
      "grad_norm": 5.430298805236816,
      "learning_rate": 2e-05,
      "loss": 0.2945,
      "step": 12700
    },
    {
      "epoch": 4.037484116899619,
      "grad_norm": 5.293789386749268,
      "learning_rate": 2e-05,
      "loss": 0.2765,
      "step": 12710
    },
    {
      "epoch": 4.040660736975858,
      "grad_norm": 5.451168537139893,
      "learning_rate": 2e-05,
      "loss": 0.3025,
      "step": 12720
    },
    {
      "epoch": 4.043837357052096,
      "grad_norm": 8.996859550476074,
      "learning_rate": 2e-05,
      "loss": 0.2702,
      "step": 12730
    },
    {
      "epoch": 4.047013977128335,
      "grad_norm": 5.271692276000977,
      "learning_rate": 2e-05,
      "loss": 0.2798,
      "step": 12740
    },
    {
      "epoch": 4.050190597204574,
      "grad_norm": 7.887998104095459,
      "learning_rate": 2e-05,
      "loss": 0.286,
      "step": 12750
    },
    {
      "epoch": 4.0533672172808135,
      "grad_norm": 6.8304266929626465,
      "learning_rate": 2e-05,
      "loss": 0.2793,
      "step": 12760
    },
    {
      "epoch": 4.056543837357052,
      "grad_norm": 7.109191417694092,
      "learning_rate": 2e-05,
      "loss": 0.2808,
      "step": 12770
    },
    {
      "epoch": 4.059720457433291,
      "grad_norm": 4.424727439880371,
      "learning_rate": 2e-05,
      "loss": 0.2828,
      "step": 12780
    },
    {
      "epoch": 4.06289707750953,
      "grad_norm": 7.280300617218018,
      "learning_rate": 2e-05,
      "loss": 0.286,
      "step": 12790
    },
    {
      "epoch": 4.066073697585769,
      "grad_norm": 6.509333610534668,
      "learning_rate": 2e-05,
      "loss": 0.2733,
      "step": 12800
    },
    {
      "epoch": 4.069250317662007,
      "grad_norm": 8.022586822509766,
      "learning_rate": 2e-05,
      "loss": 0.2828,
      "step": 12810
    },
    {
      "epoch": 4.072426937738246,
      "grad_norm": 4.867122173309326,
      "learning_rate": 2e-05,
      "loss": 0.2993,
      "step": 12820
    },
    {
      "epoch": 4.075603557814485,
      "grad_norm": 5.214595794677734,
      "learning_rate": 2e-05,
      "loss": 0.289,
      "step": 12830
    },
    {
      "epoch": 4.0787801778907244,
      "grad_norm": 6.302516460418701,
      "learning_rate": 2e-05,
      "loss": 0.2798,
      "step": 12840
    },
    {
      "epoch": 4.0819567979669635,
      "grad_norm": 12.356860160827637,
      "learning_rate": 2e-05,
      "loss": 0.3007,
      "step": 12850
    },
    {
      "epoch": 4.085133418043202,
      "grad_norm": 5.768556118011475,
      "learning_rate": 2e-05,
      "loss": 0.2657,
      "step": 12860
    },
    {
      "epoch": 4.088310038119441,
      "grad_norm": 6.9429402351379395,
      "learning_rate": 2e-05,
      "loss": 0.2832,
      "step": 12870
    },
    {
      "epoch": 4.09148665819568,
      "grad_norm": 5.801998615264893,
      "learning_rate": 2e-05,
      "loss": 0.2776,
      "step": 12880
    },
    {
      "epoch": 4.094663278271919,
      "grad_norm": 5.657394886016846,
      "learning_rate": 2e-05,
      "loss": 0.2745,
      "step": 12890
    },
    {
      "epoch": 4.097839898348157,
      "grad_norm": 5.240342617034912,
      "learning_rate": 2e-05,
      "loss": 0.2797,
      "step": 12900
    },
    {
      "epoch": 4.101016518424396,
      "grad_norm": 7.202139854431152,
      "learning_rate": 2e-05,
      "loss": 0.2681,
      "step": 12910
    },
    {
      "epoch": 4.102604828462516,
      "eval_loss": 2.0296525955200195,
      "eval_mse": 2.028859286505128,
      "eval_pearson": 0.3950246048527343,
      "eval_runtime": 7.5101,
      "eval_samples_per_second": 2870.792,
      "eval_spearmanr": 0.38802384356894853,
      "eval_steps_per_second": 11.318,
      "step": 12915
    },
    {
      "epoch": 4.104193138500635,
      "grad_norm": 5.967655658721924,
      "learning_rate": 2e-05,
      "loss": 0.2766,
      "step": 12920
    },
    {
      "epoch": 4.1073697585768745,
      "grad_norm": 6.054511547088623,
      "learning_rate": 2e-05,
      "loss": 0.2924,
      "step": 12930
    },
    {
      "epoch": 4.110546378653113,
      "grad_norm": 7.0071330070495605,
      "learning_rate": 2e-05,
      "loss": 0.2706,
      "step": 12940
    },
    {
      "epoch": 4.113722998729352,
      "grad_norm": 8.446742057800293,
      "learning_rate": 2e-05,
      "loss": 0.2947,
      "step": 12950
    },
    {
      "epoch": 4.116899618805591,
      "grad_norm": 6.352656841278076,
      "learning_rate": 2e-05,
      "loss": 0.276,
      "step": 12960
    },
    {
      "epoch": 4.12007623888183,
      "grad_norm": 4.719152927398682,
      "learning_rate": 2e-05,
      "loss": 0.2723,
      "step": 12970
    },
    {
      "epoch": 4.123252858958069,
      "grad_norm": 7.595230579376221,
      "learning_rate": 2e-05,
      "loss": 0.2718,
      "step": 12980
    },
    {
      "epoch": 4.126429479034307,
      "grad_norm": 5.908344745635986,
      "learning_rate": 2e-05,
      "loss": 0.2914,
      "step": 12990
    },
    {
      "epoch": 4.129606099110546,
      "grad_norm": 5.765925884246826,
      "learning_rate": 2e-05,
      "loss": 0.2814,
      "step": 13000
    },
    {
      "epoch": 4.132782719186785,
      "grad_norm": 4.8370184898376465,
      "learning_rate": 2e-05,
      "loss": 0.2792,
      "step": 13010
    },
    {
      "epoch": 4.1359593392630245,
      "grad_norm": 4.615434169769287,
      "learning_rate": 2e-05,
      "loss": 0.2873,
      "step": 13020
    },
    {
      "epoch": 4.139135959339263,
      "grad_norm": 6.042316913604736,
      "learning_rate": 2e-05,
      "loss": 0.2792,
      "step": 13030
    },
    {
      "epoch": 4.142312579415502,
      "grad_norm": 7.053395748138428,
      "learning_rate": 2e-05,
      "loss": 0.2733,
      "step": 13040
    },
    {
      "epoch": 4.145489199491741,
      "grad_norm": 4.09840202331543,
      "learning_rate": 2e-05,
      "loss": 0.2642,
      "step": 13050
    },
    {
      "epoch": 4.14866581956798,
      "grad_norm": 4.570496082305908,
      "learning_rate": 2e-05,
      "loss": 0.2871,
      "step": 13060
    },
    {
      "epoch": 4.151842439644218,
      "grad_norm": 5.504523277282715,
      "learning_rate": 2e-05,
      "loss": 0.2831,
      "step": 13070
    },
    {
      "epoch": 4.155019059720457,
      "grad_norm": 4.578266143798828,
      "learning_rate": 2e-05,
      "loss": 0.2659,
      "step": 13080
    },
    {
      "epoch": 4.158195679796696,
      "grad_norm": 6.905998706817627,
      "learning_rate": 2e-05,
      "loss": 0.2958,
      "step": 13090
    },
    {
      "epoch": 4.161372299872935,
      "grad_norm": 5.134213924407959,
      "learning_rate": 2e-05,
      "loss": 0.2707,
      "step": 13100
    },
    {
      "epoch": 4.1645489199491745,
      "grad_norm": 7.741466999053955,
      "learning_rate": 2e-05,
      "loss": 0.2902,
      "step": 13110
    },
    {
      "epoch": 4.167725540025413,
      "grad_norm": 12.795483589172363,
      "learning_rate": 2e-05,
      "loss": 0.2973,
      "step": 13120
    },
    {
      "epoch": 4.170902160101652,
      "grad_norm": 6.113658905029297,
      "learning_rate": 2e-05,
      "loss": 0.2902,
      "step": 13130
    },
    {
      "epoch": 4.174078780177891,
      "grad_norm": 7.650465488433838,
      "learning_rate": 2e-05,
      "loss": 0.2632,
      "step": 13140
    },
    {
      "epoch": 4.17725540025413,
      "grad_norm": 6.538068771362305,
      "learning_rate": 2e-05,
      "loss": 0.273,
      "step": 13150
    },
    {
      "epoch": 4.180432020330368,
      "grad_norm": 5.585543155670166,
      "learning_rate": 2e-05,
      "loss": 0.2801,
      "step": 13160
    },
    {
      "epoch": 4.183608640406607,
      "grad_norm": 5.624590873718262,
      "learning_rate": 2e-05,
      "loss": 0.2748,
      "step": 13170
    },
    {
      "epoch": 4.186785260482846,
      "grad_norm": 9.695820808410645,
      "learning_rate": 2e-05,
      "loss": 0.2782,
      "step": 13180
    },
    {
      "epoch": 4.1899618805590855,
      "grad_norm": 8.643453598022461,
      "learning_rate": 2e-05,
      "loss": 0.274,
      "step": 13190
    },
    {
      "epoch": 4.193138500635324,
      "grad_norm": 8.412518501281738,
      "learning_rate": 2e-05,
      "loss": 0.3051,
      "step": 13200
    },
    {
      "epoch": 4.196315120711563,
      "grad_norm": 5.122969627380371,
      "learning_rate": 2e-05,
      "loss": 0.2897,
      "step": 13210
    },
    {
      "epoch": 4.199491740787802,
      "grad_norm": 5.013923645019531,
      "learning_rate": 2e-05,
      "loss": 0.2831,
      "step": 13220
    },
    {
      "epoch": 4.202668360864041,
      "grad_norm": 4.722074508666992,
      "learning_rate": 2e-05,
      "loss": 0.2934,
      "step": 13230
    },
    {
      "epoch": 4.202668360864041,
      "eval_loss": 1.7406032085418701,
      "eval_mse": 1.7392932132356462,
      "eval_pearson": 0.37440842610581315,
      "eval_runtime": 7.5197,
      "eval_samples_per_second": 2867.123,
      "eval_spearmanr": 0.36744012626120315,
      "eval_steps_per_second": 11.304,
      "step": 13230
    },
    {
      "epoch": 4.205844980940279,
      "grad_norm": 10.821735382080078,
      "learning_rate": 2e-05,
      "loss": 0.285,
      "step": 13240
    },
    {
      "epoch": 4.209021601016518,
      "grad_norm": 10.083107948303223,
      "learning_rate": 2e-05,
      "loss": 0.3144,
      "step": 13250
    },
    {
      "epoch": 4.212198221092757,
      "grad_norm": 9.655471801757812,
      "learning_rate": 2e-05,
      "loss": 0.2751,
      "step": 13260
    },
    {
      "epoch": 4.215374841168996,
      "grad_norm": 6.739071846008301,
      "learning_rate": 2e-05,
      "loss": 0.2714,
      "step": 13270
    },
    {
      "epoch": 4.2185514612452355,
      "grad_norm": 10.394285202026367,
      "learning_rate": 2e-05,
      "loss": 0.2732,
      "step": 13280
    },
    {
      "epoch": 4.221728081321474,
      "grad_norm": 5.559955596923828,
      "learning_rate": 2e-05,
      "loss": 0.2684,
      "step": 13290
    },
    {
      "epoch": 4.224904701397713,
      "grad_norm": 6.482410430908203,
      "learning_rate": 2e-05,
      "loss": 0.2732,
      "step": 13300
    },
    {
      "epoch": 4.228081321473952,
      "grad_norm": 5.2906036376953125,
      "learning_rate": 2e-05,
      "loss": 0.2739,
      "step": 13310
    },
    {
      "epoch": 4.231257941550191,
      "grad_norm": 6.122494697570801,
      "learning_rate": 2e-05,
      "loss": 0.2817,
      "step": 13320
    },
    {
      "epoch": 4.234434561626429,
      "grad_norm": 5.011774063110352,
      "learning_rate": 2e-05,
      "loss": 0.278,
      "step": 13330
    },
    {
      "epoch": 4.237611181702668,
      "grad_norm": 6.420507907867432,
      "learning_rate": 2e-05,
      "loss": 0.2621,
      "step": 13340
    },
    {
      "epoch": 4.240787801778907,
      "grad_norm": 11.675798416137695,
      "learning_rate": 2e-05,
      "loss": 0.2874,
      "step": 13350
    },
    {
      "epoch": 4.243964421855146,
      "grad_norm": 8.381365776062012,
      "learning_rate": 2e-05,
      "loss": 0.2808,
      "step": 13360
    },
    {
      "epoch": 4.247141041931385,
      "grad_norm": 5.753515243530273,
      "learning_rate": 2e-05,
      "loss": 0.2738,
      "step": 13370
    },
    {
      "epoch": 4.250317662007624,
      "grad_norm": 4.9303741455078125,
      "learning_rate": 2e-05,
      "loss": 0.2972,
      "step": 13380
    },
    {
      "epoch": 4.253494282083863,
      "grad_norm": 4.962006568908691,
      "learning_rate": 2e-05,
      "loss": 0.2715,
      "step": 13390
    },
    {
      "epoch": 4.256670902160102,
      "grad_norm": 6.061363697052002,
      "learning_rate": 2e-05,
      "loss": 0.2794,
      "step": 13400
    },
    {
      "epoch": 4.25984752223634,
      "grad_norm": 5.57418155670166,
      "learning_rate": 2e-05,
      "loss": 0.2838,
      "step": 13410
    },
    {
      "epoch": 4.263024142312579,
      "grad_norm": 6.620855808258057,
      "learning_rate": 2e-05,
      "loss": 0.2947,
      "step": 13420
    },
    {
      "epoch": 4.266200762388818,
      "grad_norm": 4.890716075897217,
      "learning_rate": 2e-05,
      "loss": 0.2627,
      "step": 13430
    },
    {
      "epoch": 4.269377382465057,
      "grad_norm": 9.402310371398926,
      "learning_rate": 2e-05,
      "loss": 0.2875,
      "step": 13440
    },
    {
      "epoch": 4.2725540025412965,
      "grad_norm": 4.283604145050049,
      "learning_rate": 2e-05,
      "loss": 0.2658,
      "step": 13450
    },
    {
      "epoch": 4.275730622617535,
      "grad_norm": 7.375030517578125,
      "learning_rate": 2e-05,
      "loss": 0.2686,
      "step": 13460
    },
    {
      "epoch": 4.278907242693774,
      "grad_norm": 5.2808308601379395,
      "learning_rate": 2e-05,
      "loss": 0.2922,
      "step": 13470
    },
    {
      "epoch": 4.282083862770013,
      "grad_norm": 4.1991448402404785,
      "learning_rate": 2e-05,
      "loss": 0.2705,
      "step": 13480
    },
    {
      "epoch": 4.285260482846252,
      "grad_norm": 4.746638298034668,
      "learning_rate": 2e-05,
      "loss": 0.2861,
      "step": 13490
    },
    {
      "epoch": 4.28843710292249,
      "grad_norm": 6.617621421813965,
      "learning_rate": 2e-05,
      "loss": 0.2725,
      "step": 13500
    },
    {
      "epoch": 4.291613722998729,
      "grad_norm": 11.969206809997559,
      "learning_rate": 2e-05,
      "loss": 0.2663,
      "step": 13510
    },
    {
      "epoch": 4.294790343074968,
      "grad_norm": 9.176475524902344,
      "learning_rate": 2e-05,
      "loss": 0.2699,
      "step": 13520
    },
    {
      "epoch": 4.297966963151207,
      "grad_norm": 11.31222915649414,
      "learning_rate": 2e-05,
      "loss": 0.292,
      "step": 13530
    },
    {
      "epoch": 4.301143583227446,
      "grad_norm": 4.545205116271973,
      "learning_rate": 2e-05,
      "loss": 0.275,
      "step": 13540
    },
    {
      "epoch": 4.302731893265565,
      "eval_loss": 1.8358193635940552,
      "eval_mse": 1.8343261270443452,
      "eval_pearson": 0.4003935354705692,
      "eval_runtime": 7.6051,
      "eval_samples_per_second": 2834.941,
      "eval_spearmanr": 0.3929823934091937,
      "eval_steps_per_second": 11.177,
      "step": 13545
    },
    {
      "epoch": 4.304320203303685,
      "grad_norm": 5.4008378982543945,
      "learning_rate": 2e-05,
      "loss": 0.2842,
      "step": 13550
    },
    {
      "epoch": 4.307496823379924,
      "grad_norm": 4.9894280433654785,
      "learning_rate": 2e-05,
      "loss": 0.2545,
      "step": 13560
    },
    {
      "epoch": 4.310673443456163,
      "grad_norm": 6.8894362449646,
      "learning_rate": 2e-05,
      "loss": 0.2827,
      "step": 13570
    },
    {
      "epoch": 4.313850063532401,
      "grad_norm": 7.990213394165039,
      "learning_rate": 2e-05,
      "loss": 0.2824,
      "step": 13580
    },
    {
      "epoch": 4.31702668360864,
      "grad_norm": 4.041579723358154,
      "learning_rate": 2e-05,
      "loss": 0.2676,
      "step": 13590
    },
    {
      "epoch": 4.320203303684879,
      "grad_norm": 4.652061939239502,
      "learning_rate": 2e-05,
      "loss": 0.2799,
      "step": 13600
    },
    {
      "epoch": 4.323379923761118,
      "grad_norm": 7.164708614349365,
      "learning_rate": 2e-05,
      "loss": 0.2766,
      "step": 13610
    },
    {
      "epoch": 4.326556543837357,
      "grad_norm": 6.249497890472412,
      "learning_rate": 2e-05,
      "loss": 0.2743,
      "step": 13620
    },
    {
      "epoch": 4.329733163913596,
      "grad_norm": 6.979284286499023,
      "learning_rate": 2e-05,
      "loss": 0.2667,
      "step": 13630
    },
    {
      "epoch": 4.332909783989835,
      "grad_norm": 7.190919399261475,
      "learning_rate": 2e-05,
      "loss": 0.2827,
      "step": 13640
    },
    {
      "epoch": 4.336086404066074,
      "grad_norm": 4.774279594421387,
      "learning_rate": 2e-05,
      "loss": 0.2771,
      "step": 13650
    },
    {
      "epoch": 4.339263024142313,
      "grad_norm": 4.604471683502197,
      "learning_rate": 2e-05,
      "loss": 0.2943,
      "step": 13660
    },
    {
      "epoch": 4.342439644218551,
      "grad_norm": 5.4810919761657715,
      "learning_rate": 2e-05,
      "loss": 0.2746,
      "step": 13670
    },
    {
      "epoch": 4.34561626429479,
      "grad_norm": 6.014404296875,
      "learning_rate": 2e-05,
      "loss": 0.2867,
      "step": 13680
    },
    {
      "epoch": 4.348792884371029,
      "grad_norm": 6.42691707611084,
      "learning_rate": 2e-05,
      "loss": 0.2814,
      "step": 13690
    },
    {
      "epoch": 4.351969504447268,
      "grad_norm": 9.13647174835205,
      "learning_rate": 2e-05,
      "loss": 0.2796,
      "step": 13700
    },
    {
      "epoch": 4.355146124523507,
      "grad_norm": 4.614048480987549,
      "learning_rate": 2e-05,
      "loss": 0.2635,
      "step": 13710
    },
    {
      "epoch": 4.358322744599746,
      "grad_norm": 4.357560634613037,
      "learning_rate": 2e-05,
      "loss": 0.271,
      "step": 13720
    },
    {
      "epoch": 4.361499364675985,
      "grad_norm": 7.882737636566162,
      "learning_rate": 2e-05,
      "loss": 0.2828,
      "step": 13730
    },
    {
      "epoch": 4.364675984752224,
      "grad_norm": 4.628849983215332,
      "learning_rate": 2e-05,
      "loss": 0.2622,
      "step": 13740
    },
    {
      "epoch": 4.367852604828462,
      "grad_norm": 5.735790252685547,
      "learning_rate": 2e-05,
      "loss": 0.2804,
      "step": 13750
    },
    {
      "epoch": 4.371029224904701,
      "grad_norm": 4.551815986633301,
      "learning_rate": 2e-05,
      "loss": 0.2636,
      "step": 13760
    },
    {
      "epoch": 4.37420584498094,
      "grad_norm": 7.666717529296875,
      "learning_rate": 2e-05,
      "loss": 0.2663,
      "step": 13770
    },
    {
      "epoch": 4.377382465057179,
      "grad_norm": 6.403076648712158,
      "learning_rate": 2e-05,
      "loss": 0.2594,
      "step": 13780
    },
    {
      "epoch": 4.380559085133418,
      "grad_norm": 10.152063369750977,
      "learning_rate": 2e-05,
      "loss": 0.2846,
      "step": 13790
    },
    {
      "epoch": 4.383735705209657,
      "grad_norm": 7.425503730773926,
      "learning_rate": 2e-05,
      "loss": 0.2802,
      "step": 13800
    },
    {
      "epoch": 4.386912325285896,
      "grad_norm": 5.686579704284668,
      "learning_rate": 2e-05,
      "loss": 0.2827,
      "step": 13810
    },
    {
      "epoch": 4.390088945362135,
      "grad_norm": 4.234973907470703,
      "learning_rate": 2e-05,
      "loss": 0.2822,
      "step": 13820
    },
    {
      "epoch": 4.393265565438374,
      "grad_norm": 5.159128665924072,
      "learning_rate": 2e-05,
      "loss": 0.2641,
      "step": 13830
    },
    {
      "epoch": 4.396442185514612,
      "grad_norm": 11.152982711791992,
      "learning_rate": 2e-05,
      "loss": 0.2872,
      "step": 13840
    },
    {
      "epoch": 4.399618805590851,
      "grad_norm": 5.338028907775879,
      "learning_rate": 2e-05,
      "loss": 0.2857,
      "step": 13850
    },
    {
      "epoch": 4.40279542566709,
      "grad_norm": 4.844392776489258,
      "learning_rate": 2e-05,
      "loss": 0.2667,
      "step": 13860
    },
    {
      "epoch": 4.40279542566709,
      "eval_loss": 1.950056552886963,
      "eval_mse": 1.9485862663482247,
      "eval_pearson": 0.348726884959957,
      "eval_runtime": 7.5907,
      "eval_samples_per_second": 2840.323,
      "eval_spearmanr": 0.34931785431148316,
      "eval_steps_per_second": 11.198,
      "step": 13860
    },
    {
      "epoch": 4.405972045743329,
      "grad_norm": 10.826156616210938,
      "learning_rate": 2e-05,
      "loss": 0.272,
      "step": 13870
    },
    {
      "epoch": 4.409148665819568,
      "grad_norm": 5.722609043121338,
      "learning_rate": 2e-05,
      "loss": 0.2791,
      "step": 13880
    },
    {
      "epoch": 4.412325285895807,
      "grad_norm": 8.410093307495117,
      "learning_rate": 2e-05,
      "loss": 0.2645,
      "step": 13890
    },
    {
      "epoch": 4.415501905972046,
      "grad_norm": 9.450054168701172,
      "learning_rate": 2e-05,
      "loss": 0.3016,
      "step": 13900
    },
    {
      "epoch": 4.418678526048285,
      "grad_norm": 5.112576007843018,
      "learning_rate": 2e-05,
      "loss": 0.2843,
      "step": 13910
    },
    {
      "epoch": 4.421855146124524,
      "grad_norm": 4.60124397277832,
      "learning_rate": 2e-05,
      "loss": 0.2778,
      "step": 13920
    },
    {
      "epoch": 4.425031766200762,
      "grad_norm": 6.398189067840576,
      "learning_rate": 2e-05,
      "loss": 0.2793,
      "step": 13930
    },
    {
      "epoch": 4.428208386277001,
      "grad_norm": 4.56209135055542,
      "learning_rate": 2e-05,
      "loss": 0.2673,
      "step": 13940
    },
    {
      "epoch": 4.43138500635324,
      "grad_norm": 4.950029373168945,
      "learning_rate": 2e-05,
      "loss": 0.2757,
      "step": 13950
    },
    {
      "epoch": 4.434561626429479,
      "grad_norm": 5.391984462738037,
      "learning_rate": 2e-05,
      "loss": 0.2714,
      "step": 13960
    },
    {
      "epoch": 4.437738246505718,
      "grad_norm": 5.457276821136475,
      "learning_rate": 2e-05,
      "loss": 0.2596,
      "step": 13970
    },
    {
      "epoch": 4.440914866581957,
      "grad_norm": 5.818925380706787,
      "learning_rate": 2e-05,
      "loss": 0.2697,
      "step": 13980
    },
    {
      "epoch": 4.444091486658196,
      "grad_norm": 5.299205303192139,
      "learning_rate": 2e-05,
      "loss": 0.2675,
      "step": 13990
    },
    {
      "epoch": 4.447268106734435,
      "grad_norm": 6.019679546356201,
      "learning_rate": 2e-05,
      "loss": 0.2747,
      "step": 14000
    },
    {
      "epoch": 4.450444726810673,
      "grad_norm": 8.960990905761719,
      "learning_rate": 2e-05,
      "loss": 0.2958,
      "step": 14010
    },
    {
      "epoch": 4.453621346886912,
      "grad_norm": 4.6507978439331055,
      "learning_rate": 2e-05,
      "loss": 0.2725,
      "step": 14020
    },
    {
      "epoch": 4.456797966963151,
      "grad_norm": 5.460712432861328,
      "learning_rate": 2e-05,
      "loss": 0.2706,
      "step": 14030
    },
    {
      "epoch": 4.45997458703939,
      "grad_norm": 6.783434867858887,
      "learning_rate": 2e-05,
      "loss": 0.2673,
      "step": 14040
    },
    {
      "epoch": 4.463151207115629,
      "grad_norm": 9.817584991455078,
      "learning_rate": 2e-05,
      "loss": 0.2883,
      "step": 14050
    },
    {
      "epoch": 4.466327827191868,
      "grad_norm": 5.515993595123291,
      "learning_rate": 2e-05,
      "loss": 0.2773,
      "step": 14060
    },
    {
      "epoch": 4.469504447268107,
      "grad_norm": 6.377174377441406,
      "learning_rate": 2e-05,
      "loss": 0.2652,
      "step": 14070
    },
    {
      "epoch": 4.472681067344346,
      "grad_norm": 5.282459259033203,
      "learning_rate": 2e-05,
      "loss": 0.2754,
      "step": 14080
    },
    {
      "epoch": 4.475857687420585,
      "grad_norm": 6.685486793518066,
      "learning_rate": 2e-05,
      "loss": 0.2784,
      "step": 14090
    },
    {
      "epoch": 4.479034307496823,
      "grad_norm": 4.957388877868652,
      "learning_rate": 2e-05,
      "loss": 0.2905,
      "step": 14100
    },
    {
      "epoch": 4.482210927573062,
      "grad_norm": 7.495612144470215,
      "learning_rate": 2e-05,
      "loss": 0.2694,
      "step": 14110
    },
    {
      "epoch": 4.485387547649301,
      "grad_norm": 10.931886672973633,
      "learning_rate": 2e-05,
      "loss": 0.2709,
      "step": 14120
    },
    {
      "epoch": 4.48856416772554,
      "grad_norm": 5.875077724456787,
      "learning_rate": 2e-05,
      "loss": 0.2611,
      "step": 14130
    },
    {
      "epoch": 4.4917407878017785,
      "grad_norm": 6.109969615936279,
      "learning_rate": 2e-05,
      "loss": 0.2784,
      "step": 14140
    },
    {
      "epoch": 4.494917407878018,
      "grad_norm": 4.56961727142334,
      "learning_rate": 2e-05,
      "loss": 0.2856,
      "step": 14150
    },
    {
      "epoch": 4.498094027954257,
      "grad_norm": 7.25883150100708,
      "learning_rate": 2e-05,
      "loss": 0.2958,
      "step": 14160
    },
    {
      "epoch": 4.501270648030496,
      "grad_norm": 5.32763147354126,
      "learning_rate": 2e-05,
      "loss": 0.2691,
      "step": 14170
    },
    {
      "epoch": 4.502858958068615,
      "eval_loss": 1.84230637550354,
      "eval_mse": 1.8402638706293972,
      "eval_pearson": 0.37976818680395424,
      "eval_runtime": 7.5114,
      "eval_samples_per_second": 2870.292,
      "eval_spearmanr": 0.3799953418605274,
      "eval_steps_per_second": 11.316,
      "step": 14175
    },
    {
      "epoch": 4.504447268106734,
      "grad_norm": 5.605052947998047,
      "learning_rate": 2e-05,
      "loss": 0.2671,
      "step": 14180
    },
    {
      "epoch": 4.507623888182973,
      "grad_norm": 4.801552772521973,
      "learning_rate": 2e-05,
      "loss": 0.2641,
      "step": 14190
    },
    {
      "epoch": 4.510800508259212,
      "grad_norm": 7.514833450317383,
      "learning_rate": 2e-05,
      "loss": 0.2923,
      "step": 14200
    },
    {
      "epoch": 4.513977128335451,
      "grad_norm": 5.163625717163086,
      "learning_rate": 2e-05,
      "loss": 0.2671,
      "step": 14210
    },
    {
      "epoch": 4.51715374841169,
      "grad_norm": 6.551292896270752,
      "learning_rate": 2e-05,
      "loss": 0.2668,
      "step": 14220
    },
    {
      "epoch": 4.520330368487929,
      "grad_norm": 5.239560127258301,
      "learning_rate": 2e-05,
      "loss": 0.2657,
      "step": 14230
    },
    {
      "epoch": 4.523506988564168,
      "grad_norm": 4.272144317626953,
      "learning_rate": 2e-05,
      "loss": 0.258,
      "step": 14240
    },
    {
      "epoch": 4.526683608640407,
      "grad_norm": 8.708818435668945,
      "learning_rate": 2e-05,
      "loss": 0.2671,
      "step": 14250
    },
    {
      "epoch": 4.529860228716646,
      "grad_norm": 5.827938556671143,
      "learning_rate": 2e-05,
      "loss": 0.2814,
      "step": 14260
    },
    {
      "epoch": 4.533036848792884,
      "grad_norm": 5.556466579437256,
      "learning_rate": 2e-05,
      "loss": 0.2493,
      "step": 14270
    },
    {
      "epoch": 4.536213468869123,
      "grad_norm": 10.115199089050293,
      "learning_rate": 2e-05,
      "loss": 0.2511,
      "step": 14280
    },
    {
      "epoch": 4.539390088945362,
      "grad_norm": 4.979372978210449,
      "learning_rate": 2e-05,
      "loss": 0.2702,
      "step": 14290
    },
    {
      "epoch": 4.542566709021601,
      "grad_norm": 9.883634567260742,
      "learning_rate": 2e-05,
      "loss": 0.2588,
      "step": 14300
    },
    {
      "epoch": 4.5457433290978395,
      "grad_norm": 5.977548599243164,
      "learning_rate": 2e-05,
      "loss": 0.2594,
      "step": 14310
    },
    {
      "epoch": 4.548919949174079,
      "grad_norm": 5.652538776397705,
      "learning_rate": 2e-05,
      "loss": 0.2807,
      "step": 14320
    },
    {
      "epoch": 4.552096569250318,
      "grad_norm": 6.386992931365967,
      "learning_rate": 2e-05,
      "loss": 0.2567,
      "step": 14330
    },
    {
      "epoch": 4.555273189326557,
      "grad_norm": 4.320425987243652,
      "learning_rate": 2e-05,
      "loss": 0.2681,
      "step": 14340
    },
    {
      "epoch": 4.558449809402795,
      "grad_norm": 5.434116840362549,
      "learning_rate": 2e-05,
      "loss": 0.2597,
      "step": 14350
    },
    {
      "epoch": 4.561626429479034,
      "grad_norm": 6.116066932678223,
      "learning_rate": 2e-05,
      "loss": 0.2557,
      "step": 14360
    },
    {
      "epoch": 4.564803049555273,
      "grad_norm": 5.45340633392334,
      "learning_rate": 2e-05,
      "loss": 0.2677,
      "step": 14370
    },
    {
      "epoch": 4.567979669631512,
      "grad_norm": 6.909956455230713,
      "learning_rate": 2e-05,
      "loss": 0.2541,
      "step": 14380
    },
    {
      "epoch": 4.571156289707751,
      "grad_norm": 4.114919662475586,
      "learning_rate": 2e-05,
      "loss": 0.2736,
      "step": 14390
    },
    {
      "epoch": 4.5743329097839895,
      "grad_norm": 6.169317722320557,
      "learning_rate": 2e-05,
      "loss": 0.2872,
      "step": 14400
    },
    {
      "epoch": 4.577509529860229,
      "grad_norm": 4.283586502075195,
      "learning_rate": 2e-05,
      "loss": 0.2841,
      "step": 14410
    },
    {
      "epoch": 4.580686149936468,
      "grad_norm": 8.567370414733887,
      "learning_rate": 2e-05,
      "loss": 0.2751,
      "step": 14420
    },
    {
      "epoch": 4.583862770012707,
      "grad_norm": 6.671688556671143,
      "learning_rate": 2e-05,
      "loss": 0.2709,
      "step": 14430
    },
    {
      "epoch": 4.587039390088945,
      "grad_norm": 5.377408027648926,
      "learning_rate": 2e-05,
      "loss": 0.2912,
      "step": 14440
    },
    {
      "epoch": 4.590216010165184,
      "grad_norm": 6.48272180557251,
      "learning_rate": 2e-05,
      "loss": 0.2691,
      "step": 14450
    },
    {
      "epoch": 4.593392630241423,
      "grad_norm": 7.844117641448975,
      "learning_rate": 2e-05,
      "loss": 0.2549,
      "step": 14460
    },
    {
      "epoch": 4.596569250317662,
      "grad_norm": 7.480242729187012,
      "learning_rate": 2e-05,
      "loss": 0.2663,
      "step": 14470
    },
    {
      "epoch": 4.599745870393901,
      "grad_norm": 6.371362686157227,
      "learning_rate": 2e-05,
      "loss": 0.245,
      "step": 14480
    },
    {
      "epoch": 4.60292249047014,
      "grad_norm": 4.060647487640381,
      "learning_rate": 2e-05,
      "loss": 0.252,
      "step": 14490
    },
    {
      "epoch": 4.60292249047014,
      "eval_loss": 1.7351737022399902,
      "eval_mse": 1.7340244536718323,
      "eval_pearson": 0.40879211441165103,
      "eval_runtime": 7.4952,
      "eval_samples_per_second": 2876.504,
      "eval_spearmanr": 0.4049980843184804,
      "eval_steps_per_second": 11.341,
      "step": 14490
    },
    {
      "epoch": 4.606099110546379,
      "grad_norm": 4.768277645111084,
      "learning_rate": 2e-05,
      "loss": 0.2544,
      "step": 14500
    },
    {
      "epoch": 4.609275730622618,
      "grad_norm": 5.373359203338623,
      "learning_rate": 2e-05,
      "loss": 0.267,
      "step": 14510
    },
    {
      "epoch": 4.612452350698856,
      "grad_norm": 5.405158996582031,
      "learning_rate": 2e-05,
      "loss": 0.2794,
      "step": 14520
    },
    {
      "epoch": 4.615628970775095,
      "grad_norm": 5.478062629699707,
      "learning_rate": 2e-05,
      "loss": 0.2496,
      "step": 14530
    },
    {
      "epoch": 4.618805590851334,
      "grad_norm": 7.5671820640563965,
      "learning_rate": 2e-05,
      "loss": 0.2871,
      "step": 14540
    },
    {
      "epoch": 4.621982210927573,
      "grad_norm": 4.798323154449463,
      "learning_rate": 2e-05,
      "loss": 0.2422,
      "step": 14550
    },
    {
      "epoch": 4.625158831003812,
      "grad_norm": 6.578803062438965,
      "learning_rate": 2e-05,
      "loss": 0.2609,
      "step": 14560
    },
    {
      "epoch": 4.6283354510800505,
      "grad_norm": 6.859597682952881,
      "learning_rate": 2e-05,
      "loss": 0.2731,
      "step": 14570
    },
    {
      "epoch": 4.63151207115629,
      "grad_norm": 6.725753307342529,
      "learning_rate": 2e-05,
      "loss": 0.2816,
      "step": 14580
    },
    {
      "epoch": 4.634688691232529,
      "grad_norm": 7.428661346435547,
      "learning_rate": 2e-05,
      "loss": 0.2763,
      "step": 14590
    },
    {
      "epoch": 4.637865311308768,
      "grad_norm": 6.387252330780029,
      "learning_rate": 2e-05,
      "loss": 0.2627,
      "step": 14600
    },
    {
      "epoch": 4.641041931385006,
      "grad_norm": 7.361461162567139,
      "learning_rate": 2e-05,
      "loss": 0.2527,
      "step": 14610
    },
    {
      "epoch": 4.644218551461245,
      "grad_norm": 4.977965831756592,
      "learning_rate": 2e-05,
      "loss": 0.2644,
      "step": 14620
    },
    {
      "epoch": 4.647395171537484,
      "grad_norm": 5.724605560302734,
      "learning_rate": 2e-05,
      "loss": 0.2593,
      "step": 14630
    },
    {
      "epoch": 4.650571791613723,
      "grad_norm": 5.552927017211914,
      "learning_rate": 2e-05,
      "loss": 0.2605,
      "step": 14640
    },
    {
      "epoch": 4.653748411689962,
      "grad_norm": 8.489227294921875,
      "learning_rate": 2e-05,
      "loss": 0.2625,
      "step": 14650
    },
    {
      "epoch": 4.6569250317662005,
      "grad_norm": 5.366273403167725,
      "learning_rate": 2e-05,
      "loss": 0.2602,
      "step": 14660
    },
    {
      "epoch": 4.66010165184244,
      "grad_norm": 4.035733699798584,
      "learning_rate": 2e-05,
      "loss": 0.2678,
      "step": 14670
    },
    {
      "epoch": 4.663278271918679,
      "grad_norm": 7.316432476043701,
      "learning_rate": 2e-05,
      "loss": 0.262,
      "step": 14680
    },
    {
      "epoch": 4.666454891994917,
      "grad_norm": 5.628673076629639,
      "learning_rate": 2e-05,
      "loss": 0.2711,
      "step": 14690
    },
    {
      "epoch": 4.669631512071156,
      "grad_norm": 4.5440778732299805,
      "learning_rate": 2e-05,
      "loss": 0.2761,
      "step": 14700
    },
    {
      "epoch": 4.672808132147395,
      "grad_norm": 7.185235977172852,
      "learning_rate": 2e-05,
      "loss": 0.2661,
      "step": 14710
    },
    {
      "epoch": 4.675984752223634,
      "grad_norm": 6.7014689445495605,
      "learning_rate": 2e-05,
      "loss": 0.2633,
      "step": 14720
    },
    {
      "epoch": 4.679161372299873,
      "grad_norm": 5.492427349090576,
      "learning_rate": 2e-05,
      "loss": 0.2606,
      "step": 14730
    },
    {
      "epoch": 4.6823379923761115,
      "grad_norm": 4.863950252532959,
      "learning_rate": 2e-05,
      "loss": 0.2652,
      "step": 14740
    },
    {
      "epoch": 4.685514612452351,
      "grad_norm": 4.3237624168396,
      "learning_rate": 2e-05,
      "loss": 0.2731,
      "step": 14750
    },
    {
      "epoch": 4.68869123252859,
      "grad_norm": 4.913020133972168,
      "learning_rate": 2e-05,
      "loss": 0.2502,
      "step": 14760
    },
    {
      "epoch": 4.691867852604829,
      "grad_norm": 5.873133659362793,
      "learning_rate": 2e-05,
      "loss": 0.2592,
      "step": 14770
    },
    {
      "epoch": 4.695044472681067,
      "grad_norm": 8.408060073852539,
      "learning_rate": 2e-05,
      "loss": 0.264,
      "step": 14780
    },
    {
      "epoch": 4.698221092757306,
      "grad_norm": 4.488544940948486,
      "learning_rate": 2e-05,
      "loss": 0.2477,
      "step": 14790
    },
    {
      "epoch": 4.701397712833545,
      "grad_norm": 4.188958644866943,
      "learning_rate": 2e-05,
      "loss": 0.2559,
      "step": 14800
    },
    {
      "epoch": 4.702986022871665,
      "eval_loss": 1.922354817390442,
      "eval_mse": 1.9212497225636012,
      "eval_pearson": 0.36113172848311914,
      "eval_runtime": 7.3875,
      "eval_samples_per_second": 2918.428,
      "eval_spearmanr": 0.3608871380533864,
      "eval_steps_per_second": 11.506,
      "step": 14805
    },
    {
      "epoch": 4.704574332909784,
      "grad_norm": 4.407984733581543,
      "learning_rate": 2e-05,
      "loss": 0.27,
      "step": 14810
    },
    {
      "epoch": 4.707750952986023,
      "grad_norm": 6.3233208656311035,
      "learning_rate": 2e-05,
      "loss": 0.2566,
      "step": 14820
    },
    {
      "epoch": 4.7109275730622615,
      "grad_norm": 4.7597246170043945,
      "learning_rate": 2e-05,
      "loss": 0.2591,
      "step": 14830
    },
    {
      "epoch": 4.714104193138501,
      "grad_norm": 7.277591228485107,
      "learning_rate": 2e-05,
      "loss": 0.2973,
      "step": 14840
    },
    {
      "epoch": 4.71728081321474,
      "grad_norm": 4.214722633361816,
      "learning_rate": 2e-05,
      "loss": 0.2504,
      "step": 14850
    },
    {
      "epoch": 4.720457433290979,
      "grad_norm": 7.182274341583252,
      "learning_rate": 2e-05,
      "loss": 0.2707,
      "step": 14860
    },
    {
      "epoch": 4.723634053367217,
      "grad_norm": 4.673086643218994,
      "learning_rate": 2e-05,
      "loss": 0.2532,
      "step": 14870
    },
    {
      "epoch": 4.726810673443456,
      "grad_norm": 6.501355171203613,
      "learning_rate": 2e-05,
      "loss": 0.241,
      "step": 14880
    },
    {
      "epoch": 4.729987293519695,
      "grad_norm": 6.269479751586914,
      "learning_rate": 2e-05,
      "loss": 0.2554,
      "step": 14890
    },
    {
      "epoch": 4.733163913595934,
      "grad_norm": 5.005936145782471,
      "learning_rate": 2e-05,
      "loss": 0.2677,
      "step": 14900
    },
    {
      "epoch": 4.7363405336721724,
      "grad_norm": 5.711889266967773,
      "learning_rate": 2e-05,
      "loss": 0.2537,
      "step": 14910
    },
    {
      "epoch": 4.7395171537484115,
      "grad_norm": 5.883796691894531,
      "learning_rate": 2e-05,
      "loss": 0.2598,
      "step": 14920
    },
    {
      "epoch": 4.742693773824651,
      "grad_norm": 4.6734466552734375,
      "learning_rate": 2e-05,
      "loss": 0.2626,
      "step": 14930
    },
    {
      "epoch": 4.74587039390089,
      "grad_norm": 6.016146659851074,
      "learning_rate": 2e-05,
      "loss": 0.2528,
      "step": 14940
    },
    {
      "epoch": 4.749047013977128,
      "grad_norm": 5.583605766296387,
      "learning_rate": 2e-05,
      "loss": 0.2595,
      "step": 14950
    },
    {
      "epoch": 4.752223634053367,
      "grad_norm": 5.303300857543945,
      "learning_rate": 2e-05,
      "loss": 0.2529,
      "step": 14960
    },
    {
      "epoch": 4.755400254129606,
      "grad_norm": 6.4120025634765625,
      "learning_rate": 2e-05,
      "loss": 0.2537,
      "step": 14970
    },
    {
      "epoch": 4.758576874205845,
      "grad_norm": 4.446783065795898,
      "learning_rate": 2e-05,
      "loss": 0.264,
      "step": 14980
    },
    {
      "epoch": 4.761753494282084,
      "grad_norm": 6.717803955078125,
      "learning_rate": 2e-05,
      "loss": 0.2567,
      "step": 14990
    },
    {
      "epoch": 4.7649301143583225,
      "grad_norm": 4.577381134033203,
      "learning_rate": 2e-05,
      "loss": 0.2531,
      "step": 15000
    },
    {
      "epoch": 4.768106734434562,
      "grad_norm": 7.615378379821777,
      "learning_rate": 2e-05,
      "loss": 0.2611,
      "step": 15010
    },
    {
      "epoch": 4.771283354510801,
      "grad_norm": 5.369521141052246,
      "learning_rate": 2e-05,
      "loss": 0.2548,
      "step": 15020
    },
    {
      "epoch": 4.77445997458704,
      "grad_norm": 8.13452434539795,
      "learning_rate": 2e-05,
      "loss": 0.2824,
      "step": 15030
    },
    {
      "epoch": 4.777636594663278,
      "grad_norm": 5.416879653930664,
      "learning_rate": 2e-05,
      "loss": 0.2673,
      "step": 15040
    },
    {
      "epoch": 4.780813214739517,
      "grad_norm": 5.715744495391846,
      "learning_rate": 2e-05,
      "loss": 0.2594,
      "step": 15050
    },
    {
      "epoch": 4.783989834815756,
      "grad_norm": 5.257898330688477,
      "learning_rate": 2e-05,
      "loss": 0.2453,
      "step": 15060
    },
    {
      "epoch": 4.787166454891995,
      "grad_norm": 7.062392711639404,
      "learning_rate": 2e-05,
      "loss": 0.255,
      "step": 15070
    },
    {
      "epoch": 4.790343074968233,
      "grad_norm": 5.576462268829346,
      "learning_rate": 2e-05,
      "loss": 0.2534,
      "step": 15080
    },
    {
      "epoch": 4.7935196950444725,
      "grad_norm": 4.593119144439697,
      "learning_rate": 2e-05,
      "loss": 0.2545,
      "step": 15090
    },
    {
      "epoch": 4.796696315120712,
      "grad_norm": 4.5886406898498535,
      "learning_rate": 2e-05,
      "loss": 0.2526,
      "step": 15100
    },
    {
      "epoch": 4.799872935196951,
      "grad_norm": 5.584711074829102,
      "learning_rate": 2e-05,
      "loss": 0.2402,
      "step": 15110
    },
    {
      "epoch": 4.803049555273189,
      "grad_norm": 4.32464599609375,
      "learning_rate": 2e-05,
      "loss": 0.2553,
      "step": 15120
    },
    {
      "epoch": 4.803049555273189,
      "eval_loss": 1.9618701934814453,
      "eval_mse": 1.960236420281964,
      "eval_pearson": 0.3689565654537169,
      "eval_runtime": 7.6542,
      "eval_samples_per_second": 2816.745,
      "eval_spearmanr": 0.37129156486032633,
      "eval_steps_per_second": 11.105,
      "step": 15120
    },
    {
      "epoch": 4.806226175349428,
      "grad_norm": 5.555439472198486,
      "learning_rate": 2e-05,
      "loss": 0.2625,
      "step": 15130
    },
    {
      "epoch": 4.809402795425667,
      "grad_norm": 5.665910720825195,
      "learning_rate": 2e-05,
      "loss": 0.2493,
      "step": 15140
    },
    {
      "epoch": 4.812579415501906,
      "grad_norm": 6.910638332366943,
      "learning_rate": 2e-05,
      "loss": 0.2687,
      "step": 15150
    },
    {
      "epoch": 4.815756035578145,
      "grad_norm": 4.9870171546936035,
      "learning_rate": 2e-05,
      "loss": 0.2624,
      "step": 15160
    },
    {
      "epoch": 4.8189326556543834,
      "grad_norm": 6.781315326690674,
      "learning_rate": 2e-05,
      "loss": 0.2627,
      "step": 15170
    },
    {
      "epoch": 4.8221092757306225,
      "grad_norm": 6.824065685272217,
      "learning_rate": 2e-05,
      "loss": 0.2492,
      "step": 15180
    },
    {
      "epoch": 4.825285895806862,
      "grad_norm": 4.43982458114624,
      "learning_rate": 2e-05,
      "loss": 0.2704,
      "step": 15190
    },
    {
      "epoch": 4.828462515883101,
      "grad_norm": 4.551756858825684,
      "learning_rate": 2e-05,
      "loss": 0.261,
      "step": 15200
    },
    {
      "epoch": 4.831639135959339,
      "grad_norm": 6.315411567687988,
      "learning_rate": 2e-05,
      "loss": 0.2592,
      "step": 15210
    },
    {
      "epoch": 4.834815756035578,
      "grad_norm": 6.0039591789245605,
      "learning_rate": 2e-05,
      "loss": 0.2605,
      "step": 15220
    },
    {
      "epoch": 4.837992376111817,
      "grad_norm": 4.893156051635742,
      "learning_rate": 2e-05,
      "loss": 0.2523,
      "step": 15230
    },
    {
      "epoch": 4.841168996188056,
      "grad_norm": 4.915698051452637,
      "learning_rate": 2e-05,
      "loss": 0.26,
      "step": 15240
    },
    {
      "epoch": 4.844345616264295,
      "grad_norm": 7.351757049560547,
      "learning_rate": 2e-05,
      "loss": 0.2588,
      "step": 15250
    },
    {
      "epoch": 4.8475222363405335,
      "grad_norm": 4.670780181884766,
      "learning_rate": 2e-05,
      "loss": 0.2423,
      "step": 15260
    },
    {
      "epoch": 4.850698856416773,
      "grad_norm": 5.906061172485352,
      "learning_rate": 2e-05,
      "loss": 0.2652,
      "step": 15270
    },
    {
      "epoch": 4.853875476493012,
      "grad_norm": 4.377664089202881,
      "learning_rate": 2e-05,
      "loss": 0.2618,
      "step": 15280
    },
    {
      "epoch": 4.85705209656925,
      "grad_norm": 6.056003093719482,
      "learning_rate": 2e-05,
      "loss": 0.2539,
      "step": 15290
    },
    {
      "epoch": 4.860228716645489,
      "grad_norm": 5.801131725311279,
      "learning_rate": 2e-05,
      "loss": 0.2598,
      "step": 15300
    },
    {
      "epoch": 4.863405336721728,
      "grad_norm": 6.089645862579346,
      "learning_rate": 2e-05,
      "loss": 0.2496,
      "step": 15310
    },
    {
      "epoch": 4.866581956797967,
      "grad_norm": 7.392794609069824,
      "learning_rate": 2e-05,
      "loss": 0.2518,
      "step": 15320
    },
    {
      "epoch": 4.869758576874206,
      "grad_norm": 5.400365829467773,
      "learning_rate": 2e-05,
      "loss": 0.2617,
      "step": 15330
    },
    {
      "epoch": 4.872935196950444,
      "grad_norm": 4.638934135437012,
      "learning_rate": 2e-05,
      "loss": 0.2488,
      "step": 15340
    },
    {
      "epoch": 4.8761118170266835,
      "grad_norm": 4.5447306632995605,
      "learning_rate": 2e-05,
      "loss": 0.2563,
      "step": 15350
    },
    {
      "epoch": 4.879288437102923,
      "grad_norm": 7.654353141784668,
      "learning_rate": 2e-05,
      "loss": 0.2629,
      "step": 15360
    },
    {
      "epoch": 4.882465057179162,
      "grad_norm": 4.239601135253906,
      "learning_rate": 2e-05,
      "loss": 0.2605,
      "step": 15370
    },
    {
      "epoch": 4.8856416772554,
      "grad_norm": 4.2520751953125,
      "learning_rate": 2e-05,
      "loss": 0.2457,
      "step": 15380
    },
    {
      "epoch": 4.888818297331639,
      "grad_norm": 6.375646591186523,
      "learning_rate": 2e-05,
      "loss": 0.2479,
      "step": 15390
    },
    {
      "epoch": 4.891994917407878,
      "grad_norm": 5.29463529586792,
      "learning_rate": 2e-05,
      "loss": 0.2633,
      "step": 15400
    },
    {
      "epoch": 4.895171537484117,
      "grad_norm": 10.191719055175781,
      "learning_rate": 2e-05,
      "loss": 0.2477,
      "step": 15410
    },
    {
      "epoch": 4.898348157560356,
      "grad_norm": 8.37722110748291,
      "learning_rate": 2e-05,
      "loss": 0.2729,
      "step": 15420
    },
    {
      "epoch": 4.901524777636594,
      "grad_norm": 8.540587425231934,
      "learning_rate": 2e-05,
      "loss": 0.257,
      "step": 15430
    },
    {
      "epoch": 4.903113087674714,
      "eval_loss": 1.9192413091659546,
      "eval_mse": 1.917984024845824,
      "eval_pearson": 0.3792636578367131,
      "eval_runtime": 7.5013,
      "eval_samples_per_second": 2874.153,
      "eval_spearmanr": 0.37590367643617456,
      "eval_steps_per_second": 11.331,
      "step": 15435
    },
    {
      "epoch": 4.9047013977128335,
      "grad_norm": 4.293777942657471,
      "learning_rate": 2e-05,
      "loss": 0.2443,
      "step": 15440
    },
    {
      "epoch": 4.907878017789073,
      "grad_norm": 4.199134349822998,
      "learning_rate": 2e-05,
      "loss": 0.2461,
      "step": 15450
    },
    {
      "epoch": 4.911054637865311,
      "grad_norm": 6.498924255371094,
      "learning_rate": 2e-05,
      "loss": 0.259,
      "step": 15460
    },
    {
      "epoch": 4.91423125794155,
      "grad_norm": 5.066330432891846,
      "learning_rate": 2e-05,
      "loss": 0.2502,
      "step": 15470
    },
    {
      "epoch": 4.917407878017789,
      "grad_norm": 6.39182186126709,
      "learning_rate": 2e-05,
      "loss": 0.2521,
      "step": 15480
    },
    {
      "epoch": 4.920584498094028,
      "grad_norm": 7.4841156005859375,
      "learning_rate": 2e-05,
      "loss": 0.2565,
      "step": 15490
    },
    {
      "epoch": 4.923761118170267,
      "grad_norm": 4.553487777709961,
      "learning_rate": 2e-05,
      "loss": 0.2696,
      "step": 15500
    },
    {
      "epoch": 4.926937738246505,
      "grad_norm": 3.7891149520874023,
      "learning_rate": 2e-05,
      "loss": 0.2384,
      "step": 15510
    },
    {
      "epoch": 4.9301143583227445,
      "grad_norm": 5.9791388511657715,
      "learning_rate": 2e-05,
      "loss": 0.2642,
      "step": 15520
    },
    {
      "epoch": 4.933290978398984,
      "grad_norm": 6.25862455368042,
      "learning_rate": 2e-05,
      "loss": 0.2618,
      "step": 15530
    },
    {
      "epoch": 4.936467598475223,
      "grad_norm": 5.475295543670654,
      "learning_rate": 2e-05,
      "loss": 0.2415,
      "step": 15540
    },
    {
      "epoch": 4.939644218551461,
      "grad_norm": 5.384828090667725,
      "learning_rate": 2e-05,
      "loss": 0.2636,
      "step": 15550
    },
    {
      "epoch": 4.9428208386277,
      "grad_norm": 5.397160530090332,
      "learning_rate": 2e-05,
      "loss": 0.2599,
      "step": 15560
    },
    {
      "epoch": 4.945997458703939,
      "grad_norm": 5.671107769012451,
      "learning_rate": 2e-05,
      "loss": 0.2333,
      "step": 15570
    },
    {
      "epoch": 4.949174078780178,
      "grad_norm": 5.789602279663086,
      "learning_rate": 2e-05,
      "loss": 0.2697,
      "step": 15580
    },
    {
      "epoch": 4.952350698856417,
      "grad_norm": 7.883219242095947,
      "learning_rate": 2e-05,
      "loss": 0.2626,
      "step": 15590
    },
    {
      "epoch": 4.955527318932655,
      "grad_norm": 6.0579752922058105,
      "learning_rate": 2e-05,
      "loss": 0.2749,
      "step": 15600
    },
    {
      "epoch": 4.9587039390088945,
      "grad_norm": 5.50504207611084,
      "learning_rate": 2e-05,
      "loss": 0.2454,
      "step": 15610
    },
    {
      "epoch": 4.961880559085134,
      "grad_norm": 6.042123317718506,
      "learning_rate": 2e-05,
      "loss": 0.2295,
      "step": 15620
    },
    {
      "epoch": 4.965057179161373,
      "grad_norm": 3.8193273544311523,
      "learning_rate": 2e-05,
      "loss": 0.2451,
      "step": 15630
    },
    {
      "epoch": 4.968233799237611,
      "grad_norm": 6.396854877471924,
      "learning_rate": 2e-05,
      "loss": 0.2583,
      "step": 15640
    },
    {
      "epoch": 4.97141041931385,
      "grad_norm": 4.771857738494873,
      "learning_rate": 2e-05,
      "loss": 0.2569,
      "step": 15650
    },
    {
      "epoch": 4.974587039390089,
      "grad_norm": 4.680007457733154,
      "learning_rate": 2e-05,
      "loss": 0.2407,
      "step": 15660
    },
    {
      "epoch": 4.977763659466328,
      "grad_norm": 6.69942045211792,
      "learning_rate": 2e-05,
      "loss": 0.2509,
      "step": 15670
    },
    {
      "epoch": 4.980940279542566,
      "grad_norm": 7.8842315673828125,
      "learning_rate": 2e-05,
      "loss": 0.2532,
      "step": 15680
    },
    {
      "epoch": 4.984116899618805,
      "grad_norm": 5.171146869659424,
      "learning_rate": 2e-05,
      "loss": 0.2451,
      "step": 15690
    },
    {
      "epoch": 4.9872935196950445,
      "grad_norm": 5.399862289428711,
      "learning_rate": 2e-05,
      "loss": 0.2682,
      "step": 15700
    },
    {
      "epoch": 4.990470139771284,
      "grad_norm": 5.889913082122803,
      "learning_rate": 2e-05,
      "loss": 0.2518,
      "step": 15710
    },
    {
      "epoch": 4.993646759847522,
      "grad_norm": 4.982795715332031,
      "learning_rate": 2e-05,
      "loss": 0.2584,
      "step": 15720
    },
    {
      "epoch": 4.996823379923761,
      "grad_norm": 4.600299835205078,
      "learning_rate": 2e-05,
      "loss": 0.2605,
      "step": 15730
    },
    {
      "epoch": 5.0,
      "grad_norm": 4.563834190368652,
      "learning_rate": 2e-05,
      "loss": 0.2443,
      "step": 15740
    },
    {
      "epoch": 5.003176620076239,
      "grad_norm": 3.896789073944092,
      "learning_rate": 2e-05,
      "loss": 0.2116,
      "step": 15750
    },
    {
      "epoch": 5.003176620076239,
      "eval_loss": 1.9217380285263062,
      "eval_mse": 1.9202851614697682,
      "eval_pearson": 0.4171726076045934,
      "eval_runtime": 7.2865,
      "eval_samples_per_second": 2958.884,
      "eval_spearmanr": 0.41130848297116906,
      "eval_steps_per_second": 11.665,
      "step": 15750
    },
    {
      "epoch": 5.006353240152478,
      "grad_norm": 11.403264045715332,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 15760
    },
    {
      "epoch": 5.009529860228716,
      "grad_norm": 7.425755977630615,
      "learning_rate": 2e-05,
      "loss": 0.2525,
      "step": 15770
    },
    {
      "epoch": 5.0127064803049555,
      "grad_norm": 4.539307117462158,
      "learning_rate": 2e-05,
      "loss": 0.2088,
      "step": 15780
    },
    {
      "epoch": 5.015883100381195,
      "grad_norm": 5.625363826751709,
      "learning_rate": 2e-05,
      "loss": 0.251,
      "step": 15790
    },
    {
      "epoch": 5.019059720457434,
      "grad_norm": 4.420601844787598,
      "learning_rate": 2e-05,
      "loss": 0.2301,
      "step": 15800
    },
    {
      "epoch": 5.022236340533672,
      "grad_norm": 6.987155437469482,
      "learning_rate": 2e-05,
      "loss": 0.232,
      "step": 15810
    },
    {
      "epoch": 5.025412960609911,
      "grad_norm": 3.6854088306427,
      "learning_rate": 2e-05,
      "loss": 0.2247,
      "step": 15820
    },
    {
      "epoch": 5.02858958068615,
      "grad_norm": 6.7104878425598145,
      "learning_rate": 2e-05,
      "loss": 0.2371,
      "step": 15830
    },
    {
      "epoch": 5.031766200762389,
      "grad_norm": 4.4205322265625,
      "learning_rate": 2e-05,
      "loss": 0.2208,
      "step": 15840
    },
    {
      "epoch": 5.034942820838627,
      "grad_norm": 8.519813537597656,
      "learning_rate": 2e-05,
      "loss": 0.2462,
      "step": 15850
    },
    {
      "epoch": 5.038119440914866,
      "grad_norm": 6.027523517608643,
      "learning_rate": 2e-05,
      "loss": 0.227,
      "step": 15860
    },
    {
      "epoch": 5.0412960609911055,
      "grad_norm": 8.611465454101562,
      "learning_rate": 2e-05,
      "loss": 0.2293,
      "step": 15870
    },
    {
      "epoch": 5.044472681067345,
      "grad_norm": 5.866784572601318,
      "learning_rate": 2e-05,
      "loss": 0.2295,
      "step": 15880
    },
    {
      "epoch": 5.047649301143583,
      "grad_norm": 5.514492511749268,
      "learning_rate": 2e-05,
      "loss": 0.2325,
      "step": 15890
    },
    {
      "epoch": 5.050825921219822,
      "grad_norm": 4.616154670715332,
      "learning_rate": 2e-05,
      "loss": 0.2197,
      "step": 15900
    },
    {
      "epoch": 5.054002541296061,
      "grad_norm": 6.795986652374268,
      "learning_rate": 2e-05,
      "loss": 0.2126,
      "step": 15910
    },
    {
      "epoch": 5.0571791613723,
      "grad_norm": 4.2689208984375,
      "learning_rate": 2e-05,
      "loss": 0.2308,
      "step": 15920
    },
    {
      "epoch": 5.060355781448539,
      "grad_norm": 4.038673400878906,
      "learning_rate": 2e-05,
      "loss": 0.2232,
      "step": 15930
    },
    {
      "epoch": 5.063532401524777,
      "grad_norm": 4.104979991912842,
      "learning_rate": 2e-05,
      "loss": 0.211,
      "step": 15940
    },
    {
      "epoch": 5.066709021601016,
      "grad_norm": 7.656097412109375,
      "learning_rate": 2e-05,
      "loss": 0.2308,
      "step": 15950
    },
    {
      "epoch": 5.0698856416772555,
      "grad_norm": 10.823786735534668,
      "learning_rate": 2e-05,
      "loss": 0.236,
      "step": 15960
    },
    {
      "epoch": 5.073062261753495,
      "grad_norm": 8.098716735839844,
      "learning_rate": 2e-05,
      "loss": 0.2298,
      "step": 15970
    },
    {
      "epoch": 5.076238881829733,
      "grad_norm": 6.303865909576416,
      "learning_rate": 2e-05,
      "loss": 0.2348,
      "step": 15980
    },
    {
      "epoch": 5.079415501905972,
      "grad_norm": 6.5119309425354,
      "learning_rate": 2e-05,
      "loss": 0.2161,
      "step": 15990
    },
    {
      "epoch": 5.082592121982211,
      "grad_norm": 6.138505458831787,
      "learning_rate": 2e-05,
      "loss": 0.2292,
      "step": 16000
    },
    {
      "epoch": 5.08576874205845,
      "grad_norm": 6.075383186340332,
      "learning_rate": 2e-05,
      "loss": 0.2228,
      "step": 16010
    },
    {
      "epoch": 5.088945362134688,
      "grad_norm": 3.4761805534362793,
      "learning_rate": 2e-05,
      "loss": 0.2143,
      "step": 16020
    },
    {
      "epoch": 5.092121982210927,
      "grad_norm": 5.395332336425781,
      "learning_rate": 2e-05,
      "loss": 0.2242,
      "step": 16030
    },
    {
      "epoch": 5.0952986022871665,
      "grad_norm": 6.384178638458252,
      "learning_rate": 2e-05,
      "loss": 0.233,
      "step": 16040
    },
    {
      "epoch": 5.098475222363406,
      "grad_norm": 6.388936996459961,
      "learning_rate": 2e-05,
      "loss": 0.2333,
      "step": 16050
    },
    {
      "epoch": 5.101651842439645,
      "grad_norm": 3.8359689712524414,
      "learning_rate": 2e-05,
      "loss": 0.2237,
      "step": 16060
    },
    {
      "epoch": 5.103240152477763,
      "eval_loss": 1.770548701286316,
      "eval_mse": 1.7687830684136372,
      "eval_pearson": 0.39924395127218315,
      "eval_runtime": 7.4106,
      "eval_samples_per_second": 2909.347,
      "eval_spearmanr": 0.39350434245239535,
      "eval_steps_per_second": 11.47,
      "step": 16065
    },
    {
      "epoch": 5.104828462515883,
      "grad_norm": 5.2300920486450195,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 16070
    },
    {
      "epoch": 5.108005082592122,
      "grad_norm": 7.505787372589111,
      "learning_rate": 2e-05,
      "loss": 0.2211,
      "step": 16080
    },
    {
      "epoch": 5.111181702668361,
      "grad_norm": 5.445562362670898,
      "learning_rate": 2e-05,
      "loss": 0.2266,
      "step": 16090
    },
    {
      "epoch": 5.1143583227446,
      "grad_norm": 4.956284999847412,
      "learning_rate": 2e-05,
      "loss": 0.23,
      "step": 16100
    },
    {
      "epoch": 5.117534942820838,
      "grad_norm": 6.872167110443115,
      "learning_rate": 2e-05,
      "loss": 0.2233,
      "step": 16110
    },
    {
      "epoch": 5.120711562897077,
      "grad_norm": 4.099207401275635,
      "learning_rate": 2e-05,
      "loss": 0.2315,
      "step": 16120
    },
    {
      "epoch": 5.1238881829733165,
      "grad_norm": 4.908313274383545,
      "learning_rate": 2e-05,
      "loss": 0.2203,
      "step": 16130
    },
    {
      "epoch": 5.127064803049556,
      "grad_norm": 4.3247785568237305,
      "learning_rate": 2e-05,
      "loss": 0.2253,
      "step": 16140
    },
    {
      "epoch": 5.130241423125794,
      "grad_norm": 4.700505256652832,
      "learning_rate": 2e-05,
      "loss": 0.2252,
      "step": 16150
    },
    {
      "epoch": 5.133418043202033,
      "grad_norm": 6.5829033851623535,
      "learning_rate": 2e-05,
      "loss": 0.2268,
      "step": 16160
    },
    {
      "epoch": 5.136594663278272,
      "grad_norm": 4.280997276306152,
      "learning_rate": 2e-05,
      "loss": 0.2224,
      "step": 16170
    },
    {
      "epoch": 5.139771283354511,
      "grad_norm": 7.374261379241943,
      "learning_rate": 2e-05,
      "loss": 0.2301,
      "step": 16180
    },
    {
      "epoch": 5.142947903430749,
      "grad_norm": 4.085953235626221,
      "learning_rate": 2e-05,
      "loss": 0.2104,
      "step": 16190
    },
    {
      "epoch": 5.146124523506988,
      "grad_norm": 4.964304447174072,
      "learning_rate": 2e-05,
      "loss": 0.2165,
      "step": 16200
    },
    {
      "epoch": 5.149301143583227,
      "grad_norm": 4.210855007171631,
      "learning_rate": 2e-05,
      "loss": 0.2328,
      "step": 16210
    },
    {
      "epoch": 5.1524777636594665,
      "grad_norm": 4.234318256378174,
      "learning_rate": 2e-05,
      "loss": 0.2042,
      "step": 16220
    },
    {
      "epoch": 5.155654383735706,
      "grad_norm": 6.072836875915527,
      "learning_rate": 2e-05,
      "loss": 0.2317,
      "step": 16230
    },
    {
      "epoch": 5.158831003811944,
      "grad_norm": 7.144199371337891,
      "learning_rate": 2e-05,
      "loss": 0.2347,
      "step": 16240
    },
    {
      "epoch": 5.162007623888183,
      "grad_norm": 3.793647289276123,
      "learning_rate": 2e-05,
      "loss": 0.2134,
      "step": 16250
    },
    {
      "epoch": 5.165184243964422,
      "grad_norm": 5.0071001052856445,
      "learning_rate": 2e-05,
      "loss": 0.2164,
      "step": 16260
    },
    {
      "epoch": 5.168360864040661,
      "grad_norm": 7.677377700805664,
      "learning_rate": 2e-05,
      "loss": 0.22,
      "step": 16270
    },
    {
      "epoch": 5.171537484116899,
      "grad_norm": 5.255474090576172,
      "learning_rate": 2e-05,
      "loss": 0.2188,
      "step": 16280
    },
    {
      "epoch": 5.174714104193138,
      "grad_norm": 7.301168441772461,
      "learning_rate": 2e-05,
      "loss": 0.2165,
      "step": 16290
    },
    {
      "epoch": 5.1778907242693775,
      "grad_norm": 4.487781047821045,
      "learning_rate": 2e-05,
      "loss": 0.2436,
      "step": 16300
    },
    {
      "epoch": 5.1810673443456166,
      "grad_norm": 5.421137809753418,
      "learning_rate": 2e-05,
      "loss": 0.2389,
      "step": 16310
    },
    {
      "epoch": 5.184243964421855,
      "grad_norm": 5.611364841461182,
      "learning_rate": 2e-05,
      "loss": 0.2342,
      "step": 16320
    },
    {
      "epoch": 5.187420584498094,
      "grad_norm": 4.496296405792236,
      "learning_rate": 2e-05,
      "loss": 0.2213,
      "step": 16330
    },
    {
      "epoch": 5.190597204574333,
      "grad_norm": 9.52362060546875,
      "learning_rate": 2e-05,
      "loss": 0.2317,
      "step": 16340
    },
    {
      "epoch": 5.193773824650572,
      "grad_norm": 3.961235761642456,
      "learning_rate": 2e-05,
      "loss": 0.215,
      "step": 16350
    },
    {
      "epoch": 5.196950444726811,
      "grad_norm": 4.535350799560547,
      "learning_rate": 2e-05,
      "loss": 0.2198,
      "step": 16360
    },
    {
      "epoch": 5.200127064803049,
      "grad_norm": 4.465869426727295,
      "learning_rate": 2e-05,
      "loss": 0.231,
      "step": 16370
    },
    {
      "epoch": 5.203303684879288,
      "grad_norm": 5.850367546081543,
      "learning_rate": 2e-05,
      "loss": 0.2281,
      "step": 16380
    },
    {
      "epoch": 5.203303684879288,
      "eval_loss": 1.8468409776687622,
      "eval_mse": 1.8452317974452406,
      "eval_pearson": 0.3939155826281508,
      "eval_runtime": 7.3963,
      "eval_samples_per_second": 2914.954,
      "eval_spearmanr": 0.388227082860406,
      "eval_steps_per_second": 11.492,
      "step": 16380
    },
    {
      "epoch": 5.2064803049555275,
      "grad_norm": 4.407447814941406,
      "learning_rate": 2e-05,
      "loss": 0.2375,
      "step": 16390
    },
    {
      "epoch": 5.209656925031767,
      "grad_norm": 4.326844692230225,
      "learning_rate": 2e-05,
      "loss": 0.211,
      "step": 16400
    },
    {
      "epoch": 5.212833545108005,
      "grad_norm": 9.387606620788574,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 16410
    },
    {
      "epoch": 5.216010165184244,
      "grad_norm": 5.1257710456848145,
      "learning_rate": 2e-05,
      "loss": 0.2408,
      "step": 16420
    },
    {
      "epoch": 5.219186785260483,
      "grad_norm": 4.837272644042969,
      "learning_rate": 2e-05,
      "loss": 0.2268,
      "step": 16430
    },
    {
      "epoch": 5.222363405336722,
      "grad_norm": 5.586409091949463,
      "learning_rate": 2e-05,
      "loss": 0.225,
      "step": 16440
    },
    {
      "epoch": 5.22554002541296,
      "grad_norm": 5.574900150299072,
      "learning_rate": 2e-05,
      "loss": 0.2356,
      "step": 16450
    },
    {
      "epoch": 5.228716645489199,
      "grad_norm": 4.537824630737305,
      "learning_rate": 2e-05,
      "loss": 0.2233,
      "step": 16460
    },
    {
      "epoch": 5.231893265565438,
      "grad_norm": 4.9519429206848145,
      "learning_rate": 2e-05,
      "loss": 0.22,
      "step": 16470
    },
    {
      "epoch": 5.2350698856416775,
      "grad_norm": 4.774900913238525,
      "learning_rate": 2e-05,
      "loss": 0.2376,
      "step": 16480
    },
    {
      "epoch": 5.238246505717916,
      "grad_norm": 4.456934928894043,
      "learning_rate": 2e-05,
      "loss": 0.2249,
      "step": 16490
    },
    {
      "epoch": 5.241423125794155,
      "grad_norm": 4.934289455413818,
      "learning_rate": 2e-05,
      "loss": 0.2229,
      "step": 16500
    },
    {
      "epoch": 5.244599745870394,
      "grad_norm": 4.066357135772705,
      "learning_rate": 2e-05,
      "loss": 0.2232,
      "step": 16510
    },
    {
      "epoch": 5.247776365946633,
      "grad_norm": 5.189879894256592,
      "learning_rate": 2e-05,
      "loss": 0.2192,
      "step": 16520
    },
    {
      "epoch": 5.250952986022872,
      "grad_norm": 5.342756748199463,
      "learning_rate": 2e-05,
      "loss": 0.2323,
      "step": 16530
    },
    {
      "epoch": 5.25412960609911,
      "grad_norm": 4.275149822235107,
      "learning_rate": 2e-05,
      "loss": 0.2314,
      "step": 16540
    },
    {
      "epoch": 5.257306226175349,
      "grad_norm": 4.819647789001465,
      "learning_rate": 2e-05,
      "loss": 0.2282,
      "step": 16550
    },
    {
      "epoch": 5.2604828462515885,
      "grad_norm": 5.285641193389893,
      "learning_rate": 2e-05,
      "loss": 0.2375,
      "step": 16560
    },
    {
      "epoch": 5.2636594663278276,
      "grad_norm": 3.4849910736083984,
      "learning_rate": 2e-05,
      "loss": 0.2176,
      "step": 16570
    },
    {
      "epoch": 5.266836086404066,
      "grad_norm": 3.8357677459716797,
      "learning_rate": 2e-05,
      "loss": 0.2386,
      "step": 16580
    },
    {
      "epoch": 5.270012706480305,
      "grad_norm": 5.1989426612854,
      "learning_rate": 2e-05,
      "loss": 0.2277,
      "step": 16590
    },
    {
      "epoch": 5.273189326556544,
      "grad_norm": 5.15686559677124,
      "learning_rate": 2e-05,
      "loss": 0.2331,
      "step": 16600
    },
    {
      "epoch": 5.276365946632783,
      "grad_norm": 6.587932586669922,
      "learning_rate": 2e-05,
      "loss": 0.2225,
      "step": 16610
    },
    {
      "epoch": 5.279542566709021,
      "grad_norm": 5.16064453125,
      "learning_rate": 2e-05,
      "loss": 0.2158,
      "step": 16620
    },
    {
      "epoch": 5.28271918678526,
      "grad_norm": 6.459850311279297,
      "learning_rate": 2e-05,
      "loss": 0.2302,
      "step": 16630
    },
    {
      "epoch": 5.285895806861499,
      "grad_norm": 5.214120388031006,
      "learning_rate": 2e-05,
      "loss": 0.24,
      "step": 16640
    },
    {
      "epoch": 5.2890724269377385,
      "grad_norm": 4.502641201019287,
      "learning_rate": 2e-05,
      "loss": 0.2231,
      "step": 16650
    },
    {
      "epoch": 5.292249047013977,
      "grad_norm": 4.362120628356934,
      "learning_rate": 2e-05,
      "loss": 0.2244,
      "step": 16660
    },
    {
      "epoch": 5.295425667090216,
      "grad_norm": 3.8852734565734863,
      "learning_rate": 2e-05,
      "loss": 0.2243,
      "step": 16670
    },
    {
      "epoch": 5.298602287166455,
      "grad_norm": 8.311966896057129,
      "learning_rate": 2e-05,
      "loss": 0.2295,
      "step": 16680
    },
    {
      "epoch": 5.301778907242694,
      "grad_norm": 3.834094762802124,
      "learning_rate": 2e-05,
      "loss": 0.2231,
      "step": 16690
    },
    {
      "epoch": 5.3033672172808135,
      "eval_loss": 1.7791062593460083,
      "eval_mse": 1.7774179597390842,
      "eval_pearson": 0.4185677327580022,
      "eval_runtime": 7.3905,
      "eval_samples_per_second": 2917.255,
      "eval_spearmanr": 0.4199740128546875,
      "eval_steps_per_second": 11.501,
      "step": 16695
    },
    {
      "epoch": 5.304955527318933,
      "grad_norm": 4.567781925201416,
      "learning_rate": 2e-05,
      "loss": 0.2277,
      "step": 16700
    },
    {
      "epoch": 5.308132147395171,
      "grad_norm": 6.102242469787598,
      "learning_rate": 2e-05,
      "loss": 0.2372,
      "step": 16710
    },
    {
      "epoch": 5.31130876747141,
      "grad_norm": 4.63271427154541,
      "learning_rate": 2e-05,
      "loss": 0.2117,
      "step": 16720
    },
    {
      "epoch": 5.314485387547649,
      "grad_norm": 5.163181304931641,
      "learning_rate": 2e-05,
      "loss": 0.2192,
      "step": 16730
    },
    {
      "epoch": 5.3176620076238885,
      "grad_norm": 3.972827434539795,
      "learning_rate": 2e-05,
      "loss": 0.2034,
      "step": 16740
    },
    {
      "epoch": 5.320838627700127,
      "grad_norm": 6.153090953826904,
      "learning_rate": 2e-05,
      "loss": 0.2131,
      "step": 16750
    },
    {
      "epoch": 5.324015247776366,
      "grad_norm": 6.8562798500061035,
      "learning_rate": 2e-05,
      "loss": 0.2166,
      "step": 16760
    },
    {
      "epoch": 5.327191867852605,
      "grad_norm": 7.32175350189209,
      "learning_rate": 2e-05,
      "loss": 0.2177,
      "step": 16770
    },
    {
      "epoch": 5.330368487928844,
      "grad_norm": 4.222492218017578,
      "learning_rate": 2e-05,
      "loss": 0.2288,
      "step": 16780
    },
    {
      "epoch": 5.333545108005082,
      "grad_norm": 3.9279346466064453,
      "learning_rate": 2e-05,
      "loss": 0.2416,
      "step": 16790
    },
    {
      "epoch": 5.336721728081321,
      "grad_norm": 6.089547634124756,
      "learning_rate": 2e-05,
      "loss": 0.2197,
      "step": 16800
    },
    {
      "epoch": 5.33989834815756,
      "grad_norm": 4.646677017211914,
      "learning_rate": 2e-05,
      "loss": 0.2364,
      "step": 16810
    },
    {
      "epoch": 5.3430749682337995,
      "grad_norm": 4.767277717590332,
      "learning_rate": 2e-05,
      "loss": 0.2254,
      "step": 16820
    },
    {
      "epoch": 5.346251588310038,
      "grad_norm": 5.294421195983887,
      "learning_rate": 2e-05,
      "loss": 0.2131,
      "step": 16830
    },
    {
      "epoch": 5.349428208386277,
      "grad_norm": 5.297114849090576,
      "learning_rate": 2e-05,
      "loss": 0.2058,
      "step": 16840
    },
    {
      "epoch": 5.352604828462516,
      "grad_norm": 4.8066253662109375,
      "learning_rate": 2e-05,
      "loss": 0.213,
      "step": 16850
    },
    {
      "epoch": 5.355781448538755,
      "grad_norm": 4.499720096588135,
      "learning_rate": 2e-05,
      "loss": 0.2289,
      "step": 16860
    },
    {
      "epoch": 5.358958068614994,
      "grad_norm": 6.470526695251465,
      "learning_rate": 2e-05,
      "loss": 0.2085,
      "step": 16870
    },
    {
      "epoch": 5.362134688691232,
      "grad_norm": 4.46138858795166,
      "learning_rate": 2e-05,
      "loss": 0.2396,
      "step": 16880
    },
    {
      "epoch": 5.365311308767471,
      "grad_norm": 3.962259292602539,
      "learning_rate": 2e-05,
      "loss": 0.2311,
      "step": 16890
    },
    {
      "epoch": 5.36848792884371,
      "grad_norm": 10.117583274841309,
      "learning_rate": 2e-05,
      "loss": 0.2238,
      "step": 16900
    },
    {
      "epoch": 5.3716645489199495,
      "grad_norm": 5.349684238433838,
      "learning_rate": 2e-05,
      "loss": 0.2263,
      "step": 16910
    },
    {
      "epoch": 5.374841168996188,
      "grad_norm": 4.389296054840088,
      "learning_rate": 2e-05,
      "loss": 0.2373,
      "step": 16920
    },
    {
      "epoch": 5.378017789072427,
      "grad_norm": 9.285245895385742,
      "learning_rate": 2e-05,
      "loss": 0.2249,
      "step": 16930
    },
    {
      "epoch": 5.381194409148666,
      "grad_norm": 4.624654293060303,
      "learning_rate": 2e-05,
      "loss": 0.2401,
      "step": 16940
    },
    {
      "epoch": 5.384371029224905,
      "grad_norm": 6.921512126922607,
      "learning_rate": 2e-05,
      "loss": 0.2227,
      "step": 16950
    },
    {
      "epoch": 5.387547649301144,
      "grad_norm": 3.5672521591186523,
      "learning_rate": 2e-05,
      "loss": 0.2266,
      "step": 16960
    },
    {
      "epoch": 5.390724269377382,
      "grad_norm": 4.091825485229492,
      "learning_rate": 2e-05,
      "loss": 0.2218,
      "step": 16970
    },
    {
      "epoch": 5.393900889453621,
      "grad_norm": 4.352066516876221,
      "learning_rate": 2e-05,
      "loss": 0.2389,
      "step": 16980
    },
    {
      "epoch": 5.39707750952986,
      "grad_norm": 6.909728050231934,
      "learning_rate": 2e-05,
      "loss": 0.2219,
      "step": 16990
    },
    {
      "epoch": 5.4002541296060995,
      "grad_norm": 6.039434909820557,
      "learning_rate": 2e-05,
      "loss": 0.2207,
      "step": 17000
    },
    {
      "epoch": 5.403430749682338,
      "grad_norm": 4.726781368255615,
      "learning_rate": 2e-05,
      "loss": 0.2301,
      "step": 17010
    },
    {
      "epoch": 5.403430749682338,
      "eval_loss": 2.041980266571045,
      "eval_mse": 2.040427126933117,
      "eval_pearson": 0.40878135603395166,
      "eval_runtime": 7.5912,
      "eval_samples_per_second": 2840.118,
      "eval_spearmanr": 0.4128688645634311,
      "eval_steps_per_second": 11.197,
      "step": 17010
    },
    {
      "epoch": 5.406607369758577,
      "grad_norm": 4.162114143371582,
      "learning_rate": 2e-05,
      "loss": 0.2242,
      "step": 17020
    },
    {
      "epoch": 5.409783989834816,
      "grad_norm": 4.040860652923584,
      "learning_rate": 2e-05,
      "loss": 0.2303,
      "step": 17030
    },
    {
      "epoch": 5.412960609911055,
      "grad_norm": 8.046249389648438,
      "learning_rate": 2e-05,
      "loss": 0.2415,
      "step": 17040
    },
    {
      "epoch": 5.416137229987293,
      "grad_norm": 6.037308216094971,
      "learning_rate": 2e-05,
      "loss": 0.222,
      "step": 17050
    },
    {
      "epoch": 5.419313850063532,
      "grad_norm": 4.521266460418701,
      "learning_rate": 2e-05,
      "loss": 0.2329,
      "step": 17060
    },
    {
      "epoch": 5.422490470139771,
      "grad_norm": 7.101837635040283,
      "learning_rate": 2e-05,
      "loss": 0.2234,
      "step": 17070
    },
    {
      "epoch": 5.4256670902160105,
      "grad_norm": 4.641354084014893,
      "learning_rate": 2e-05,
      "loss": 0.2187,
      "step": 17080
    },
    {
      "epoch": 5.428843710292249,
      "grad_norm": 4.602793216705322,
      "learning_rate": 2e-05,
      "loss": 0.2328,
      "step": 17090
    },
    {
      "epoch": 5.432020330368488,
      "grad_norm": 5.535208225250244,
      "learning_rate": 2e-05,
      "loss": 0.2195,
      "step": 17100
    },
    {
      "epoch": 5.435196950444727,
      "grad_norm": 6.156194686889648,
      "learning_rate": 2e-05,
      "loss": 0.213,
      "step": 17110
    },
    {
      "epoch": 5.438373570520966,
      "grad_norm": 4.197371959686279,
      "learning_rate": 2e-05,
      "loss": 0.2111,
      "step": 17120
    },
    {
      "epoch": 5.441550190597205,
      "grad_norm": 3.950540065765381,
      "learning_rate": 2e-05,
      "loss": 0.2198,
      "step": 17130
    },
    {
      "epoch": 5.444726810673443,
      "grad_norm": 4.527599334716797,
      "learning_rate": 2e-05,
      "loss": 0.23,
      "step": 17140
    },
    {
      "epoch": 5.447903430749682,
      "grad_norm": 4.936319351196289,
      "learning_rate": 2e-05,
      "loss": 0.2194,
      "step": 17150
    },
    {
      "epoch": 5.451080050825921,
      "grad_norm": 4.709982395172119,
      "learning_rate": 2e-05,
      "loss": 0.2145,
      "step": 17160
    },
    {
      "epoch": 5.4542566709021605,
      "grad_norm": 6.928956985473633,
      "learning_rate": 2e-05,
      "loss": 0.2198,
      "step": 17170
    },
    {
      "epoch": 5.457433290978399,
      "grad_norm": 4.073151111602783,
      "learning_rate": 2e-05,
      "loss": 0.2179,
      "step": 17180
    },
    {
      "epoch": 5.460609911054638,
      "grad_norm": 6.340932846069336,
      "learning_rate": 2e-05,
      "loss": 0.2256,
      "step": 17190
    },
    {
      "epoch": 5.463786531130877,
      "grad_norm": 5.726085662841797,
      "learning_rate": 2e-05,
      "loss": 0.208,
      "step": 17200
    },
    {
      "epoch": 5.466963151207116,
      "grad_norm": 5.145788192749023,
      "learning_rate": 2e-05,
      "loss": 0.2138,
      "step": 17210
    },
    {
      "epoch": 5.470139771283354,
      "grad_norm": 5.294156551361084,
      "learning_rate": 2e-05,
      "loss": 0.2187,
      "step": 17220
    },
    {
      "epoch": 5.473316391359593,
      "grad_norm": 5.086425304412842,
      "learning_rate": 2e-05,
      "loss": 0.2083,
      "step": 17230
    },
    {
      "epoch": 5.476493011435832,
      "grad_norm": 5.052274227142334,
      "learning_rate": 2e-05,
      "loss": 0.23,
      "step": 17240
    },
    {
      "epoch": 5.479669631512071,
      "grad_norm": 5.506632328033447,
      "learning_rate": 2e-05,
      "loss": 0.2298,
      "step": 17250
    },
    {
      "epoch": 5.48284625158831,
      "grad_norm": 4.1312689781188965,
      "learning_rate": 2e-05,
      "loss": 0.2157,
      "step": 17260
    },
    {
      "epoch": 5.486022871664549,
      "grad_norm": 4.898463249206543,
      "learning_rate": 2e-05,
      "loss": 0.1996,
      "step": 17270
    },
    {
      "epoch": 5.489199491740788,
      "grad_norm": 5.522577285766602,
      "learning_rate": 2e-05,
      "loss": 0.2145,
      "step": 17280
    },
    {
      "epoch": 5.492376111817027,
      "grad_norm": 8.367574691772461,
      "learning_rate": 2e-05,
      "loss": 0.2359,
      "step": 17290
    },
    {
      "epoch": 5.495552731893266,
      "grad_norm": 6.0376973152160645,
      "learning_rate": 2e-05,
      "loss": 0.2342,
      "step": 17300
    },
    {
      "epoch": 5.498729351969504,
      "grad_norm": 3.9091928005218506,
      "learning_rate": 2e-05,
      "loss": 0.2197,
      "step": 17310
    },
    {
      "epoch": 5.501905972045743,
      "grad_norm": 4.7123122215271,
      "learning_rate": 2e-05,
      "loss": 0.2157,
      "step": 17320
    },
    {
      "epoch": 5.503494282083863,
      "eval_loss": 1.7900713682174683,
      "eval_mse": 1.7879816504931405,
      "eval_pearson": 0.3969434393075848,
      "eval_runtime": 7.3066,
      "eval_samples_per_second": 2950.759,
      "eval_spearmanr": 0.39512276770033916,
      "eval_steps_per_second": 11.633,
      "step": 17325
    },
    {
      "epoch": 5.505082592121982,
      "grad_norm": 4.434991359710693,
      "learning_rate": 2e-05,
      "loss": 0.2239,
      "step": 17330
    },
    {
      "epoch": 5.5082592121982215,
      "grad_norm": 4.199239253997803,
      "learning_rate": 2e-05,
      "loss": 0.2336,
      "step": 17340
    },
    {
      "epoch": 5.51143583227446,
      "grad_norm": 4.441694259643555,
      "learning_rate": 2e-05,
      "loss": 0.2103,
      "step": 17350
    },
    {
      "epoch": 5.514612452350699,
      "grad_norm": 6.083128452301025,
      "learning_rate": 2e-05,
      "loss": 0.223,
      "step": 17360
    },
    {
      "epoch": 5.517789072426938,
      "grad_norm": 5.383369445800781,
      "learning_rate": 2e-05,
      "loss": 0.238,
      "step": 17370
    },
    {
      "epoch": 5.520965692503177,
      "grad_norm": 4.51693058013916,
      "learning_rate": 2e-05,
      "loss": 0.2179,
      "step": 17380
    },
    {
      "epoch": 5.524142312579415,
      "grad_norm": 4.983378887176514,
      "learning_rate": 2e-05,
      "loss": 0.2256,
      "step": 17390
    },
    {
      "epoch": 5.527318932655654,
      "grad_norm": 4.417083263397217,
      "learning_rate": 2e-05,
      "loss": 0.229,
      "step": 17400
    },
    {
      "epoch": 5.530495552731893,
      "grad_norm": 5.476747035980225,
      "learning_rate": 2e-05,
      "loss": 0.2171,
      "step": 17410
    },
    {
      "epoch": 5.533672172808132,
      "grad_norm": 5.3502020835876465,
      "learning_rate": 2e-05,
      "loss": 0.226,
      "step": 17420
    },
    {
      "epoch": 5.536848792884371,
      "grad_norm": 6.511203289031982,
      "learning_rate": 2e-05,
      "loss": 0.2325,
      "step": 17430
    },
    {
      "epoch": 5.54002541296061,
      "grad_norm": 5.650363922119141,
      "learning_rate": 2e-05,
      "loss": 0.2273,
      "step": 17440
    },
    {
      "epoch": 5.543202033036849,
      "grad_norm": 4.391951084136963,
      "learning_rate": 2e-05,
      "loss": 0.2193,
      "step": 17450
    },
    {
      "epoch": 5.546378653113088,
      "grad_norm": 3.3729355335235596,
      "learning_rate": 2e-05,
      "loss": 0.2032,
      "step": 17460
    },
    {
      "epoch": 5.549555273189327,
      "grad_norm": 9.001389503479004,
      "learning_rate": 2e-05,
      "loss": 0.2164,
      "step": 17470
    },
    {
      "epoch": 5.552731893265565,
      "grad_norm": 5.924999713897705,
      "learning_rate": 2e-05,
      "loss": 0.2049,
      "step": 17480
    },
    {
      "epoch": 5.555908513341804,
      "grad_norm": 6.217920303344727,
      "learning_rate": 2e-05,
      "loss": 0.2161,
      "step": 17490
    },
    {
      "epoch": 5.559085133418043,
      "grad_norm": 4.499606609344482,
      "learning_rate": 2e-05,
      "loss": 0.2247,
      "step": 17500
    },
    {
      "epoch": 5.562261753494282,
      "grad_norm": 3.934612512588501,
      "learning_rate": 2e-05,
      "loss": 0.2127,
      "step": 17510
    },
    {
      "epoch": 5.565438373570521,
      "grad_norm": 3.9489071369171143,
      "learning_rate": 2e-05,
      "loss": 0.2138,
      "step": 17520
    },
    {
      "epoch": 5.56861499364676,
      "grad_norm": 4.552408695220947,
      "learning_rate": 2e-05,
      "loss": 0.2232,
      "step": 17530
    },
    {
      "epoch": 5.571791613722999,
      "grad_norm": 6.606417655944824,
      "learning_rate": 2e-05,
      "loss": 0.2173,
      "step": 17540
    },
    {
      "epoch": 5.574968233799238,
      "grad_norm": 7.370362281799316,
      "learning_rate": 2e-05,
      "loss": 0.2317,
      "step": 17550
    },
    {
      "epoch": 5.578144853875477,
      "grad_norm": 6.723299026489258,
      "learning_rate": 2e-05,
      "loss": 0.2316,
      "step": 17560
    },
    {
      "epoch": 5.581321473951715,
      "grad_norm": 3.9827396869659424,
      "learning_rate": 2e-05,
      "loss": 0.2169,
      "step": 17570
    },
    {
      "epoch": 5.584498094027954,
      "grad_norm": 3.3751819133758545,
      "learning_rate": 2e-05,
      "loss": 0.2232,
      "step": 17580
    },
    {
      "epoch": 5.587674714104193,
      "grad_norm": 3.94875168800354,
      "learning_rate": 2e-05,
      "loss": 0.2186,
      "step": 17590
    },
    {
      "epoch": 5.590851334180432,
      "grad_norm": 6.527545928955078,
      "learning_rate": 2e-05,
      "loss": 0.2367,
      "step": 17600
    },
    {
      "epoch": 5.594027954256671,
      "grad_norm": 5.904245853424072,
      "learning_rate": 2e-05,
      "loss": 0.246,
      "step": 17610
    },
    {
      "epoch": 5.59720457433291,
      "grad_norm": 5.800980091094971,
      "learning_rate": 2e-05,
      "loss": 0.2181,
      "step": 17620
    },
    {
      "epoch": 5.600381194409149,
      "grad_norm": 4.540384292602539,
      "learning_rate": 2e-05,
      "loss": 0.2104,
      "step": 17630
    },
    {
      "epoch": 5.603557814485388,
      "grad_norm": 5.434105396270752,
      "learning_rate": 2e-05,
      "loss": 0.2078,
      "step": 17640
    },
    {
      "epoch": 5.603557814485388,
      "eval_loss": 1.7877029180526733,
      "eval_mse": 1.7860873251075204,
      "eval_pearson": 0.38422778086477494,
      "eval_runtime": 7.4239,
      "eval_samples_per_second": 2904.122,
      "eval_spearmanr": 0.38281103693197066,
      "eval_steps_per_second": 11.449,
      "step": 17640
    },
    {
      "epoch": 5.606734434561626,
      "grad_norm": 5.56123685836792,
      "learning_rate": 2e-05,
      "loss": 0.2205,
      "step": 17650
    },
    {
      "epoch": 5.609911054637865,
      "grad_norm": 4.827754020690918,
      "learning_rate": 2e-05,
      "loss": 0.2235,
      "step": 17660
    },
    {
      "epoch": 5.613087674714104,
      "grad_norm": 4.303493022918701,
      "learning_rate": 2e-05,
      "loss": 0.1974,
      "step": 17670
    },
    {
      "epoch": 5.616264294790343,
      "grad_norm": 5.400953769683838,
      "learning_rate": 2e-05,
      "loss": 0.2341,
      "step": 17680
    },
    {
      "epoch": 5.619440914866582,
      "grad_norm": 3.345940351486206,
      "learning_rate": 2e-05,
      "loss": 0.2157,
      "step": 17690
    },
    {
      "epoch": 5.622617534942821,
      "grad_norm": 4.438691139221191,
      "learning_rate": 2e-05,
      "loss": 0.2093,
      "step": 17700
    },
    {
      "epoch": 5.62579415501906,
      "grad_norm": 4.369130611419678,
      "learning_rate": 2e-05,
      "loss": 0.2149,
      "step": 17710
    },
    {
      "epoch": 5.628970775095299,
      "grad_norm": 4.184670448303223,
      "learning_rate": 2e-05,
      "loss": 0.204,
      "step": 17720
    },
    {
      "epoch": 5.632147395171538,
      "grad_norm": 6.574100494384766,
      "learning_rate": 2e-05,
      "loss": 0.2058,
      "step": 17730
    },
    {
      "epoch": 5.635324015247776,
      "grad_norm": 4.164758682250977,
      "learning_rate": 2e-05,
      "loss": 0.2171,
      "step": 17740
    },
    {
      "epoch": 5.638500635324015,
      "grad_norm": 5.870837688446045,
      "learning_rate": 2e-05,
      "loss": 0.2243,
      "step": 17750
    },
    {
      "epoch": 5.641677255400254,
      "grad_norm": 3.86893892288208,
      "learning_rate": 2e-05,
      "loss": 0.2085,
      "step": 17760
    },
    {
      "epoch": 5.6448538754764925,
      "grad_norm": 3.674652576446533,
      "learning_rate": 2e-05,
      "loss": 0.2003,
      "step": 17770
    },
    {
      "epoch": 5.648030495552732,
      "grad_norm": 4.923914909362793,
      "learning_rate": 2e-05,
      "loss": 0.2156,
      "step": 17780
    },
    {
      "epoch": 5.651207115628971,
      "grad_norm": 3.843141794204712,
      "learning_rate": 2e-05,
      "loss": 0.2113,
      "step": 17790
    },
    {
      "epoch": 5.65438373570521,
      "grad_norm": 4.456954479217529,
      "learning_rate": 2e-05,
      "loss": 0.2232,
      "step": 17800
    },
    {
      "epoch": 5.657560355781449,
      "grad_norm": 5.6613569259643555,
      "learning_rate": 2e-05,
      "loss": 0.214,
      "step": 17810
    },
    {
      "epoch": 5.660736975857687,
      "grad_norm": 5.744074821472168,
      "learning_rate": 2e-05,
      "loss": 0.2154,
      "step": 17820
    },
    {
      "epoch": 5.663913595933926,
      "grad_norm": 4.178938865661621,
      "learning_rate": 2e-05,
      "loss": 0.2212,
      "step": 17830
    },
    {
      "epoch": 5.667090216010165,
      "grad_norm": 5.41148567199707,
      "learning_rate": 2e-05,
      "loss": 0.2012,
      "step": 17840
    },
    {
      "epoch": 5.670266836086404,
      "grad_norm": 4.072415828704834,
      "learning_rate": 2e-05,
      "loss": 0.2061,
      "step": 17850
    },
    {
      "epoch": 5.673443456162643,
      "grad_norm": 5.040156364440918,
      "learning_rate": 2e-05,
      "loss": 0.2252,
      "step": 17860
    },
    {
      "epoch": 5.676620076238882,
      "grad_norm": 4.026952743530273,
      "learning_rate": 2e-05,
      "loss": 0.2117,
      "step": 17870
    },
    {
      "epoch": 5.679796696315121,
      "grad_norm": 5.938722610473633,
      "learning_rate": 2e-05,
      "loss": 0.2303,
      "step": 17880
    },
    {
      "epoch": 5.68297331639136,
      "grad_norm": 6.037301063537598,
      "learning_rate": 2e-05,
      "loss": 0.2205,
      "step": 17890
    },
    {
      "epoch": 5.686149936467599,
      "grad_norm": 4.9774675369262695,
      "learning_rate": 2e-05,
      "loss": 0.2091,
      "step": 17900
    },
    {
      "epoch": 5.689326556543837,
      "grad_norm": 5.152227401733398,
      "learning_rate": 2e-05,
      "loss": 0.2177,
      "step": 17910
    },
    {
      "epoch": 5.692503176620076,
      "grad_norm": 5.116433620452881,
      "learning_rate": 2e-05,
      "loss": 0.2223,
      "step": 17920
    },
    {
      "epoch": 5.695679796696315,
      "grad_norm": 7.755491733551025,
      "learning_rate": 2e-05,
      "loss": 0.2052,
      "step": 17930
    },
    {
      "epoch": 5.698856416772554,
      "grad_norm": 6.263247489929199,
      "learning_rate": 2e-05,
      "loss": 0.2297,
      "step": 17940
    },
    {
      "epoch": 5.702033036848793,
      "grad_norm": 6.1420793533325195,
      "learning_rate": 2e-05,
      "loss": 0.2135,
      "step": 17950
    },
    {
      "epoch": 5.703621346886912,
      "eval_loss": 1.8311108350753784,
      "eval_mse": 1.8297676955337647,
      "eval_pearson": 0.38798699864183017,
      "eval_runtime": 7.3809,
      "eval_samples_per_second": 2921.067,
      "eval_spearmanr": 0.3904708488743771,
      "eval_steps_per_second": 11.516,
      "step": 17955
    },
    {
      "epoch": 5.705209656925032,
      "grad_norm": 4.114720821380615,
      "learning_rate": 2e-05,
      "loss": 0.2145,
      "step": 17960
    },
    {
      "epoch": 5.708386277001271,
      "grad_norm": 5.57487154006958,
      "learning_rate": 2e-05,
      "loss": 0.2144,
      "step": 17970
    },
    {
      "epoch": 5.71156289707751,
      "grad_norm": 3.693718671798706,
      "learning_rate": 2e-05,
      "loss": 0.2178,
      "step": 17980
    },
    {
      "epoch": 5.714739517153748,
      "grad_norm": 5.104541778564453,
      "learning_rate": 2e-05,
      "loss": 0.2141,
      "step": 17990
    },
    {
      "epoch": 5.717916137229987,
      "grad_norm": 4.440484523773193,
      "learning_rate": 2e-05,
      "loss": 0.2272,
      "step": 18000
    },
    {
      "epoch": 5.721092757306226,
      "grad_norm": 5.315109729766846,
      "learning_rate": 2e-05,
      "loss": 0.2031,
      "step": 18010
    },
    {
      "epoch": 5.724269377382465,
      "grad_norm": 4.266186714172363,
      "learning_rate": 2e-05,
      "loss": 0.2078,
      "step": 18020
    },
    {
      "epoch": 5.7274459974587035,
      "grad_norm": 3.712759256362915,
      "learning_rate": 2e-05,
      "loss": 0.2149,
      "step": 18030
    },
    {
      "epoch": 5.730622617534943,
      "grad_norm": 4.117588996887207,
      "learning_rate": 2e-05,
      "loss": 0.2199,
      "step": 18040
    },
    {
      "epoch": 5.733799237611182,
      "grad_norm": 4.592304229736328,
      "learning_rate": 2e-05,
      "loss": 0.2198,
      "step": 18050
    },
    {
      "epoch": 5.736975857687421,
      "grad_norm": 3.547490358352661,
      "learning_rate": 2e-05,
      "loss": 0.2066,
      "step": 18060
    },
    {
      "epoch": 5.74015247776366,
      "grad_norm": 4.703005790710449,
      "learning_rate": 2e-05,
      "loss": 0.2224,
      "step": 18070
    },
    {
      "epoch": 5.743329097839898,
      "grad_norm": 8.284479141235352,
      "learning_rate": 2e-05,
      "loss": 0.2161,
      "step": 18080
    },
    {
      "epoch": 5.746505717916137,
      "grad_norm": 8.435074806213379,
      "learning_rate": 2e-05,
      "loss": 0.2288,
      "step": 18090
    },
    {
      "epoch": 5.749682337992376,
      "grad_norm": 8.718483924865723,
      "learning_rate": 2e-05,
      "loss": 0.2271,
      "step": 18100
    },
    {
      "epoch": 5.752858958068615,
      "grad_norm": 4.3479766845703125,
      "learning_rate": 2e-05,
      "loss": 0.2111,
      "step": 18110
    },
    {
      "epoch": 5.756035578144854,
      "grad_norm": 4.8995795249938965,
      "learning_rate": 2e-05,
      "loss": 0.2076,
      "step": 18120
    },
    {
      "epoch": 5.759212198221093,
      "grad_norm": 5.1957106590271,
      "learning_rate": 2e-05,
      "loss": 0.2123,
      "step": 18130
    },
    {
      "epoch": 5.762388818297332,
      "grad_norm": 4.594116687774658,
      "learning_rate": 2e-05,
      "loss": 0.1973,
      "step": 18140
    },
    {
      "epoch": 5.765565438373571,
      "grad_norm": 4.355137348175049,
      "learning_rate": 2e-05,
      "loss": 0.2107,
      "step": 18150
    },
    {
      "epoch": 5.768742058449809,
      "grad_norm": 3.7992117404937744,
      "learning_rate": 2e-05,
      "loss": 0.2088,
      "step": 18160
    },
    {
      "epoch": 5.771918678526048,
      "grad_norm": 8.66751766204834,
      "learning_rate": 2e-05,
      "loss": 0.2161,
      "step": 18170
    },
    {
      "epoch": 5.775095298602287,
      "grad_norm": 4.755786418914795,
      "learning_rate": 2e-05,
      "loss": 0.2144,
      "step": 18180
    },
    {
      "epoch": 5.778271918678526,
      "grad_norm": 4.372878074645996,
      "learning_rate": 2e-05,
      "loss": 0.2035,
      "step": 18190
    },
    {
      "epoch": 5.7814485387547645,
      "grad_norm": 4.15923547744751,
      "learning_rate": 2e-05,
      "loss": 0.2027,
      "step": 18200
    },
    {
      "epoch": 5.784625158831004,
      "grad_norm": 5.429042816162109,
      "learning_rate": 2e-05,
      "loss": 0.2195,
      "step": 18210
    },
    {
      "epoch": 5.787801778907243,
      "grad_norm": 4.751276016235352,
      "learning_rate": 2e-05,
      "loss": 0.2262,
      "step": 18220
    },
    {
      "epoch": 5.790978398983482,
      "grad_norm": 6.061533451080322,
      "learning_rate": 2e-05,
      "loss": 0.2196,
      "step": 18230
    },
    {
      "epoch": 5.794155019059721,
      "grad_norm": 3.33764910697937,
      "learning_rate": 2e-05,
      "loss": 0.2031,
      "step": 18240
    },
    {
      "epoch": 5.797331639135959,
      "grad_norm": 5.477983474731445,
      "learning_rate": 2e-05,
      "loss": 0.2118,
      "step": 18250
    },
    {
      "epoch": 5.800508259212198,
      "grad_norm": 5.073936462402344,
      "learning_rate": 2e-05,
      "loss": 0.2236,
      "step": 18260
    },
    {
      "epoch": 5.803684879288437,
      "grad_norm": 6.985803604125977,
      "learning_rate": 2e-05,
      "loss": 0.2095,
      "step": 18270
    },
    {
      "epoch": 5.803684879288437,
      "eval_loss": 1.9419193267822266,
      "eval_mse": 1.9405531932947144,
      "eval_pearson": 0.39687311568292516,
      "eval_runtime": 7.3714,
      "eval_samples_per_second": 2924.815,
      "eval_spearmanr": 0.40009510072973103,
      "eval_steps_per_second": 11.531,
      "step": 18270
    },
    {
      "epoch": 5.806861499364676,
      "grad_norm": 8.888773918151855,
      "learning_rate": 2e-05,
      "loss": 0.2364,
      "step": 18280
    },
    {
      "epoch": 5.8100381194409145,
      "grad_norm": 6.529788017272949,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 18290
    },
    {
      "epoch": 5.813214739517154,
      "grad_norm": 4.803988933563232,
      "learning_rate": 2e-05,
      "loss": 0.203,
      "step": 18300
    },
    {
      "epoch": 5.816391359593393,
      "grad_norm": 4.876813888549805,
      "learning_rate": 2e-05,
      "loss": 0.2103,
      "step": 18310
    },
    {
      "epoch": 5.819567979669632,
      "grad_norm": 6.42055606842041,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 18320
    },
    {
      "epoch": 5.822744599745871,
      "grad_norm": 4.218966007232666,
      "learning_rate": 2e-05,
      "loss": 0.207,
      "step": 18330
    },
    {
      "epoch": 5.825921219822109,
      "grad_norm": 5.373350620269775,
      "learning_rate": 2e-05,
      "loss": 0.2184,
      "step": 18340
    },
    {
      "epoch": 5.829097839898348,
      "grad_norm": 4.876260280609131,
      "learning_rate": 2e-05,
      "loss": 0.2188,
      "step": 18350
    },
    {
      "epoch": 5.832274459974587,
      "grad_norm": 5.132651329040527,
      "learning_rate": 2e-05,
      "loss": 0.2039,
      "step": 18360
    },
    {
      "epoch": 5.8354510800508255,
      "grad_norm": 4.402166366577148,
      "learning_rate": 2e-05,
      "loss": 0.2104,
      "step": 18370
    },
    {
      "epoch": 5.838627700127065,
      "grad_norm": 3.699115514755249,
      "learning_rate": 2e-05,
      "loss": 0.2158,
      "step": 18380
    },
    {
      "epoch": 5.841804320203304,
      "grad_norm": 5.052114963531494,
      "learning_rate": 2e-05,
      "loss": 0.2112,
      "step": 18390
    },
    {
      "epoch": 5.844980940279543,
      "grad_norm": 4.70169734954834,
      "learning_rate": 2e-05,
      "loss": 0.2113,
      "step": 18400
    },
    {
      "epoch": 5.848157560355782,
      "grad_norm": 5.267746448516846,
      "learning_rate": 2e-05,
      "loss": 0.2078,
      "step": 18410
    },
    {
      "epoch": 5.85133418043202,
      "grad_norm": 6.558760643005371,
      "learning_rate": 2e-05,
      "loss": 0.2008,
      "step": 18420
    },
    {
      "epoch": 5.854510800508259,
      "grad_norm": 4.907571792602539,
      "learning_rate": 2e-05,
      "loss": 0.2094,
      "step": 18430
    },
    {
      "epoch": 5.857687420584498,
      "grad_norm": 4.399781703948975,
      "learning_rate": 2e-05,
      "loss": 0.216,
      "step": 18440
    },
    {
      "epoch": 5.860864040660737,
      "grad_norm": 5.281630039215088,
      "learning_rate": 2e-05,
      "loss": 0.2128,
      "step": 18450
    },
    {
      "epoch": 5.8640406607369755,
      "grad_norm": 4.771023750305176,
      "learning_rate": 2e-05,
      "loss": 0.2022,
      "step": 18460
    },
    {
      "epoch": 5.867217280813215,
      "grad_norm": 3.8732035160064697,
      "learning_rate": 2e-05,
      "loss": 0.2016,
      "step": 18470
    },
    {
      "epoch": 5.870393900889454,
      "grad_norm": 4.658328533172607,
      "learning_rate": 2e-05,
      "loss": 0.2204,
      "step": 18480
    },
    {
      "epoch": 5.873570520965693,
      "grad_norm": 5.824246406555176,
      "learning_rate": 2e-05,
      "loss": 0.2121,
      "step": 18490
    },
    {
      "epoch": 5.876747141041932,
      "grad_norm": 4.930690765380859,
      "learning_rate": 2e-05,
      "loss": 0.2128,
      "step": 18500
    },
    {
      "epoch": 5.87992376111817,
      "grad_norm": 5.495189189910889,
      "learning_rate": 2e-05,
      "loss": 0.2062,
      "step": 18510
    },
    {
      "epoch": 5.883100381194409,
      "grad_norm": 5.126668453216553,
      "learning_rate": 2e-05,
      "loss": 0.2099,
      "step": 18520
    },
    {
      "epoch": 5.886277001270648,
      "grad_norm": 3.623227119445801,
      "learning_rate": 2e-05,
      "loss": 0.2068,
      "step": 18530
    },
    {
      "epoch": 5.889453621346886,
      "grad_norm": 3.6730599403381348,
      "learning_rate": 2e-05,
      "loss": 0.2072,
      "step": 18540
    },
    {
      "epoch": 5.8926302414231255,
      "grad_norm": 4.731211185455322,
      "learning_rate": 2e-05,
      "loss": 0.2114,
      "step": 18550
    },
    {
      "epoch": 5.895806861499365,
      "grad_norm": 4.441227912902832,
      "learning_rate": 2e-05,
      "loss": 0.2332,
      "step": 18560
    },
    {
      "epoch": 5.898983481575604,
      "grad_norm": 4.754647254943848,
      "learning_rate": 2e-05,
      "loss": 0.2155,
      "step": 18570
    },
    {
      "epoch": 5.902160101651843,
      "grad_norm": 4.099281311035156,
      "learning_rate": 2e-05,
      "loss": 0.1995,
      "step": 18580
    },
    {
      "epoch": 5.903748411689962,
      "eval_loss": 1.7851171493530273,
      "eval_mse": 1.7840276154383659,
      "eval_pearson": 0.4032594415133254,
      "eval_runtime": 7.3962,
      "eval_samples_per_second": 2915.012,
      "eval_spearmanr": 0.40107577278215195,
      "eval_steps_per_second": 11.492,
      "step": 18585
    },
    {
      "epoch": 5.905336721728081,
      "grad_norm": 8.87248420715332,
      "learning_rate": 2e-05,
      "loss": 0.2239,
      "step": 18590
    },
    {
      "epoch": 5.90851334180432,
      "grad_norm": 4.519835948944092,
      "learning_rate": 2e-05,
      "loss": 0.2047,
      "step": 18600
    },
    {
      "epoch": 5.911689961880559,
      "grad_norm": 5.058769226074219,
      "learning_rate": 2e-05,
      "loss": 0.222,
      "step": 18610
    },
    {
      "epoch": 5.914866581956798,
      "grad_norm": 3.824270248413086,
      "learning_rate": 2e-05,
      "loss": 0.2004,
      "step": 18620
    },
    {
      "epoch": 5.9180432020330365,
      "grad_norm": 8.700023651123047,
      "learning_rate": 2e-05,
      "loss": 0.2039,
      "step": 18630
    },
    {
      "epoch": 5.9212198221092756,
      "grad_norm": 3.987426519393921,
      "learning_rate": 2e-05,
      "loss": 0.1978,
      "step": 18640
    },
    {
      "epoch": 5.924396442185515,
      "grad_norm": 3.9368834495544434,
      "learning_rate": 2e-05,
      "loss": 0.1989,
      "step": 18650
    },
    {
      "epoch": 5.927573062261754,
      "grad_norm": 5.1321234703063965,
      "learning_rate": 2e-05,
      "loss": 0.2147,
      "step": 18660
    },
    {
      "epoch": 5.930749682337993,
      "grad_norm": 5.100681781768799,
      "learning_rate": 2e-05,
      "loss": 0.2202,
      "step": 18670
    },
    {
      "epoch": 5.933926302414231,
      "grad_norm": 4.8104705810546875,
      "learning_rate": 2e-05,
      "loss": 0.2093,
      "step": 18680
    },
    {
      "epoch": 5.93710292249047,
      "grad_norm": 4.708157062530518,
      "learning_rate": 2e-05,
      "loss": 0.2201,
      "step": 18690
    },
    {
      "epoch": 5.940279542566709,
      "grad_norm": 4.343157768249512,
      "learning_rate": 2e-05,
      "loss": 0.2198,
      "step": 18700
    },
    {
      "epoch": 5.943456162642947,
      "grad_norm": 7.251887321472168,
      "learning_rate": 2e-05,
      "loss": 0.2127,
      "step": 18710
    },
    {
      "epoch": 5.9466327827191865,
      "grad_norm": 4.490999698638916,
      "learning_rate": 2e-05,
      "loss": 0.2042,
      "step": 18720
    },
    {
      "epoch": 5.949809402795426,
      "grad_norm": 4.357463836669922,
      "learning_rate": 2e-05,
      "loss": 0.2021,
      "step": 18730
    },
    {
      "epoch": 5.952986022871665,
      "grad_norm": 5.536838054656982,
      "learning_rate": 2e-05,
      "loss": 0.2178,
      "step": 18740
    },
    {
      "epoch": 5.956162642947904,
      "grad_norm": 4.601485729217529,
      "learning_rate": 2e-05,
      "loss": 0.2144,
      "step": 18750
    },
    {
      "epoch": 5.959339263024142,
      "grad_norm": 3.7236411571502686,
      "learning_rate": 2e-05,
      "loss": 0.2071,
      "step": 18760
    },
    {
      "epoch": 5.962515883100381,
      "grad_norm": 5.165084362030029,
      "learning_rate": 2e-05,
      "loss": 0.2003,
      "step": 18770
    },
    {
      "epoch": 5.96569250317662,
      "grad_norm": 8.062772750854492,
      "learning_rate": 2e-05,
      "loss": 0.2261,
      "step": 18780
    },
    {
      "epoch": 5.968869123252859,
      "grad_norm": 4.089705944061279,
      "learning_rate": 2e-05,
      "loss": 0.2108,
      "step": 18790
    },
    {
      "epoch": 5.972045743329097,
      "grad_norm": 5.4228386878967285,
      "learning_rate": 2e-05,
      "loss": 0.1993,
      "step": 18800
    },
    {
      "epoch": 5.9752223634053365,
      "grad_norm": 4.6009111404418945,
      "learning_rate": 2e-05,
      "loss": 0.2064,
      "step": 18810
    },
    {
      "epoch": 5.978398983481576,
      "grad_norm": 3.663372039794922,
      "learning_rate": 2e-05,
      "loss": 0.2026,
      "step": 18820
    },
    {
      "epoch": 5.981575603557815,
      "grad_norm": 3.1710009574890137,
      "learning_rate": 2e-05,
      "loss": 0.2084,
      "step": 18830
    },
    {
      "epoch": 5.984752223634054,
      "grad_norm": 4.51059627532959,
      "learning_rate": 2e-05,
      "loss": 0.209,
      "step": 18840
    },
    {
      "epoch": 5.987928843710292,
      "grad_norm": 7.5119452476501465,
      "learning_rate": 2e-05,
      "loss": 0.2162,
      "step": 18850
    },
    {
      "epoch": 5.991105463786531,
      "grad_norm": 6.073416233062744,
      "learning_rate": 2e-05,
      "loss": 0.1975,
      "step": 18860
    },
    {
      "epoch": 5.99428208386277,
      "grad_norm": 4.621971607208252,
      "learning_rate": 2e-05,
      "loss": 0.209,
      "step": 18870
    },
    {
      "epoch": 5.997458703939009,
      "grad_norm": 4.454184532165527,
      "learning_rate": 2e-05,
      "loss": 0.2056,
      "step": 18880
    },
    {
      "epoch": 6.0006353240152475,
      "grad_norm": 4.153463840484619,
      "learning_rate": 2e-05,
      "loss": 0.199,
      "step": 18890
    },
    {
      "epoch": 6.0038119440914866,
      "grad_norm": 4.059896945953369,
      "learning_rate": 2e-05,
      "loss": 0.1963,
      "step": 18900
    },
    {
      "epoch": 6.0038119440914866,
      "eval_loss": 1.6596648693084717,
      "eval_mse": 1.6583563433811483,
      "eval_pearson": 0.4151036380782516,
      "eval_runtime": 7.3671,
      "eval_samples_per_second": 2926.538,
      "eval_spearmanr": 0.40901614765445693,
      "eval_steps_per_second": 11.538,
      "step": 18900
    },
    {
      "epoch": 6.006988564167726,
      "grad_norm": 3.561676263809204,
      "learning_rate": 2e-05,
      "loss": 0.1786,
      "step": 18910
    },
    {
      "epoch": 6.010165184243965,
      "grad_norm": 2.8106610774993896,
      "learning_rate": 2e-05,
      "loss": 0.1807,
      "step": 18920
    },
    {
      "epoch": 6.013341804320203,
      "grad_norm": 5.7088398933410645,
      "learning_rate": 2e-05,
      "loss": 0.1876,
      "step": 18930
    },
    {
      "epoch": 6.016518424396442,
      "grad_norm": 5.046566963195801,
      "learning_rate": 2e-05,
      "loss": 0.1776,
      "step": 18940
    },
    {
      "epoch": 6.019695044472681,
      "grad_norm": 4.130315780639648,
      "learning_rate": 2e-05,
      "loss": 0.1806,
      "step": 18950
    },
    {
      "epoch": 6.02287166454892,
      "grad_norm": 3.965128183364868,
      "learning_rate": 2e-05,
      "loss": 0.1857,
      "step": 18960
    },
    {
      "epoch": 6.026048284625158,
      "grad_norm": 3.4892382621765137,
      "learning_rate": 2e-05,
      "loss": 0.1801,
      "step": 18970
    },
    {
      "epoch": 6.0292249047013975,
      "grad_norm": 4.807620525360107,
      "learning_rate": 2e-05,
      "loss": 0.1866,
      "step": 18980
    },
    {
      "epoch": 6.032401524777637,
      "grad_norm": 5.244062423706055,
      "learning_rate": 2e-05,
      "loss": 0.1912,
      "step": 18990
    },
    {
      "epoch": 6.035578144853876,
      "grad_norm": 4.230711936950684,
      "learning_rate": 2e-05,
      "loss": 0.1982,
      "step": 19000
    },
    {
      "epoch": 6.038754764930115,
      "grad_norm": 5.476994037628174,
      "learning_rate": 2e-05,
      "loss": 0.1793,
      "step": 19010
    },
    {
      "epoch": 6.041931385006353,
      "grad_norm": 4.536403656005859,
      "learning_rate": 2e-05,
      "loss": 0.1931,
      "step": 19020
    },
    {
      "epoch": 6.045108005082592,
      "grad_norm": 5.1854166984558105,
      "learning_rate": 2e-05,
      "loss": 0.1949,
      "step": 19030
    },
    {
      "epoch": 6.048284625158831,
      "grad_norm": 4.5851006507873535,
      "learning_rate": 2e-05,
      "loss": 0.1919,
      "step": 19040
    },
    {
      "epoch": 6.05146124523507,
      "grad_norm": 3.175429582595825,
      "learning_rate": 2e-05,
      "loss": 0.1896,
      "step": 19050
    },
    {
      "epoch": 6.054637865311308,
      "grad_norm": 5.192786693572998,
      "learning_rate": 2e-05,
      "loss": 0.1839,
      "step": 19060
    },
    {
      "epoch": 6.0578144853875475,
      "grad_norm": 3.2258102893829346,
      "learning_rate": 2e-05,
      "loss": 0.1784,
      "step": 19070
    },
    {
      "epoch": 6.060991105463787,
      "grad_norm": 5.52752161026001,
      "learning_rate": 2e-05,
      "loss": 0.194,
      "step": 19080
    },
    {
      "epoch": 6.064167725540026,
      "grad_norm": 2.8327059745788574,
      "learning_rate": 2e-05,
      "loss": 0.1839,
      "step": 19090
    },
    {
      "epoch": 6.067344345616264,
      "grad_norm": 3.8224892616271973,
      "learning_rate": 2e-05,
      "loss": 0.1885,
      "step": 19100
    },
    {
      "epoch": 6.070520965692503,
      "grad_norm": 3.6825144290924072,
      "learning_rate": 2e-05,
      "loss": 0.1754,
      "step": 19110
    },
    {
      "epoch": 6.073697585768742,
      "grad_norm": 3.693753957748413,
      "learning_rate": 2e-05,
      "loss": 0.19,
      "step": 19120
    },
    {
      "epoch": 6.076874205844981,
      "grad_norm": 4.439991474151611,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 19130
    },
    {
      "epoch": 6.08005082592122,
      "grad_norm": 7.629377841949463,
      "learning_rate": 2e-05,
      "loss": 0.1878,
      "step": 19140
    },
    {
      "epoch": 6.0832274459974585,
      "grad_norm": 3.5244176387786865,
      "learning_rate": 2e-05,
      "loss": 0.1875,
      "step": 19150
    },
    {
      "epoch": 6.0864040660736975,
      "grad_norm": 6.059737205505371,
      "learning_rate": 2e-05,
      "loss": 0.1968,
      "step": 19160
    },
    {
      "epoch": 6.089580686149937,
      "grad_norm": 3.8149282932281494,
      "learning_rate": 2e-05,
      "loss": 0.1795,
      "step": 19170
    },
    {
      "epoch": 6.092757306226176,
      "grad_norm": 4.216599941253662,
      "learning_rate": 2e-05,
      "loss": 0.1854,
      "step": 19180
    },
    {
      "epoch": 6.095933926302414,
      "grad_norm": 4.652143478393555,
      "learning_rate": 2e-05,
      "loss": 0.1825,
      "step": 19190
    },
    {
      "epoch": 6.099110546378653,
      "grad_norm": 4.353381156921387,
      "learning_rate": 2e-05,
      "loss": 0.175,
      "step": 19200
    },
    {
      "epoch": 6.102287166454892,
      "grad_norm": 6.942592144012451,
      "learning_rate": 2e-05,
      "loss": 0.1962,
      "step": 19210
    },
    {
      "epoch": 6.103875476493012,
      "eval_loss": 1.7404873371124268,
      "eval_mse": 1.7394739397602754,
      "eval_pearson": 0.39248314961836656,
      "eval_runtime": 7.3765,
      "eval_samples_per_second": 2922.782,
      "eval_spearmanr": 0.38795857347067014,
      "eval_steps_per_second": 11.523,
      "step": 19215
    },
    {
      "epoch": 6.105463786531131,
      "grad_norm": 7.126350402832031,
      "learning_rate": 2e-05,
      "loss": 0.1919,
      "step": 19220
    },
    {
      "epoch": 6.108640406607369,
      "grad_norm": 7.3580522537231445,
      "learning_rate": 2e-05,
      "loss": 0.1943,
      "step": 19230
    },
    {
      "epoch": 6.1118170266836085,
      "grad_norm": 4.723766326904297,
      "learning_rate": 2e-05,
      "loss": 0.1882,
      "step": 19240
    },
    {
      "epoch": 6.114993646759848,
      "grad_norm": 8.836686134338379,
      "learning_rate": 2e-05,
      "loss": 0.1887,
      "step": 19250
    },
    {
      "epoch": 6.118170266836087,
      "grad_norm": 5.4352593421936035,
      "learning_rate": 2e-05,
      "loss": 0.1908,
      "step": 19260
    },
    {
      "epoch": 6.121346886912325,
      "grad_norm": 4.4312543869018555,
      "learning_rate": 2e-05,
      "loss": 0.1826,
      "step": 19270
    },
    {
      "epoch": 6.124523506988564,
      "grad_norm": 4.213290214538574,
      "learning_rate": 2e-05,
      "loss": 0.1823,
      "step": 19280
    },
    {
      "epoch": 6.127700127064803,
      "grad_norm": 3.60894775390625,
      "learning_rate": 2e-05,
      "loss": 0.1777,
      "step": 19290
    },
    {
      "epoch": 6.130876747141042,
      "grad_norm": 5.641872882843018,
      "learning_rate": 2e-05,
      "loss": 0.189,
      "step": 19300
    },
    {
      "epoch": 6.134053367217281,
      "grad_norm": 4.026326656341553,
      "learning_rate": 2e-05,
      "loss": 0.1906,
      "step": 19310
    },
    {
      "epoch": 6.137229987293519,
      "grad_norm": 6.5829386711120605,
      "learning_rate": 2e-05,
      "loss": 0.1899,
      "step": 19320
    },
    {
      "epoch": 6.1404066073697585,
      "grad_norm": 4.14912223815918,
      "learning_rate": 2e-05,
      "loss": 0.1712,
      "step": 19330
    },
    {
      "epoch": 6.143583227445998,
      "grad_norm": 5.710400104522705,
      "learning_rate": 2e-05,
      "loss": 0.194,
      "step": 19340
    },
    {
      "epoch": 6.146759847522237,
      "grad_norm": 4.5150532722473145,
      "learning_rate": 2e-05,
      "loss": 0.1974,
      "step": 19350
    },
    {
      "epoch": 6.149936467598475,
      "grad_norm": 4.737242221832275,
      "learning_rate": 2e-05,
      "loss": 0.1845,
      "step": 19360
    },
    {
      "epoch": 6.153113087674714,
      "grad_norm": 3.5838212966918945,
      "learning_rate": 2e-05,
      "loss": 0.208,
      "step": 19370
    },
    {
      "epoch": 6.156289707750953,
      "grad_norm": 4.901084899902344,
      "learning_rate": 2e-05,
      "loss": 0.19,
      "step": 19380
    },
    {
      "epoch": 6.159466327827192,
      "grad_norm": 4.028258800506592,
      "learning_rate": 2e-05,
      "loss": 0.1868,
      "step": 19390
    },
    {
      "epoch": 6.16264294790343,
      "grad_norm": 5.2598652839660645,
      "learning_rate": 2e-05,
      "loss": 0.1881,
      "step": 19400
    },
    {
      "epoch": 6.1658195679796695,
      "grad_norm": 6.835177421569824,
      "learning_rate": 2e-05,
      "loss": 0.1913,
      "step": 19410
    },
    {
      "epoch": 6.1689961880559085,
      "grad_norm": 3.683218240737915,
      "learning_rate": 2e-05,
      "loss": 0.1851,
      "step": 19420
    },
    {
      "epoch": 6.172172808132148,
      "grad_norm": 4.247264862060547,
      "learning_rate": 2e-05,
      "loss": 0.1873,
      "step": 19430
    },
    {
      "epoch": 6.175349428208387,
      "grad_norm": 5.807541370391846,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 19440
    },
    {
      "epoch": 6.178526048284625,
      "grad_norm": 4.103124141693115,
      "learning_rate": 2e-05,
      "loss": 0.1725,
      "step": 19450
    },
    {
      "epoch": 6.181702668360864,
      "grad_norm": 5.614225387573242,
      "learning_rate": 2e-05,
      "loss": 0.1711,
      "step": 19460
    },
    {
      "epoch": 6.184879288437103,
      "grad_norm": 10.431944847106934,
      "learning_rate": 2e-05,
      "loss": 0.1932,
      "step": 19470
    },
    {
      "epoch": 6.188055908513342,
      "grad_norm": 5.00376033782959,
      "learning_rate": 2e-05,
      "loss": 0.1699,
      "step": 19480
    },
    {
      "epoch": 6.19123252858958,
      "grad_norm": 7.336653709411621,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 19490
    },
    {
      "epoch": 6.1944091486658195,
      "grad_norm": 4.315217971801758,
      "learning_rate": 2e-05,
      "loss": 0.1961,
      "step": 19500
    },
    {
      "epoch": 6.197585768742059,
      "grad_norm": 3.6420810222625732,
      "learning_rate": 2e-05,
      "loss": 0.1998,
      "step": 19510
    },
    {
      "epoch": 6.200762388818298,
      "grad_norm": 4.776374340057373,
      "learning_rate": 2e-05,
      "loss": 0.1792,
      "step": 19520
    },
    {
      "epoch": 6.203939008894536,
      "grad_norm": 3.3096747398376465,
      "learning_rate": 2e-05,
      "loss": 0.1751,
      "step": 19530
    },
    {
      "epoch": 6.203939008894536,
      "eval_loss": 1.8259199857711792,
      "eval_mse": 1.8247055586973457,
      "eval_pearson": 0.3818758416670433,
      "eval_runtime": 7.4761,
      "eval_samples_per_second": 2883.876,
      "eval_spearmanr": 0.3851484773216006,
      "eval_steps_per_second": 11.37,
      "step": 19530
    },
    {
      "epoch": 6.207115628970775,
      "grad_norm": 7.018922328948975,
      "learning_rate": 2e-05,
      "loss": 0.1923,
      "step": 19540
    },
    {
      "epoch": 6.210292249047014,
      "grad_norm": 4.515854358673096,
      "learning_rate": 2e-05,
      "loss": 0.1869,
      "step": 19550
    },
    {
      "epoch": 6.213468869123253,
      "grad_norm": 5.615147113800049,
      "learning_rate": 2e-05,
      "loss": 0.1963,
      "step": 19560
    },
    {
      "epoch": 6.216645489199491,
      "grad_norm": 5.068775177001953,
      "learning_rate": 2e-05,
      "loss": 0.1924,
      "step": 19570
    },
    {
      "epoch": 6.21982210927573,
      "grad_norm": 4.861398220062256,
      "learning_rate": 2e-05,
      "loss": 0.2038,
      "step": 19580
    },
    {
      "epoch": 6.2229987293519695,
      "grad_norm": 8.092065811157227,
      "learning_rate": 2e-05,
      "loss": 0.1969,
      "step": 19590
    },
    {
      "epoch": 6.226175349428209,
      "grad_norm": 5.073419570922852,
      "learning_rate": 2e-05,
      "loss": 0.1928,
      "step": 19600
    },
    {
      "epoch": 6.229351969504448,
      "grad_norm": 5.2241315841674805,
      "learning_rate": 2e-05,
      "loss": 0.1851,
      "step": 19610
    },
    {
      "epoch": 6.232528589580686,
      "grad_norm": 4.105112075805664,
      "learning_rate": 2e-05,
      "loss": 0.1884,
      "step": 19620
    },
    {
      "epoch": 6.235705209656925,
      "grad_norm": 3.8200292587280273,
      "learning_rate": 2e-05,
      "loss": 0.1843,
      "step": 19630
    },
    {
      "epoch": 6.238881829733164,
      "grad_norm": 6.016852378845215,
      "learning_rate": 2e-05,
      "loss": 0.1873,
      "step": 19640
    },
    {
      "epoch": 6.242058449809403,
      "grad_norm": 3.6074087619781494,
      "learning_rate": 2e-05,
      "loss": 0.1821,
      "step": 19650
    },
    {
      "epoch": 6.245235069885641,
      "grad_norm": 4.119032382965088,
      "learning_rate": 2e-05,
      "loss": 0.1844,
      "step": 19660
    },
    {
      "epoch": 6.2484116899618805,
      "grad_norm": 3.996458053588867,
      "learning_rate": 2e-05,
      "loss": 0.2005,
      "step": 19670
    },
    {
      "epoch": 6.2515883100381195,
      "grad_norm": 5.457088947296143,
      "learning_rate": 2e-05,
      "loss": 0.1969,
      "step": 19680
    },
    {
      "epoch": 6.254764930114359,
      "grad_norm": 4.935695171356201,
      "learning_rate": 2e-05,
      "loss": 0.1864,
      "step": 19690
    },
    {
      "epoch": 6.257941550190597,
      "grad_norm": 3.898594379425049,
      "learning_rate": 2e-05,
      "loss": 0.1934,
      "step": 19700
    },
    {
      "epoch": 6.261118170266836,
      "grad_norm": 4.441177845001221,
      "learning_rate": 2e-05,
      "loss": 0.1844,
      "step": 19710
    },
    {
      "epoch": 6.264294790343075,
      "grad_norm": 5.037661075592041,
      "learning_rate": 2e-05,
      "loss": 0.1735,
      "step": 19720
    },
    {
      "epoch": 6.267471410419314,
      "grad_norm": 8.359010696411133,
      "learning_rate": 2e-05,
      "loss": 0.1809,
      "step": 19730
    },
    {
      "epoch": 6.270648030495552,
      "grad_norm": 4.358034133911133,
      "learning_rate": 2e-05,
      "loss": 0.199,
      "step": 19740
    },
    {
      "epoch": 6.273824650571791,
      "grad_norm": 3.5762016773223877,
      "learning_rate": 2e-05,
      "loss": 0.1922,
      "step": 19750
    },
    {
      "epoch": 6.2770012706480305,
      "grad_norm": 3.29141902923584,
      "learning_rate": 2e-05,
      "loss": 0.1806,
      "step": 19760
    },
    {
      "epoch": 6.28017789072427,
      "grad_norm": 4.520618915557861,
      "learning_rate": 2e-05,
      "loss": 0.1889,
      "step": 19770
    },
    {
      "epoch": 6.283354510800509,
      "grad_norm": 3.1896109580993652,
      "learning_rate": 2e-05,
      "loss": 0.171,
      "step": 19780
    },
    {
      "epoch": 6.286531130876747,
      "grad_norm": 6.269120693206787,
      "learning_rate": 2e-05,
      "loss": 0.1838,
      "step": 19790
    },
    {
      "epoch": 6.289707750952986,
      "grad_norm": 3.359642505645752,
      "learning_rate": 2e-05,
      "loss": 0.1952,
      "step": 19800
    },
    {
      "epoch": 6.292884371029225,
      "grad_norm": 4.897444725036621,
      "learning_rate": 2e-05,
      "loss": 0.1836,
      "step": 19810
    },
    {
      "epoch": 6.296060991105464,
      "grad_norm": 7.31226110458374,
      "learning_rate": 2e-05,
      "loss": 0.1954,
      "step": 19820
    },
    {
      "epoch": 6.299237611181702,
      "grad_norm": 4.027825355529785,
      "learning_rate": 2e-05,
      "loss": 0.1999,
      "step": 19830
    },
    {
      "epoch": 6.302414231257941,
      "grad_norm": 3.692574977874756,
      "learning_rate": 2e-05,
      "loss": 0.1942,
      "step": 19840
    },
    {
      "epoch": 6.304002541296061,
      "eval_loss": 1.8474873304367065,
      "eval_mse": 1.846088053287555,
      "eval_pearson": 0.4009980122090261,
      "eval_runtime": 7.2834,
      "eval_samples_per_second": 2960.157,
      "eval_spearmanr": 0.40390725622057605,
      "eval_steps_per_second": 11.67,
      "step": 19845
    },
    {
      "epoch": 6.3055908513341805,
      "grad_norm": 6.084158897399902,
      "learning_rate": 2e-05,
      "loss": 0.1965,
      "step": 19850
    },
    {
      "epoch": 6.30876747141042,
      "grad_norm": 7.592573642730713,
      "learning_rate": 2e-05,
      "loss": 0.1841,
      "step": 19860
    },
    {
      "epoch": 6.311944091486658,
      "grad_norm": 6.001180648803711,
      "learning_rate": 2e-05,
      "loss": 0.1818,
      "step": 19870
    },
    {
      "epoch": 6.315120711562897,
      "grad_norm": 5.3195390701293945,
      "learning_rate": 2e-05,
      "loss": 0.1852,
      "step": 19880
    },
    {
      "epoch": 6.318297331639136,
      "grad_norm": 4.248191833496094,
      "learning_rate": 2e-05,
      "loss": 0.1791,
      "step": 19890
    },
    {
      "epoch": 6.321473951715375,
      "grad_norm": 3.4535655975341797,
      "learning_rate": 2e-05,
      "loss": 0.1865,
      "step": 19900
    },
    {
      "epoch": 6.324650571791613,
      "grad_norm": 6.668551921844482,
      "learning_rate": 2e-05,
      "loss": 0.2011,
      "step": 19910
    },
    {
      "epoch": 6.327827191867852,
      "grad_norm": 3.9809491634368896,
      "learning_rate": 2e-05,
      "loss": 0.1911,
      "step": 19920
    },
    {
      "epoch": 6.3310038119440915,
      "grad_norm": 6.517160415649414,
      "learning_rate": 2e-05,
      "loss": 0.1928,
      "step": 19930
    },
    {
      "epoch": 6.3341804320203305,
      "grad_norm": 7.563091278076172,
      "learning_rate": 2e-05,
      "loss": 0.1991,
      "step": 19940
    },
    {
      "epoch": 6.33735705209657,
      "grad_norm": 6.1395721435546875,
      "learning_rate": 2e-05,
      "loss": 0.192,
      "step": 19950
    },
    {
      "epoch": 6.340533672172808,
      "grad_norm": 4.484879493713379,
      "learning_rate": 2e-05,
      "loss": 0.1937,
      "step": 19960
    },
    {
      "epoch": 6.343710292249047,
      "grad_norm": 3.870054006576538,
      "learning_rate": 2e-05,
      "loss": 0.1832,
      "step": 19970
    },
    {
      "epoch": 6.346886912325286,
      "grad_norm": 5.2427473068237305,
      "learning_rate": 2e-05,
      "loss": 0.1857,
      "step": 19980
    },
    {
      "epoch": 6.350063532401525,
      "grad_norm": 4.756399154663086,
      "learning_rate": 2e-05,
      "loss": 0.1868,
      "step": 19990
    },
    {
      "epoch": 6.353240152477763,
      "grad_norm": 6.243668556213379,
      "learning_rate": 2e-05,
      "loss": 0.1891,
      "step": 20000
    },
    {
      "epoch": 6.356416772554002,
      "grad_norm": 3.936440944671631,
      "learning_rate": 2e-05,
      "loss": 0.1857,
      "step": 20010
    },
    {
      "epoch": 6.3595933926302415,
      "grad_norm": 7.078125476837158,
      "learning_rate": 2e-05,
      "loss": 0.1785,
      "step": 20020
    },
    {
      "epoch": 6.362770012706481,
      "grad_norm": 7.46225118637085,
      "learning_rate": 2e-05,
      "loss": 0.1739,
      "step": 20030
    },
    {
      "epoch": 6.365946632782719,
      "grad_norm": 3.2907752990722656,
      "learning_rate": 2e-05,
      "loss": 0.1986,
      "step": 20040
    },
    {
      "epoch": 6.369123252858958,
      "grad_norm": 3.9678802490234375,
      "learning_rate": 2e-05,
      "loss": 0.1819,
      "step": 20050
    },
    {
      "epoch": 6.372299872935197,
      "grad_norm": 7.212926387786865,
      "learning_rate": 2e-05,
      "loss": 0.1899,
      "step": 20060
    },
    {
      "epoch": 6.375476493011436,
      "grad_norm": 5.006043910980225,
      "learning_rate": 2e-05,
      "loss": 0.1879,
      "step": 20070
    },
    {
      "epoch": 6.378653113087674,
      "grad_norm": 5.216212749481201,
      "learning_rate": 2e-05,
      "loss": 0.1883,
      "step": 20080
    },
    {
      "epoch": 6.381829733163913,
      "grad_norm": 5.173018932342529,
      "learning_rate": 2e-05,
      "loss": 0.1848,
      "step": 20090
    },
    {
      "epoch": 6.385006353240152,
      "grad_norm": 3.961683511734009,
      "learning_rate": 2e-05,
      "loss": 0.1915,
      "step": 20100
    },
    {
      "epoch": 6.3881829733163915,
      "grad_norm": 5.796480655670166,
      "learning_rate": 2e-05,
      "loss": 0.1898,
      "step": 20110
    },
    {
      "epoch": 6.391359593392631,
      "grad_norm": 7.320460319519043,
      "learning_rate": 2e-05,
      "loss": 0.206,
      "step": 20120
    },
    {
      "epoch": 6.394536213468869,
      "grad_norm": 4.709805011749268,
      "learning_rate": 2e-05,
      "loss": 0.1802,
      "step": 20130
    },
    {
      "epoch": 6.397712833545108,
      "grad_norm": 4.020937442779541,
      "learning_rate": 2e-05,
      "loss": 0.1829,
      "step": 20140
    },
    {
      "epoch": 6.400889453621347,
      "grad_norm": 3.706662893295288,
      "learning_rate": 2e-05,
      "loss": 0.197,
      "step": 20150
    },
    {
      "epoch": 6.404066073697586,
      "grad_norm": 3.734192132949829,
      "learning_rate": 2e-05,
      "loss": 0.1977,
      "step": 20160
    },
    {
      "epoch": 6.404066073697586,
      "eval_loss": 1.7473312616348267,
      "eval_mse": 1.7459510974185144,
      "eval_pearson": 0.3950892035880482,
      "eval_runtime": 7.5778,
      "eval_samples_per_second": 2845.136,
      "eval_spearmanr": 0.39785182764059757,
      "eval_steps_per_second": 11.217,
      "step": 20160
    },
    {
      "epoch": 6.407242693773824,
      "grad_norm": 5.830999851226807,
      "learning_rate": 2e-05,
      "loss": 0.1928,
      "step": 20170
    },
    {
      "epoch": 6.410419313850063,
      "grad_norm": 4.065641403198242,
      "learning_rate": 2e-05,
      "loss": 0.1719,
      "step": 20180
    },
    {
      "epoch": 6.4135959339263025,
      "grad_norm": 3.096524238586426,
      "learning_rate": 2e-05,
      "loss": 0.183,
      "step": 20190
    },
    {
      "epoch": 6.4167725540025415,
      "grad_norm": 7.832237243652344,
      "learning_rate": 2e-05,
      "loss": 0.1898,
      "step": 20200
    },
    {
      "epoch": 6.419949174078781,
      "grad_norm": 5.46011209487915,
      "learning_rate": 2e-05,
      "loss": 0.2095,
      "step": 20210
    },
    {
      "epoch": 6.423125794155019,
      "grad_norm": 10.571666717529297,
      "learning_rate": 2e-05,
      "loss": 0.1738,
      "step": 20220
    },
    {
      "epoch": 6.426302414231258,
      "grad_norm": 8.830689430236816,
      "learning_rate": 2e-05,
      "loss": 0.198,
      "step": 20230
    },
    {
      "epoch": 6.429479034307497,
      "grad_norm": 3.348757028579712,
      "learning_rate": 2e-05,
      "loss": 0.18,
      "step": 20240
    },
    {
      "epoch": 6.432655654383736,
      "grad_norm": 3.209566593170166,
      "learning_rate": 2e-05,
      "loss": 0.1799,
      "step": 20250
    },
    {
      "epoch": 6.435832274459974,
      "grad_norm": 7.48115873336792,
      "learning_rate": 2e-05,
      "loss": 0.1893,
      "step": 20260
    },
    {
      "epoch": 6.439008894536213,
      "grad_norm": 4.186143398284912,
      "learning_rate": 2e-05,
      "loss": 0.1865,
      "step": 20270
    },
    {
      "epoch": 6.4421855146124525,
      "grad_norm": 4.3767991065979,
      "learning_rate": 2e-05,
      "loss": 0.1853,
      "step": 20280
    },
    {
      "epoch": 6.445362134688692,
      "grad_norm": 5.585137844085693,
      "learning_rate": 2e-05,
      "loss": 0.1904,
      "step": 20290
    },
    {
      "epoch": 6.44853875476493,
      "grad_norm": 3.4140045642852783,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 20300
    },
    {
      "epoch": 6.451715374841169,
      "grad_norm": 8.184976577758789,
      "learning_rate": 2e-05,
      "loss": 0.1745,
      "step": 20310
    },
    {
      "epoch": 6.454891994917408,
      "grad_norm": 7.304995536804199,
      "learning_rate": 2e-05,
      "loss": 0.1759,
      "step": 20320
    },
    {
      "epoch": 6.458068614993647,
      "grad_norm": 3.820462942123413,
      "learning_rate": 2e-05,
      "loss": 0.1836,
      "step": 20330
    },
    {
      "epoch": 6.461245235069885,
      "grad_norm": 5.674333572387695,
      "learning_rate": 2e-05,
      "loss": 0.1841,
      "step": 20340
    },
    {
      "epoch": 6.464421855146124,
      "grad_norm": 8.534956932067871,
      "learning_rate": 2e-05,
      "loss": 0.1976,
      "step": 20350
    },
    {
      "epoch": 6.467598475222363,
      "grad_norm": 4.36352014541626,
      "learning_rate": 2e-05,
      "loss": 0.1926,
      "step": 20360
    },
    {
      "epoch": 6.4707750952986025,
      "grad_norm": 7.729356288909912,
      "learning_rate": 2e-05,
      "loss": 0.1842,
      "step": 20370
    },
    {
      "epoch": 6.473951715374842,
      "grad_norm": 3.6228809356689453,
      "learning_rate": 2e-05,
      "loss": 0.1824,
      "step": 20380
    },
    {
      "epoch": 6.47712833545108,
      "grad_norm": 7.512564182281494,
      "learning_rate": 2e-05,
      "loss": 0.1781,
      "step": 20390
    },
    {
      "epoch": 6.480304955527319,
      "grad_norm": 6.046713352203369,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 20400
    },
    {
      "epoch": 6.483481575603558,
      "grad_norm": 3.803607702255249,
      "learning_rate": 2e-05,
      "loss": 0.1808,
      "step": 20410
    },
    {
      "epoch": 6.486658195679797,
      "grad_norm": 6.7400970458984375,
      "learning_rate": 2e-05,
      "loss": 0.1735,
      "step": 20420
    },
    {
      "epoch": 6.489834815756035,
      "grad_norm": 5.355151653289795,
      "learning_rate": 2e-05,
      "loss": 0.1889,
      "step": 20430
    },
    {
      "epoch": 6.493011435832274,
      "grad_norm": 5.645556926727295,
      "learning_rate": 2e-05,
      "loss": 0.1742,
      "step": 20440
    },
    {
      "epoch": 6.4961880559085134,
      "grad_norm": 4.081892013549805,
      "learning_rate": 2e-05,
      "loss": 0.1962,
      "step": 20450
    },
    {
      "epoch": 6.4993646759847525,
      "grad_norm": 3.681525945663452,
      "learning_rate": 2e-05,
      "loss": 0.183,
      "step": 20460
    },
    {
      "epoch": 6.502541296060991,
      "grad_norm": 5.179543495178223,
      "learning_rate": 2e-05,
      "loss": 0.1856,
      "step": 20470
    },
    {
      "epoch": 6.50412960609911,
      "eval_loss": 1.787500023841858,
      "eval_mse": 1.7860986524477518,
      "eval_pearson": 0.405946085531097,
      "eval_runtime": 7.2854,
      "eval_samples_per_second": 2959.328,
      "eval_spearmanr": 0.40349833418431835,
      "eval_steps_per_second": 11.667,
      "step": 20475
    },
    {
      "epoch": 6.50571791613723,
      "grad_norm": 3.914463520050049,
      "learning_rate": 2e-05,
      "loss": 0.1676,
      "step": 20480
    },
    {
      "epoch": 6.508894536213469,
      "grad_norm": 4.103381633758545,
      "learning_rate": 2e-05,
      "loss": 0.1891,
      "step": 20490
    },
    {
      "epoch": 6.512071156289708,
      "grad_norm": 3.872340440750122,
      "learning_rate": 2e-05,
      "loss": 0.1766,
      "step": 20500
    },
    {
      "epoch": 6.515247776365946,
      "grad_norm": 3.973353862762451,
      "learning_rate": 2e-05,
      "loss": 0.1853,
      "step": 20510
    },
    {
      "epoch": 6.518424396442185,
      "grad_norm": 3.698065757751465,
      "learning_rate": 2e-05,
      "loss": 0.1902,
      "step": 20520
    },
    {
      "epoch": 6.521601016518424,
      "grad_norm": 4.6832380294799805,
      "learning_rate": 2e-05,
      "loss": 0.1895,
      "step": 20530
    },
    {
      "epoch": 6.5247776365946635,
      "grad_norm": 3.114438533782959,
      "learning_rate": 2e-05,
      "loss": 0.1804,
      "step": 20540
    },
    {
      "epoch": 6.527954256670903,
      "grad_norm": 3.5612661838531494,
      "learning_rate": 2e-05,
      "loss": 0.196,
      "step": 20550
    },
    {
      "epoch": 6.531130876747141,
      "grad_norm": 6.027698040008545,
      "learning_rate": 2e-05,
      "loss": 0.1965,
      "step": 20560
    },
    {
      "epoch": 6.53430749682338,
      "grad_norm": 5.118901252746582,
      "learning_rate": 2e-05,
      "loss": 0.1872,
      "step": 20570
    },
    {
      "epoch": 6.537484116899619,
      "grad_norm": 5.487534999847412,
      "learning_rate": 2e-05,
      "loss": 0.1944,
      "step": 20580
    },
    {
      "epoch": 6.540660736975858,
      "grad_norm": 5.164812088012695,
      "learning_rate": 2e-05,
      "loss": 0.1752,
      "step": 20590
    },
    {
      "epoch": 6.543837357052096,
      "grad_norm": 5.509205341339111,
      "learning_rate": 2e-05,
      "loss": 0.1782,
      "step": 20600
    },
    {
      "epoch": 6.547013977128335,
      "grad_norm": 4.2637224197387695,
      "learning_rate": 2e-05,
      "loss": 0.1908,
      "step": 20610
    },
    {
      "epoch": 6.550190597204574,
      "grad_norm": 4.905063152313232,
      "learning_rate": 2e-05,
      "loss": 0.1903,
      "step": 20620
    },
    {
      "epoch": 6.5533672172808135,
      "grad_norm": 4.4745283126831055,
      "learning_rate": 2e-05,
      "loss": 0.1881,
      "step": 20630
    },
    {
      "epoch": 6.556543837357053,
      "grad_norm": 3.851224660873413,
      "learning_rate": 2e-05,
      "loss": 0.1814,
      "step": 20640
    },
    {
      "epoch": 6.559720457433291,
      "grad_norm": 6.559844970703125,
      "learning_rate": 2e-05,
      "loss": 0.1666,
      "step": 20650
    },
    {
      "epoch": 6.56289707750953,
      "grad_norm": 3.856020212173462,
      "learning_rate": 2e-05,
      "loss": 0.1788,
      "step": 20660
    },
    {
      "epoch": 6.566073697585769,
      "grad_norm": 3.0119309425354004,
      "learning_rate": 2e-05,
      "loss": 0.1754,
      "step": 20670
    },
    {
      "epoch": 6.569250317662007,
      "grad_norm": 4.595954418182373,
      "learning_rate": 2e-05,
      "loss": 0.19,
      "step": 20680
    },
    {
      "epoch": 6.572426937738246,
      "grad_norm": 14.394648551940918,
      "learning_rate": 2e-05,
      "loss": 0.1751,
      "step": 20690
    },
    {
      "epoch": 6.575603557814485,
      "grad_norm": 4.973628520965576,
      "learning_rate": 2e-05,
      "loss": 0.1888,
      "step": 20700
    },
    {
      "epoch": 6.5787801778907244,
      "grad_norm": 3.6471240520477295,
      "learning_rate": 2e-05,
      "loss": 0.1771,
      "step": 20710
    },
    {
      "epoch": 6.5819567979669635,
      "grad_norm": 4.433504104614258,
      "learning_rate": 2e-05,
      "loss": 0.1918,
      "step": 20720
    },
    {
      "epoch": 6.585133418043202,
      "grad_norm": 6.772341251373291,
      "learning_rate": 2e-05,
      "loss": 0.1812,
      "step": 20730
    },
    {
      "epoch": 6.588310038119441,
      "grad_norm": 4.5488057136535645,
      "learning_rate": 2e-05,
      "loss": 0.1833,
      "step": 20740
    },
    {
      "epoch": 6.59148665819568,
      "grad_norm": 3.7153079509735107,
      "learning_rate": 2e-05,
      "loss": 0.1861,
      "step": 20750
    },
    {
      "epoch": 6.594663278271919,
      "grad_norm": 4.691158771514893,
      "learning_rate": 2e-05,
      "loss": 0.1956,
      "step": 20760
    },
    {
      "epoch": 6.597839898348157,
      "grad_norm": 6.945799827575684,
      "learning_rate": 2e-05,
      "loss": 0.1854,
      "step": 20770
    },
    {
      "epoch": 6.601016518424396,
      "grad_norm": 3.618150234222412,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 20780
    },
    {
      "epoch": 6.604193138500635,
      "grad_norm": 3.8023579120635986,
      "learning_rate": 2e-05,
      "loss": 0.1781,
      "step": 20790
    },
    {
      "epoch": 6.604193138500635,
      "eval_loss": 1.8560398817062378,
      "eval_mse": 1.8540076142228823,
      "eval_pearson": 0.3405316046697334,
      "eval_runtime": 7.3789,
      "eval_samples_per_second": 2921.832,
      "eval_spearmanr": 0.3451055165879576,
      "eval_steps_per_second": 11.519,
      "step": 20790
    },
    {
      "epoch": 6.6073697585768745,
      "grad_norm": 3.7894182205200195,
      "learning_rate": 2e-05,
      "loss": 0.1862,
      "step": 20800
    },
    {
      "epoch": 6.610546378653114,
      "grad_norm": 3.9197793006896973,
      "learning_rate": 2e-05,
      "loss": 0.1818,
      "step": 20810
    },
    {
      "epoch": 6.613722998729352,
      "grad_norm": 3.8170506954193115,
      "learning_rate": 2e-05,
      "loss": 0.1895,
      "step": 20820
    },
    {
      "epoch": 6.616899618805591,
      "grad_norm": 4.411282539367676,
      "learning_rate": 2e-05,
      "loss": 0.1919,
      "step": 20830
    },
    {
      "epoch": 6.62007623888183,
      "grad_norm": 3.4674758911132812,
      "learning_rate": 2e-05,
      "loss": 0.184,
      "step": 20840
    },
    {
      "epoch": 6.623252858958068,
      "grad_norm": 5.578627109527588,
      "learning_rate": 2e-05,
      "loss": 0.1784,
      "step": 20850
    },
    {
      "epoch": 6.626429479034307,
      "grad_norm": 4.085848808288574,
      "learning_rate": 2e-05,
      "loss": 0.194,
      "step": 20860
    },
    {
      "epoch": 6.629606099110546,
      "grad_norm": 4.615901470184326,
      "learning_rate": 2e-05,
      "loss": 0.1742,
      "step": 20870
    },
    {
      "epoch": 6.632782719186785,
      "grad_norm": 4.854286193847656,
      "learning_rate": 2e-05,
      "loss": 0.1716,
      "step": 20880
    },
    {
      "epoch": 6.6359593392630245,
      "grad_norm": 3.3303682804107666,
      "learning_rate": 2e-05,
      "loss": 0.173,
      "step": 20890
    },
    {
      "epoch": 6.639135959339263,
      "grad_norm": 5.030858993530273,
      "learning_rate": 2e-05,
      "loss": 0.1815,
      "step": 20900
    },
    {
      "epoch": 6.642312579415502,
      "grad_norm": 4.384069442749023,
      "learning_rate": 2e-05,
      "loss": 0.1831,
      "step": 20910
    },
    {
      "epoch": 6.645489199491741,
      "grad_norm": 7.1879777908325195,
      "learning_rate": 2e-05,
      "loss": 0.1787,
      "step": 20920
    },
    {
      "epoch": 6.64866581956798,
      "grad_norm": 4.468546390533447,
      "learning_rate": 2e-05,
      "loss": 0.1846,
      "step": 20930
    },
    {
      "epoch": 6.651842439644218,
      "grad_norm": 4.518668174743652,
      "learning_rate": 2e-05,
      "loss": 0.1842,
      "step": 20940
    },
    {
      "epoch": 6.655019059720457,
      "grad_norm": 4.4466142654418945,
      "learning_rate": 2e-05,
      "loss": 0.1739,
      "step": 20950
    },
    {
      "epoch": 6.658195679796696,
      "grad_norm": 2.899441719055176,
      "learning_rate": 2e-05,
      "loss": 0.1792,
      "step": 20960
    },
    {
      "epoch": 6.661372299872935,
      "grad_norm": 4.013421058654785,
      "learning_rate": 2e-05,
      "loss": 0.1814,
      "step": 20970
    },
    {
      "epoch": 6.6645489199491745,
      "grad_norm": 7.787140369415283,
      "learning_rate": 2e-05,
      "loss": 0.1714,
      "step": 20980
    },
    {
      "epoch": 6.667725540025413,
      "grad_norm": 4.047985076904297,
      "learning_rate": 2e-05,
      "loss": 0.1878,
      "step": 20990
    },
    {
      "epoch": 6.670902160101652,
      "grad_norm": 3.8997228145599365,
      "learning_rate": 2e-05,
      "loss": 0.1954,
      "step": 21000
    },
    {
      "epoch": 6.674078780177891,
      "grad_norm": 4.8752593994140625,
      "learning_rate": 2e-05,
      "loss": 0.1843,
      "step": 21010
    },
    {
      "epoch": 6.677255400254129,
      "grad_norm": 5.029972553253174,
      "learning_rate": 2e-05,
      "loss": 0.1862,
      "step": 21020
    },
    {
      "epoch": 6.680432020330368,
      "grad_norm": 6.349854946136475,
      "learning_rate": 2e-05,
      "loss": 0.1831,
      "step": 21030
    },
    {
      "epoch": 6.683608640406607,
      "grad_norm": 5.219573020935059,
      "learning_rate": 2e-05,
      "loss": 0.1967,
      "step": 21040
    },
    {
      "epoch": 6.686785260482846,
      "grad_norm": 4.350371837615967,
      "learning_rate": 2e-05,
      "loss": 0.1746,
      "step": 21050
    },
    {
      "epoch": 6.6899618805590855,
      "grad_norm": 6.32115364074707,
      "learning_rate": 2e-05,
      "loss": 0.1719,
      "step": 21060
    },
    {
      "epoch": 6.693138500635324,
      "grad_norm": 3.3850629329681396,
      "learning_rate": 2e-05,
      "loss": 0.1643,
      "step": 21070
    },
    {
      "epoch": 6.696315120711563,
      "grad_norm": 3.655194044113159,
      "learning_rate": 2e-05,
      "loss": 0.1752,
      "step": 21080
    },
    {
      "epoch": 6.699491740787802,
      "grad_norm": 5.272923469543457,
      "learning_rate": 2e-05,
      "loss": 0.2014,
      "step": 21090
    },
    {
      "epoch": 6.702668360864041,
      "grad_norm": 4.08376932144165,
      "learning_rate": 2e-05,
      "loss": 0.1656,
      "step": 21100
    },
    {
      "epoch": 6.7042566709021605,
      "eval_loss": 1.872880220413208,
      "eval_mse": 1.8714267757142409,
      "eval_pearson": 0.3842367022086806,
      "eval_runtime": 7.3861,
      "eval_samples_per_second": 2919.011,
      "eval_spearmanr": 0.38731618214059177,
      "eval_steps_per_second": 11.508,
      "step": 21105
    },
    {
      "epoch": 6.705844980940279,
      "grad_norm": 5.893400192260742,
      "learning_rate": 2e-05,
      "loss": 0.1853,
      "step": 21110
    },
    {
      "epoch": 6.709021601016518,
      "grad_norm": 3.3611714839935303,
      "learning_rate": 2e-05,
      "loss": 0.1872,
      "step": 21120
    },
    {
      "epoch": 6.712198221092757,
      "grad_norm": 3.132471799850464,
      "learning_rate": 2e-05,
      "loss": 0.1775,
      "step": 21130
    },
    {
      "epoch": 6.715374841168996,
      "grad_norm": 3.896355628967285,
      "learning_rate": 2e-05,
      "loss": 0.1743,
      "step": 21140
    },
    {
      "epoch": 6.7185514612452355,
      "grad_norm": 4.901011943817139,
      "learning_rate": 2e-05,
      "loss": 0.1734,
      "step": 21150
    },
    {
      "epoch": 6.721728081321474,
      "grad_norm": 6.937206268310547,
      "learning_rate": 2e-05,
      "loss": 0.1839,
      "step": 21160
    },
    {
      "epoch": 6.724904701397713,
      "grad_norm": 5.503350734710693,
      "learning_rate": 2e-05,
      "loss": 0.1682,
      "step": 21170
    },
    {
      "epoch": 6.728081321473952,
      "grad_norm": 3.8745920658111572,
      "learning_rate": 2e-05,
      "loss": 0.1758,
      "step": 21180
    },
    {
      "epoch": 6.731257941550191,
      "grad_norm": 5.667962074279785,
      "learning_rate": 2e-05,
      "loss": 0.1876,
      "step": 21190
    },
    {
      "epoch": 6.734434561626429,
      "grad_norm": 2.845695972442627,
      "learning_rate": 2e-05,
      "loss": 0.1709,
      "step": 21200
    },
    {
      "epoch": 6.737611181702668,
      "grad_norm": 5.663738250732422,
      "learning_rate": 2e-05,
      "loss": 0.173,
      "step": 21210
    },
    {
      "epoch": 6.740787801778907,
      "grad_norm": 4.221717834472656,
      "learning_rate": 2e-05,
      "loss": 0.1724,
      "step": 21220
    },
    {
      "epoch": 6.743964421855146,
      "grad_norm": 5.365452766418457,
      "learning_rate": 2e-05,
      "loss": 0.1845,
      "step": 21230
    },
    {
      "epoch": 6.747141041931385,
      "grad_norm": 4.066117763519287,
      "learning_rate": 2e-05,
      "loss": 0.1896,
      "step": 21240
    },
    {
      "epoch": 6.750317662007624,
      "grad_norm": 4.035703659057617,
      "learning_rate": 2e-05,
      "loss": 0.1715,
      "step": 21250
    },
    {
      "epoch": 6.753494282083863,
      "grad_norm": 5.228061199188232,
      "learning_rate": 2e-05,
      "loss": 0.1725,
      "step": 21260
    },
    {
      "epoch": 6.756670902160102,
      "grad_norm": 7.029428958892822,
      "learning_rate": 2e-05,
      "loss": 0.1805,
      "step": 21270
    },
    {
      "epoch": 6.75984752223634,
      "grad_norm": 5.665808200836182,
      "learning_rate": 2e-05,
      "loss": 0.1783,
      "step": 21280
    },
    {
      "epoch": 6.763024142312579,
      "grad_norm": 3.9769256114959717,
      "learning_rate": 2e-05,
      "loss": 0.1684,
      "step": 21290
    },
    {
      "epoch": 6.766200762388818,
      "grad_norm": 6.606021881103516,
      "learning_rate": 2e-05,
      "loss": 0.1807,
      "step": 21300
    },
    {
      "epoch": 6.769377382465057,
      "grad_norm": 3.8186230659484863,
      "learning_rate": 2e-05,
      "loss": 0.1729,
      "step": 21310
    },
    {
      "epoch": 6.7725540025412965,
      "grad_norm": 5.082793712615967,
      "learning_rate": 2e-05,
      "loss": 0.1827,
      "step": 21320
    },
    {
      "epoch": 6.775730622617535,
      "grad_norm": 4.441988468170166,
      "learning_rate": 2e-05,
      "loss": 0.1757,
      "step": 21330
    },
    {
      "epoch": 6.778907242693774,
      "grad_norm": 4.078949451446533,
      "learning_rate": 2e-05,
      "loss": 0.1761,
      "step": 21340
    },
    {
      "epoch": 6.782083862770013,
      "grad_norm": 5.337971210479736,
      "learning_rate": 2e-05,
      "loss": 0.1701,
      "step": 21350
    },
    {
      "epoch": 6.785260482846252,
      "grad_norm": 3.69728684425354,
      "learning_rate": 2e-05,
      "loss": 0.1699,
      "step": 21360
    },
    {
      "epoch": 6.78843710292249,
      "grad_norm": 9.987685203552246,
      "learning_rate": 2e-05,
      "loss": 0.177,
      "step": 21370
    },
    {
      "epoch": 6.791613722998729,
      "grad_norm": 3.526914358139038,
      "learning_rate": 2e-05,
      "loss": 0.1801,
      "step": 21380
    },
    {
      "epoch": 6.794790343074968,
      "grad_norm": 5.849725246429443,
      "learning_rate": 2e-05,
      "loss": 0.1779,
      "step": 21390
    },
    {
      "epoch": 6.797966963151207,
      "grad_norm": 3.627685546875,
      "learning_rate": 2e-05,
      "loss": 0.1817,
      "step": 21400
    },
    {
      "epoch": 6.801143583227446,
      "grad_norm": 2.9715096950531006,
      "learning_rate": 2e-05,
      "loss": 0.1838,
      "step": 21410
    },
    {
      "epoch": 6.804320203303685,
      "grad_norm": 4.776641845703125,
      "learning_rate": 2e-05,
      "loss": 0.1863,
      "step": 21420
    },
    {
      "epoch": 6.804320203303685,
      "eval_loss": 1.8847837448120117,
      "eval_mse": 1.8835572907225646,
      "eval_pearson": 0.37461326592872923,
      "eval_runtime": 7.3811,
      "eval_samples_per_second": 2920.965,
      "eval_spearmanr": 0.3748617978201137,
      "eval_steps_per_second": 11.516,
      "step": 21420
    },
    {
      "epoch": 6.807496823379924,
      "grad_norm": 4.393675804138184,
      "learning_rate": 2e-05,
      "loss": 0.1917,
      "step": 21430
    },
    {
      "epoch": 6.810673443456163,
      "grad_norm": 7.012402057647705,
      "learning_rate": 2e-05,
      "loss": 0.1706,
      "step": 21440
    },
    {
      "epoch": 6.813850063532401,
      "grad_norm": 6.012296676635742,
      "learning_rate": 2e-05,
      "loss": 0.1766,
      "step": 21450
    },
    {
      "epoch": 6.81702668360864,
      "grad_norm": 3.892383337020874,
      "learning_rate": 2e-05,
      "loss": 0.1866,
      "step": 21460
    },
    {
      "epoch": 6.820203303684879,
      "grad_norm": 7.3823771476745605,
      "learning_rate": 2e-05,
      "loss": 0.1768,
      "step": 21470
    },
    {
      "epoch": 6.823379923761118,
      "grad_norm": 4.0281805992126465,
      "learning_rate": 2e-05,
      "loss": 0.1706,
      "step": 21480
    },
    {
      "epoch": 6.826556543837357,
      "grad_norm": 4.814470291137695,
      "learning_rate": 2e-05,
      "loss": 0.1741,
      "step": 21490
    },
    {
      "epoch": 6.829733163913596,
      "grad_norm": 4.37487268447876,
      "learning_rate": 2e-05,
      "loss": 0.1815,
      "step": 21500
    },
    {
      "epoch": 6.832909783989835,
      "grad_norm": 5.27985954284668,
      "learning_rate": 2e-05,
      "loss": 0.1762,
      "step": 21510
    },
    {
      "epoch": 6.836086404066074,
      "grad_norm": 3.011974573135376,
      "learning_rate": 2e-05,
      "loss": 0.1722,
      "step": 21520
    },
    {
      "epoch": 6.839263024142313,
      "grad_norm": 3.569890260696411,
      "learning_rate": 2e-05,
      "loss": 0.1778,
      "step": 21530
    },
    {
      "epoch": 6.842439644218551,
      "grad_norm": 4.139035224914551,
      "learning_rate": 2e-05,
      "loss": 0.1787,
      "step": 21540
    },
    {
      "epoch": 6.84561626429479,
      "grad_norm": 3.1364383697509766,
      "learning_rate": 2e-05,
      "loss": 0.1716,
      "step": 21550
    },
    {
      "epoch": 6.848792884371029,
      "grad_norm": 4.32155179977417,
      "learning_rate": 2e-05,
      "loss": 0.1706,
      "step": 21560
    },
    {
      "epoch": 6.851969504447268,
      "grad_norm": 2.9967703819274902,
      "learning_rate": 2e-05,
      "loss": 0.1659,
      "step": 21570
    },
    {
      "epoch": 6.8551461245235075,
      "grad_norm": 7.292048931121826,
      "learning_rate": 2e-05,
      "loss": 0.1757,
      "step": 21580
    },
    {
      "epoch": 6.858322744599746,
      "grad_norm": 5.973872661590576,
      "learning_rate": 2e-05,
      "loss": 0.2024,
      "step": 21590
    },
    {
      "epoch": 6.861499364675985,
      "grad_norm": 5.656501293182373,
      "learning_rate": 2e-05,
      "loss": 0.1839,
      "step": 21600
    },
    {
      "epoch": 6.864675984752224,
      "grad_norm": 3.216439723968506,
      "learning_rate": 2e-05,
      "loss": 0.1859,
      "step": 21610
    },
    {
      "epoch": 6.867852604828462,
      "grad_norm": 5.172473430633545,
      "learning_rate": 2e-05,
      "loss": 0.1798,
      "step": 21620
    },
    {
      "epoch": 6.871029224904701,
      "grad_norm": 5.148773670196533,
      "learning_rate": 2e-05,
      "loss": 0.1867,
      "step": 21630
    },
    {
      "epoch": 6.87420584498094,
      "grad_norm": 6.505862712860107,
      "learning_rate": 2e-05,
      "loss": 0.1732,
      "step": 21640
    },
    {
      "epoch": 6.877382465057179,
      "grad_norm": 4.0701003074646,
      "learning_rate": 2e-05,
      "loss": 0.1985,
      "step": 21650
    },
    {
      "epoch": 6.880559085133418,
      "grad_norm": 3.3477210998535156,
      "learning_rate": 2e-05,
      "loss": 0.1647,
      "step": 21660
    },
    {
      "epoch": 6.883735705209657,
      "grad_norm": 4.338942050933838,
      "learning_rate": 2e-05,
      "loss": 0.1765,
      "step": 21670
    },
    {
      "epoch": 6.886912325285896,
      "grad_norm": 4.972555160522461,
      "learning_rate": 2e-05,
      "loss": 0.1777,
      "step": 21680
    },
    {
      "epoch": 6.890088945362135,
      "grad_norm": 6.165543556213379,
      "learning_rate": 2e-05,
      "loss": 0.1782,
      "step": 21690
    },
    {
      "epoch": 6.893265565438374,
      "grad_norm": 3.513033390045166,
      "learning_rate": 2e-05,
      "loss": 0.1639,
      "step": 21700
    },
    {
      "epoch": 6.896442185514612,
      "grad_norm": 4.458404541015625,
      "learning_rate": 2e-05,
      "loss": 0.1861,
      "step": 21710
    },
    {
      "epoch": 6.899618805590851,
      "grad_norm": 4.134284019470215,
      "learning_rate": 2e-05,
      "loss": 0.1808,
      "step": 21720
    },
    {
      "epoch": 6.90279542566709,
      "grad_norm": 3.945399045944214,
      "learning_rate": 2e-05,
      "loss": 0.1799,
      "step": 21730
    },
    {
      "epoch": 6.90438373570521,
      "eval_loss": 1.8987737894058228,
      "eval_mse": 1.8977348360539275,
      "eval_pearson": 0.3784602320678323,
      "eval_runtime": 7.4918,
      "eval_samples_per_second": 2877.81,
      "eval_spearmanr": 0.3820783660545475,
      "eval_steps_per_second": 11.346,
      "step": 21735
    },
    {
      "epoch": 6.905972045743329,
      "grad_norm": 3.671351671218872,
      "learning_rate": 2e-05,
      "loss": 0.1779,
      "step": 21740
    },
    {
      "epoch": 6.909148665819568,
      "grad_norm": 4.7951459884643555,
      "learning_rate": 2e-05,
      "loss": 0.1793,
      "step": 21750
    },
    {
      "epoch": 6.912325285895807,
      "grad_norm": 5.3493781089782715,
      "learning_rate": 2e-05,
      "loss": 0.1721,
      "step": 21760
    },
    {
      "epoch": 6.915501905972046,
      "grad_norm": 9.731500625610352,
      "learning_rate": 2e-05,
      "loss": 0.1746,
      "step": 21770
    },
    {
      "epoch": 6.918678526048285,
      "grad_norm": 3.459914445877075,
      "learning_rate": 2e-05,
      "loss": 0.1682,
      "step": 21780
    },
    {
      "epoch": 6.921855146124523,
      "grad_norm": 6.400112152099609,
      "learning_rate": 2e-05,
      "loss": 0.1697,
      "step": 21790
    },
    {
      "epoch": 6.925031766200762,
      "grad_norm": 4.125205039978027,
      "learning_rate": 2e-05,
      "loss": 0.1755,
      "step": 21800
    },
    {
      "epoch": 6.928208386277001,
      "grad_norm": 3.646679401397705,
      "learning_rate": 2e-05,
      "loss": 0.1858,
      "step": 21810
    },
    {
      "epoch": 6.93138500635324,
      "grad_norm": 3.969454288482666,
      "learning_rate": 2e-05,
      "loss": 0.1892,
      "step": 21820
    },
    {
      "epoch": 6.934561626429479,
      "grad_norm": 5.075425148010254,
      "learning_rate": 2e-05,
      "loss": 0.1877,
      "step": 21830
    },
    {
      "epoch": 6.937738246505718,
      "grad_norm": 4.450221061706543,
      "learning_rate": 2e-05,
      "loss": 0.1775,
      "step": 21840
    },
    {
      "epoch": 6.940914866581957,
      "grad_norm": 4.229465484619141,
      "learning_rate": 2e-05,
      "loss": 0.1685,
      "step": 21850
    },
    {
      "epoch": 6.944091486658196,
      "grad_norm": 5.532689094543457,
      "learning_rate": 2e-05,
      "loss": 0.1714,
      "step": 21860
    },
    {
      "epoch": 6.947268106734435,
      "grad_norm": 4.327013969421387,
      "learning_rate": 2e-05,
      "loss": 0.1743,
      "step": 21870
    },
    {
      "epoch": 6.950444726810673,
      "grad_norm": 5.855107307434082,
      "learning_rate": 2e-05,
      "loss": 0.1703,
      "step": 21880
    },
    {
      "epoch": 6.953621346886912,
      "grad_norm": 3.308580160140991,
      "learning_rate": 2e-05,
      "loss": 0.1769,
      "step": 21890
    },
    {
      "epoch": 6.956797966963151,
      "grad_norm": 4.77557897567749,
      "learning_rate": 2e-05,
      "loss": 0.1873,
      "step": 21900
    },
    {
      "epoch": 6.95997458703939,
      "grad_norm": 4.007937431335449,
      "learning_rate": 2e-05,
      "loss": 0.1656,
      "step": 21910
    },
    {
      "epoch": 6.963151207115629,
      "grad_norm": 6.377922534942627,
      "learning_rate": 2e-05,
      "loss": 0.1708,
      "step": 21920
    },
    {
      "epoch": 6.966327827191868,
      "grad_norm": 3.3416810035705566,
      "learning_rate": 2e-05,
      "loss": 0.1772,
      "step": 21930
    },
    {
      "epoch": 6.969504447268107,
      "grad_norm": 3.306931972503662,
      "learning_rate": 2e-05,
      "loss": 0.1685,
      "step": 21940
    },
    {
      "epoch": 6.972681067344346,
      "grad_norm": 4.52747106552124,
      "learning_rate": 2e-05,
      "loss": 0.1665,
      "step": 21950
    },
    {
      "epoch": 6.975857687420585,
      "grad_norm": 5.139047145843506,
      "learning_rate": 2e-05,
      "loss": 0.1924,
      "step": 21960
    },
    {
      "epoch": 6.979034307496823,
      "grad_norm": 5.102491855621338,
      "learning_rate": 2e-05,
      "loss": 0.1882,
      "step": 21970
    },
    {
      "epoch": 6.982210927573062,
      "grad_norm": 4.819954872131348,
      "learning_rate": 2e-05,
      "loss": 0.1692,
      "step": 21980
    },
    {
      "epoch": 6.985387547649301,
      "grad_norm": 3.831956624984741,
      "learning_rate": 2e-05,
      "loss": 0.161,
      "step": 21990
    },
    {
      "epoch": 6.98856416772554,
      "grad_norm": 4.518287181854248,
      "learning_rate": 2e-05,
      "loss": 0.1737,
      "step": 22000
    },
    {
      "epoch": 6.9917407878017785,
      "grad_norm": 5.774888038635254,
      "learning_rate": 2e-05,
      "loss": 0.1722,
      "step": 22010
    },
    {
      "epoch": 6.994917407878018,
      "grad_norm": 4.957565784454346,
      "learning_rate": 2e-05,
      "loss": 0.1725,
      "step": 22020
    },
    {
      "epoch": 6.998094027954257,
      "grad_norm": 6.109321594238281,
      "learning_rate": 2e-05,
      "loss": 0.1812,
      "step": 22030
    },
    {
      "epoch": 7.001270648030496,
      "grad_norm": 4.5447773933410645,
      "learning_rate": 2e-05,
      "loss": 0.1667,
      "step": 22040
    },
    {
      "epoch": 7.004447268106734,
      "grad_norm": 5.433434963226318,
      "learning_rate": 2e-05,
      "loss": 0.161,
      "step": 22050
    },
    {
      "epoch": 7.004447268106734,
      "eval_loss": 1.8701987266540527,
      "eval_mse": 1.8686917711733555,
      "eval_pearson": 0.3653752480585367,
      "eval_runtime": 7.3784,
      "eval_samples_per_second": 2922.03,
      "eval_spearmanr": 0.367445930455597,
      "eval_steps_per_second": 11.52,
      "step": 22050
    },
    {
      "epoch": 7.007623888182973,
      "grad_norm": 4.157687187194824,
      "learning_rate": 2e-05,
      "loss": 0.1508,
      "step": 22060
    },
    {
      "epoch": 7.010800508259212,
      "grad_norm": 3.6672604084014893,
      "learning_rate": 2e-05,
      "loss": 0.1592,
      "step": 22070
    },
    {
      "epoch": 7.013977128335451,
      "grad_norm": 2.966444969177246,
      "learning_rate": 2e-05,
      "loss": 0.1521,
      "step": 22080
    },
    {
      "epoch": 7.01715374841169,
      "grad_norm": 3.633650064468384,
      "learning_rate": 2e-05,
      "loss": 0.1513,
      "step": 22090
    },
    {
      "epoch": 7.020330368487929,
      "grad_norm": 3.6619865894317627,
      "learning_rate": 2e-05,
      "loss": 0.1638,
      "step": 22100
    },
    {
      "epoch": 7.023506988564168,
      "grad_norm": 4.371842384338379,
      "learning_rate": 2e-05,
      "loss": 0.1631,
      "step": 22110
    },
    {
      "epoch": 7.026683608640407,
      "grad_norm": 4.843018054962158,
      "learning_rate": 2e-05,
      "loss": 0.1566,
      "step": 22120
    },
    {
      "epoch": 7.029860228716646,
      "grad_norm": 27.615177154541016,
      "learning_rate": 2e-05,
      "loss": 0.1652,
      "step": 22130
    },
    {
      "epoch": 7.033036848792884,
      "grad_norm": 5.221574783325195,
      "learning_rate": 2e-05,
      "loss": 0.1591,
      "step": 22140
    },
    {
      "epoch": 7.036213468869123,
      "grad_norm": 2.863116979598999,
      "learning_rate": 2e-05,
      "loss": 0.1526,
      "step": 22150
    },
    {
      "epoch": 7.039390088945362,
      "grad_norm": 4.794731616973877,
      "learning_rate": 2e-05,
      "loss": 0.1637,
      "step": 22160
    },
    {
      "epoch": 7.042566709021601,
      "grad_norm": 3.4862008094787598,
      "learning_rate": 2e-05,
      "loss": 0.1556,
      "step": 22170
    },
    {
      "epoch": 7.0457433290978395,
      "grad_norm": 5.804200172424316,
      "learning_rate": 2e-05,
      "loss": 0.1501,
      "step": 22180
    },
    {
      "epoch": 7.048919949174079,
      "grad_norm": 5.813156604766846,
      "learning_rate": 2e-05,
      "loss": 0.1752,
      "step": 22190
    },
    {
      "epoch": 7.052096569250318,
      "grad_norm": 2.7658884525299072,
      "learning_rate": 2e-05,
      "loss": 0.1537,
      "step": 22200
    },
    {
      "epoch": 7.055273189326557,
      "grad_norm": 3.6963791847229004,
      "learning_rate": 2e-05,
      "loss": 0.153,
      "step": 22210
    },
    {
      "epoch": 7.058449809402795,
      "grad_norm": 4.7106099128723145,
      "learning_rate": 2e-05,
      "loss": 0.1579,
      "step": 22220
    },
    {
      "epoch": 7.061626429479034,
      "grad_norm": 4.90877628326416,
      "learning_rate": 2e-05,
      "loss": 0.1619,
      "step": 22230
    },
    {
      "epoch": 7.064803049555273,
      "grad_norm": 4.710103988647461,
      "learning_rate": 2e-05,
      "loss": 0.1543,
      "step": 22240
    },
    {
      "epoch": 7.067979669631512,
      "grad_norm": 3.728295087814331,
      "learning_rate": 2e-05,
      "loss": 0.1679,
      "step": 22250
    },
    {
      "epoch": 7.071156289707751,
      "grad_norm": 4.173122882843018,
      "learning_rate": 2e-05,
      "loss": 0.1525,
      "step": 22260
    },
    {
      "epoch": 7.0743329097839895,
      "grad_norm": 6.668185234069824,
      "learning_rate": 2e-05,
      "loss": 0.1626,
      "step": 22270
    },
    {
      "epoch": 7.077509529860229,
      "grad_norm": 4.304904460906982,
      "learning_rate": 2e-05,
      "loss": 0.1604,
      "step": 22280
    },
    {
      "epoch": 7.080686149936468,
      "grad_norm": 3.4311695098876953,
      "learning_rate": 2e-05,
      "loss": 0.1576,
      "step": 22290
    },
    {
      "epoch": 7.083862770012707,
      "grad_norm": 3.5131077766418457,
      "learning_rate": 2e-05,
      "loss": 0.15,
      "step": 22300
    },
    {
      "epoch": 7.087039390088945,
      "grad_norm": 3.432281494140625,
      "learning_rate": 2e-05,
      "loss": 0.1468,
      "step": 22310
    },
    {
      "epoch": 7.090216010165184,
      "grad_norm": 2.4026601314544678,
      "learning_rate": 2e-05,
      "loss": 0.1459,
      "step": 22320
    },
    {
      "epoch": 7.093392630241423,
      "grad_norm": 3.4626824855804443,
      "learning_rate": 2e-05,
      "loss": 0.1672,
      "step": 22330
    },
    {
      "epoch": 7.096569250317662,
      "grad_norm": 6.68741512298584,
      "learning_rate": 2e-05,
      "loss": 0.1654,
      "step": 22340
    },
    {
      "epoch": 7.0997458703939005,
      "grad_norm": 4.527693271636963,
      "learning_rate": 2e-05,
      "loss": 0.1682,
      "step": 22350
    },
    {
      "epoch": 7.10292249047014,
      "grad_norm": 7.556695461273193,
      "learning_rate": 2e-05,
      "loss": 0.1596,
      "step": 22360
    },
    {
      "epoch": 7.104510800508259,
      "eval_loss": 1.7746325731277466,
      "eval_mse": 1.7733685745600203,
      "eval_pearson": 0.4056623855214257,
      "eval_runtime": 7.4671,
      "eval_samples_per_second": 2887.315,
      "eval_spearmanr": 0.40674207805285584,
      "eval_steps_per_second": 11.383,
      "step": 22365
    },
    {
      "epoch": 7.106099110546379,
      "grad_norm": 9.711060523986816,
      "learning_rate": 2e-05,
      "loss": 0.1649,
      "step": 22370
    },
    {
      "epoch": 7.109275730622618,
      "grad_norm": 3.229325532913208,
      "learning_rate": 2e-05,
      "loss": 0.156,
      "step": 22380
    },
    {
      "epoch": 7.112452350698857,
      "grad_norm": 4.410171031951904,
      "learning_rate": 2e-05,
      "loss": 0.1599,
      "step": 22390
    },
    {
      "epoch": 7.115628970775095,
      "grad_norm": 4.0340471267700195,
      "learning_rate": 2e-05,
      "loss": 0.1676,
      "step": 22400
    },
    {
      "epoch": 7.118805590851334,
      "grad_norm": 5.498843669891357,
      "learning_rate": 2e-05,
      "loss": 0.1505,
      "step": 22410
    },
    {
      "epoch": 7.121982210927573,
      "grad_norm": 3.713253974914551,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 22420
    },
    {
      "epoch": 7.125158831003812,
      "grad_norm": 2.9675705432891846,
      "learning_rate": 2e-05,
      "loss": 0.1543,
      "step": 22430
    },
    {
      "epoch": 7.1283354510800505,
      "grad_norm": 5.828207969665527,
      "learning_rate": 2e-05,
      "loss": 0.1627,
      "step": 22440
    },
    {
      "epoch": 7.13151207115629,
      "grad_norm": 4.864591598510742,
      "learning_rate": 2e-05,
      "loss": 0.1472,
      "step": 22450
    },
    {
      "epoch": 7.134688691232529,
      "grad_norm": 2.947999954223633,
      "learning_rate": 2e-05,
      "loss": 0.1589,
      "step": 22460
    },
    {
      "epoch": 7.137865311308768,
      "grad_norm": 3.147042989730835,
      "learning_rate": 2e-05,
      "loss": 0.1695,
      "step": 22470
    },
    {
      "epoch": 7.141041931385006,
      "grad_norm": 3.638709545135498,
      "learning_rate": 2e-05,
      "loss": 0.1658,
      "step": 22480
    },
    {
      "epoch": 7.144218551461245,
      "grad_norm": 3.8519694805145264,
      "learning_rate": 2e-05,
      "loss": 0.1517,
      "step": 22490
    },
    {
      "epoch": 7.147395171537484,
      "grad_norm": 5.54977560043335,
      "learning_rate": 2e-05,
      "loss": 0.1554,
      "step": 22500
    },
    {
      "epoch": 7.150571791613723,
      "grad_norm": 3.4169461727142334,
      "learning_rate": 2e-05,
      "loss": 0.1606,
      "step": 22510
    },
    {
      "epoch": 7.1537484116899615,
      "grad_norm": 3.7621803283691406,
      "learning_rate": 2e-05,
      "loss": 0.1585,
      "step": 22520
    },
    {
      "epoch": 7.1569250317662005,
      "grad_norm": 4.654877662658691,
      "learning_rate": 2e-05,
      "loss": 0.157,
      "step": 22530
    },
    {
      "epoch": 7.16010165184244,
      "grad_norm": 7.115988731384277,
      "learning_rate": 2e-05,
      "loss": 0.1783,
      "step": 22540
    },
    {
      "epoch": 7.163278271918679,
      "grad_norm": 3.5832526683807373,
      "learning_rate": 2e-05,
      "loss": 0.1581,
      "step": 22550
    },
    {
      "epoch": 7.166454891994918,
      "grad_norm": 4.957088470458984,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 22560
    },
    {
      "epoch": 7.169631512071156,
      "grad_norm": 3.1056785583496094,
      "learning_rate": 2e-05,
      "loss": 0.1671,
      "step": 22570
    },
    {
      "epoch": 7.172808132147395,
      "grad_norm": 3.4947993755340576,
      "learning_rate": 2e-05,
      "loss": 0.1626,
      "step": 22580
    },
    {
      "epoch": 7.175984752223634,
      "grad_norm": 8.35772705078125,
      "learning_rate": 2e-05,
      "loss": 0.1745,
      "step": 22590
    },
    {
      "epoch": 7.179161372299873,
      "grad_norm": 3.0945000648498535,
      "learning_rate": 2e-05,
      "loss": 0.1592,
      "step": 22600
    },
    {
      "epoch": 7.1823379923761115,
      "grad_norm": 2.9239511489868164,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 22610
    },
    {
      "epoch": 7.185514612452351,
      "grad_norm": 3.8461546897888184,
      "learning_rate": 2e-05,
      "loss": 0.1671,
      "step": 22620
    },
    {
      "epoch": 7.18869123252859,
      "grad_norm": 4.079312801361084,
      "learning_rate": 2e-05,
      "loss": 0.1528,
      "step": 22630
    },
    {
      "epoch": 7.191867852604829,
      "grad_norm": 3.889967203140259,
      "learning_rate": 2e-05,
      "loss": 0.1671,
      "step": 22640
    },
    {
      "epoch": 7.195044472681067,
      "grad_norm": 13.586886405944824,
      "learning_rate": 2e-05,
      "loss": 0.1689,
      "step": 22650
    },
    {
      "epoch": 7.198221092757306,
      "grad_norm": 3.8967702388763428,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 22660
    },
    {
      "epoch": 7.201397712833545,
      "grad_norm": 3.0876519680023193,
      "learning_rate": 2e-05,
      "loss": 0.1615,
      "step": 22670
    },
    {
      "epoch": 7.204574332909784,
      "grad_norm": 11.948954582214355,
      "learning_rate": 2e-05,
      "loss": 0.1563,
      "step": 22680
    },
    {
      "epoch": 7.204574332909784,
      "eval_loss": 1.8768248558044434,
      "eval_mse": 1.8763742431528272,
      "eval_pearson": 0.37605105782855847,
      "eval_runtime": 7.2883,
      "eval_samples_per_second": 2958.174,
      "eval_spearmanr": 0.3775815294513511,
      "eval_steps_per_second": 11.663,
      "step": 22680
    },
    {
      "epoch": 7.207750952986023,
      "grad_norm": 3.969874143600464,
      "learning_rate": 2e-05,
      "loss": 0.1667,
      "step": 22690
    },
    {
      "epoch": 7.2109275730622615,
      "grad_norm": 5.580011367797852,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 22700
    },
    {
      "epoch": 7.214104193138501,
      "grad_norm": 3.8965532779693604,
      "learning_rate": 2e-05,
      "loss": 0.1654,
      "step": 22710
    },
    {
      "epoch": 7.21728081321474,
      "grad_norm": 3.7275009155273438,
      "learning_rate": 2e-05,
      "loss": 0.1625,
      "step": 22720
    },
    {
      "epoch": 7.220457433290979,
      "grad_norm": 3.0505077838897705,
      "learning_rate": 2e-05,
      "loss": 0.1565,
      "step": 22730
    },
    {
      "epoch": 7.223634053367217,
      "grad_norm": 3.2104241847991943,
      "learning_rate": 2e-05,
      "loss": 0.1543,
      "step": 22740
    },
    {
      "epoch": 7.226810673443456,
      "grad_norm": 4.501924514770508,
      "learning_rate": 2e-05,
      "loss": 0.1461,
      "step": 22750
    },
    {
      "epoch": 7.229987293519695,
      "grad_norm": 4.077107906341553,
      "learning_rate": 2e-05,
      "loss": 0.1698,
      "step": 22760
    },
    {
      "epoch": 7.233163913595934,
      "grad_norm": 3.059476852416992,
      "learning_rate": 2e-05,
      "loss": 0.1696,
      "step": 22770
    },
    {
      "epoch": 7.2363405336721724,
      "grad_norm": 4.076052665710449,
      "learning_rate": 2e-05,
      "loss": 0.1564,
      "step": 22780
    },
    {
      "epoch": 7.2395171537484115,
      "grad_norm": 3.310345411300659,
      "learning_rate": 2e-05,
      "loss": 0.157,
      "step": 22790
    },
    {
      "epoch": 7.242693773824651,
      "grad_norm": 4.171814441680908,
      "learning_rate": 2e-05,
      "loss": 0.1609,
      "step": 22800
    },
    {
      "epoch": 7.24587039390089,
      "grad_norm": 3.431373119354248,
      "learning_rate": 2e-05,
      "loss": 0.1552,
      "step": 22810
    },
    {
      "epoch": 7.249047013977128,
      "grad_norm": 5.822288990020752,
      "learning_rate": 2e-05,
      "loss": 0.1722,
      "step": 22820
    },
    {
      "epoch": 7.252223634053367,
      "grad_norm": 5.293865203857422,
      "learning_rate": 2e-05,
      "loss": 0.1642,
      "step": 22830
    },
    {
      "epoch": 7.255400254129606,
      "grad_norm": 4.368415832519531,
      "learning_rate": 2e-05,
      "loss": 0.1576,
      "step": 22840
    },
    {
      "epoch": 7.258576874205845,
      "grad_norm": 7.798318862915039,
      "learning_rate": 2e-05,
      "loss": 0.1488,
      "step": 22850
    },
    {
      "epoch": 7.261753494282084,
      "grad_norm": 6.341175079345703,
      "learning_rate": 2e-05,
      "loss": 0.1637,
      "step": 22860
    },
    {
      "epoch": 7.2649301143583225,
      "grad_norm": 5.135983943939209,
      "learning_rate": 2e-05,
      "loss": 0.1606,
      "step": 22870
    },
    {
      "epoch": 7.268106734434562,
      "grad_norm": 4.809017181396484,
      "learning_rate": 2e-05,
      "loss": 0.1521,
      "step": 22880
    },
    {
      "epoch": 7.271283354510801,
      "grad_norm": 4.301117897033691,
      "learning_rate": 2e-05,
      "loss": 0.1563,
      "step": 22890
    },
    {
      "epoch": 7.27445997458704,
      "grad_norm": 4.412938117980957,
      "learning_rate": 2e-05,
      "loss": 0.157,
      "step": 22900
    },
    {
      "epoch": 7.277636594663278,
      "grad_norm": 4.049612522125244,
      "learning_rate": 2e-05,
      "loss": 0.156,
      "step": 22910
    },
    {
      "epoch": 7.280813214739517,
      "grad_norm": 5.0089874267578125,
      "learning_rate": 2e-05,
      "loss": 0.15,
      "step": 22920
    },
    {
      "epoch": 7.283989834815756,
      "grad_norm": 3.942720413208008,
      "learning_rate": 2e-05,
      "loss": 0.158,
      "step": 22930
    },
    {
      "epoch": 7.287166454891995,
      "grad_norm": 4.403225421905518,
      "learning_rate": 2e-05,
      "loss": 0.1637,
      "step": 22940
    },
    {
      "epoch": 7.290343074968233,
      "grad_norm": 3.6117615699768066,
      "learning_rate": 2e-05,
      "loss": 0.1634,
      "step": 22950
    },
    {
      "epoch": 7.2935196950444725,
      "grad_norm": 6.565289497375488,
      "learning_rate": 2e-05,
      "loss": 0.1696,
      "step": 22960
    },
    {
      "epoch": 7.296696315120712,
      "grad_norm": 5.747542858123779,
      "learning_rate": 2e-05,
      "loss": 0.1474,
      "step": 22970
    },
    {
      "epoch": 7.299872935196951,
      "grad_norm": 3.518117666244507,
      "learning_rate": 2e-05,
      "loss": 0.1617,
      "step": 22980
    },
    {
      "epoch": 7.303049555273189,
      "grad_norm": 5.397368431091309,
      "learning_rate": 2e-05,
      "loss": 0.1602,
      "step": 22990
    },
    {
      "epoch": 7.304637865311308,
      "eval_loss": 1.8570935726165771,
      "eval_mse": 1.8554994688184452,
      "eval_pearson": 0.35245520728093493,
      "eval_runtime": 7.3771,
      "eval_samples_per_second": 2922.556,
      "eval_spearmanr": 0.3580001280378519,
      "eval_steps_per_second": 11.522,
      "step": 22995
    },
    {
      "epoch": 7.306226175349428,
      "grad_norm": 3.1792285442352295,
      "learning_rate": 2e-05,
      "loss": 0.1547,
      "step": 23000
    },
    {
      "epoch": 7.309402795425667,
      "grad_norm": 5.753742694854736,
      "learning_rate": 2e-05,
      "loss": 0.1647,
      "step": 23010
    },
    {
      "epoch": 7.312579415501906,
      "grad_norm": 8.15308666229248,
      "learning_rate": 2e-05,
      "loss": 0.1546,
      "step": 23020
    },
    {
      "epoch": 7.315756035578145,
      "grad_norm": 3.6311447620391846,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 23030
    },
    {
      "epoch": 7.3189326556543834,
      "grad_norm": 4.178608417510986,
      "learning_rate": 2e-05,
      "loss": 0.1637,
      "step": 23040
    },
    {
      "epoch": 7.3221092757306225,
      "grad_norm": 3.0371201038360596,
      "learning_rate": 2e-05,
      "loss": 0.1511,
      "step": 23050
    },
    {
      "epoch": 7.325285895806862,
      "grad_norm": 5.645447254180908,
      "learning_rate": 2e-05,
      "loss": 0.1545,
      "step": 23060
    },
    {
      "epoch": 7.328462515883101,
      "grad_norm": 3.2464065551757812,
      "learning_rate": 2e-05,
      "loss": 0.1658,
      "step": 23070
    },
    {
      "epoch": 7.331639135959339,
      "grad_norm": 2.7331206798553467,
      "learning_rate": 2e-05,
      "loss": 0.1606,
      "step": 23080
    },
    {
      "epoch": 7.334815756035578,
      "grad_norm": 3.934764862060547,
      "learning_rate": 2e-05,
      "loss": 0.1543,
      "step": 23090
    },
    {
      "epoch": 7.337992376111817,
      "grad_norm": 4.207302093505859,
      "learning_rate": 2e-05,
      "loss": 0.1457,
      "step": 23100
    },
    {
      "epoch": 7.341168996188056,
      "grad_norm": 3.6167080402374268,
      "learning_rate": 2e-05,
      "loss": 0.1514,
      "step": 23110
    },
    {
      "epoch": 7.344345616264294,
      "grad_norm": 3.868514060974121,
      "learning_rate": 2e-05,
      "loss": 0.1524,
      "step": 23120
    },
    {
      "epoch": 7.3475222363405335,
      "grad_norm": 4.201895236968994,
      "learning_rate": 2e-05,
      "loss": 0.158,
      "step": 23130
    },
    {
      "epoch": 7.350698856416773,
      "grad_norm": 5.729025840759277,
      "learning_rate": 2e-05,
      "loss": 0.1628,
      "step": 23140
    },
    {
      "epoch": 7.353875476493012,
      "grad_norm": 4.772963523864746,
      "learning_rate": 2e-05,
      "loss": 0.1673,
      "step": 23150
    },
    {
      "epoch": 7.35705209656925,
      "grad_norm": 5.353960990905762,
      "learning_rate": 2e-05,
      "loss": 0.1493,
      "step": 23160
    },
    {
      "epoch": 7.360228716645489,
      "grad_norm": 5.381017684936523,
      "learning_rate": 2e-05,
      "loss": 0.1514,
      "step": 23170
    },
    {
      "epoch": 7.363405336721728,
      "grad_norm": 3.0205814838409424,
      "learning_rate": 2e-05,
      "loss": 0.1627,
      "step": 23180
    },
    {
      "epoch": 7.366581956797967,
      "grad_norm": 5.8155012130737305,
      "learning_rate": 2e-05,
      "loss": 0.1726,
      "step": 23190
    },
    {
      "epoch": 7.369758576874206,
      "grad_norm": 3.5324230194091797,
      "learning_rate": 2e-05,
      "loss": 0.1629,
      "step": 23200
    },
    {
      "epoch": 7.372935196950444,
      "grad_norm": 3.419644832611084,
      "learning_rate": 2e-05,
      "loss": 0.1591,
      "step": 23210
    },
    {
      "epoch": 7.3761118170266835,
      "grad_norm": 3.7275938987731934,
      "learning_rate": 2e-05,
      "loss": 0.1525,
      "step": 23220
    },
    {
      "epoch": 7.379288437102923,
      "grad_norm": 5.843517303466797,
      "learning_rate": 2e-05,
      "loss": 0.1514,
      "step": 23230
    },
    {
      "epoch": 7.382465057179162,
      "grad_norm": 5.660618782043457,
      "learning_rate": 2e-05,
      "loss": 0.1628,
      "step": 23240
    },
    {
      "epoch": 7.3856416772554,
      "grad_norm": 3.3111469745635986,
      "learning_rate": 2e-05,
      "loss": 0.1538,
      "step": 23250
    },
    {
      "epoch": 7.388818297331639,
      "grad_norm": 7.706808567047119,
      "learning_rate": 2e-05,
      "loss": 0.1658,
      "step": 23260
    },
    {
      "epoch": 7.391994917407878,
      "grad_norm": 4.932754039764404,
      "learning_rate": 2e-05,
      "loss": 0.1659,
      "step": 23270
    },
    {
      "epoch": 7.395171537484117,
      "grad_norm": 3.284334897994995,
      "learning_rate": 2e-05,
      "loss": 0.1589,
      "step": 23280
    },
    {
      "epoch": 7.398348157560356,
      "grad_norm": 3.6266376972198486,
      "learning_rate": 2e-05,
      "loss": 0.1562,
      "step": 23290
    },
    {
      "epoch": 7.401524777636594,
      "grad_norm": 4.373465538024902,
      "learning_rate": 2e-05,
      "loss": 0.1568,
      "step": 23300
    },
    {
      "epoch": 7.4047013977128335,
      "grad_norm": 4.970595836639404,
      "learning_rate": 2e-05,
      "loss": 0.1677,
      "step": 23310
    },
    {
      "epoch": 7.4047013977128335,
      "eval_loss": 1.7180246114730835,
      "eval_mse": 1.7161866221445612,
      "eval_pearson": 0.4066504893361068,
      "eval_runtime": 7.404,
      "eval_samples_per_second": 2911.929,
      "eval_spearmanr": 0.4123928572292871,
      "eval_steps_per_second": 11.48,
      "step": 23310
    },
    {
      "epoch": 7.407878017789073,
      "grad_norm": 5.665832996368408,
      "learning_rate": 2e-05,
      "loss": 0.1644,
      "step": 23320
    },
    {
      "epoch": 7.411054637865312,
      "grad_norm": 5.273681163787842,
      "learning_rate": 2e-05,
      "loss": 0.1612,
      "step": 23330
    },
    {
      "epoch": 7.41423125794155,
      "grad_norm": 3.446079969406128,
      "learning_rate": 2e-05,
      "loss": 0.1532,
      "step": 23340
    },
    {
      "epoch": 7.417407878017789,
      "grad_norm": 7.634118556976318,
      "learning_rate": 2e-05,
      "loss": 0.161,
      "step": 23350
    },
    {
      "epoch": 7.420584498094028,
      "grad_norm": 4.815098285675049,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 23360
    },
    {
      "epoch": 7.423761118170267,
      "grad_norm": 4.706335067749023,
      "learning_rate": 2e-05,
      "loss": 0.1512,
      "step": 23370
    },
    {
      "epoch": 7.426937738246505,
      "grad_norm": 3.287163257598877,
      "learning_rate": 2e-05,
      "loss": 0.1517,
      "step": 23380
    },
    {
      "epoch": 7.4301143583227445,
      "grad_norm": 3.9052700996398926,
      "learning_rate": 2e-05,
      "loss": 0.1498,
      "step": 23390
    },
    {
      "epoch": 7.433290978398984,
      "grad_norm": 3.1903889179229736,
      "learning_rate": 2e-05,
      "loss": 0.1518,
      "step": 23400
    },
    {
      "epoch": 7.436467598475223,
      "grad_norm": 5.809498310089111,
      "learning_rate": 2e-05,
      "loss": 0.1656,
      "step": 23410
    },
    {
      "epoch": 7.439644218551461,
      "grad_norm": 2.820209503173828,
      "learning_rate": 2e-05,
      "loss": 0.1537,
      "step": 23420
    },
    {
      "epoch": 7.4428208386277,
      "grad_norm": 4.618488311767578,
      "learning_rate": 2e-05,
      "loss": 0.1432,
      "step": 23430
    },
    {
      "epoch": 7.445997458703939,
      "grad_norm": 3.18609881401062,
      "learning_rate": 2e-05,
      "loss": 0.1588,
      "step": 23440
    },
    {
      "epoch": 7.449174078780178,
      "grad_norm": 3.666177988052368,
      "learning_rate": 2e-05,
      "loss": 0.1619,
      "step": 23450
    },
    {
      "epoch": 7.452350698856417,
      "grad_norm": 3.46748685836792,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 23460
    },
    {
      "epoch": 7.455527318932655,
      "grad_norm": 3.449000358581543,
      "learning_rate": 2e-05,
      "loss": 0.1561,
      "step": 23470
    },
    {
      "epoch": 7.4587039390088945,
      "grad_norm": 6.87791109085083,
      "learning_rate": 2e-05,
      "loss": 0.1668,
      "step": 23480
    },
    {
      "epoch": 7.461880559085134,
      "grad_norm": 4.909304141998291,
      "learning_rate": 2e-05,
      "loss": 0.1603,
      "step": 23490
    },
    {
      "epoch": 7.465057179161373,
      "grad_norm": 6.254884719848633,
      "learning_rate": 2e-05,
      "loss": 0.1495,
      "step": 23500
    },
    {
      "epoch": 7.468233799237611,
      "grad_norm": 3.33144211769104,
      "learning_rate": 2e-05,
      "loss": 0.1516,
      "step": 23510
    },
    {
      "epoch": 7.47141041931385,
      "grad_norm": 7.89081335067749,
      "learning_rate": 2e-05,
      "loss": 0.16,
      "step": 23520
    },
    {
      "epoch": 7.474587039390089,
      "grad_norm": 3.1697700023651123,
      "learning_rate": 2e-05,
      "loss": 0.1625,
      "step": 23530
    },
    {
      "epoch": 7.477763659466328,
      "grad_norm": 3.5361974239349365,
      "learning_rate": 2e-05,
      "loss": 0.1536,
      "step": 23540
    },
    {
      "epoch": 7.480940279542566,
      "grad_norm": 3.9042141437530518,
      "learning_rate": 2e-05,
      "loss": 0.1634,
      "step": 23550
    },
    {
      "epoch": 7.484116899618805,
      "grad_norm": 4.611155986785889,
      "learning_rate": 2e-05,
      "loss": 0.1592,
      "step": 23560
    },
    {
      "epoch": 7.4872935196950445,
      "grad_norm": 5.912824630737305,
      "learning_rate": 2e-05,
      "loss": 0.16,
      "step": 23570
    },
    {
      "epoch": 7.490470139771284,
      "grad_norm": 5.470908164978027,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 23580
    },
    {
      "epoch": 7.493646759847522,
      "grad_norm": 3.1440083980560303,
      "learning_rate": 2e-05,
      "loss": 0.1609,
      "step": 23590
    },
    {
      "epoch": 7.496823379923761,
      "grad_norm": 3.284029483795166,
      "learning_rate": 2e-05,
      "loss": 0.1572,
      "step": 23600
    },
    {
      "epoch": 7.5,
      "grad_norm": 2.978672742843628,
      "learning_rate": 2e-05,
      "loss": 0.1635,
      "step": 23610
    },
    {
      "epoch": 7.503176620076239,
      "grad_norm": 3.1412947177886963,
      "learning_rate": 2e-05,
      "loss": 0.1721,
      "step": 23620
    },
    {
      "epoch": 7.504764930114359,
      "eval_loss": 1.863019585609436,
      "eval_mse": 1.8615536653293308,
      "eval_pearson": 0.40357443115785785,
      "eval_runtime": 7.5013,
      "eval_samples_per_second": 2874.177,
      "eval_spearmanr": 0.4083187292032419,
      "eval_steps_per_second": 11.331,
      "step": 23625
    },
    {
      "epoch": 7.506353240152478,
      "grad_norm": 3.3494272232055664,
      "learning_rate": 2e-05,
      "loss": 0.1529,
      "step": 23630
    },
    {
      "epoch": 7.509529860228716,
      "grad_norm": 2.5339596271514893,
      "learning_rate": 2e-05,
      "loss": 0.1578,
      "step": 23640
    },
    {
      "epoch": 7.5127064803049555,
      "grad_norm": 3.941958427429199,
      "learning_rate": 2e-05,
      "loss": 0.1382,
      "step": 23650
    },
    {
      "epoch": 7.515883100381195,
      "grad_norm": 3.5341196060180664,
      "learning_rate": 2e-05,
      "loss": 0.1657,
      "step": 23660
    },
    {
      "epoch": 7.519059720457434,
      "grad_norm": 5.431127071380615,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 23670
    },
    {
      "epoch": 7.522236340533672,
      "grad_norm": 3.4110872745513916,
      "learning_rate": 2e-05,
      "loss": 0.1532,
      "step": 23680
    },
    {
      "epoch": 7.525412960609911,
      "grad_norm": 3.307828426361084,
      "learning_rate": 2e-05,
      "loss": 0.1617,
      "step": 23690
    },
    {
      "epoch": 7.52858958068615,
      "grad_norm": 4.149054050445557,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 23700
    },
    {
      "epoch": 7.531766200762389,
      "grad_norm": 3.8617396354675293,
      "learning_rate": 2e-05,
      "loss": 0.1598,
      "step": 23710
    },
    {
      "epoch": 7.534942820838627,
      "grad_norm": 6.221413612365723,
      "learning_rate": 2e-05,
      "loss": 0.1436,
      "step": 23720
    },
    {
      "epoch": 7.538119440914866,
      "grad_norm": 3.065201759338379,
      "learning_rate": 2e-05,
      "loss": 0.1687,
      "step": 23730
    },
    {
      "epoch": 7.5412960609911055,
      "grad_norm": 3.1453700065612793,
      "learning_rate": 2e-05,
      "loss": 0.1704,
      "step": 23740
    },
    {
      "epoch": 7.544472681067345,
      "grad_norm": 2.8576290607452393,
      "learning_rate": 2e-05,
      "loss": 0.1536,
      "step": 23750
    },
    {
      "epoch": 7.547649301143583,
      "grad_norm": 5.473209857940674,
      "learning_rate": 2e-05,
      "loss": 0.1667,
      "step": 23760
    },
    {
      "epoch": 7.550825921219822,
      "grad_norm": 2.9365527629852295,
      "learning_rate": 2e-05,
      "loss": 0.1464,
      "step": 23770
    },
    {
      "epoch": 7.554002541296061,
      "grad_norm": 5.008683204650879,
      "learning_rate": 2e-05,
      "loss": 0.1605,
      "step": 23780
    },
    {
      "epoch": 7.5571791613723,
      "grad_norm": 3.1928608417510986,
      "learning_rate": 2e-05,
      "loss": 0.1573,
      "step": 23790
    },
    {
      "epoch": 7.560355781448539,
      "grad_norm": 4.735845565795898,
      "learning_rate": 2e-05,
      "loss": 0.1563,
      "step": 23800
    },
    {
      "epoch": 7.563532401524777,
      "grad_norm": 4.38819694519043,
      "learning_rate": 2e-05,
      "loss": 0.1458,
      "step": 23810
    },
    {
      "epoch": 7.566709021601016,
      "grad_norm": 4.717341423034668,
      "learning_rate": 2e-05,
      "loss": 0.1654,
      "step": 23820
    },
    {
      "epoch": 7.5698856416772555,
      "grad_norm": 3.482438802719116,
      "learning_rate": 2e-05,
      "loss": 0.1682,
      "step": 23830
    },
    {
      "epoch": 7.573062261753495,
      "grad_norm": 4.175316333770752,
      "learning_rate": 2e-05,
      "loss": 0.1566,
      "step": 23840
    },
    {
      "epoch": 7.576238881829733,
      "grad_norm": 6.121145725250244,
      "learning_rate": 2e-05,
      "loss": 0.1685,
      "step": 23850
    },
    {
      "epoch": 7.579415501905972,
      "grad_norm": 3.0676398277282715,
      "learning_rate": 2e-05,
      "loss": 0.1402,
      "step": 23860
    },
    {
      "epoch": 7.582592121982211,
      "grad_norm": 2.8262405395507812,
      "learning_rate": 2e-05,
      "loss": 0.1507,
      "step": 23870
    },
    {
      "epoch": 7.58576874205845,
      "grad_norm": 3.992117404937744,
      "learning_rate": 2e-05,
      "loss": 0.1607,
      "step": 23880
    },
    {
      "epoch": 7.588945362134689,
      "grad_norm": 4.678308486938477,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 23890
    },
    {
      "epoch": 7.592121982210927,
      "grad_norm": 3.645521402359009,
      "learning_rate": 2e-05,
      "loss": 0.1605,
      "step": 23900
    },
    {
      "epoch": 7.5952986022871665,
      "grad_norm": 3.9990615844726562,
      "learning_rate": 2e-05,
      "loss": 0.1752,
      "step": 23910
    },
    {
      "epoch": 7.598475222363406,
      "grad_norm": 5.6755900382995605,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 23920
    },
    {
      "epoch": 7.601651842439644,
      "grad_norm": 7.404361724853516,
      "learning_rate": 2e-05,
      "loss": 0.1568,
      "step": 23930
    },
    {
      "epoch": 7.604828462515883,
      "grad_norm": 4.586530685424805,
      "learning_rate": 2e-05,
      "loss": 0.1746,
      "step": 23940
    },
    {
      "epoch": 7.604828462515883,
      "eval_loss": 1.7835488319396973,
      "eval_mse": 1.7814108895542229,
      "eval_pearson": 0.38212689523835347,
      "eval_runtime": 7.2879,
      "eval_samples_per_second": 2958.326,
      "eval_spearmanr": 0.38762040154208544,
      "eval_steps_per_second": 11.663,
      "step": 23940
    },
    {
      "epoch": 7.608005082592122,
      "grad_norm": 4.110146522521973,
      "learning_rate": 2e-05,
      "loss": 0.1545,
      "step": 23950
    },
    {
      "epoch": 7.611181702668361,
      "grad_norm": 3.19289231300354,
      "learning_rate": 2e-05,
      "loss": 0.1586,
      "step": 23960
    },
    {
      "epoch": 7.6143583227446,
      "grad_norm": 2.8830337524414062,
      "learning_rate": 2e-05,
      "loss": 0.1496,
      "step": 23970
    },
    {
      "epoch": 7.617534942820838,
      "grad_norm": 3.417633533477783,
      "learning_rate": 2e-05,
      "loss": 0.1528,
      "step": 23980
    },
    {
      "epoch": 7.620711562897077,
      "grad_norm": 2.532327890396118,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 23990
    },
    {
      "epoch": 7.6238881829733165,
      "grad_norm": 5.052576541900635,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 24000
    },
    {
      "epoch": 7.627064803049556,
      "grad_norm": 4.839084148406982,
      "learning_rate": 2e-05,
      "loss": 0.1611,
      "step": 24010
    },
    {
      "epoch": 7.630241423125794,
      "grad_norm": 3.5110068321228027,
      "learning_rate": 2e-05,
      "loss": 0.1546,
      "step": 24020
    },
    {
      "epoch": 7.633418043202033,
      "grad_norm": 4.120371341705322,
      "learning_rate": 2e-05,
      "loss": 0.145,
      "step": 24030
    },
    {
      "epoch": 7.636594663278272,
      "grad_norm": 2.7754156589508057,
      "learning_rate": 2e-05,
      "loss": 0.1615,
      "step": 24040
    },
    {
      "epoch": 7.639771283354511,
      "grad_norm": 2.832977771759033,
      "learning_rate": 2e-05,
      "loss": 0.1433,
      "step": 24050
    },
    {
      "epoch": 7.64294790343075,
      "grad_norm": 5.414550304412842,
      "learning_rate": 2e-05,
      "loss": 0.1635,
      "step": 24060
    },
    {
      "epoch": 7.646124523506988,
      "grad_norm": 3.9763004779815674,
      "learning_rate": 2e-05,
      "loss": 0.1558,
      "step": 24070
    },
    {
      "epoch": 7.649301143583227,
      "grad_norm": 3.5781524181365967,
      "learning_rate": 2e-05,
      "loss": 0.1485,
      "step": 24080
    },
    {
      "epoch": 7.6524777636594665,
      "grad_norm": 5.792533874511719,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 24090
    },
    {
      "epoch": 7.655654383735705,
      "grad_norm": 3.2814126014709473,
      "learning_rate": 2e-05,
      "loss": 0.1423,
      "step": 24100
    },
    {
      "epoch": 7.658831003811944,
      "grad_norm": 4.144772529602051,
      "learning_rate": 2e-05,
      "loss": 0.1442,
      "step": 24110
    },
    {
      "epoch": 7.662007623888183,
      "grad_norm": 5.39717435836792,
      "learning_rate": 2e-05,
      "loss": 0.1605,
      "step": 24120
    },
    {
      "epoch": 7.665184243964422,
      "grad_norm": 5.522481918334961,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 24130
    },
    {
      "epoch": 7.668360864040661,
      "grad_norm": 3.235992670059204,
      "learning_rate": 2e-05,
      "loss": 0.1573,
      "step": 24140
    },
    {
      "epoch": 7.671537484116899,
      "grad_norm": 3.1764919757843018,
      "learning_rate": 2e-05,
      "loss": 0.1565,
      "step": 24150
    },
    {
      "epoch": 7.674714104193138,
      "grad_norm": 3.1836585998535156,
      "learning_rate": 2e-05,
      "loss": 0.1589,
      "step": 24160
    },
    {
      "epoch": 7.6778907242693775,
      "grad_norm": 5.193765640258789,
      "learning_rate": 2e-05,
      "loss": 0.159,
      "step": 24170
    },
    {
      "epoch": 7.6810673443456166,
      "grad_norm": 4.953022480010986,
      "learning_rate": 2e-05,
      "loss": 0.1601,
      "step": 24180
    },
    {
      "epoch": 7.684243964421855,
      "grad_norm": 3.974148750305176,
      "learning_rate": 2e-05,
      "loss": 0.1564,
      "step": 24190
    },
    {
      "epoch": 7.687420584498094,
      "grad_norm": 3.3456661701202393,
      "learning_rate": 2e-05,
      "loss": 0.1495,
      "step": 24200
    },
    {
      "epoch": 7.690597204574333,
      "grad_norm": 5.051788806915283,
      "learning_rate": 2e-05,
      "loss": 0.1463,
      "step": 24210
    },
    {
      "epoch": 7.693773824650572,
      "grad_norm": 3.412155866622925,
      "learning_rate": 2e-05,
      "loss": 0.155,
      "step": 24220
    },
    {
      "epoch": 7.696950444726811,
      "grad_norm": 3.963716745376587,
      "learning_rate": 2e-05,
      "loss": 0.163,
      "step": 24230
    },
    {
      "epoch": 7.700127064803049,
      "grad_norm": 3.731787919998169,
      "learning_rate": 2e-05,
      "loss": 0.1418,
      "step": 24240
    },
    {
      "epoch": 7.703303684879288,
      "grad_norm": 3.621764659881592,
      "learning_rate": 2e-05,
      "loss": 0.1545,
      "step": 24250
    },
    {
      "epoch": 7.704891994917408,
      "eval_loss": 1.8446508646011353,
      "eval_mse": 1.8438063567101692,
      "eval_pearson": 0.4027088961936667,
      "eval_runtime": 7.4939,
      "eval_samples_per_second": 2877.0,
      "eval_spearmanr": 0.407067572920605,
      "eval_steps_per_second": 11.343,
      "step": 24255
    },
    {
      "epoch": 7.7064803049555275,
      "grad_norm": 6.972725868225098,
      "learning_rate": 2e-05,
      "loss": 0.1503,
      "step": 24260
    },
    {
      "epoch": 7.709656925031767,
      "grad_norm": 3.1665778160095215,
      "learning_rate": 2e-05,
      "loss": 0.1495,
      "step": 24270
    },
    {
      "epoch": 7.712833545108005,
      "grad_norm": 3.8232498168945312,
      "learning_rate": 2e-05,
      "loss": 0.1538,
      "step": 24280
    },
    {
      "epoch": 7.716010165184244,
      "grad_norm": 6.421877861022949,
      "learning_rate": 2e-05,
      "loss": 0.1587,
      "step": 24290
    },
    {
      "epoch": 7.719186785260483,
      "grad_norm": 4.567174911499023,
      "learning_rate": 2e-05,
      "loss": 0.1538,
      "step": 24300
    },
    {
      "epoch": 7.722363405336722,
      "grad_norm": 3.059476375579834,
      "learning_rate": 2e-05,
      "loss": 0.153,
      "step": 24310
    },
    {
      "epoch": 7.72554002541296,
      "grad_norm": 5.262125015258789,
      "learning_rate": 2e-05,
      "loss": 0.1506,
      "step": 24320
    },
    {
      "epoch": 7.728716645489199,
      "grad_norm": 4.479394912719727,
      "learning_rate": 2e-05,
      "loss": 0.1539,
      "step": 24330
    },
    {
      "epoch": 7.731893265565438,
      "grad_norm": 3.5918240547180176,
      "learning_rate": 2e-05,
      "loss": 0.1554,
      "step": 24340
    },
    {
      "epoch": 7.7350698856416775,
      "grad_norm": 6.752242088317871,
      "learning_rate": 2e-05,
      "loss": 0.1602,
      "step": 24350
    },
    {
      "epoch": 7.738246505717916,
      "grad_norm": 5.2621259689331055,
      "learning_rate": 2e-05,
      "loss": 0.1527,
      "step": 24360
    },
    {
      "epoch": 7.741423125794155,
      "grad_norm": 2.991018772125244,
      "learning_rate": 2e-05,
      "loss": 0.1555,
      "step": 24370
    },
    {
      "epoch": 7.744599745870394,
      "grad_norm": 3.1041979789733887,
      "learning_rate": 2e-05,
      "loss": 0.1515,
      "step": 24380
    },
    {
      "epoch": 7.747776365946633,
      "grad_norm": 4.975674629211426,
      "learning_rate": 2e-05,
      "loss": 0.1449,
      "step": 24390
    },
    {
      "epoch": 7.750952986022872,
      "grad_norm": 6.1158270835876465,
      "learning_rate": 2e-05,
      "loss": 0.1528,
      "step": 24400
    },
    {
      "epoch": 7.75412960609911,
      "grad_norm": 4.689221382141113,
      "learning_rate": 2e-05,
      "loss": 0.1622,
      "step": 24410
    },
    {
      "epoch": 7.757306226175349,
      "grad_norm": 4.622063159942627,
      "learning_rate": 2e-05,
      "loss": 0.1545,
      "step": 24420
    },
    {
      "epoch": 7.7604828462515885,
      "grad_norm": 4.9387617111206055,
      "learning_rate": 2e-05,
      "loss": 0.1556,
      "step": 24430
    },
    {
      "epoch": 7.7636594663278276,
      "grad_norm": 4.213411331176758,
      "learning_rate": 2e-05,
      "loss": 0.1442,
      "step": 24440
    },
    {
      "epoch": 7.766836086404066,
      "grad_norm": 3.59904146194458,
      "learning_rate": 2e-05,
      "loss": 0.1655,
      "step": 24450
    },
    {
      "epoch": 7.770012706480305,
      "grad_norm": 3.925907850265503,
      "learning_rate": 2e-05,
      "loss": 0.1593,
      "step": 24460
    },
    {
      "epoch": 7.773189326556544,
      "grad_norm": 3.0263280868530273,
      "learning_rate": 2e-05,
      "loss": 0.1551,
      "step": 24470
    },
    {
      "epoch": 7.776365946632783,
      "grad_norm": 3.296668767929077,
      "learning_rate": 2e-05,
      "loss": 0.1473,
      "step": 24480
    },
    {
      "epoch": 7.779542566709021,
      "grad_norm": 5.585457801818848,
      "learning_rate": 2e-05,
      "loss": 0.1492,
      "step": 24490
    },
    {
      "epoch": 7.78271918678526,
      "grad_norm": 4.8876051902771,
      "learning_rate": 2e-05,
      "loss": 0.1651,
      "step": 24500
    },
    {
      "epoch": 7.785895806861499,
      "grad_norm": 5.822315216064453,
      "learning_rate": 2e-05,
      "loss": 0.178,
      "step": 24510
    },
    {
      "epoch": 7.7890724269377385,
      "grad_norm": 4.871471405029297,
      "learning_rate": 2e-05,
      "loss": 0.1558,
      "step": 24520
    },
    {
      "epoch": 7.792249047013977,
      "grad_norm": 3.3430569171905518,
      "learning_rate": 2e-05,
      "loss": 0.1458,
      "step": 24530
    },
    {
      "epoch": 7.795425667090216,
      "grad_norm": 3.7576963901519775,
      "learning_rate": 2e-05,
      "loss": 0.1423,
      "step": 24540
    },
    {
      "epoch": 7.798602287166455,
      "grad_norm": 4.182971477508545,
      "learning_rate": 2e-05,
      "loss": 0.1467,
      "step": 24550
    },
    {
      "epoch": 7.801778907242694,
      "grad_norm": 4.9298577308654785,
      "learning_rate": 2e-05,
      "loss": 0.1525,
      "step": 24560
    },
    {
      "epoch": 7.804955527318933,
      "grad_norm": 6.760189533233643,
      "learning_rate": 2e-05,
      "loss": 0.1458,
      "step": 24570
    },
    {
      "epoch": 7.804955527318933,
      "eval_loss": 1.873128056526184,
      "eval_mse": 1.871713847021446,
      "eval_pearson": 0.3799059960091616,
      "eval_runtime": 7.3871,
      "eval_samples_per_second": 2918.583,
      "eval_spearmanr": 0.384152864232765,
      "eval_steps_per_second": 11.506,
      "step": 24570
    },
    {
      "epoch": 7.808132147395171,
      "grad_norm": 5.626201629638672,
      "learning_rate": 2e-05,
      "loss": 0.1515,
      "step": 24580
    },
    {
      "epoch": 7.81130876747141,
      "grad_norm": 3.416217088699341,
      "learning_rate": 2e-05,
      "loss": 0.1546,
      "step": 24590
    },
    {
      "epoch": 7.814485387547649,
      "grad_norm": 6.6201372146606445,
      "learning_rate": 2e-05,
      "loss": 0.1535,
      "step": 24600
    },
    {
      "epoch": 7.8176620076238885,
      "grad_norm": 4.565145492553711,
      "learning_rate": 2e-05,
      "loss": 0.1496,
      "step": 24610
    },
    {
      "epoch": 7.820838627700127,
      "grad_norm": 2.8776533603668213,
      "learning_rate": 2e-05,
      "loss": 0.1491,
      "step": 24620
    },
    {
      "epoch": 7.824015247776366,
      "grad_norm": 3.3542792797088623,
      "learning_rate": 2e-05,
      "loss": 0.1576,
      "step": 24630
    },
    {
      "epoch": 7.827191867852605,
      "grad_norm": 4.006499767303467,
      "learning_rate": 2e-05,
      "loss": 0.1408,
      "step": 24640
    },
    {
      "epoch": 7.830368487928844,
      "grad_norm": 3.6532838344573975,
      "learning_rate": 2e-05,
      "loss": 0.158,
      "step": 24650
    },
    {
      "epoch": 7.833545108005083,
      "grad_norm": 4.3980865478515625,
      "learning_rate": 2e-05,
      "loss": 0.1445,
      "step": 24660
    },
    {
      "epoch": 7.836721728081321,
      "grad_norm": 4.764953136444092,
      "learning_rate": 2e-05,
      "loss": 0.1484,
      "step": 24670
    },
    {
      "epoch": 7.83989834815756,
      "grad_norm": 3.8000407218933105,
      "learning_rate": 2e-05,
      "loss": 0.1527,
      "step": 24680
    },
    {
      "epoch": 7.8430749682337995,
      "grad_norm": 3.5501418113708496,
      "learning_rate": 2e-05,
      "loss": 0.1377,
      "step": 24690
    },
    {
      "epoch": 7.846251588310038,
      "grad_norm": 3.0605714321136475,
      "learning_rate": 2e-05,
      "loss": 0.1611,
      "step": 24700
    },
    {
      "epoch": 7.849428208386277,
      "grad_norm": 5.953707218170166,
      "learning_rate": 2e-05,
      "loss": 0.1566,
      "step": 24710
    },
    {
      "epoch": 7.852604828462516,
      "grad_norm": 3.659700870513916,
      "learning_rate": 2e-05,
      "loss": 0.1622,
      "step": 24720
    },
    {
      "epoch": 7.855781448538755,
      "grad_norm": 4.036818027496338,
      "learning_rate": 2e-05,
      "loss": 0.1648,
      "step": 24730
    },
    {
      "epoch": 7.858958068614994,
      "grad_norm": 3.102773666381836,
      "learning_rate": 2e-05,
      "loss": 0.165,
      "step": 24740
    },
    {
      "epoch": 7.862134688691232,
      "grad_norm": 5.863457679748535,
      "learning_rate": 2e-05,
      "loss": 0.1518,
      "step": 24750
    },
    {
      "epoch": 7.865311308767471,
      "grad_norm": 5.74409294128418,
      "learning_rate": 2e-05,
      "loss": 0.1603,
      "step": 24760
    },
    {
      "epoch": 7.86848792884371,
      "grad_norm": 5.433129787445068,
      "learning_rate": 2e-05,
      "loss": 0.1525,
      "step": 24770
    },
    {
      "epoch": 7.8716645489199495,
      "grad_norm": 3.258000612258911,
      "learning_rate": 2e-05,
      "loss": 0.167,
      "step": 24780
    },
    {
      "epoch": 7.874841168996188,
      "grad_norm": 5.264748573303223,
      "learning_rate": 2e-05,
      "loss": 0.1541,
      "step": 24790
    },
    {
      "epoch": 7.878017789072427,
      "grad_norm": 4.372788906097412,
      "learning_rate": 2e-05,
      "loss": 0.1468,
      "step": 24800
    },
    {
      "epoch": 7.881194409148666,
      "grad_norm": 3.4153478145599365,
      "learning_rate": 2e-05,
      "loss": 0.1533,
      "step": 24810
    },
    {
      "epoch": 7.884371029224905,
      "grad_norm": 3.211103916168213,
      "learning_rate": 2e-05,
      "loss": 0.1467,
      "step": 24820
    },
    {
      "epoch": 7.887547649301144,
      "grad_norm": 3.978839635848999,
      "learning_rate": 2e-05,
      "loss": 0.1363,
      "step": 24830
    },
    {
      "epoch": 7.890724269377382,
      "grad_norm": 3.4295899868011475,
      "learning_rate": 2e-05,
      "loss": 0.1509,
      "step": 24840
    },
    {
      "epoch": 7.893900889453621,
      "grad_norm": 2.7195470333099365,
      "learning_rate": 2e-05,
      "loss": 0.1416,
      "step": 24850
    },
    {
      "epoch": 7.89707750952986,
      "grad_norm": 4.078720569610596,
      "learning_rate": 2e-05,
      "loss": 0.1603,
      "step": 24860
    },
    {
      "epoch": 7.900254129606099,
      "grad_norm": 3.638756275177002,
      "learning_rate": 2e-05,
      "loss": 0.1644,
      "step": 24870
    },
    {
      "epoch": 7.903430749682338,
      "grad_norm": 3.7387454509735107,
      "learning_rate": 2e-05,
      "loss": 0.1529,
      "step": 24880
    },
    {
      "epoch": 7.905019059720457,
      "eval_loss": 1.839404582977295,
      "eval_mse": 1.838020034115036,
      "eval_pearson": 0.38954783378959296,
      "eval_runtime": 7.2951,
      "eval_samples_per_second": 2955.39,
      "eval_spearmanr": 0.3930311650593301,
      "eval_steps_per_second": 11.652,
      "step": 24885
    },
    {
      "epoch": 7.906607369758577,
      "grad_norm": 3.8048958778381348,
      "learning_rate": 2e-05,
      "loss": 0.1495,
      "step": 24890
    },
    {
      "epoch": 7.909783989834816,
      "grad_norm": 2.99057674407959,
      "learning_rate": 2e-05,
      "loss": 0.1493,
      "step": 24900
    },
    {
      "epoch": 7.912960609911055,
      "grad_norm": 2.5604021549224854,
      "learning_rate": 2e-05,
      "loss": 0.1476,
      "step": 24910
    },
    {
      "epoch": 7.916137229987293,
      "grad_norm": 2.8857851028442383,
      "learning_rate": 2e-05,
      "loss": 0.1577,
      "step": 24920
    },
    {
      "epoch": 7.919313850063532,
      "grad_norm": 4.40434455871582,
      "learning_rate": 2e-05,
      "loss": 0.1463,
      "step": 24930
    },
    {
      "epoch": 7.922490470139771,
      "grad_norm": 3.496192455291748,
      "learning_rate": 2e-05,
      "loss": 0.1535,
      "step": 24940
    },
    {
      "epoch": 7.9256670902160105,
      "grad_norm": 3.7804534435272217,
      "learning_rate": 2e-05,
      "loss": 0.1539,
      "step": 24950
    },
    {
      "epoch": 7.928843710292249,
      "grad_norm": 3.913975477218628,
      "learning_rate": 2e-05,
      "loss": 0.1457,
      "step": 24960
    },
    {
      "epoch": 7.932020330368488,
      "grad_norm": 3.730008363723755,
      "learning_rate": 2e-05,
      "loss": 0.1554,
      "step": 24970
    },
    {
      "epoch": 7.935196950444727,
      "grad_norm": 5.651547908782959,
      "learning_rate": 2e-05,
      "loss": 0.1468,
      "step": 24980
    },
    {
      "epoch": 7.938373570520966,
      "grad_norm": 3.259842872619629,
      "learning_rate": 2e-05,
      "loss": 0.1569,
      "step": 24990
    },
    {
      "epoch": 7.941550190597205,
      "grad_norm": 5.211953163146973,
      "learning_rate": 2e-05,
      "loss": 0.1559,
      "step": 25000
    },
    {
      "epoch": 7.944726810673443,
      "grad_norm": 5.610556125640869,
      "learning_rate": 2e-05,
      "loss": 0.1597,
      "step": 25010
    },
    {
      "epoch": 7.947903430749682,
      "grad_norm": 4.467710971832275,
      "learning_rate": 2e-05,
      "loss": 0.1471,
      "step": 25020
    },
    {
      "epoch": 7.951080050825921,
      "grad_norm": 4.474841594696045,
      "learning_rate": 2e-05,
      "loss": 0.1683,
      "step": 25030
    },
    {
      "epoch": 7.9542566709021605,
      "grad_norm": 4.920477390289307,
      "learning_rate": 2e-05,
      "loss": 0.1631,
      "step": 25040
    },
    {
      "epoch": 7.957433290978399,
      "grad_norm": 3.4893710613250732,
      "learning_rate": 2e-05,
      "loss": 0.1535,
      "step": 25050
    },
    {
      "epoch": 7.960609911054638,
      "grad_norm": 2.900869607925415,
      "learning_rate": 2e-05,
      "loss": 0.1567,
      "step": 25060
    },
    {
      "epoch": 7.963786531130877,
      "grad_norm": 2.982767105102539,
      "learning_rate": 2e-05,
      "loss": 0.1516,
      "step": 25070
    },
    {
      "epoch": 7.966963151207116,
      "grad_norm": 4.357304096221924,
      "learning_rate": 2e-05,
      "loss": 0.1466,
      "step": 25080
    },
    {
      "epoch": 7.970139771283354,
      "grad_norm": 4.191261291503906,
      "learning_rate": 2e-05,
      "loss": 0.1575,
      "step": 25090
    },
    {
      "epoch": 7.973316391359593,
      "grad_norm": 4.105010032653809,
      "learning_rate": 2e-05,
      "loss": 0.1591,
      "step": 25100
    },
    {
      "epoch": 7.976493011435832,
      "grad_norm": 5.76698637008667,
      "learning_rate": 2e-05,
      "loss": 0.1495,
      "step": 25110
    },
    {
      "epoch": 7.979669631512071,
      "grad_norm": 6.961573123931885,
      "learning_rate": 2e-05,
      "loss": 0.1578,
      "step": 25120
    },
    {
      "epoch": 7.98284625158831,
      "grad_norm": 4.502096652984619,
      "learning_rate": 2e-05,
      "loss": 0.1743,
      "step": 25130
    },
    {
      "epoch": 7.986022871664549,
      "grad_norm": 3.875945568084717,
      "learning_rate": 2e-05,
      "loss": 0.1618,
      "step": 25140
    },
    {
      "epoch": 7.989199491740788,
      "grad_norm": 4.1985297203063965,
      "learning_rate": 2e-05,
      "loss": 0.1488,
      "step": 25150
    },
    {
      "epoch": 7.992376111817027,
      "grad_norm": 3.1894493103027344,
      "learning_rate": 2e-05,
      "loss": 0.1551,
      "step": 25160
    },
    {
      "epoch": 7.995552731893266,
      "grad_norm": 3.8715360164642334,
      "learning_rate": 2e-05,
      "loss": 0.151,
      "step": 25170
    },
    {
      "epoch": 7.998729351969504,
      "grad_norm": 3.3963112831115723,
      "learning_rate": 2e-05,
      "loss": 0.1516,
      "step": 25180
    },
    {
      "epoch": 8.001905972045744,
      "grad_norm": 2.598602771759033,
      "learning_rate": 2e-05,
      "loss": 0.1517,
      "step": 25190
    },
    {
      "epoch": 8.005082592121981,
      "grad_norm": 2.5985772609710693,
      "learning_rate": 2e-05,
      "loss": 0.1309,
      "step": 25200
    },
    {
      "epoch": 8.005082592121981,
      "eval_loss": 1.7646640539169312,
      "eval_mse": 1.7627818016401027,
      "eval_pearson": 0.3991153503529081,
      "eval_runtime": 7.5005,
      "eval_samples_per_second": 2874.483,
      "eval_spearmanr": 0.4027401641590186,
      "eval_steps_per_second": 11.333,
      "step": 25200
    },
    {
      "epoch": 8.00825921219822,
      "grad_norm": 3.7493247985839844,
      "learning_rate": 2e-05,
      "loss": 0.1522,
      "step": 25210
    },
    {
      "epoch": 8.01143583227446,
      "grad_norm": 3.3362388610839844,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 25220
    },
    {
      "epoch": 8.014612452350699,
      "grad_norm": 3.7033071517944336,
      "learning_rate": 2e-05,
      "loss": 0.1394,
      "step": 25230
    },
    {
      "epoch": 8.017789072426938,
      "grad_norm": 2.9317145347595215,
      "learning_rate": 2e-05,
      "loss": 0.1362,
      "step": 25240
    },
    {
      "epoch": 8.020965692503177,
      "grad_norm": 13.539697647094727,
      "learning_rate": 2e-05,
      "loss": 0.134,
      "step": 25250
    },
    {
      "epoch": 8.024142312579416,
      "grad_norm": 4.3678879737854,
      "learning_rate": 2e-05,
      "loss": 0.1451,
      "step": 25260
    },
    {
      "epoch": 8.027318932655655,
      "grad_norm": 3.469383716583252,
      "learning_rate": 2e-05,
      "loss": 0.1441,
      "step": 25270
    },
    {
      "epoch": 8.030495552731892,
      "grad_norm": 3.8739027976989746,
      "learning_rate": 2e-05,
      "loss": 0.141,
      "step": 25280
    },
    {
      "epoch": 8.033672172808132,
      "grad_norm": 4.540639877319336,
      "learning_rate": 2e-05,
      "loss": 0.1339,
      "step": 25290
    },
    {
      "epoch": 8.03684879288437,
      "grad_norm": 4.277133464813232,
      "learning_rate": 2e-05,
      "loss": 0.1379,
      "step": 25300
    },
    {
      "epoch": 8.04002541296061,
      "grad_norm": 4.447510242462158,
      "learning_rate": 2e-05,
      "loss": 0.132,
      "step": 25310
    },
    {
      "epoch": 8.043202033036849,
      "grad_norm": 5.461979389190674,
      "learning_rate": 2e-05,
      "loss": 0.1348,
      "step": 25320
    },
    {
      "epoch": 8.046378653113088,
      "grad_norm": 4.027660846710205,
      "learning_rate": 2e-05,
      "loss": 0.1389,
      "step": 25330
    },
    {
      "epoch": 8.049555273189327,
      "grad_norm": 5.008690357208252,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 25340
    },
    {
      "epoch": 8.052731893265566,
      "grad_norm": 3.607225179672241,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 25350
    },
    {
      "epoch": 8.055908513341805,
      "grad_norm": 2.851607084274292,
      "learning_rate": 2e-05,
      "loss": 0.148,
      "step": 25360
    },
    {
      "epoch": 8.059085133418042,
      "grad_norm": 2.784715414047241,
      "learning_rate": 2e-05,
      "loss": 0.1516,
      "step": 25370
    },
    {
      "epoch": 8.062261753494282,
      "grad_norm": 3.9913129806518555,
      "learning_rate": 2e-05,
      "loss": 0.1481,
      "step": 25380
    },
    {
      "epoch": 8.06543837357052,
      "grad_norm": 4.180010795593262,
      "learning_rate": 2e-05,
      "loss": 0.137,
      "step": 25390
    },
    {
      "epoch": 8.06861499364676,
      "grad_norm": 8.47726821899414,
      "learning_rate": 2e-05,
      "loss": 0.1349,
      "step": 25400
    },
    {
      "epoch": 8.071791613722999,
      "grad_norm": 3.526223659515381,
      "learning_rate": 2e-05,
      "loss": 0.141,
      "step": 25410
    },
    {
      "epoch": 8.074968233799238,
      "grad_norm": 6.294383525848389,
      "learning_rate": 2e-05,
      "loss": 0.1453,
      "step": 25420
    },
    {
      "epoch": 8.078144853875477,
      "grad_norm": 5.815690040588379,
      "learning_rate": 2e-05,
      "loss": 0.1407,
      "step": 25430
    },
    {
      "epoch": 8.081321473951716,
      "grad_norm": 2.733384847640991,
      "learning_rate": 2e-05,
      "loss": 0.1402,
      "step": 25440
    },
    {
      "epoch": 8.084498094027953,
      "grad_norm": 3.4618167877197266,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 25450
    },
    {
      "epoch": 8.087674714104192,
      "grad_norm": 3.08961820602417,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 25460
    },
    {
      "epoch": 8.090851334180432,
      "grad_norm": 3.59371280670166,
      "learning_rate": 2e-05,
      "loss": 0.1431,
      "step": 25470
    },
    {
      "epoch": 8.09402795425667,
      "grad_norm": 3.786585807800293,
      "learning_rate": 2e-05,
      "loss": 0.1279,
      "step": 25480
    },
    {
      "epoch": 8.09720457433291,
      "grad_norm": 6.492402076721191,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 25490
    },
    {
      "epoch": 8.100381194409149,
      "grad_norm": 5.4502410888671875,
      "learning_rate": 2e-05,
      "loss": 0.1431,
      "step": 25500
    },
    {
      "epoch": 8.103557814485388,
      "grad_norm": 3.7291386127471924,
      "learning_rate": 2e-05,
      "loss": 0.1416,
      "step": 25510
    },
    {
      "epoch": 8.105146124523507,
      "eval_loss": 1.7943134307861328,
      "eval_mse": 1.7926255056693292,
      "eval_pearson": 0.37717816591912623,
      "eval_runtime": 7.4936,
      "eval_samples_per_second": 2877.125,
      "eval_spearmanr": 0.37874798883054633,
      "eval_steps_per_second": 11.343,
      "step": 25515
    },
    {
      "epoch": 8.106734434561627,
      "grad_norm": 2.8412485122680664,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 25520
    },
    {
      "epoch": 8.109911054637866,
      "grad_norm": 3.162461280822754,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 25530
    },
    {
      "epoch": 8.113087674714103,
      "grad_norm": 3.322720527648926,
      "learning_rate": 2e-05,
      "loss": 0.1294,
      "step": 25540
    },
    {
      "epoch": 8.116264294790343,
      "grad_norm": 3.506556987762451,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 25550
    },
    {
      "epoch": 8.119440914866582,
      "grad_norm": 3.709239959716797,
      "learning_rate": 2e-05,
      "loss": 0.1307,
      "step": 25560
    },
    {
      "epoch": 8.12261753494282,
      "grad_norm": 2.9822847843170166,
      "learning_rate": 2e-05,
      "loss": 0.1406,
      "step": 25570
    },
    {
      "epoch": 8.12579415501906,
      "grad_norm": 4.064267158508301,
      "learning_rate": 2e-05,
      "loss": 0.1337,
      "step": 25580
    },
    {
      "epoch": 8.128970775095299,
      "grad_norm": 3.2542240619659424,
      "learning_rate": 2e-05,
      "loss": 0.1297,
      "step": 25590
    },
    {
      "epoch": 8.132147395171538,
      "grad_norm": 3.3865578174591064,
      "learning_rate": 2e-05,
      "loss": 0.1319,
      "step": 25600
    },
    {
      "epoch": 8.135324015247777,
      "grad_norm": 2.3901867866516113,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 25610
    },
    {
      "epoch": 8.138500635324014,
      "grad_norm": 3.0447444915771484,
      "learning_rate": 2e-05,
      "loss": 0.1461,
      "step": 25620
    },
    {
      "epoch": 8.141677255400253,
      "grad_norm": 5.185070514678955,
      "learning_rate": 2e-05,
      "loss": 0.1462,
      "step": 25630
    },
    {
      "epoch": 8.144853875476493,
      "grad_norm": 3.645216226577759,
      "learning_rate": 2e-05,
      "loss": 0.1279,
      "step": 25640
    },
    {
      "epoch": 8.148030495552732,
      "grad_norm": 3.1119508743286133,
      "learning_rate": 2e-05,
      "loss": 0.1361,
      "step": 25650
    },
    {
      "epoch": 8.15120711562897,
      "grad_norm": 3.058607578277588,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 25660
    },
    {
      "epoch": 8.15438373570521,
      "grad_norm": 3.541693925857544,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 25670
    },
    {
      "epoch": 8.157560355781449,
      "grad_norm": 5.194299697875977,
      "learning_rate": 2e-05,
      "loss": 0.1382,
      "step": 25680
    },
    {
      "epoch": 8.160736975857688,
      "grad_norm": 3.416048526763916,
      "learning_rate": 2e-05,
      "loss": 0.1353,
      "step": 25690
    },
    {
      "epoch": 8.163913595933927,
      "grad_norm": 3.608314275741577,
      "learning_rate": 2e-05,
      "loss": 0.1306,
      "step": 25700
    },
    {
      "epoch": 8.167090216010164,
      "grad_norm": 3.743650197982788,
      "learning_rate": 2e-05,
      "loss": 0.1459,
      "step": 25710
    },
    {
      "epoch": 8.170266836086403,
      "grad_norm": 6.545010089874268,
      "learning_rate": 2e-05,
      "loss": 0.1351,
      "step": 25720
    },
    {
      "epoch": 8.173443456162643,
      "grad_norm": 12.524633407592773,
      "learning_rate": 2e-05,
      "loss": 0.1452,
      "step": 25730
    },
    {
      "epoch": 8.176620076238882,
      "grad_norm": 4.381573677062988,
      "learning_rate": 2e-05,
      "loss": 0.1351,
      "step": 25740
    },
    {
      "epoch": 8.17979669631512,
      "grad_norm": 2.947122573852539,
      "learning_rate": 2e-05,
      "loss": 0.14,
      "step": 25750
    },
    {
      "epoch": 8.18297331639136,
      "grad_norm": 2.670508623123169,
      "learning_rate": 2e-05,
      "loss": 0.1338,
      "step": 25760
    },
    {
      "epoch": 8.186149936467599,
      "grad_norm": 3.7799224853515625,
      "learning_rate": 2e-05,
      "loss": 0.1294,
      "step": 25770
    },
    {
      "epoch": 8.189326556543838,
      "grad_norm": 14.294888496398926,
      "learning_rate": 2e-05,
      "loss": 0.1275,
      "step": 25780
    },
    {
      "epoch": 8.192503176620077,
      "grad_norm": 3.8589906692504883,
      "learning_rate": 2e-05,
      "loss": 0.139,
      "step": 25790
    },
    {
      "epoch": 8.195679796696314,
      "grad_norm": 3.2557244300842285,
      "learning_rate": 2e-05,
      "loss": 0.1432,
      "step": 25800
    },
    {
      "epoch": 8.198856416772554,
      "grad_norm": 4.846573352813721,
      "learning_rate": 2e-05,
      "loss": 0.1491,
      "step": 25810
    },
    {
      "epoch": 8.202033036848793,
      "grad_norm": 5.2242536544799805,
      "learning_rate": 2e-05,
      "loss": 0.1435,
      "step": 25820
    },
    {
      "epoch": 8.205209656925032,
      "grad_norm": 5.027510166168213,
      "learning_rate": 2e-05,
      "loss": 0.1348,
      "step": 25830
    },
    {
      "epoch": 8.205209656925032,
      "eval_loss": 1.826300859451294,
      "eval_mse": 1.8248330622254862,
      "eval_pearson": 0.38958280944100954,
      "eval_runtime": 7.5059,
      "eval_samples_per_second": 2872.408,
      "eval_spearmanr": 0.3978983059132454,
      "eval_steps_per_second": 11.324,
      "step": 25830
    },
    {
      "epoch": 8.20838627700127,
      "grad_norm": 4.627904891967773,
      "learning_rate": 2e-05,
      "loss": 0.128,
      "step": 25840
    },
    {
      "epoch": 8.21156289707751,
      "grad_norm": 3.0949208736419678,
      "learning_rate": 2e-05,
      "loss": 0.1449,
      "step": 25850
    },
    {
      "epoch": 8.214739517153749,
      "grad_norm": 5.117992877960205,
      "learning_rate": 2e-05,
      "loss": 0.1476,
      "step": 25860
    },
    {
      "epoch": 8.217916137229988,
      "grad_norm": 3.6005184650421143,
      "learning_rate": 2e-05,
      "loss": 0.1296,
      "step": 25870
    },
    {
      "epoch": 8.221092757306225,
      "grad_norm": 4.772389888763428,
      "learning_rate": 2e-05,
      "loss": 0.1414,
      "step": 25880
    },
    {
      "epoch": 8.224269377382464,
      "grad_norm": 6.911815166473389,
      "learning_rate": 2e-05,
      "loss": 0.1498,
      "step": 25890
    },
    {
      "epoch": 8.227445997458704,
      "grad_norm": 3.617662191390991,
      "learning_rate": 2e-05,
      "loss": 0.1314,
      "step": 25900
    },
    {
      "epoch": 8.230622617534943,
      "grad_norm": 3.1221923828125,
      "learning_rate": 2e-05,
      "loss": 0.1408,
      "step": 25910
    },
    {
      "epoch": 8.233799237611182,
      "grad_norm": 3.415416955947876,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 25920
    },
    {
      "epoch": 8.23697585768742,
      "grad_norm": 3.9567291736602783,
      "learning_rate": 2e-05,
      "loss": 0.1379,
      "step": 25930
    },
    {
      "epoch": 8.24015247776366,
      "grad_norm": 2.2988712787628174,
      "learning_rate": 2e-05,
      "loss": 0.1294,
      "step": 25940
    },
    {
      "epoch": 8.243329097839899,
      "grad_norm": 3.9005677700042725,
      "learning_rate": 2e-05,
      "loss": 0.1312,
      "step": 25950
    },
    {
      "epoch": 8.246505717916138,
      "grad_norm": 8.389444351196289,
      "learning_rate": 2e-05,
      "loss": 0.1371,
      "step": 25960
    },
    {
      "epoch": 8.249682337992375,
      "grad_norm": 2.8772153854370117,
      "learning_rate": 2e-05,
      "loss": 0.136,
      "step": 25970
    },
    {
      "epoch": 8.252858958068614,
      "grad_norm": 3.313225507736206,
      "learning_rate": 2e-05,
      "loss": 0.145,
      "step": 25980
    },
    {
      "epoch": 8.256035578144854,
      "grad_norm": 5.821633815765381,
      "learning_rate": 2e-05,
      "loss": 0.1413,
      "step": 25990
    },
    {
      "epoch": 8.259212198221093,
      "grad_norm": 3.606870412826538,
      "learning_rate": 2e-05,
      "loss": 0.1467,
      "step": 26000
    },
    {
      "epoch": 8.262388818297332,
      "grad_norm": 3.7268929481506348,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 26010
    },
    {
      "epoch": 8.26556543837357,
      "grad_norm": 2.318023681640625,
      "learning_rate": 2e-05,
      "loss": 0.1381,
      "step": 26020
    },
    {
      "epoch": 8.26874205844981,
      "grad_norm": 3.2914679050445557,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 26030
    },
    {
      "epoch": 8.271918678526049,
      "grad_norm": 5.702235698699951,
      "learning_rate": 2e-05,
      "loss": 0.1494,
      "step": 26040
    },
    {
      "epoch": 8.275095298602286,
      "grad_norm": 3.2708287239074707,
      "learning_rate": 2e-05,
      "loss": 0.1512,
      "step": 26050
    },
    {
      "epoch": 8.278271918678525,
      "grad_norm": 3.270782947540283,
      "learning_rate": 2e-05,
      "loss": 0.1458,
      "step": 26060
    },
    {
      "epoch": 8.281448538754764,
      "grad_norm": 3.155733346939087,
      "learning_rate": 2e-05,
      "loss": 0.1402,
      "step": 26070
    },
    {
      "epoch": 8.284625158831004,
      "grad_norm": 3.2034735679626465,
      "learning_rate": 2e-05,
      "loss": 0.1486,
      "step": 26080
    },
    {
      "epoch": 8.287801778907243,
      "grad_norm": 3.1324751377105713,
      "learning_rate": 2e-05,
      "loss": 0.134,
      "step": 26090
    },
    {
      "epoch": 8.290978398983482,
      "grad_norm": 2.5107648372650146,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 26100
    },
    {
      "epoch": 8.29415501905972,
      "grad_norm": 3.09240984916687,
      "learning_rate": 2e-05,
      "loss": 0.1334,
      "step": 26110
    },
    {
      "epoch": 8.29733163913596,
      "grad_norm": 5.497359752655029,
      "learning_rate": 2e-05,
      "loss": 0.1317,
      "step": 26120
    },
    {
      "epoch": 8.300508259212199,
      "grad_norm": 2.9688756465911865,
      "learning_rate": 2e-05,
      "loss": 0.1346,
      "step": 26130
    },
    {
      "epoch": 8.303684879288436,
      "grad_norm": 3.1936604976654053,
      "learning_rate": 2e-05,
      "loss": 0.1396,
      "step": 26140
    },
    {
      "epoch": 8.305273189326556,
      "eval_loss": 1.928543210029602,
      "eval_mse": 1.9273270389421089,
      "eval_pearson": 0.38743604423830447,
      "eval_runtime": 7.6001,
      "eval_samples_per_second": 2836.796,
      "eval_spearmanr": 0.3924944978040429,
      "eval_steps_per_second": 11.184,
      "step": 26145
    },
    {
      "epoch": 8.306861499364675,
      "grad_norm": 3.886690139770508,
      "learning_rate": 2e-05,
      "loss": 0.1366,
      "step": 26150
    },
    {
      "epoch": 8.310038119440915,
      "grad_norm": 3.726243257522583,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 26160
    },
    {
      "epoch": 8.313214739517154,
      "grad_norm": 3.1083884239196777,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 26170
    },
    {
      "epoch": 8.316391359593393,
      "grad_norm": 3.0364620685577393,
      "learning_rate": 2e-05,
      "loss": 0.1424,
      "step": 26180
    },
    {
      "epoch": 8.319567979669632,
      "grad_norm": 5.3597002029418945,
      "learning_rate": 2e-05,
      "loss": 0.1364,
      "step": 26190
    },
    {
      "epoch": 8.32274459974587,
      "grad_norm": 3.266568183898926,
      "learning_rate": 2e-05,
      "loss": 0.1394,
      "step": 26200
    },
    {
      "epoch": 8.32592121982211,
      "grad_norm": 3.797407865524292,
      "learning_rate": 2e-05,
      "loss": 0.1281,
      "step": 26210
    },
    {
      "epoch": 8.329097839898349,
      "grad_norm": 3.5138652324676514,
      "learning_rate": 2e-05,
      "loss": 0.1299,
      "step": 26220
    },
    {
      "epoch": 8.332274459974586,
      "grad_norm": 2.8982763290405273,
      "learning_rate": 2e-05,
      "loss": 0.1311,
      "step": 26230
    },
    {
      "epoch": 8.335451080050825,
      "grad_norm": 3.435465097427368,
      "learning_rate": 2e-05,
      "loss": 0.1284,
      "step": 26240
    },
    {
      "epoch": 8.338627700127065,
      "grad_norm": 5.117199420928955,
      "learning_rate": 2e-05,
      "loss": 0.1524,
      "step": 26250
    },
    {
      "epoch": 8.341804320203304,
      "grad_norm": 4.4460906982421875,
      "learning_rate": 2e-05,
      "loss": 0.1383,
      "step": 26260
    },
    {
      "epoch": 8.344980940279543,
      "grad_norm": 6.58304500579834,
      "learning_rate": 2e-05,
      "loss": 0.1421,
      "step": 26270
    },
    {
      "epoch": 8.348157560355782,
      "grad_norm": 11.665566444396973,
      "learning_rate": 2e-05,
      "loss": 0.1443,
      "step": 26280
    },
    {
      "epoch": 8.351334180432021,
      "grad_norm": 3.003059148788452,
      "learning_rate": 2e-05,
      "loss": 0.1399,
      "step": 26290
    },
    {
      "epoch": 8.35451080050826,
      "grad_norm": 8.395699501037598,
      "learning_rate": 2e-05,
      "loss": 0.1419,
      "step": 26300
    },
    {
      "epoch": 8.357687420584497,
      "grad_norm": 3.4419219493865967,
      "learning_rate": 2e-05,
      "loss": 0.1448,
      "step": 26310
    },
    {
      "epoch": 8.360864040660736,
      "grad_norm": 3.476491928100586,
      "learning_rate": 2e-05,
      "loss": 0.1397,
      "step": 26320
    },
    {
      "epoch": 8.364040660736975,
      "grad_norm": 4.439158916473389,
      "learning_rate": 2e-05,
      "loss": 0.149,
      "step": 26330
    },
    {
      "epoch": 8.367217280813215,
      "grad_norm": 3.296860456466675,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 26340
    },
    {
      "epoch": 8.370393900889454,
      "grad_norm": 2.7490739822387695,
      "learning_rate": 2e-05,
      "loss": 0.1371,
      "step": 26350
    },
    {
      "epoch": 8.373570520965693,
      "grad_norm": 3.2522549629211426,
      "learning_rate": 2e-05,
      "loss": 0.1302,
      "step": 26360
    },
    {
      "epoch": 8.376747141041932,
      "grad_norm": 3.063429594039917,
      "learning_rate": 2e-05,
      "loss": 0.1392,
      "step": 26370
    },
    {
      "epoch": 8.379923761118171,
      "grad_norm": 4.417738437652588,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 26380
    },
    {
      "epoch": 8.38310038119441,
      "grad_norm": 2.707456350326538,
      "learning_rate": 2e-05,
      "loss": 0.137,
      "step": 26390
    },
    {
      "epoch": 8.386277001270647,
      "grad_norm": 6.507127285003662,
      "learning_rate": 2e-05,
      "loss": 0.1279,
      "step": 26400
    },
    {
      "epoch": 8.389453621346886,
      "grad_norm": 2.929295778274536,
      "learning_rate": 2e-05,
      "loss": 0.1334,
      "step": 26410
    },
    {
      "epoch": 8.392630241423126,
      "grad_norm": 3.169367551803589,
      "learning_rate": 2e-05,
      "loss": 0.143,
      "step": 26420
    },
    {
      "epoch": 8.395806861499365,
      "grad_norm": 2.453153371810913,
      "learning_rate": 2e-05,
      "loss": 0.1281,
      "step": 26430
    },
    {
      "epoch": 8.398983481575604,
      "grad_norm": 3.346538782119751,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 26440
    },
    {
      "epoch": 8.402160101651843,
      "grad_norm": 3.6261448860168457,
      "learning_rate": 2e-05,
      "loss": 0.1411,
      "step": 26450
    },
    {
      "epoch": 8.405336721728082,
      "grad_norm": 4.042938232421875,
      "learning_rate": 2e-05,
      "loss": 0.1398,
      "step": 26460
    },
    {
      "epoch": 8.405336721728082,
      "eval_loss": 1.8959770202636719,
      "eval_mse": 1.894364105012952,
      "eval_pearson": 0.37883810710856114,
      "eval_runtime": 7.3947,
      "eval_samples_per_second": 2915.611,
      "eval_spearmanr": 0.3851206444607828,
      "eval_steps_per_second": 11.495,
      "step": 26460
    },
    {
      "epoch": 8.408513341804321,
      "grad_norm": 3.100308418273926,
      "learning_rate": 2e-05,
      "loss": 0.1333,
      "step": 26470
    },
    {
      "epoch": 8.411689961880558,
      "grad_norm": 3.5970873832702637,
      "learning_rate": 2e-05,
      "loss": 0.1373,
      "step": 26480
    },
    {
      "epoch": 8.414866581956797,
      "grad_norm": 5.674417972564697,
      "learning_rate": 2e-05,
      "loss": 0.1416,
      "step": 26490
    },
    {
      "epoch": 8.418043202033036,
      "grad_norm": 4.4143290519714355,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 26500
    },
    {
      "epoch": 8.421219822109276,
      "grad_norm": 3.3825297355651855,
      "learning_rate": 2e-05,
      "loss": 0.1463,
      "step": 26510
    },
    {
      "epoch": 8.424396442185515,
      "grad_norm": 3.293137788772583,
      "learning_rate": 2e-05,
      "loss": 0.1396,
      "step": 26520
    },
    {
      "epoch": 8.427573062261754,
      "grad_norm": 3.375908374786377,
      "learning_rate": 2e-05,
      "loss": 0.1377,
      "step": 26530
    },
    {
      "epoch": 8.430749682337993,
      "grad_norm": 3.645695447921753,
      "learning_rate": 2e-05,
      "loss": 0.1441,
      "step": 26540
    },
    {
      "epoch": 8.433926302414232,
      "grad_norm": 3.7923214435577393,
      "learning_rate": 2e-05,
      "loss": 0.1382,
      "step": 26550
    },
    {
      "epoch": 8.437102922490471,
      "grad_norm": 5.81501579284668,
      "learning_rate": 2e-05,
      "loss": 0.1332,
      "step": 26560
    },
    {
      "epoch": 8.440279542566708,
      "grad_norm": 5.771376132965088,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 26570
    },
    {
      "epoch": 8.443456162642947,
      "grad_norm": 2.7402498722076416,
      "learning_rate": 2e-05,
      "loss": 0.1396,
      "step": 26580
    },
    {
      "epoch": 8.446632782719186,
      "grad_norm": 3.2687008380889893,
      "learning_rate": 2e-05,
      "loss": 0.1337,
      "step": 26590
    },
    {
      "epoch": 8.449809402795426,
      "grad_norm": 4.515091896057129,
      "learning_rate": 2e-05,
      "loss": 0.1303,
      "step": 26600
    },
    {
      "epoch": 8.452986022871665,
      "grad_norm": 4.1135172843933105,
      "learning_rate": 2e-05,
      "loss": 0.1299,
      "step": 26610
    },
    {
      "epoch": 8.456162642947904,
      "grad_norm": 4.347967147827148,
      "learning_rate": 2e-05,
      "loss": 0.1404,
      "step": 26620
    },
    {
      "epoch": 8.459339263024143,
      "grad_norm": 2.4685745239257812,
      "learning_rate": 2e-05,
      "loss": 0.1368,
      "step": 26630
    },
    {
      "epoch": 8.462515883100382,
      "grad_norm": 3.1556239128112793,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 26640
    },
    {
      "epoch": 8.46569250317662,
      "grad_norm": 4.205617904663086,
      "learning_rate": 2e-05,
      "loss": 0.1377,
      "step": 26650
    },
    {
      "epoch": 8.468869123252858,
      "grad_norm": 3.5547797679901123,
      "learning_rate": 2e-05,
      "loss": 0.138,
      "step": 26660
    },
    {
      "epoch": 8.472045743329097,
      "grad_norm": 2.5972795486450195,
      "learning_rate": 2e-05,
      "loss": 0.1426,
      "step": 26670
    },
    {
      "epoch": 8.475222363405337,
      "grad_norm": 4.137385368347168,
      "learning_rate": 2e-05,
      "loss": 0.1407,
      "step": 26680
    },
    {
      "epoch": 8.478398983481576,
      "grad_norm": 3.7551119327545166,
      "learning_rate": 2e-05,
      "loss": 0.1398,
      "step": 26690
    },
    {
      "epoch": 8.481575603557815,
      "grad_norm": 3.4358248710632324,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 26700
    },
    {
      "epoch": 8.484752223634054,
      "grad_norm": 3.417118787765503,
      "learning_rate": 2e-05,
      "loss": 0.1385,
      "step": 26710
    },
    {
      "epoch": 8.487928843710293,
      "grad_norm": 3.5816736221313477,
      "learning_rate": 2e-05,
      "loss": 0.1413,
      "step": 26720
    },
    {
      "epoch": 8.491105463786532,
      "grad_norm": 2.3701634407043457,
      "learning_rate": 2e-05,
      "loss": 0.1342,
      "step": 26730
    },
    {
      "epoch": 8.49428208386277,
      "grad_norm": 3.439434051513672,
      "learning_rate": 2e-05,
      "loss": 0.1283,
      "step": 26740
    },
    {
      "epoch": 8.497458703939008,
      "grad_norm": 3.30844783782959,
      "learning_rate": 2e-05,
      "loss": 0.1329,
      "step": 26750
    },
    {
      "epoch": 8.500635324015247,
      "grad_norm": 3.410860538482666,
      "learning_rate": 2e-05,
      "loss": 0.1389,
      "step": 26760
    },
    {
      "epoch": 8.503811944091487,
      "grad_norm": 3.523902416229248,
      "learning_rate": 2e-05,
      "loss": 0.138,
      "step": 26770
    },
    {
      "epoch": 8.505400254129606,
      "eval_loss": 1.8417330980300903,
      "eval_mse": 1.8403795154174316,
      "eval_pearson": 0.4080096613719415,
      "eval_runtime": 7.3768,
      "eval_samples_per_second": 2922.666,
      "eval_spearmanr": 0.4131115915039917,
      "eval_steps_per_second": 11.523,
      "step": 26775
    },
    {
      "epoch": 8.506988564167726,
      "grad_norm": 4.181560039520264,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 26780
    },
    {
      "epoch": 8.510165184243965,
      "grad_norm": 5.3031697273254395,
      "learning_rate": 2e-05,
      "loss": 0.1326,
      "step": 26790
    },
    {
      "epoch": 8.513341804320204,
      "grad_norm": 3.810533285140991,
      "learning_rate": 2e-05,
      "loss": 0.1418,
      "step": 26800
    },
    {
      "epoch": 8.516518424396443,
      "grad_norm": 3.3067572116851807,
      "learning_rate": 2e-05,
      "loss": 0.1298,
      "step": 26810
    },
    {
      "epoch": 8.51969504447268,
      "grad_norm": 3.6554815769195557,
      "learning_rate": 2e-05,
      "loss": 0.1287,
      "step": 26820
    },
    {
      "epoch": 8.52287166454892,
      "grad_norm": 4.593667507171631,
      "learning_rate": 2e-05,
      "loss": 0.1383,
      "step": 26830
    },
    {
      "epoch": 8.526048284625158,
      "grad_norm": 4.094736099243164,
      "learning_rate": 2e-05,
      "loss": 0.1274,
      "step": 26840
    },
    {
      "epoch": 8.529224904701397,
      "grad_norm": 2.9649717807769775,
      "learning_rate": 2e-05,
      "loss": 0.135,
      "step": 26850
    },
    {
      "epoch": 8.532401524777637,
      "grad_norm": 5.227104187011719,
      "learning_rate": 2e-05,
      "loss": 0.1351,
      "step": 26860
    },
    {
      "epoch": 8.535578144853876,
      "grad_norm": 3.5887160301208496,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 26870
    },
    {
      "epoch": 8.538754764930115,
      "grad_norm": 3.5218873023986816,
      "learning_rate": 2e-05,
      "loss": 0.1421,
      "step": 26880
    },
    {
      "epoch": 8.541931385006354,
      "grad_norm": 3.708498477935791,
      "learning_rate": 2e-05,
      "loss": 0.124,
      "step": 26890
    },
    {
      "epoch": 8.545108005082593,
      "grad_norm": 3.0860495567321777,
      "learning_rate": 2e-05,
      "loss": 0.1389,
      "step": 26900
    },
    {
      "epoch": 8.54828462515883,
      "grad_norm": 3.7542481422424316,
      "learning_rate": 2e-05,
      "loss": 0.1385,
      "step": 26910
    },
    {
      "epoch": 8.55146124523507,
      "grad_norm": 3.5894455909729004,
      "learning_rate": 2e-05,
      "loss": 0.1361,
      "step": 26920
    },
    {
      "epoch": 8.554637865311308,
      "grad_norm": 4.114934921264648,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 26930
    },
    {
      "epoch": 8.557814485387548,
      "grad_norm": 3.6384758949279785,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 26940
    },
    {
      "epoch": 8.560991105463787,
      "grad_norm": 3.0341408252716064,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 26950
    },
    {
      "epoch": 8.564167725540026,
      "grad_norm": 4.410109519958496,
      "learning_rate": 2e-05,
      "loss": 0.138,
      "step": 26960
    },
    {
      "epoch": 8.567344345616265,
      "grad_norm": 3.1289212703704834,
      "learning_rate": 2e-05,
      "loss": 0.1261,
      "step": 26970
    },
    {
      "epoch": 8.570520965692504,
      "grad_norm": 2.904038190841675,
      "learning_rate": 2e-05,
      "loss": 0.1352,
      "step": 26980
    },
    {
      "epoch": 8.573697585768741,
      "grad_norm": 3.6236073970794678,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 26990
    },
    {
      "epoch": 8.57687420584498,
      "grad_norm": 2.417106866836548,
      "learning_rate": 2e-05,
      "loss": 0.1464,
      "step": 27000
    },
    {
      "epoch": 8.58005082592122,
      "grad_norm": 4.460938453674316,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 27010
    },
    {
      "epoch": 8.583227445997458,
      "grad_norm": 3.2866322994232178,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 27020
    },
    {
      "epoch": 8.586404066073698,
      "grad_norm": 2.8454339504241943,
      "learning_rate": 2e-05,
      "loss": 0.1427,
      "step": 27030
    },
    {
      "epoch": 8.589580686149937,
      "grad_norm": 3.577747106552124,
      "learning_rate": 2e-05,
      "loss": 0.1347,
      "step": 27040
    },
    {
      "epoch": 8.592757306226176,
      "grad_norm": 4.869448661804199,
      "learning_rate": 2e-05,
      "loss": 0.1292,
      "step": 27050
    },
    {
      "epoch": 8.595933926302415,
      "grad_norm": 3.7797935009002686,
      "learning_rate": 2e-05,
      "loss": 0.1365,
      "step": 27060
    },
    {
      "epoch": 8.599110546378654,
      "grad_norm": 3.469980001449585,
      "learning_rate": 2e-05,
      "loss": 0.1438,
      "step": 27070
    },
    {
      "epoch": 8.602287166454891,
      "grad_norm": 3.9136312007904053,
      "learning_rate": 2e-05,
      "loss": 0.1271,
      "step": 27080
    },
    {
      "epoch": 8.60546378653113,
      "grad_norm": 2.8773601055145264,
      "learning_rate": 2e-05,
      "loss": 0.1301,
      "step": 27090
    },
    {
      "epoch": 8.60546378653113,
      "eval_loss": 1.8189738988876343,
      "eval_mse": 1.817407120910779,
      "eval_pearson": 0.3994891596251068,
      "eval_runtime": 7.3936,
      "eval_samples_per_second": 2916.04,
      "eval_spearmanr": 0.4013638216254964,
      "eval_steps_per_second": 11.496,
      "step": 27090
    },
    {
      "epoch": 8.60864040660737,
      "grad_norm": 3.43916654586792,
      "learning_rate": 2e-05,
      "loss": 0.1322,
      "step": 27100
    },
    {
      "epoch": 8.611817026683608,
      "grad_norm": 5.150484085083008,
      "learning_rate": 2e-05,
      "loss": 0.146,
      "step": 27110
    },
    {
      "epoch": 8.614993646759848,
      "grad_norm": 4.263176441192627,
      "learning_rate": 2e-05,
      "loss": 0.154,
      "step": 27120
    },
    {
      "epoch": 8.618170266836087,
      "grad_norm": 4.090502738952637,
      "learning_rate": 2e-05,
      "loss": 0.1427,
      "step": 27130
    },
    {
      "epoch": 8.621346886912326,
      "grad_norm": 3.0882513523101807,
      "learning_rate": 2e-05,
      "loss": 0.1381,
      "step": 27140
    },
    {
      "epoch": 8.624523506988565,
      "grad_norm": 2.617746114730835,
      "learning_rate": 2e-05,
      "loss": 0.1409,
      "step": 27150
    },
    {
      "epoch": 8.627700127064802,
      "grad_norm": 3.486374616622925,
      "learning_rate": 2e-05,
      "loss": 0.1347,
      "step": 27160
    },
    {
      "epoch": 8.630876747141041,
      "grad_norm": 3.921658992767334,
      "learning_rate": 2e-05,
      "loss": 0.1457,
      "step": 27170
    },
    {
      "epoch": 8.63405336721728,
      "grad_norm": 4.024326801300049,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 27180
    },
    {
      "epoch": 8.63722998729352,
      "grad_norm": 5.011946678161621,
      "learning_rate": 2e-05,
      "loss": 0.1284,
      "step": 27190
    },
    {
      "epoch": 8.640406607369759,
      "grad_norm": 3.830181360244751,
      "learning_rate": 2e-05,
      "loss": 0.1401,
      "step": 27200
    },
    {
      "epoch": 8.643583227445998,
      "grad_norm": 3.101649045944214,
      "learning_rate": 2e-05,
      "loss": 0.1257,
      "step": 27210
    },
    {
      "epoch": 8.646759847522237,
      "grad_norm": 2.865161657333374,
      "learning_rate": 2e-05,
      "loss": 0.1371,
      "step": 27220
    },
    {
      "epoch": 8.649936467598476,
      "grad_norm": 2.623563528060913,
      "learning_rate": 2e-05,
      "loss": 0.1412,
      "step": 27230
    },
    {
      "epoch": 8.653113087674715,
      "grad_norm": 5.8855462074279785,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 27240
    },
    {
      "epoch": 8.656289707750952,
      "grad_norm": 4.818676948547363,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 27250
    },
    {
      "epoch": 8.659466327827191,
      "grad_norm": 4.3277716636657715,
      "learning_rate": 2e-05,
      "loss": 0.1533,
      "step": 27260
    },
    {
      "epoch": 8.66264294790343,
      "grad_norm": 5.085461139678955,
      "learning_rate": 2e-05,
      "loss": 0.1369,
      "step": 27270
    },
    {
      "epoch": 8.66581956797967,
      "grad_norm": 3.292194128036499,
      "learning_rate": 2e-05,
      "loss": 0.1384,
      "step": 27280
    },
    {
      "epoch": 8.668996188055909,
      "grad_norm": 5.920746803283691,
      "learning_rate": 2e-05,
      "loss": 0.1355,
      "step": 27290
    },
    {
      "epoch": 8.672172808132148,
      "grad_norm": 3.5044620037078857,
      "learning_rate": 2e-05,
      "loss": 0.1416,
      "step": 27300
    },
    {
      "epoch": 8.675349428208387,
      "grad_norm": 2.45908784866333,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 27310
    },
    {
      "epoch": 8.678526048284626,
      "grad_norm": 3.7048158645629883,
      "learning_rate": 2e-05,
      "loss": 0.1271,
      "step": 27320
    },
    {
      "epoch": 8.681702668360863,
      "grad_norm": 4.414817810058594,
      "learning_rate": 2e-05,
      "loss": 0.1357,
      "step": 27330
    },
    {
      "epoch": 8.684879288437102,
      "grad_norm": 2.665541410446167,
      "learning_rate": 2e-05,
      "loss": 0.1431,
      "step": 27340
    },
    {
      "epoch": 8.688055908513341,
      "grad_norm": 3.8475492000579834,
      "learning_rate": 2e-05,
      "loss": 0.129,
      "step": 27350
    },
    {
      "epoch": 8.69123252858958,
      "grad_norm": 4.502318859100342,
      "learning_rate": 2e-05,
      "loss": 0.1338,
      "step": 27360
    },
    {
      "epoch": 8.69440914866582,
      "grad_norm": 3.6785078048706055,
      "learning_rate": 2e-05,
      "loss": 0.1433,
      "step": 27370
    },
    {
      "epoch": 8.697585768742059,
      "grad_norm": 5.515091896057129,
      "learning_rate": 2e-05,
      "loss": 0.137,
      "step": 27380
    },
    {
      "epoch": 8.700762388818298,
      "grad_norm": 4.085609436035156,
      "learning_rate": 2e-05,
      "loss": 0.1382,
      "step": 27390
    },
    {
      "epoch": 8.703939008894537,
      "grad_norm": 3.7357635498046875,
      "learning_rate": 2e-05,
      "loss": 0.1396,
      "step": 27400
    },
    {
      "epoch": 8.705527318932656,
      "eval_loss": 1.735280156135559,
      "eval_mse": 1.7338796758813462,
      "eval_pearson": 0.41214363141534543,
      "eval_runtime": 7.2813,
      "eval_samples_per_second": 2961.015,
      "eval_spearmanr": 0.41589201446132,
      "eval_steps_per_second": 11.674,
      "step": 27405
    },
    {
      "epoch": 8.707115628970776,
      "grad_norm": 2.440887689590454,
      "learning_rate": 2e-05,
      "loss": 0.1321,
      "step": 27410
    },
    {
      "epoch": 8.710292249047013,
      "grad_norm": 3.57970929145813,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 27420
    },
    {
      "epoch": 8.713468869123252,
      "grad_norm": 4.250665187835693,
      "learning_rate": 2e-05,
      "loss": 0.1327,
      "step": 27430
    },
    {
      "epoch": 8.716645489199491,
      "grad_norm": 3.701235771179199,
      "learning_rate": 2e-05,
      "loss": 0.1401,
      "step": 27440
    },
    {
      "epoch": 8.71982210927573,
      "grad_norm": 3.327118158340454,
      "learning_rate": 2e-05,
      "loss": 0.1424,
      "step": 27450
    },
    {
      "epoch": 8.72299872935197,
      "grad_norm": 2.9980688095092773,
      "learning_rate": 2e-05,
      "loss": 0.1476,
      "step": 27460
    },
    {
      "epoch": 8.726175349428209,
      "grad_norm": 3.7759459018707275,
      "learning_rate": 2e-05,
      "loss": 0.1283,
      "step": 27470
    },
    {
      "epoch": 8.729351969504448,
      "grad_norm": 3.111557960510254,
      "learning_rate": 2e-05,
      "loss": 0.1254,
      "step": 27480
    },
    {
      "epoch": 8.732528589580687,
      "grad_norm": 4.747195720672607,
      "learning_rate": 2e-05,
      "loss": 0.1401,
      "step": 27490
    },
    {
      "epoch": 8.735705209656924,
      "grad_norm": 2.728926420211792,
      "learning_rate": 2e-05,
      "loss": 0.1334,
      "step": 27500
    },
    {
      "epoch": 8.738881829733163,
      "grad_norm": 4.389988422393799,
      "learning_rate": 2e-05,
      "loss": 0.1371,
      "step": 27510
    },
    {
      "epoch": 8.742058449809402,
      "grad_norm": 2.8541736602783203,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 27520
    },
    {
      "epoch": 8.745235069885641,
      "grad_norm": 4.804666996002197,
      "learning_rate": 2e-05,
      "loss": 0.1376,
      "step": 27530
    },
    {
      "epoch": 8.74841168996188,
      "grad_norm": 2.5099737644195557,
      "learning_rate": 2e-05,
      "loss": 0.1295,
      "step": 27540
    },
    {
      "epoch": 8.75158831003812,
      "grad_norm": 3.6375250816345215,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 27550
    },
    {
      "epoch": 8.754764930114359,
      "grad_norm": 3.527414083480835,
      "learning_rate": 2e-05,
      "loss": 0.1354,
      "step": 27560
    },
    {
      "epoch": 8.757941550190598,
      "grad_norm": 3.109802484512329,
      "learning_rate": 2e-05,
      "loss": 0.1444,
      "step": 27570
    },
    {
      "epoch": 8.761118170266837,
      "grad_norm": 4.127822399139404,
      "learning_rate": 2e-05,
      "loss": 0.1322,
      "step": 27580
    },
    {
      "epoch": 8.764294790343076,
      "grad_norm": 2.741145610809326,
      "learning_rate": 2e-05,
      "loss": 0.1202,
      "step": 27590
    },
    {
      "epoch": 8.767471410419313,
      "grad_norm": 3.500511646270752,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 27600
    },
    {
      "epoch": 8.770648030495552,
      "grad_norm": 3.0607128143310547,
      "learning_rate": 2e-05,
      "loss": 0.1314,
      "step": 27610
    },
    {
      "epoch": 8.773824650571791,
      "grad_norm": 2.758125066757202,
      "learning_rate": 2e-05,
      "loss": 0.136,
      "step": 27620
    },
    {
      "epoch": 8.77700127064803,
      "grad_norm": 4.942961692810059,
      "learning_rate": 2e-05,
      "loss": 0.1385,
      "step": 27630
    },
    {
      "epoch": 8.78017789072427,
      "grad_norm": 4.436404228210449,
      "learning_rate": 2e-05,
      "loss": 0.1391,
      "step": 27640
    },
    {
      "epoch": 8.783354510800509,
      "grad_norm": 3.882122039794922,
      "learning_rate": 2e-05,
      "loss": 0.146,
      "step": 27650
    },
    {
      "epoch": 8.786531130876748,
      "grad_norm": 3.1819541454315186,
      "learning_rate": 2e-05,
      "loss": 0.1324,
      "step": 27660
    },
    {
      "epoch": 8.789707750952987,
      "grad_norm": 3.1330337524414062,
      "learning_rate": 2e-05,
      "loss": 0.1346,
      "step": 27670
    },
    {
      "epoch": 8.792884371029224,
      "grad_norm": 3.782759666442871,
      "learning_rate": 2e-05,
      "loss": 0.1288,
      "step": 27680
    },
    {
      "epoch": 8.796060991105463,
      "grad_norm": 2.8772544860839844,
      "learning_rate": 2e-05,
      "loss": 0.1304,
      "step": 27690
    },
    {
      "epoch": 8.799237611181702,
      "grad_norm": 7.2665696144104,
      "learning_rate": 2e-05,
      "loss": 0.1358,
      "step": 27700
    },
    {
      "epoch": 8.802414231257941,
      "grad_norm": 3.132270574569702,
      "learning_rate": 2e-05,
      "loss": 0.1301,
      "step": 27710
    },
    {
      "epoch": 8.80559085133418,
      "grad_norm": 3.057830333709717,
      "learning_rate": 2e-05,
      "loss": 0.1425,
      "step": 27720
    },
    {
      "epoch": 8.80559085133418,
      "eval_loss": 1.8127304315567017,
      "eval_mse": 1.8119030590817085,
      "eval_pearson": 0.41040871817913244,
      "eval_runtime": 7.3164,
      "eval_samples_per_second": 2946.797,
      "eval_spearmanr": 0.4135012562213704,
      "eval_steps_per_second": 11.618,
      "step": 27720
    },
    {
      "epoch": 8.80876747141042,
      "grad_norm": 5.026275634765625,
      "learning_rate": 2e-05,
      "loss": 0.14,
      "step": 27730
    },
    {
      "epoch": 8.811944091486659,
      "grad_norm": 4.964007377624512,
      "learning_rate": 2e-05,
      "loss": 0.1409,
      "step": 27740
    },
    {
      "epoch": 8.815120711562898,
      "grad_norm": 3.0909810066223145,
      "learning_rate": 2e-05,
      "loss": 0.1443,
      "step": 27750
    },
    {
      "epoch": 8.818297331639137,
      "grad_norm": 2.9775149822235107,
      "learning_rate": 2e-05,
      "loss": 0.1323,
      "step": 27760
    },
    {
      "epoch": 8.821473951715374,
      "grad_norm": 7.669606685638428,
      "learning_rate": 2e-05,
      "loss": 0.1357,
      "step": 27770
    },
    {
      "epoch": 8.824650571791613,
      "grad_norm": 2.7711455821990967,
      "learning_rate": 2e-05,
      "loss": 0.1342,
      "step": 27780
    },
    {
      "epoch": 8.827827191867852,
      "grad_norm": 2.758192539215088,
      "learning_rate": 2e-05,
      "loss": 0.1342,
      "step": 27790
    },
    {
      "epoch": 8.831003811944091,
      "grad_norm": 2.2298178672790527,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 27800
    },
    {
      "epoch": 8.83418043202033,
      "grad_norm": 5.801419258117676,
      "learning_rate": 2e-05,
      "loss": 0.1314,
      "step": 27810
    },
    {
      "epoch": 8.83735705209657,
      "grad_norm": 3.8769428730010986,
      "learning_rate": 2e-05,
      "loss": 0.14,
      "step": 27820
    },
    {
      "epoch": 8.840533672172809,
      "grad_norm": 3.761423110961914,
      "learning_rate": 2e-05,
      "loss": 0.1336,
      "step": 27830
    },
    {
      "epoch": 8.843710292249048,
      "grad_norm": 4.124151706695557,
      "learning_rate": 2e-05,
      "loss": 0.1337,
      "step": 27840
    },
    {
      "epoch": 8.846886912325285,
      "grad_norm": 2.430978298187256,
      "learning_rate": 2e-05,
      "loss": 0.1258,
      "step": 27850
    },
    {
      "epoch": 8.850063532401524,
      "grad_norm": 3.3751697540283203,
      "learning_rate": 2e-05,
      "loss": 0.1344,
      "step": 27860
    },
    {
      "epoch": 8.853240152477763,
      "grad_norm": 2.528921604156494,
      "learning_rate": 2e-05,
      "loss": 0.1293,
      "step": 27870
    },
    {
      "epoch": 8.856416772554002,
      "grad_norm": 3.5944178104400635,
      "learning_rate": 2e-05,
      "loss": 0.1294,
      "step": 27880
    },
    {
      "epoch": 8.859593392630241,
      "grad_norm": 2.5292246341705322,
      "learning_rate": 2e-05,
      "loss": 0.1254,
      "step": 27890
    },
    {
      "epoch": 8.86277001270648,
      "grad_norm": 3.048676013946533,
      "learning_rate": 2e-05,
      "loss": 0.1261,
      "step": 27900
    },
    {
      "epoch": 8.86594663278272,
      "grad_norm": 3.329092264175415,
      "learning_rate": 2e-05,
      "loss": 0.1221,
      "step": 27910
    },
    {
      "epoch": 8.869123252858959,
      "grad_norm": 4.286048412322998,
      "learning_rate": 2e-05,
      "loss": 0.1326,
      "step": 27920
    },
    {
      "epoch": 8.872299872935198,
      "grad_norm": 3.4300217628479004,
      "learning_rate": 2e-05,
      "loss": 0.1526,
      "step": 27930
    },
    {
      "epoch": 8.875476493011435,
      "grad_norm": 3.1098780632019043,
      "learning_rate": 2e-05,
      "loss": 0.1388,
      "step": 27940
    },
    {
      "epoch": 8.878653113087674,
      "grad_norm": 3.8622429370880127,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 27950
    },
    {
      "epoch": 8.881829733163913,
      "grad_norm": 3.668198823928833,
      "learning_rate": 2e-05,
      "loss": 0.1427,
      "step": 27960
    },
    {
      "epoch": 8.885006353240152,
      "grad_norm": 2.829059362411499,
      "learning_rate": 2e-05,
      "loss": 0.1306,
      "step": 27970
    },
    {
      "epoch": 8.888182973316392,
      "grad_norm": 5.058202266693115,
      "learning_rate": 2e-05,
      "loss": 0.1372,
      "step": 27980
    },
    {
      "epoch": 8.89135959339263,
      "grad_norm": 3.806502103805542,
      "learning_rate": 2e-05,
      "loss": 0.141,
      "step": 27990
    },
    {
      "epoch": 8.89453621346887,
      "grad_norm": 2.5088298320770264,
      "learning_rate": 2e-05,
      "loss": 0.1457,
      "step": 28000
    },
    {
      "epoch": 8.897712833545109,
      "grad_norm": 9.493041038513184,
      "learning_rate": 2e-05,
      "loss": 0.1506,
      "step": 28010
    },
    {
      "epoch": 8.900889453621346,
      "grad_norm": 2.5856130123138428,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 28020
    },
    {
      "epoch": 8.904066073697585,
      "grad_norm": 7.585700988769531,
      "learning_rate": 2e-05,
      "loss": 0.1208,
      "step": 28030
    },
    {
      "epoch": 8.905654383735705,
      "eval_loss": 1.804433822631836,
      "eval_mse": 1.8027946250534606,
      "eval_pearson": 0.3865517328179988,
      "eval_runtime": 7.363,
      "eval_samples_per_second": 2928.148,
      "eval_spearmanr": 0.391394571149066,
      "eval_steps_per_second": 11.544,
      "step": 28035
    },
    {
      "epoch": 8.907242693773824,
      "grad_norm": 3.851971387863159,
      "learning_rate": 2e-05,
      "loss": 0.1341,
      "step": 28040
    },
    {
      "epoch": 8.910419313850063,
      "grad_norm": 2.8718461990356445,
      "learning_rate": 2e-05,
      "loss": 0.128,
      "step": 28050
    },
    {
      "epoch": 8.913595933926302,
      "grad_norm": 2.77468204498291,
      "learning_rate": 2e-05,
      "loss": 0.1361,
      "step": 28060
    },
    {
      "epoch": 8.916772554002542,
      "grad_norm": 3.3888120651245117,
      "learning_rate": 2e-05,
      "loss": 0.1191,
      "step": 28070
    },
    {
      "epoch": 8.91994917407878,
      "grad_norm": 2.7990171909332275,
      "learning_rate": 2e-05,
      "loss": 0.1326,
      "step": 28080
    },
    {
      "epoch": 8.92312579415502,
      "grad_norm": 4.2982072830200195,
      "learning_rate": 2e-05,
      "loss": 0.135,
      "step": 28090
    },
    {
      "epoch": 8.926302414231259,
      "grad_norm": 12.481427192687988,
      "learning_rate": 2e-05,
      "loss": 0.1331,
      "step": 28100
    },
    {
      "epoch": 8.929479034307496,
      "grad_norm": 3.5176005363464355,
      "learning_rate": 2e-05,
      "loss": 0.1373,
      "step": 28110
    },
    {
      "epoch": 8.932655654383735,
      "grad_norm": 2.767193078994751,
      "learning_rate": 2e-05,
      "loss": 0.1177,
      "step": 28120
    },
    {
      "epoch": 8.935832274459974,
      "grad_norm": 3.2806754112243652,
      "learning_rate": 2e-05,
      "loss": 0.1362,
      "step": 28130
    },
    {
      "epoch": 8.939008894536213,
      "grad_norm": 4.622621059417725,
      "learning_rate": 2e-05,
      "loss": 0.1368,
      "step": 28140
    },
    {
      "epoch": 8.942185514612452,
      "grad_norm": 4.471310138702393,
      "learning_rate": 2e-05,
      "loss": 0.1409,
      "step": 28150
    },
    {
      "epoch": 8.945362134688692,
      "grad_norm": 5.561882972717285,
      "learning_rate": 2e-05,
      "loss": 0.1402,
      "step": 28160
    },
    {
      "epoch": 8.94853875476493,
      "grad_norm": 3.075354814529419,
      "learning_rate": 2e-05,
      "loss": 0.1504,
      "step": 28170
    },
    {
      "epoch": 8.95171537484117,
      "grad_norm": 2.477797031402588,
      "learning_rate": 2e-05,
      "loss": 0.1309,
      "step": 28180
    },
    {
      "epoch": 8.954891994917407,
      "grad_norm": 3.602543592453003,
      "learning_rate": 2e-05,
      "loss": 0.1374,
      "step": 28190
    },
    {
      "epoch": 8.958068614993646,
      "grad_norm": 5.24180793762207,
      "learning_rate": 2e-05,
      "loss": 0.1201,
      "step": 28200
    },
    {
      "epoch": 8.961245235069885,
      "grad_norm": 3.6518397331237793,
      "learning_rate": 2e-05,
      "loss": 0.1316,
      "step": 28210
    },
    {
      "epoch": 8.964421855146124,
      "grad_norm": 4.587817192077637,
      "learning_rate": 2e-05,
      "loss": 0.1318,
      "step": 28220
    },
    {
      "epoch": 8.967598475222363,
      "grad_norm": 3.025054931640625,
      "learning_rate": 2e-05,
      "loss": 0.14,
      "step": 28230
    },
    {
      "epoch": 8.970775095298603,
      "grad_norm": 5.172028541564941,
      "learning_rate": 2e-05,
      "loss": 0.1428,
      "step": 28240
    },
    {
      "epoch": 8.973951715374842,
      "grad_norm": 4.986506938934326,
      "learning_rate": 2e-05,
      "loss": 0.1418,
      "step": 28250
    },
    {
      "epoch": 8.97712833545108,
      "grad_norm": 3.4202210903167725,
      "learning_rate": 2e-05,
      "loss": 0.1399,
      "step": 28260
    },
    {
      "epoch": 8.98030495552732,
      "grad_norm": 3.138536214828491,
      "learning_rate": 2e-05,
      "loss": 0.1305,
      "step": 28270
    },
    {
      "epoch": 8.983481575603557,
      "grad_norm": 3.7618157863616943,
      "learning_rate": 2e-05,
      "loss": 0.1285,
      "step": 28280
    },
    {
      "epoch": 8.986658195679796,
      "grad_norm": 4.652539253234863,
      "learning_rate": 2e-05,
      "loss": 0.134,
      "step": 28290
    },
    {
      "epoch": 8.989834815756035,
      "grad_norm": 3.550457239151001,
      "learning_rate": 2e-05,
      "loss": 0.1297,
      "step": 28300
    },
    {
      "epoch": 8.993011435832274,
      "grad_norm": 13.539017677307129,
      "learning_rate": 2e-05,
      "loss": 0.1428,
      "step": 28310
    },
    {
      "epoch": 8.996188055908513,
      "grad_norm": 4.65765380859375,
      "learning_rate": 2e-05,
      "loss": 0.1262,
      "step": 28320
    },
    {
      "epoch": 8.999364675984753,
      "grad_norm": 2.8099560737609863,
      "learning_rate": 2e-05,
      "loss": 0.1392,
      "step": 28330
    },
    {
      "epoch": 9.002541296060992,
      "grad_norm": 2.9118733406066895,
      "learning_rate": 2e-05,
      "loss": 0.1223,
      "step": 28340
    },
    {
      "epoch": 9.00571791613723,
      "grad_norm": 3.7535336017608643,
      "learning_rate": 2e-05,
      "loss": 0.129,
      "step": 28350
    },
    {
      "epoch": 9.00571791613723,
      "eval_loss": 1.8968837261199951,
      "eval_mse": 1.895686121474509,
      "eval_pearson": 0.3883925002313189,
      "eval_runtime": 7.2685,
      "eval_samples_per_second": 2966.221,
      "eval_spearmanr": 0.39537248551246473,
      "eval_steps_per_second": 11.694,
      "step": 28350
    },
    {
      "epoch": 9.008894536213468,
      "grad_norm": 3.20513916015625,
      "learning_rate": 2e-05,
      "loss": 0.1232,
      "step": 28360
    },
    {
      "epoch": 9.012071156289707,
      "grad_norm": 2.2891697883605957,
      "learning_rate": 2e-05,
      "loss": 0.1125,
      "step": 28370
    },
    {
      "epoch": 9.015247776365946,
      "grad_norm": 2.741028070449829,
      "learning_rate": 2e-05,
      "loss": 0.1146,
      "step": 28380
    },
    {
      "epoch": 9.018424396442185,
      "grad_norm": 2.57545804977417,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 28390
    },
    {
      "epoch": 9.021601016518424,
      "grad_norm": 2.2742464542388916,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 28400
    },
    {
      "epoch": 9.024777636594663,
      "grad_norm": 4.060885906219482,
      "learning_rate": 2e-05,
      "loss": 0.1122,
      "step": 28410
    },
    {
      "epoch": 9.027954256670903,
      "grad_norm": 2.885470390319824,
      "learning_rate": 2e-05,
      "loss": 0.1245,
      "step": 28420
    },
    {
      "epoch": 9.031130876747142,
      "grad_norm": 2.496785879135132,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 28430
    },
    {
      "epoch": 9.03430749682338,
      "grad_norm": 4.877830982208252,
      "learning_rate": 2e-05,
      "loss": 0.1176,
      "step": 28440
    },
    {
      "epoch": 9.037484116899618,
      "grad_norm": 2.2689902782440186,
      "learning_rate": 2e-05,
      "loss": 0.1136,
      "step": 28450
    },
    {
      "epoch": 9.040660736975857,
      "grad_norm": 3.557762861251831,
      "learning_rate": 2e-05,
      "loss": 0.1171,
      "step": 28460
    },
    {
      "epoch": 9.043837357052096,
      "grad_norm": 2.8807404041290283,
      "learning_rate": 2e-05,
      "loss": 0.1229,
      "step": 28470
    },
    {
      "epoch": 9.047013977128335,
      "grad_norm": 3.691200017929077,
      "learning_rate": 2e-05,
      "loss": 0.1261,
      "step": 28480
    },
    {
      "epoch": 9.050190597204574,
      "grad_norm": 3.120734453201294,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 28490
    },
    {
      "epoch": 9.053367217280814,
      "grad_norm": 6.12959098815918,
      "learning_rate": 2e-05,
      "loss": 0.1314,
      "step": 28500
    },
    {
      "epoch": 9.056543837357053,
      "grad_norm": 3.6648192405700684,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 28510
    },
    {
      "epoch": 9.059720457433292,
      "grad_norm": 3.9829816818237305,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 28520
    },
    {
      "epoch": 9.062897077509529,
      "grad_norm": 4.02268123626709,
      "learning_rate": 2e-05,
      "loss": 0.1191,
      "step": 28530
    },
    {
      "epoch": 9.066073697585768,
      "grad_norm": 3.105583667755127,
      "learning_rate": 2e-05,
      "loss": 0.1086,
      "step": 28540
    },
    {
      "epoch": 9.069250317662007,
      "grad_norm": 2.7451467514038086,
      "learning_rate": 2e-05,
      "loss": 0.1159,
      "step": 28550
    },
    {
      "epoch": 9.072426937738246,
      "grad_norm": 3.8213653564453125,
      "learning_rate": 2e-05,
      "loss": 0.1286,
      "step": 28560
    },
    {
      "epoch": 9.075603557814485,
      "grad_norm": 4.824730396270752,
      "learning_rate": 2e-05,
      "loss": 0.1272,
      "step": 28570
    },
    {
      "epoch": 9.078780177890724,
      "grad_norm": 3.0354502201080322,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 28580
    },
    {
      "epoch": 9.081956797966964,
      "grad_norm": 4.847655296325684,
      "learning_rate": 2e-05,
      "loss": 0.123,
      "step": 28590
    },
    {
      "epoch": 9.085133418043203,
      "grad_norm": 3.0703203678131104,
      "learning_rate": 2e-05,
      "loss": 0.1155,
      "step": 28600
    },
    {
      "epoch": 9.088310038119442,
      "grad_norm": 2.683279275894165,
      "learning_rate": 2e-05,
      "loss": 0.1175,
      "step": 28610
    },
    {
      "epoch": 9.091486658195679,
      "grad_norm": 3.7617626190185547,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 28620
    },
    {
      "epoch": 9.094663278271918,
      "grad_norm": 2.7309646606445312,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 28630
    },
    {
      "epoch": 9.097839898348157,
      "grad_norm": 2.3490285873413086,
      "learning_rate": 2e-05,
      "loss": 0.1135,
      "step": 28640
    },
    {
      "epoch": 9.101016518424396,
      "grad_norm": 3.967604637145996,
      "learning_rate": 2e-05,
      "loss": 0.1291,
      "step": 28650
    },
    {
      "epoch": 9.104193138500635,
      "grad_norm": 4.814784526824951,
      "learning_rate": 2e-05,
      "loss": 0.12,
      "step": 28660
    },
    {
      "epoch": 9.105781448538755,
      "eval_loss": 1.6729052066802979,
      "eval_mse": 1.6713530390487088,
      "eval_pearson": 0.4176210311684203,
      "eval_runtime": 7.489,
      "eval_samples_per_second": 2878.877,
      "eval_spearmanr": 0.4229573691509535,
      "eval_steps_per_second": 11.35,
      "step": 28665
    },
    {
      "epoch": 9.107369758576874,
      "grad_norm": 3.704359769821167,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 28670
    },
    {
      "epoch": 9.110546378653114,
      "grad_norm": 3.1659278869628906,
      "learning_rate": 2e-05,
      "loss": 0.1311,
      "step": 28680
    },
    {
      "epoch": 9.113722998729353,
      "grad_norm": 4.987766265869141,
      "learning_rate": 2e-05,
      "loss": 0.124,
      "step": 28690
    },
    {
      "epoch": 9.11689961880559,
      "grad_norm": 6.823643684387207,
      "learning_rate": 2e-05,
      "loss": 0.1094,
      "step": 28700
    },
    {
      "epoch": 9.120076238881829,
      "grad_norm": 2.3119935989379883,
      "learning_rate": 2e-05,
      "loss": 0.1166,
      "step": 28710
    },
    {
      "epoch": 9.123252858958068,
      "grad_norm": 3.106640338897705,
      "learning_rate": 2e-05,
      "loss": 0.1199,
      "step": 28720
    },
    {
      "epoch": 9.126429479034307,
      "grad_norm": 4.0701141357421875,
      "learning_rate": 2e-05,
      "loss": 0.1201,
      "step": 28730
    },
    {
      "epoch": 9.129606099110546,
      "grad_norm": 4.7373247146606445,
      "learning_rate": 2e-05,
      "loss": 0.1129,
      "step": 28740
    },
    {
      "epoch": 9.132782719186785,
      "grad_norm": 3.2929773330688477,
      "learning_rate": 2e-05,
      "loss": 0.1107,
      "step": 28750
    },
    {
      "epoch": 9.135959339263025,
      "grad_norm": 2.7611007690429688,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 28760
    },
    {
      "epoch": 9.139135959339264,
      "grad_norm": 2.531097888946533,
      "learning_rate": 2e-05,
      "loss": 0.1154,
      "step": 28770
    },
    {
      "epoch": 9.142312579415503,
      "grad_norm": 2.6821951866149902,
      "learning_rate": 2e-05,
      "loss": 0.1154,
      "step": 28780
    },
    {
      "epoch": 9.14548919949174,
      "grad_norm": 3.5200674533843994,
      "learning_rate": 2e-05,
      "loss": 0.1168,
      "step": 28790
    },
    {
      "epoch": 9.148665819567979,
      "grad_norm": 3.6318793296813965,
      "learning_rate": 2e-05,
      "loss": 0.1266,
      "step": 28800
    },
    {
      "epoch": 9.151842439644218,
      "grad_norm": 2.1915462017059326,
      "learning_rate": 2e-05,
      "loss": 0.1161,
      "step": 28810
    },
    {
      "epoch": 9.155019059720457,
      "grad_norm": 3.5198397636413574,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 28820
    },
    {
      "epoch": 9.158195679796696,
      "grad_norm": 3.559868335723877,
      "learning_rate": 2e-05,
      "loss": 0.1253,
      "step": 28830
    },
    {
      "epoch": 9.161372299872935,
      "grad_norm": 4.231799602508545,
      "learning_rate": 2e-05,
      "loss": 0.1181,
      "step": 28840
    },
    {
      "epoch": 9.164548919949175,
      "grad_norm": 3.182123899459839,
      "learning_rate": 2e-05,
      "loss": 0.1203,
      "step": 28850
    },
    {
      "epoch": 9.167725540025414,
      "grad_norm": 3.682166814804077,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 28860
    },
    {
      "epoch": 9.170902160101653,
      "grad_norm": 4.156912803649902,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 28870
    },
    {
      "epoch": 9.17407878017789,
      "grad_norm": 3.1888036727905273,
      "learning_rate": 2e-05,
      "loss": 0.1237,
      "step": 28880
    },
    {
      "epoch": 9.17725540025413,
      "grad_norm": 2.4067282676696777,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 28890
    },
    {
      "epoch": 9.180432020330368,
      "grad_norm": 3.2319188117980957,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 28900
    },
    {
      "epoch": 9.183608640406607,
      "grad_norm": 3.823392152786255,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 28910
    },
    {
      "epoch": 9.186785260482846,
      "grad_norm": 2.873260259628296,
      "learning_rate": 2e-05,
      "loss": 0.1235,
      "step": 28920
    },
    {
      "epoch": 9.189961880559085,
      "grad_norm": 2.5296759605407715,
      "learning_rate": 2e-05,
      "loss": 0.1159,
      "step": 28930
    },
    {
      "epoch": 9.193138500635325,
      "grad_norm": 3.5652191638946533,
      "learning_rate": 2e-05,
      "loss": 0.1279,
      "step": 28940
    },
    {
      "epoch": 9.196315120711564,
      "grad_norm": 3.158189296722412,
      "learning_rate": 2e-05,
      "loss": 0.1196,
      "step": 28950
    },
    {
      "epoch": 9.199491740787801,
      "grad_norm": 2.8532307147979736,
      "learning_rate": 2e-05,
      "loss": 0.1245,
      "step": 28960
    },
    {
      "epoch": 9.20266836086404,
      "grad_norm": 2.5188863277435303,
      "learning_rate": 2e-05,
      "loss": 0.1272,
      "step": 28970
    },
    {
      "epoch": 9.20584498094028,
      "grad_norm": 3.331544876098633,
      "learning_rate": 2e-05,
      "loss": 0.1202,
      "step": 28980
    },
    {
      "epoch": 9.20584498094028,
      "eval_loss": 1.7870571613311768,
      "eval_mse": 1.7859385643244232,
      "eval_pearson": 0.4174160348623911,
      "eval_runtime": 7.424,
      "eval_samples_per_second": 2904.094,
      "eval_spearmanr": 0.42305270480904833,
      "eval_steps_per_second": 11.449,
      "step": 28980
    },
    {
      "epoch": 9.209021601016518,
      "grad_norm": 3.922795057296753,
      "learning_rate": 2e-05,
      "loss": 0.1162,
      "step": 28990
    },
    {
      "epoch": 9.212198221092757,
      "grad_norm": 2.982605218887329,
      "learning_rate": 2e-05,
      "loss": 0.1211,
      "step": 29000
    },
    {
      "epoch": 9.215374841168996,
      "grad_norm": 5.613913059234619,
      "learning_rate": 2e-05,
      "loss": 0.1191,
      "step": 29010
    },
    {
      "epoch": 9.218551461245236,
      "grad_norm": 6.563008785247803,
      "learning_rate": 2e-05,
      "loss": 0.1239,
      "step": 29020
    },
    {
      "epoch": 9.221728081321475,
      "grad_norm": 4.144307613372803,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 29030
    },
    {
      "epoch": 9.224904701397714,
      "grad_norm": 4.716306686401367,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 29040
    },
    {
      "epoch": 9.228081321473951,
      "grad_norm": 4.254969120025635,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 29050
    },
    {
      "epoch": 9.23125794155019,
      "grad_norm": 2.5754435062408447,
      "learning_rate": 2e-05,
      "loss": 0.1276,
      "step": 29060
    },
    {
      "epoch": 9.23443456162643,
      "grad_norm": 3.2079684734344482,
      "learning_rate": 2e-05,
      "loss": 0.1292,
      "step": 29070
    },
    {
      "epoch": 9.237611181702668,
      "grad_norm": 4.5258684158325195,
      "learning_rate": 2e-05,
      "loss": 0.1213,
      "step": 29080
    },
    {
      "epoch": 9.240787801778907,
      "grad_norm": 2.8796660900115967,
      "learning_rate": 2e-05,
      "loss": 0.1282,
      "step": 29090
    },
    {
      "epoch": 9.243964421855146,
      "grad_norm": 3.1083943843841553,
      "learning_rate": 2e-05,
      "loss": 0.1197,
      "step": 29100
    },
    {
      "epoch": 9.247141041931386,
      "grad_norm": 2.962888479232788,
      "learning_rate": 2e-05,
      "loss": 0.1387,
      "step": 29110
    },
    {
      "epoch": 9.250317662007625,
      "grad_norm": 5.771296977996826,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 29120
    },
    {
      "epoch": 9.253494282083862,
      "grad_norm": 4.1000871658325195,
      "learning_rate": 2e-05,
      "loss": 0.1189,
      "step": 29130
    },
    {
      "epoch": 9.256670902160101,
      "grad_norm": 5.402506351470947,
      "learning_rate": 2e-05,
      "loss": 0.1213,
      "step": 29140
    },
    {
      "epoch": 9.25984752223634,
      "grad_norm": 2.9595863819122314,
      "learning_rate": 2e-05,
      "loss": 0.1274,
      "step": 29150
    },
    {
      "epoch": 9.26302414231258,
      "grad_norm": 3.885164499282837,
      "learning_rate": 2e-05,
      "loss": 0.1349,
      "step": 29160
    },
    {
      "epoch": 9.266200762388818,
      "grad_norm": 7.206809043884277,
      "learning_rate": 2e-05,
      "loss": 0.1143,
      "step": 29170
    },
    {
      "epoch": 9.269377382465057,
      "grad_norm": 3.207730770111084,
      "learning_rate": 2e-05,
      "loss": 0.1189,
      "step": 29180
    },
    {
      "epoch": 9.272554002541296,
      "grad_norm": 2.209066390991211,
      "learning_rate": 2e-05,
      "loss": 0.1187,
      "step": 29190
    },
    {
      "epoch": 9.275730622617536,
      "grad_norm": 2.6007652282714844,
      "learning_rate": 2e-05,
      "loss": 0.1216,
      "step": 29200
    },
    {
      "epoch": 9.278907242693775,
      "grad_norm": 2.4818620681762695,
      "learning_rate": 2e-05,
      "loss": 0.1045,
      "step": 29210
    },
    {
      "epoch": 9.282083862770012,
      "grad_norm": 3.6857457160949707,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 29220
    },
    {
      "epoch": 9.285260482846251,
      "grad_norm": 3.6975362300872803,
      "learning_rate": 2e-05,
      "loss": 0.1365,
      "step": 29230
    },
    {
      "epoch": 9.28843710292249,
      "grad_norm": 2.4790101051330566,
      "learning_rate": 2e-05,
      "loss": 0.1207,
      "step": 29240
    },
    {
      "epoch": 9.29161372299873,
      "grad_norm": 2.5862045288085938,
      "learning_rate": 2e-05,
      "loss": 0.1207,
      "step": 29250
    },
    {
      "epoch": 9.294790343074968,
      "grad_norm": 2.9162094593048096,
      "learning_rate": 2e-05,
      "loss": 0.1198,
      "step": 29260
    },
    {
      "epoch": 9.297966963151207,
      "grad_norm": 2.515777111053467,
      "learning_rate": 2e-05,
      "loss": 0.1187,
      "step": 29270
    },
    {
      "epoch": 9.301143583227446,
      "grad_norm": 2.85628604888916,
      "learning_rate": 2e-05,
      "loss": 0.1248,
      "step": 29280
    },
    {
      "epoch": 9.304320203303686,
      "grad_norm": 3.6360561847686768,
      "learning_rate": 2e-05,
      "loss": 0.1254,
      "step": 29290
    },
    {
      "epoch": 9.305908513341805,
      "eval_loss": 1.9139440059661865,
      "eval_mse": 1.912614192237854,
      "eval_pearson": 0.374489111557462,
      "eval_runtime": 7.3889,
      "eval_samples_per_second": 2917.886,
      "eval_spearmanr": 0.3803286110991315,
      "eval_steps_per_second": 11.504,
      "step": 29295
    },
    {
      "epoch": 9.307496823379923,
      "grad_norm": 2.4432148933410645,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 29300
    },
    {
      "epoch": 9.310673443456162,
      "grad_norm": 4.028310775756836,
      "learning_rate": 2e-05,
      "loss": 0.1291,
      "step": 29310
    },
    {
      "epoch": 9.313850063532401,
      "grad_norm": 4.283092021942139,
      "learning_rate": 2e-05,
      "loss": 0.1273,
      "step": 29320
    },
    {
      "epoch": 9.31702668360864,
      "grad_norm": 3.2359001636505127,
      "learning_rate": 2e-05,
      "loss": 0.13,
      "step": 29330
    },
    {
      "epoch": 9.32020330368488,
      "grad_norm": 2.3264100551605225,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 29340
    },
    {
      "epoch": 9.323379923761118,
      "grad_norm": 2.761662721633911,
      "learning_rate": 2e-05,
      "loss": 0.1217,
      "step": 29350
    },
    {
      "epoch": 9.326556543837357,
      "grad_norm": 3.3236091136932373,
      "learning_rate": 2e-05,
      "loss": 0.1221,
      "step": 29360
    },
    {
      "epoch": 9.329733163913597,
      "grad_norm": 2.8624966144561768,
      "learning_rate": 2e-05,
      "loss": 0.1299,
      "step": 29370
    },
    {
      "epoch": 9.332909783989836,
      "grad_norm": 3.6577250957489014,
      "learning_rate": 2e-05,
      "loss": 0.1317,
      "step": 29380
    },
    {
      "epoch": 9.336086404066073,
      "grad_norm": 3.5553665161132812,
      "learning_rate": 2e-05,
      "loss": 0.1144,
      "step": 29390
    },
    {
      "epoch": 9.339263024142312,
      "grad_norm": 3.01265287399292,
      "learning_rate": 2e-05,
      "loss": 0.1236,
      "step": 29400
    },
    {
      "epoch": 9.342439644218551,
      "grad_norm": 4.510670185089111,
      "learning_rate": 2e-05,
      "loss": 0.1153,
      "step": 29410
    },
    {
      "epoch": 9.34561626429479,
      "grad_norm": 2.556546926498413,
      "learning_rate": 2e-05,
      "loss": 0.123,
      "step": 29420
    },
    {
      "epoch": 9.34879288437103,
      "grad_norm": 3.8172154426574707,
      "learning_rate": 2e-05,
      "loss": 0.1292,
      "step": 29430
    },
    {
      "epoch": 9.351969504447268,
      "grad_norm": 2.8363125324249268,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 29440
    },
    {
      "epoch": 9.355146124523507,
      "grad_norm": 2.854112386703491,
      "learning_rate": 2e-05,
      "loss": 0.1247,
      "step": 29450
    },
    {
      "epoch": 9.358322744599747,
      "grad_norm": 2.772639513015747,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 29460
    },
    {
      "epoch": 9.361499364675986,
      "grad_norm": 4.243209362030029,
      "learning_rate": 2e-05,
      "loss": 0.133,
      "step": 29470
    },
    {
      "epoch": 9.364675984752223,
      "grad_norm": 2.2687950134277344,
      "learning_rate": 2e-05,
      "loss": 0.1176,
      "step": 29480
    },
    {
      "epoch": 9.367852604828462,
      "grad_norm": 3.6641845703125,
      "learning_rate": 2e-05,
      "loss": 0.1215,
      "step": 29490
    },
    {
      "epoch": 9.371029224904701,
      "grad_norm": 3.8783230781555176,
      "learning_rate": 2e-05,
      "loss": 0.1161,
      "step": 29500
    },
    {
      "epoch": 9.37420584498094,
      "grad_norm": 6.551675796508789,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 29510
    },
    {
      "epoch": 9.37738246505718,
      "grad_norm": 4.46119499206543,
      "learning_rate": 2e-05,
      "loss": 0.1132,
      "step": 29520
    },
    {
      "epoch": 9.380559085133418,
      "grad_norm": 4.0614471435546875,
      "learning_rate": 2e-05,
      "loss": 0.1217,
      "step": 29530
    },
    {
      "epoch": 9.383735705209657,
      "grad_norm": 4.265594959259033,
      "learning_rate": 2e-05,
      "loss": 0.1218,
      "step": 29540
    },
    {
      "epoch": 9.386912325285897,
      "grad_norm": 3.7367918491363525,
      "learning_rate": 2e-05,
      "loss": 0.1224,
      "step": 29550
    },
    {
      "epoch": 9.390088945362134,
      "grad_norm": 4.377584457397461,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 29560
    },
    {
      "epoch": 9.393265565438373,
      "grad_norm": 3.2272777557373047,
      "learning_rate": 2e-05,
      "loss": 0.1246,
      "step": 29570
    },
    {
      "epoch": 9.396442185514612,
      "grad_norm": 4.035552024841309,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 29580
    },
    {
      "epoch": 9.399618805590851,
      "grad_norm": 2.9452083110809326,
      "learning_rate": 2e-05,
      "loss": 0.1312,
      "step": 29590
    },
    {
      "epoch": 9.40279542566709,
      "grad_norm": 4.675378799438477,
      "learning_rate": 2e-05,
      "loss": 0.1252,
      "step": 29600
    },
    {
      "epoch": 9.40597204574333,
      "grad_norm": 5.782289981842041,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 29610
    },
    {
      "epoch": 9.40597204574333,
      "eval_loss": 1.9403072595596313,
      "eval_mse": 1.9393452794588273,
      "eval_pearson": 0.3582229248883812,
      "eval_runtime": 7.4037,
      "eval_samples_per_second": 2912.052,
      "eval_spearmanr": 0.3675476240211729,
      "eval_steps_per_second": 11.481,
      "step": 29610
    },
    {
      "epoch": 9.409148665819568,
      "grad_norm": 3.7172937393188477,
      "learning_rate": 2e-05,
      "loss": 0.1128,
      "step": 29620
    },
    {
      "epoch": 9.412325285895808,
      "grad_norm": 3.589442253112793,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 29630
    },
    {
      "epoch": 9.415501905972047,
      "grad_norm": 2.5553221702575684,
      "learning_rate": 2e-05,
      "loss": 0.1232,
      "step": 29640
    },
    {
      "epoch": 9.418678526048284,
      "grad_norm": 2.731198310852051,
      "learning_rate": 2e-05,
      "loss": 0.1265,
      "step": 29650
    },
    {
      "epoch": 9.421855146124523,
      "grad_norm": 3.3362820148468018,
      "learning_rate": 2e-05,
      "loss": 0.1205,
      "step": 29660
    },
    {
      "epoch": 9.425031766200762,
      "grad_norm": 6.239187240600586,
      "learning_rate": 2e-05,
      "loss": 0.1362,
      "step": 29670
    },
    {
      "epoch": 9.428208386277001,
      "grad_norm": 2.9700891971588135,
      "learning_rate": 2e-05,
      "loss": 0.1278,
      "step": 29680
    },
    {
      "epoch": 9.43138500635324,
      "grad_norm": 2.460390329360962,
      "learning_rate": 2e-05,
      "loss": 0.1224,
      "step": 29690
    },
    {
      "epoch": 9.43456162642948,
      "grad_norm": 3.482166051864624,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 29700
    },
    {
      "epoch": 9.437738246505718,
      "grad_norm": 2.411710023880005,
      "learning_rate": 2e-05,
      "loss": 0.1118,
      "step": 29710
    },
    {
      "epoch": 9.440914866581958,
      "grad_norm": 4.451786041259766,
      "learning_rate": 2e-05,
      "loss": 0.1221,
      "step": 29720
    },
    {
      "epoch": 9.444091486658195,
      "grad_norm": 4.469686985015869,
      "learning_rate": 2e-05,
      "loss": 0.1286,
      "step": 29730
    },
    {
      "epoch": 9.447268106734434,
      "grad_norm": 5.023357391357422,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 29740
    },
    {
      "epoch": 9.450444726810673,
      "grad_norm": 4.040338039398193,
      "learning_rate": 2e-05,
      "loss": 0.1243,
      "step": 29750
    },
    {
      "epoch": 9.453621346886912,
      "grad_norm": 3.871387004852295,
      "learning_rate": 2e-05,
      "loss": 0.1205,
      "step": 29760
    },
    {
      "epoch": 9.456797966963151,
      "grad_norm": 4.557561874389648,
      "learning_rate": 2e-05,
      "loss": 0.1308,
      "step": 29770
    },
    {
      "epoch": 9.45997458703939,
      "grad_norm": 3.3271117210388184,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 29780
    },
    {
      "epoch": 9.46315120711563,
      "grad_norm": 3.3404011726379395,
      "learning_rate": 2e-05,
      "loss": 0.1145,
      "step": 29790
    },
    {
      "epoch": 9.466327827191868,
      "grad_norm": 3.1717429161071777,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 29800
    },
    {
      "epoch": 9.469504447268108,
      "grad_norm": 3.6634788513183594,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 29810
    },
    {
      "epoch": 9.472681067344345,
      "grad_norm": 9.684951782226562,
      "learning_rate": 2e-05,
      "loss": 0.1228,
      "step": 29820
    },
    {
      "epoch": 9.475857687420584,
      "grad_norm": 3.694579839706421,
      "learning_rate": 2e-05,
      "loss": 0.1137,
      "step": 29830
    },
    {
      "epoch": 9.479034307496823,
      "grad_norm": 2.9691412448883057,
      "learning_rate": 2e-05,
      "loss": 0.1195,
      "step": 29840
    },
    {
      "epoch": 9.482210927573062,
      "grad_norm": 2.2158610820770264,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 29850
    },
    {
      "epoch": 9.485387547649301,
      "grad_norm": 3.3045268058776855,
      "learning_rate": 2e-05,
      "loss": 0.1287,
      "step": 29860
    },
    {
      "epoch": 9.48856416772554,
      "grad_norm": 4.165192127227783,
      "learning_rate": 2e-05,
      "loss": 0.1219,
      "step": 29870
    },
    {
      "epoch": 9.49174078780178,
      "grad_norm": 2.5528361797332764,
      "learning_rate": 2e-05,
      "loss": 0.1212,
      "step": 29880
    },
    {
      "epoch": 9.494917407878019,
      "grad_norm": 2.558803081512451,
      "learning_rate": 2e-05,
      "loss": 0.1098,
      "step": 29890
    },
    {
      "epoch": 9.498094027954256,
      "grad_norm": 2.469022035598755,
      "learning_rate": 2e-05,
      "loss": 0.122,
      "step": 29900
    },
    {
      "epoch": 9.501270648030495,
      "grad_norm": 5.706045627593994,
      "learning_rate": 2e-05,
      "loss": 0.1269,
      "step": 29910
    },
    {
      "epoch": 9.504447268106734,
      "grad_norm": 2.8267822265625,
      "learning_rate": 2e-05,
      "loss": 0.1288,
      "step": 29920
    },
    {
      "epoch": 9.506035578144854,
      "eval_loss": 1.9214361906051636,
      "eval_mse": 1.9200288410421205,
      "eval_pearson": 0.3724954076552097,
      "eval_runtime": 7.3015,
      "eval_samples_per_second": 2952.813,
      "eval_spearmanr": 0.3804542619361935,
      "eval_steps_per_second": 11.641,
      "step": 29925
    },
    {
      "epoch": 9.507623888182973,
      "grad_norm": 5.624746799468994,
      "learning_rate": 2e-05,
      "loss": 0.1148,
      "step": 29930
    },
    {
      "epoch": 9.510800508259212,
      "grad_norm": 2.8141722679138184,
      "learning_rate": 2e-05,
      "loss": 0.126,
      "step": 29940
    },
    {
      "epoch": 9.513977128335451,
      "grad_norm": 2.6253819465637207,
      "learning_rate": 2e-05,
      "loss": 0.116,
      "step": 29950
    },
    {
      "epoch": 9.51715374841169,
      "grad_norm": 4.253378868103027,
      "learning_rate": 2e-05,
      "loss": 0.132,
      "step": 29960
    },
    {
      "epoch": 9.52033036848793,
      "grad_norm": 2.4188339710235596,
      "learning_rate": 2e-05,
      "loss": 0.1207,
      "step": 29970
    },
    {
      "epoch": 9.523506988564169,
      "grad_norm": 2.9924535751342773,
      "learning_rate": 2e-05,
      "loss": 0.1234,
      "step": 29980
    },
    {
      "epoch": 9.526683608640406,
      "grad_norm": 2.6869277954101562,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 29990
    },
    {
      "epoch": 9.529860228716645,
      "grad_norm": 2.774099349975586,
      "learning_rate": 2e-05,
      "loss": 0.114,
      "step": 30000
    },
    {
      "epoch": 9.533036848792884,
      "grad_norm": 2.898521900177002,
      "learning_rate": 2e-05,
      "loss": 0.1226,
      "step": 30010
    },
    {
      "epoch": 9.536213468869123,
      "grad_norm": 3.0228874683380127,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 30020
    },
    {
      "epoch": 9.539390088945362,
      "grad_norm": 2.8147053718566895,
      "learning_rate": 2e-05,
      "loss": 0.1069,
      "step": 30030
    },
    {
      "epoch": 9.542566709021601,
      "grad_norm": 3.273245096206665,
      "learning_rate": 2e-05,
      "loss": 0.1181,
      "step": 30040
    },
    {
      "epoch": 9.54574332909784,
      "grad_norm": 2.2939178943634033,
      "learning_rate": 2e-05,
      "loss": 0.1152,
      "step": 30050
    },
    {
      "epoch": 9.54891994917408,
      "grad_norm": 2.664435863494873,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 30060
    },
    {
      "epoch": 9.552096569250317,
      "grad_norm": 3.859555244445801,
      "learning_rate": 2e-05,
      "loss": 0.1146,
      "step": 30070
    },
    {
      "epoch": 9.555273189326556,
      "grad_norm": 3.5974531173706055,
      "learning_rate": 2e-05,
      "loss": 0.1263,
      "step": 30080
    },
    {
      "epoch": 9.558449809402795,
      "grad_norm": 3.5142481327056885,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 30090
    },
    {
      "epoch": 9.561626429479034,
      "grad_norm": 3.304050922393799,
      "learning_rate": 2e-05,
      "loss": 0.1223,
      "step": 30100
    },
    {
      "epoch": 9.564803049555273,
      "grad_norm": 3.0045928955078125,
      "learning_rate": 2e-05,
      "loss": 0.1142,
      "step": 30110
    },
    {
      "epoch": 9.567979669631512,
      "grad_norm": 5.71402645111084,
      "learning_rate": 2e-05,
      "loss": 0.1291,
      "step": 30120
    },
    {
      "epoch": 9.571156289707751,
      "grad_norm": 2.6281795501708984,
      "learning_rate": 2e-05,
      "loss": 0.1111,
      "step": 30130
    },
    {
      "epoch": 9.57433290978399,
      "grad_norm": 4.072073459625244,
      "learning_rate": 2e-05,
      "loss": 0.1177,
      "step": 30140
    },
    {
      "epoch": 9.57750952986023,
      "grad_norm": 4.254561901092529,
      "learning_rate": 2e-05,
      "loss": 0.1118,
      "step": 30150
    },
    {
      "epoch": 9.580686149936467,
      "grad_norm": 2.6586670875549316,
      "learning_rate": 2e-05,
      "loss": 0.1188,
      "step": 30160
    },
    {
      "epoch": 9.583862770012706,
      "grad_norm": 4.01503849029541,
      "learning_rate": 2e-05,
      "loss": 0.1244,
      "step": 30170
    },
    {
      "epoch": 9.587039390088945,
      "grad_norm": 2.836439847946167,
      "learning_rate": 2e-05,
      "loss": 0.1136,
      "step": 30180
    },
    {
      "epoch": 9.590216010165184,
      "grad_norm": 2.007910966873169,
      "learning_rate": 2e-05,
      "loss": 0.1112,
      "step": 30190
    },
    {
      "epoch": 9.593392630241423,
      "grad_norm": 2.591283082962036,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 30200
    },
    {
      "epoch": 9.596569250317662,
      "grad_norm": 2.5349862575531006,
      "learning_rate": 2e-05,
      "loss": 0.1229,
      "step": 30210
    },
    {
      "epoch": 9.599745870393901,
      "grad_norm": 3.884901285171509,
      "learning_rate": 2e-05,
      "loss": 0.1206,
      "step": 30220
    },
    {
      "epoch": 9.60292249047014,
      "grad_norm": 4.527314186096191,
      "learning_rate": 2e-05,
      "loss": 0.1267,
      "step": 30230
    },
    {
      "epoch": 9.606099110546378,
      "grad_norm": 2.6251282691955566,
      "learning_rate": 2e-05,
      "loss": 0.1249,
      "step": 30240
    },
    {
      "epoch": 9.606099110546378,
      "eval_loss": 1.8341405391693115,
      "eval_mse": 1.8326144080251199,
      "eval_pearson": 0.39517050369567086,
      "eval_runtime": 7.4933,
      "eval_samples_per_second": 2877.252,
      "eval_spearmanr": 0.3976399434721401,
      "eval_steps_per_second": 11.344,
      "step": 30240
    },
    {
      "epoch": 9.609275730622617,
      "grad_norm": 4.62533712387085,
      "learning_rate": 2e-05,
      "loss": 0.129,
      "step": 30250
    },
    {
      "epoch": 9.612452350698856,
      "grad_norm": 3.0397450923919678,
      "learning_rate": 2e-05,
      "loss": 0.1229,
      "step": 30260
    },
    {
      "epoch": 9.615628970775095,
      "grad_norm": 4.133005619049072,
      "learning_rate": 2e-05,
      "loss": 0.1224,
      "step": 30270
    },
    {
      "epoch": 9.618805590851334,
      "grad_norm": 4.787901401519775,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 30280
    },
    {
      "epoch": 9.621982210927573,
      "grad_norm": 3.0310676097869873,
      "learning_rate": 2e-05,
      "loss": 0.1098,
      "step": 30290
    },
    {
      "epoch": 9.625158831003812,
      "grad_norm": 2.7612030506134033,
      "learning_rate": 2e-05,
      "loss": 0.1201,
      "step": 30300
    },
    {
      "epoch": 9.628335451080051,
      "grad_norm": 3.671769618988037,
      "learning_rate": 2e-05,
      "loss": 0.1104,
      "step": 30310
    },
    {
      "epoch": 9.63151207115629,
      "grad_norm": 4.967897891998291,
      "learning_rate": 2e-05,
      "loss": 0.1112,
      "step": 30320
    },
    {
      "epoch": 9.634688691232528,
      "grad_norm": 2.9418439865112305,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 30330
    },
    {
      "epoch": 9.637865311308767,
      "grad_norm": 3.4759185314178467,
      "learning_rate": 2e-05,
      "loss": 0.1127,
      "step": 30340
    },
    {
      "epoch": 9.641041931385006,
      "grad_norm": 3.3839383125305176,
      "learning_rate": 2e-05,
      "loss": 0.1169,
      "step": 30350
    },
    {
      "epoch": 9.644218551461245,
      "grad_norm": 4.375661849975586,
      "learning_rate": 2e-05,
      "loss": 0.1256,
      "step": 30360
    },
    {
      "epoch": 9.647395171537484,
      "grad_norm": 4.513117790222168,
      "learning_rate": 2e-05,
      "loss": 0.1233,
      "step": 30370
    },
    {
      "epoch": 9.650571791613723,
      "grad_norm": 2.7273097038269043,
      "learning_rate": 2e-05,
      "loss": 0.1216,
      "step": 30380
    },
    {
      "epoch": 9.653748411689962,
      "grad_norm": 2.261031150817871,
      "learning_rate": 2e-05,
      "loss": 0.1197,
      "step": 30390
    },
    {
      "epoch": 9.656925031766201,
      "grad_norm": 2.940253973007202,
      "learning_rate": 2e-05,
      "loss": 0.1137,
      "step": 30400
    },
    {
      "epoch": 9.660101651842439,
      "grad_norm": 3.0022270679473877,
      "learning_rate": 2e-05,
      "loss": 0.1267,
      "step": 30410
    },
    {
      "epoch": 9.663278271918678,
      "grad_norm": 8.61801528930664,
      "learning_rate": 2e-05,
      "loss": 0.1177,
      "step": 30420
    },
    {
      "epoch": 9.666454891994917,
      "grad_norm": 3.360460042953491,
      "learning_rate": 2e-05,
      "loss": 0.1152,
      "step": 30430
    },
    {
      "epoch": 9.669631512071156,
      "grad_norm": 4.516361236572266,
      "learning_rate": 2e-05,
      "loss": 0.1204,
      "step": 30440
    },
    {
      "epoch": 9.672808132147395,
      "grad_norm": 3.2883358001708984,
      "learning_rate": 2e-05,
      "loss": 0.1174,
      "step": 30450
    },
    {
      "epoch": 9.675984752223634,
      "grad_norm": 3.158599853515625,
      "learning_rate": 2e-05,
      "loss": 0.1196,
      "step": 30460
    },
    {
      "epoch": 9.679161372299873,
      "grad_norm": 2.4316699504852295,
      "learning_rate": 2e-05,
      "loss": 0.1191,
      "step": 30470
    },
    {
      "epoch": 9.682337992376112,
      "grad_norm": 20.150442123413086,
      "learning_rate": 2e-05,
      "loss": 0.1179,
      "step": 30480
    },
    {
      "epoch": 9.685514612452351,
      "grad_norm": 4.125955581665039,
      "learning_rate": 2e-05,
      "loss": 0.1159,
      "step": 30490
    },
    {
      "epoch": 9.688691232528589,
      "grad_norm": 3.3381152153015137,
      "learning_rate": 2e-05,
      "loss": 0.1325,
      "step": 30500
    },
    {
      "epoch": 9.691867852604828,
      "grad_norm": 2.464606285095215,
      "learning_rate": 2e-05,
      "loss": 0.1205,
      "step": 30510
    },
    {
      "epoch": 9.695044472681067,
      "grad_norm": 2.102513074874878,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 30520
    },
    {
      "epoch": 9.698221092757306,
      "grad_norm": 3.1892693042755127,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 30530
    },
    {
      "epoch": 9.701397712833545,
      "grad_norm": 2.5241146087646484,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 30540
    },
    {
      "epoch": 9.704574332909784,
      "grad_norm": 3.314431667327881,
      "learning_rate": 2e-05,
      "loss": 0.1257,
      "step": 30550
    },
    {
      "epoch": 9.706162642947904,
      "eval_loss": 1.9380747079849243,
      "eval_mse": 1.9367284467219201,
      "eval_pearson": 0.36304362427504105,
      "eval_runtime": 7.6019,
      "eval_samples_per_second": 2836.145,
      "eval_spearmanr": 0.3744112638792855,
      "eval_steps_per_second": 11.181,
      "step": 30555
    },
    {
      "epoch": 9.707750952986023,
      "grad_norm": 3.8137080669403076,
      "learning_rate": 2e-05,
      "loss": 0.1183,
      "step": 30560
    },
    {
      "epoch": 9.710927573062262,
      "grad_norm": 2.19913911819458,
      "learning_rate": 2e-05,
      "loss": 0.1102,
      "step": 30570
    },
    {
      "epoch": 9.7141041931385,
      "grad_norm": 2.7855396270751953,
      "learning_rate": 2e-05,
      "loss": 0.1153,
      "step": 30580
    },
    {
      "epoch": 9.717280813214739,
      "grad_norm": 3.1181182861328125,
      "learning_rate": 2e-05,
      "loss": 0.1145,
      "step": 30590
    },
    {
      "epoch": 9.720457433290978,
      "grad_norm": 2.5396242141723633,
      "learning_rate": 2e-05,
      "loss": 0.1264,
      "step": 30600
    },
    {
      "epoch": 9.723634053367217,
      "grad_norm": 5.359564304351807,
      "learning_rate": 2e-05,
      "loss": 0.1243,
      "step": 30610
    },
    {
      "epoch": 9.726810673443456,
      "grad_norm": 4.93734884262085,
      "learning_rate": 2e-05,
      "loss": 0.1183,
      "step": 30620
    },
    {
      "epoch": 9.729987293519695,
      "grad_norm": 2.9324848651885986,
      "learning_rate": 2e-05,
      "loss": 0.1117,
      "step": 30630
    },
    {
      "epoch": 9.733163913595934,
      "grad_norm": 2.6415090560913086,
      "learning_rate": 2e-05,
      "loss": 0.1175,
      "step": 30640
    },
    {
      "epoch": 9.736340533672173,
      "grad_norm": 2.4269042015075684,
      "learning_rate": 2e-05,
      "loss": 0.1241,
      "step": 30650
    },
    {
      "epoch": 9.739517153748412,
      "grad_norm": 2.658400774002075,
      "learning_rate": 2e-05,
      "loss": 0.1175,
      "step": 30660
    },
    {
      "epoch": 9.742693773824652,
      "grad_norm": 2.9421119689941406,
      "learning_rate": 2e-05,
      "loss": 0.1291,
      "step": 30670
    },
    {
      "epoch": 9.745870393900889,
      "grad_norm": 3.8378870487213135,
      "learning_rate": 2e-05,
      "loss": 0.1258,
      "step": 30680
    },
    {
      "epoch": 9.749047013977128,
      "grad_norm": 2.8794026374816895,
      "learning_rate": 2e-05,
      "loss": 0.1166,
      "step": 30690
    },
    {
      "epoch": 9.752223634053367,
      "grad_norm": 5.6323418617248535,
      "learning_rate": 2e-05,
      "loss": 0.1225,
      "step": 30700
    },
    {
      "epoch": 9.755400254129606,
      "grad_norm": 2.969787836074829,
      "learning_rate": 2e-05,
      "loss": 0.1122,
      "step": 30710
    },
    {
      "epoch": 9.758576874205845,
      "grad_norm": 3.0881476402282715,
      "learning_rate": 2e-05,
      "loss": 0.1181,
      "step": 30720
    },
    {
      "epoch": 9.761753494282084,
      "grad_norm": 3.6215262413024902,
      "learning_rate": 2e-05,
      "loss": 0.116,
      "step": 30730
    },
    {
      "epoch": 9.764930114358323,
      "grad_norm": 3.934575319290161,
      "learning_rate": 2e-05,
      "loss": 0.1213,
      "step": 30740
    },
    {
      "epoch": 9.768106734434562,
      "grad_norm": 2.123708724975586,
      "learning_rate": 2e-05,
      "loss": 0.1129,
      "step": 30750
    },
    {
      "epoch": 9.7712833545108,
      "grad_norm": 4.950079917907715,
      "learning_rate": 2e-05,
      "loss": 0.1146,
      "step": 30760
    },
    {
      "epoch": 9.774459974587039,
      "grad_norm": 3.036362648010254,
      "learning_rate": 2e-05,
      "loss": 0.1255,
      "step": 30770
    },
    {
      "epoch": 9.777636594663278,
      "grad_norm": 3.1461658477783203,
      "learning_rate": 2e-05,
      "loss": 0.1165,
      "step": 30780
    },
    {
      "epoch": 9.780813214739517,
      "grad_norm": 3.0322530269622803,
      "learning_rate": 2e-05,
      "loss": 0.1286,
      "step": 30790
    },
    {
      "epoch": 9.783989834815756,
      "grad_norm": 3.5653185844421387,
      "learning_rate": 2e-05,
      "loss": 0.1203,
      "step": 30800
    },
    {
      "epoch": 9.787166454891995,
      "grad_norm": 3.5690200328826904,
      "learning_rate": 2e-05,
      "loss": 0.1225,
      "step": 30810
    },
    {
      "epoch": 9.790343074968234,
      "grad_norm": 2.5365216732025146,
      "learning_rate": 2e-05,
      "loss": 0.125,
      "step": 30820
    },
    {
      "epoch": 9.793519695044473,
      "grad_norm": 2.926462173461914,
      "learning_rate": 2e-05,
      "loss": 0.1186,
      "step": 30830
    },
    {
      "epoch": 9.796696315120712,
      "grad_norm": 3.1539347171783447,
      "learning_rate": 2e-05,
      "loss": 0.1148,
      "step": 30840
    },
    {
      "epoch": 9.79987293519695,
      "grad_norm": 3.7506906986236572,
      "learning_rate": 2e-05,
      "loss": 0.1267,
      "step": 30850
    },
    {
      "epoch": 9.803049555273189,
      "grad_norm": 2.3939108848571777,
      "learning_rate": 2e-05,
      "loss": 0.1154,
      "step": 30860
    },
    {
      "epoch": 9.806226175349428,
      "grad_norm": 2.544687032699585,
      "learning_rate": 2e-05,
      "loss": 0.1223,
      "step": 30870
    },
    {
      "epoch": 9.806226175349428,
      "eval_loss": 1.8041051626205444,
      "eval_mse": 1.802411837494329,
      "eval_pearson": 0.39428435796524103,
      "eval_runtime": 7.4075,
      "eval_samples_per_second": 2910.548,
      "eval_spearmanr": 0.3999550058677643,
      "eval_steps_per_second": 11.475,
      "step": 30870
    },
    {
      "epoch": 9.809402795425667,
      "grad_norm": 3.499423027038574,
      "learning_rate": 2e-05,
      "loss": 0.128,
      "step": 30880
    },
    {
      "epoch": 9.812579415501906,
      "grad_norm": 3.467134952545166,
      "learning_rate": 2e-05,
      "loss": 0.1148,
      "step": 30890
    },
    {
      "epoch": 9.815756035578145,
      "grad_norm": 3.797905445098877,
      "learning_rate": 2e-05,
      "loss": 0.1213,
      "step": 30900
    },
    {
      "epoch": 9.818932655654384,
      "grad_norm": 4.3967413902282715,
      "learning_rate": 2e-05,
      "loss": 0.1287,
      "step": 30910
    },
    {
      "epoch": 9.822109275730623,
      "grad_norm": 2.236841917037964,
      "learning_rate": 2e-05,
      "loss": 0.1131,
      "step": 30920
    },
    {
      "epoch": 9.82528589580686,
      "grad_norm": 4.729322910308838,
      "learning_rate": 2e-05,
      "loss": 0.1142,
      "step": 30930
    },
    {
      "epoch": 9.8284625158831,
      "grad_norm": 2.846222162246704,
      "learning_rate": 2e-05,
      "loss": 0.1149,
      "step": 30940
    },
    {
      "epoch": 9.831639135959339,
      "grad_norm": 3.7563562393188477,
      "learning_rate": 2e-05,
      "loss": 0.1203,
      "step": 30950
    },
    {
      "epoch": 9.834815756035578,
      "grad_norm": 4.034526348114014,
      "learning_rate": 2e-05,
      "loss": 0.1268,
      "step": 30960
    },
    {
      "epoch": 9.837992376111817,
      "grad_norm": 3.604240655899048,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 30970
    },
    {
      "epoch": 9.841168996188056,
      "grad_norm": 2.815424680709839,
      "learning_rate": 2e-05,
      "loss": 0.1099,
      "step": 30980
    },
    {
      "epoch": 9.844345616264295,
      "grad_norm": 2.7926337718963623,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 30990
    },
    {
      "epoch": 9.847522236340534,
      "grad_norm": 2.8826467990875244,
      "learning_rate": 2e-05,
      "loss": 0.1266,
      "step": 31000
    },
    {
      "epoch": 9.850698856416773,
      "grad_norm": 3.0435609817504883,
      "learning_rate": 2e-05,
      "loss": 0.1202,
      "step": 31010
    },
    {
      "epoch": 9.85387547649301,
      "grad_norm": 3.4565517902374268,
      "learning_rate": 2e-05,
      "loss": 0.1148,
      "step": 31020
    },
    {
      "epoch": 9.85705209656925,
      "grad_norm": 3.718618392944336,
      "learning_rate": 2e-05,
      "loss": 0.113,
      "step": 31030
    },
    {
      "epoch": 9.860228716645489,
      "grad_norm": 3.4424660205841064,
      "learning_rate": 2e-05,
      "loss": 0.1048,
      "step": 31040
    },
    {
      "epoch": 9.863405336721728,
      "grad_norm": 2.9401471614837646,
      "learning_rate": 2e-05,
      "loss": 0.123,
      "step": 31050
    },
    {
      "epoch": 9.866581956797967,
      "grad_norm": 2.521141767501831,
      "learning_rate": 2e-05,
      "loss": 0.1129,
      "step": 31060
    },
    {
      "epoch": 9.869758576874206,
      "grad_norm": 3.93060564994812,
      "learning_rate": 2e-05,
      "loss": 0.1246,
      "step": 31070
    },
    {
      "epoch": 9.872935196950445,
      "grad_norm": 3.10640549659729,
      "learning_rate": 2e-05,
      "loss": 0.1247,
      "step": 31080
    },
    {
      "epoch": 9.876111817026684,
      "grad_norm": 2.7134931087493896,
      "learning_rate": 2e-05,
      "loss": 0.1149,
      "step": 31090
    },
    {
      "epoch": 9.879288437102922,
      "grad_norm": 3.155593156814575,
      "learning_rate": 2e-05,
      "loss": 0.1244,
      "step": 31100
    },
    {
      "epoch": 9.88246505717916,
      "grad_norm": 4.21933650970459,
      "learning_rate": 2e-05,
      "loss": 0.1216,
      "step": 31110
    },
    {
      "epoch": 9.8856416772554,
      "grad_norm": 2.5285212993621826,
      "learning_rate": 2e-05,
      "loss": 0.1155,
      "step": 31120
    },
    {
      "epoch": 9.888818297331639,
      "grad_norm": 2.253406524658203,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 31130
    },
    {
      "epoch": 9.891994917407878,
      "grad_norm": 2.9340407848358154,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 31140
    },
    {
      "epoch": 9.895171537484117,
      "grad_norm": 2.1598353385925293,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 31150
    },
    {
      "epoch": 9.898348157560356,
      "grad_norm": 4.368619441986084,
      "learning_rate": 2e-05,
      "loss": 0.1301,
      "step": 31160
    },
    {
      "epoch": 9.901524777636595,
      "grad_norm": 3.3342761993408203,
      "learning_rate": 2e-05,
      "loss": 0.1207,
      "step": 31170
    },
    {
      "epoch": 9.904701397712834,
      "grad_norm": 3.1900618076324463,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 31180
    },
    {
      "epoch": 9.906289707750952,
      "eval_loss": 1.8497216701507568,
      "eval_mse": 1.8480957697850613,
      "eval_pearson": 0.3962235636066637,
      "eval_runtime": 7.5818,
      "eval_samples_per_second": 2843.667,
      "eval_spearmanr": 0.4007678985634395,
      "eval_steps_per_second": 11.211,
      "step": 31185
    },
    {
      "epoch": 9.907878017789072,
      "grad_norm": 3.3869283199310303,
      "learning_rate": 2e-05,
      "loss": 0.1123,
      "step": 31190
    },
    {
      "epoch": 9.91105463786531,
      "grad_norm": 2.5209624767303467,
      "learning_rate": 2e-05,
      "loss": 0.1234,
      "step": 31200
    },
    {
      "epoch": 9.91423125794155,
      "grad_norm": 3.1500930786132812,
      "learning_rate": 2e-05,
      "loss": 0.1125,
      "step": 31210
    },
    {
      "epoch": 9.917407878017789,
      "grad_norm": 2.582836866378784,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 31220
    },
    {
      "epoch": 9.920584498094028,
      "grad_norm": 2.4723215103149414,
      "learning_rate": 2e-05,
      "loss": 0.1194,
      "step": 31230
    },
    {
      "epoch": 9.923761118170267,
      "grad_norm": 2.311335563659668,
      "learning_rate": 2e-05,
      "loss": 0.1229,
      "step": 31240
    },
    {
      "epoch": 9.926937738246506,
      "grad_norm": 2.5850906372070312,
      "learning_rate": 2e-05,
      "loss": 0.1226,
      "step": 31250
    },
    {
      "epoch": 9.930114358322745,
      "grad_norm": 5.07418966293335,
      "learning_rate": 2e-05,
      "loss": 0.1199,
      "step": 31260
    },
    {
      "epoch": 9.933290978398983,
      "grad_norm": 2.5791354179382324,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 31270
    },
    {
      "epoch": 9.936467598475222,
      "grad_norm": 2.566047191619873,
      "learning_rate": 2e-05,
      "loss": 0.1212,
      "step": 31280
    },
    {
      "epoch": 9.93964421855146,
      "grad_norm": 4.139729976654053,
      "learning_rate": 2e-05,
      "loss": 0.1223,
      "step": 31290
    },
    {
      "epoch": 9.9428208386277,
      "grad_norm": 2.866022825241089,
      "learning_rate": 2e-05,
      "loss": 0.1199,
      "step": 31300
    },
    {
      "epoch": 9.945997458703939,
      "grad_norm": 4.726361274719238,
      "learning_rate": 2e-05,
      "loss": 0.1223,
      "step": 31310
    },
    {
      "epoch": 9.949174078780178,
      "grad_norm": 5.489369869232178,
      "learning_rate": 2e-05,
      "loss": 0.1179,
      "step": 31320
    },
    {
      "epoch": 9.952350698856417,
      "grad_norm": 3.9960379600524902,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 31330
    },
    {
      "epoch": 9.955527318932656,
      "grad_norm": 3.380017042160034,
      "learning_rate": 2e-05,
      "loss": 0.1194,
      "step": 31340
    },
    {
      "epoch": 9.958703939008895,
      "grad_norm": 2.7421271800994873,
      "learning_rate": 2e-05,
      "loss": 0.1182,
      "step": 31350
    },
    {
      "epoch": 9.961880559085133,
      "grad_norm": 4.1975417137146,
      "learning_rate": 2e-05,
      "loss": 0.1226,
      "step": 31360
    },
    {
      "epoch": 9.965057179161372,
      "grad_norm": 2.4132635593414307,
      "learning_rate": 2e-05,
      "loss": 0.1217,
      "step": 31370
    },
    {
      "epoch": 9.96823379923761,
      "grad_norm": 2.7272279262542725,
      "learning_rate": 2e-05,
      "loss": 0.1328,
      "step": 31380
    },
    {
      "epoch": 9.97141041931385,
      "grad_norm": 2.630751609802246,
      "learning_rate": 2e-05,
      "loss": 0.1131,
      "step": 31390
    },
    {
      "epoch": 9.974587039390089,
      "grad_norm": 3.0109434127807617,
      "learning_rate": 2e-05,
      "loss": 0.1166,
      "step": 31400
    },
    {
      "epoch": 9.977763659466328,
      "grad_norm": 4.692381381988525,
      "learning_rate": 2e-05,
      "loss": 0.1172,
      "step": 31410
    },
    {
      "epoch": 9.980940279542567,
      "grad_norm": 3.221085786819458,
      "learning_rate": 2e-05,
      "loss": 0.1221,
      "step": 31420
    },
    {
      "epoch": 9.984116899618806,
      "grad_norm": 4.197819232940674,
      "learning_rate": 2e-05,
      "loss": 0.1231,
      "step": 31430
    },
    {
      "epoch": 9.987293519695044,
      "grad_norm": 2.8066954612731934,
      "learning_rate": 2e-05,
      "loss": 0.1212,
      "step": 31440
    },
    {
      "epoch": 9.990470139771283,
      "grad_norm": 3.85937762260437,
      "learning_rate": 2e-05,
      "loss": 0.1177,
      "step": 31450
    },
    {
      "epoch": 9.993646759847522,
      "grad_norm": 6.195188045501709,
      "learning_rate": 2e-05,
      "loss": 0.1139,
      "step": 31460
    },
    {
      "epoch": 9.996823379923761,
      "grad_norm": 4.149094104766846,
      "learning_rate": 2e-05,
      "loss": 0.1135,
      "step": 31470
    },
    {
      "epoch": 10.0,
      "grad_norm": 3.4187779426574707,
      "learning_rate": 2e-05,
      "loss": 0.1104,
      "step": 31480
    },
    {
      "epoch": 10.003176620076239,
      "grad_norm": 3.592219591140747,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 31490
    },
    {
      "epoch": 10.006353240152478,
      "grad_norm": 4.665448188781738,
      "learning_rate": 2e-05,
      "loss": 0.1092,
      "step": 31500
    },
    {
      "epoch": 10.006353240152478,
      "eval_loss": 1.786521315574646,
      "eval_mse": 1.785268759141384,
      "eval_pearson": 0.4021531674701767,
      "eval_runtime": 7.2705,
      "eval_samples_per_second": 2965.403,
      "eval_spearmanr": 0.4092291405717276,
      "eval_steps_per_second": 11.691,
      "step": 31500
    },
    {
      "epoch": 10.009529860228717,
      "grad_norm": 4.405510902404785,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 31510
    },
    {
      "epoch": 10.012706480304956,
      "grad_norm": 3.0530166625976562,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 31520
    },
    {
      "epoch": 10.015883100381194,
      "grad_norm": 2.5966594219207764,
      "learning_rate": 2e-05,
      "loss": 0.1066,
      "step": 31530
    },
    {
      "epoch": 10.019059720457433,
      "grad_norm": 3.5583786964416504,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 31540
    },
    {
      "epoch": 10.022236340533672,
      "grad_norm": 3.572115182876587,
      "learning_rate": 2e-05,
      "loss": 0.103,
      "step": 31550
    },
    {
      "epoch": 10.025412960609911,
      "grad_norm": 3.9109179973602295,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 31560
    },
    {
      "epoch": 10.02858958068615,
      "grad_norm": 1.9819364547729492,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 31570
    },
    {
      "epoch": 10.03176620076239,
      "grad_norm": 3.3682520389556885,
      "learning_rate": 2e-05,
      "loss": 0.1128,
      "step": 31580
    },
    {
      "epoch": 10.034942820838628,
      "grad_norm": 2.8676795959472656,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 31590
    },
    {
      "epoch": 10.038119440914867,
      "grad_norm": 2.3669333457946777,
      "learning_rate": 2e-05,
      "loss": 0.1013,
      "step": 31600
    },
    {
      "epoch": 10.041296060991105,
      "grad_norm": 2.7087841033935547,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 31610
    },
    {
      "epoch": 10.044472681067344,
      "grad_norm": 2.7762293815612793,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 31620
    },
    {
      "epoch": 10.047649301143583,
      "grad_norm": 2.461508274078369,
      "learning_rate": 2e-05,
      "loss": 0.1038,
      "step": 31630
    },
    {
      "epoch": 10.050825921219822,
      "grad_norm": 3.0219058990478516,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 31640
    },
    {
      "epoch": 10.054002541296061,
      "grad_norm": 2.8250253200531006,
      "learning_rate": 2e-05,
      "loss": 0.1063,
      "step": 31650
    },
    {
      "epoch": 10.0571791613723,
      "grad_norm": 3.6367874145507812,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 31660
    },
    {
      "epoch": 10.06035578144854,
      "grad_norm": 2.8784148693084717,
      "learning_rate": 2e-05,
      "loss": 0.1099,
      "step": 31670
    },
    {
      "epoch": 10.063532401524778,
      "grad_norm": 2.754213809967041,
      "learning_rate": 2e-05,
      "loss": 0.1043,
      "step": 31680
    },
    {
      "epoch": 10.066709021601017,
      "grad_norm": 4.641336441040039,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 31690
    },
    {
      "epoch": 10.069885641677255,
      "grad_norm": 3.172886848449707,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 31700
    },
    {
      "epoch": 10.073062261753494,
      "grad_norm": 3.26275897026062,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 31710
    },
    {
      "epoch": 10.076238881829733,
      "grad_norm": 2.975367784500122,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 31720
    },
    {
      "epoch": 10.079415501905972,
      "grad_norm": 3.2144789695739746,
      "learning_rate": 2e-05,
      "loss": 0.1071,
      "step": 31730
    },
    {
      "epoch": 10.082592121982211,
      "grad_norm": 1.872931718826294,
      "learning_rate": 2e-05,
      "loss": 0.1138,
      "step": 31740
    },
    {
      "epoch": 10.08576874205845,
      "grad_norm": 4.469855785369873,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 31750
    },
    {
      "epoch": 10.08894536213469,
      "grad_norm": 3.1550614833831787,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 31760
    },
    {
      "epoch": 10.092121982210928,
      "grad_norm": 5.0943121910095215,
      "learning_rate": 2e-05,
      "loss": 0.1127,
      "step": 31770
    },
    {
      "epoch": 10.095298602287166,
      "grad_norm": 3.2763490676879883,
      "learning_rate": 2e-05,
      "loss": 0.0998,
      "step": 31780
    },
    {
      "epoch": 10.098475222363405,
      "grad_norm": 4.053614139556885,
      "learning_rate": 2e-05,
      "loss": 0.1104,
      "step": 31790
    },
    {
      "epoch": 10.101651842439644,
      "grad_norm": 3.5142529010772705,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 31800
    },
    {
      "epoch": 10.104828462515883,
      "grad_norm": 3.5278990268707275,
      "learning_rate": 2e-05,
      "loss": 0.1059,
      "step": 31810
    },
    {
      "epoch": 10.106416772554002,
      "eval_loss": 1.8602265119552612,
      "eval_mse": 1.8590829985344022,
      "eval_pearson": 0.3696850756523755,
      "eval_runtime": 7.4911,
      "eval_samples_per_second": 2878.09,
      "eval_spearmanr": 0.3791097017104172,
      "eval_steps_per_second": 11.347,
      "step": 31815
    },
    {
      "epoch": 10.108005082592122,
      "grad_norm": 2.844581127166748,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 31820
    },
    {
      "epoch": 10.111181702668361,
      "grad_norm": 3.0332014560699463,
      "learning_rate": 2e-05,
      "loss": 0.1083,
      "step": 31830
    },
    {
      "epoch": 10.1143583227446,
      "grad_norm": 2.5262227058410645,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 31840
    },
    {
      "epoch": 10.11753494282084,
      "grad_norm": 2.4384026527404785,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 31850
    },
    {
      "epoch": 10.120711562897078,
      "grad_norm": 3.4436194896698,
      "learning_rate": 2e-05,
      "loss": 0.1025,
      "step": 31860
    },
    {
      "epoch": 10.123888182973316,
      "grad_norm": 2.6390039920806885,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 31870
    },
    {
      "epoch": 10.127064803049555,
      "grad_norm": 2.051666021347046,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 31880
    },
    {
      "epoch": 10.130241423125794,
      "grad_norm": 4.874866962432861,
      "learning_rate": 2e-05,
      "loss": 0.1105,
      "step": 31890
    },
    {
      "epoch": 10.133418043202033,
      "grad_norm": 3.649029016494751,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 31900
    },
    {
      "epoch": 10.136594663278272,
      "grad_norm": 2.911717414855957,
      "learning_rate": 2e-05,
      "loss": 0.1098,
      "step": 31910
    },
    {
      "epoch": 10.139771283354511,
      "grad_norm": 4.847589015960693,
      "learning_rate": 2e-05,
      "loss": 0.1142,
      "step": 31920
    },
    {
      "epoch": 10.14294790343075,
      "grad_norm": 4.403130531311035,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 31930
    },
    {
      "epoch": 10.14612452350699,
      "grad_norm": 3.5788204669952393,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 31940
    },
    {
      "epoch": 10.149301143583227,
      "grad_norm": 3.0751044750213623,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 31950
    },
    {
      "epoch": 10.152477763659466,
      "grad_norm": 3.076241970062256,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 31960
    },
    {
      "epoch": 10.155654383735705,
      "grad_norm": 5.056516647338867,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 31970
    },
    {
      "epoch": 10.158831003811944,
      "grad_norm": 2.358381748199463,
      "learning_rate": 2e-05,
      "loss": 0.1157,
      "step": 31980
    },
    {
      "epoch": 10.162007623888183,
      "grad_norm": 2.304516077041626,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 31990
    },
    {
      "epoch": 10.165184243964422,
      "grad_norm": 2.0678083896636963,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 32000
    },
    {
      "epoch": 10.168360864040661,
      "grad_norm": 2.7113089561462402,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 32010
    },
    {
      "epoch": 10.1715374841169,
      "grad_norm": 2.767463445663452,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 32020
    },
    {
      "epoch": 10.17471410419314,
      "grad_norm": 2.087791919708252,
      "learning_rate": 2e-05,
      "loss": 0.1066,
      "step": 32030
    },
    {
      "epoch": 10.177890724269377,
      "grad_norm": 4.690811634063721,
      "learning_rate": 2e-05,
      "loss": 0.1002,
      "step": 32040
    },
    {
      "epoch": 10.181067344345616,
      "grad_norm": 2.5955686569213867,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 32050
    },
    {
      "epoch": 10.184243964421855,
      "grad_norm": 3.0971312522888184,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 32060
    },
    {
      "epoch": 10.187420584498094,
      "grad_norm": 2.503591299057007,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 32070
    },
    {
      "epoch": 10.190597204574333,
      "grad_norm": 2.4086215496063232,
      "learning_rate": 2e-05,
      "loss": 0.1085,
      "step": 32080
    },
    {
      "epoch": 10.193773824650572,
      "grad_norm": 2.293386697769165,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 32090
    },
    {
      "epoch": 10.196950444726811,
      "grad_norm": 3.112834930419922,
      "learning_rate": 2e-05,
      "loss": 0.1128,
      "step": 32100
    },
    {
      "epoch": 10.20012706480305,
      "grad_norm": 2.5650742053985596,
      "learning_rate": 2e-05,
      "loss": 0.0963,
      "step": 32110
    },
    {
      "epoch": 10.20330368487929,
      "grad_norm": 2.9266586303710938,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 32120
    },
    {
      "epoch": 10.206480304955527,
      "grad_norm": 3.9558684825897217,
      "learning_rate": 2e-05,
      "loss": 0.112,
      "step": 32130
    },
    {
      "epoch": 10.206480304955527,
      "eval_loss": 1.9226765632629395,
      "eval_mse": 1.9209969314247204,
      "eval_pearson": 0.3688869671278382,
      "eval_runtime": 7.3936,
      "eval_samples_per_second": 2916.018,
      "eval_spearmanr": 0.38128182412272665,
      "eval_steps_per_second": 11.496,
      "step": 32130
    },
    {
      "epoch": 10.209656925031766,
      "grad_norm": 2.5269463062286377,
      "learning_rate": 2e-05,
      "loss": 0.116,
      "step": 32140
    },
    {
      "epoch": 10.212833545108005,
      "grad_norm": 2.5941081047058105,
      "learning_rate": 2e-05,
      "loss": 0.1037,
      "step": 32150
    },
    {
      "epoch": 10.216010165184244,
      "grad_norm": 3.3410613536834717,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 32160
    },
    {
      "epoch": 10.219186785260483,
      "grad_norm": 4.217228412628174,
      "learning_rate": 2e-05,
      "loss": 0.113,
      "step": 32170
    },
    {
      "epoch": 10.222363405336722,
      "grad_norm": 3.2193119525909424,
      "learning_rate": 2e-05,
      "loss": 0.1085,
      "step": 32180
    },
    {
      "epoch": 10.225540025412961,
      "grad_norm": 8.285914421081543,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 32190
    },
    {
      "epoch": 10.2287166454892,
      "grad_norm": 2.4007935523986816,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 32200
    },
    {
      "epoch": 10.231893265565438,
      "grad_norm": 3.3223047256469727,
      "learning_rate": 2e-05,
      "loss": 0.1051,
      "step": 32210
    },
    {
      "epoch": 10.235069885641677,
      "grad_norm": 2.851698160171509,
      "learning_rate": 2e-05,
      "loss": 0.1084,
      "step": 32220
    },
    {
      "epoch": 10.238246505717916,
      "grad_norm": 3.034820795059204,
      "learning_rate": 2e-05,
      "loss": 0.1057,
      "step": 32230
    },
    {
      "epoch": 10.241423125794155,
      "grad_norm": 2.597898483276367,
      "learning_rate": 2e-05,
      "loss": 0.1038,
      "step": 32240
    },
    {
      "epoch": 10.244599745870394,
      "grad_norm": 2.3747048377990723,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 32250
    },
    {
      "epoch": 10.247776365946633,
      "grad_norm": 2.547576904296875,
      "learning_rate": 2e-05,
      "loss": 0.1099,
      "step": 32260
    },
    {
      "epoch": 10.250952986022872,
      "grad_norm": 3.931427001953125,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 32270
    },
    {
      "epoch": 10.254129606099111,
      "grad_norm": 3.20410418510437,
      "learning_rate": 2e-05,
      "loss": 0.1099,
      "step": 32280
    },
    {
      "epoch": 10.25730622617535,
      "grad_norm": 2.8389174938201904,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 32290
    },
    {
      "epoch": 10.260482846251588,
      "grad_norm": 4.183247089385986,
      "learning_rate": 2e-05,
      "loss": 0.1122,
      "step": 32300
    },
    {
      "epoch": 10.263659466327827,
      "grad_norm": 2.2174482345581055,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 32310
    },
    {
      "epoch": 10.266836086404066,
      "grad_norm": 4.778441905975342,
      "learning_rate": 2e-05,
      "loss": 0.118,
      "step": 32320
    },
    {
      "epoch": 10.270012706480305,
      "grad_norm": 2.4329135417938232,
      "learning_rate": 2e-05,
      "loss": 0.1231,
      "step": 32330
    },
    {
      "epoch": 10.273189326556544,
      "grad_norm": 2.9597420692443848,
      "learning_rate": 2e-05,
      "loss": 0.1133,
      "step": 32340
    },
    {
      "epoch": 10.276365946632783,
      "grad_norm": 2.301130533218384,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 32350
    },
    {
      "epoch": 10.279542566709022,
      "grad_norm": 2.2900636196136475,
      "learning_rate": 2e-05,
      "loss": 0.1029,
      "step": 32360
    },
    {
      "epoch": 10.282719186785261,
      "grad_norm": 3.1326308250427246,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 32370
    },
    {
      "epoch": 10.285895806861499,
      "grad_norm": 3.4088857173919678,
      "learning_rate": 2e-05,
      "loss": 0.1018,
      "step": 32380
    },
    {
      "epoch": 10.289072426937738,
      "grad_norm": 2.366112232208252,
      "learning_rate": 2e-05,
      "loss": 0.1045,
      "step": 32390
    },
    {
      "epoch": 10.292249047013977,
      "grad_norm": 5.851408004760742,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 32400
    },
    {
      "epoch": 10.295425667090216,
      "grad_norm": 1.9986672401428223,
      "learning_rate": 2e-05,
      "loss": 0.1079,
      "step": 32410
    },
    {
      "epoch": 10.298602287166455,
      "grad_norm": 2.6826069355010986,
      "learning_rate": 2e-05,
      "loss": 0.1115,
      "step": 32420
    },
    {
      "epoch": 10.301778907242694,
      "grad_norm": 3.8573756217956543,
      "learning_rate": 2e-05,
      "loss": 0.1064,
      "step": 32430
    },
    {
      "epoch": 10.304955527318933,
      "grad_norm": 2.74251389503479,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 32440
    },
    {
      "epoch": 10.306543837357053,
      "eval_loss": 1.845345139503479,
      "eval_mse": 1.8438975995347762,
      "eval_pearson": 0.3909386294045329,
      "eval_runtime": 7.4777,
      "eval_samples_per_second": 2883.255,
      "eval_spearmanr": 0.39909694272574614,
      "eval_steps_per_second": 11.367,
      "step": 32445
    },
    {
      "epoch": 10.308132147395172,
      "grad_norm": 1.9066582918167114,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 32450
    },
    {
      "epoch": 10.311308767471411,
      "grad_norm": 4.101150989532471,
      "learning_rate": 2e-05,
      "loss": 0.1123,
      "step": 32460
    },
    {
      "epoch": 10.314485387547649,
      "grad_norm": 4.239230632781982,
      "learning_rate": 2e-05,
      "loss": 0.1057,
      "step": 32470
    },
    {
      "epoch": 10.317662007623888,
      "grad_norm": 3.5010790824890137,
      "learning_rate": 2e-05,
      "loss": 0.1153,
      "step": 32480
    },
    {
      "epoch": 10.320838627700127,
      "grad_norm": 5.402279376983643,
      "learning_rate": 2e-05,
      "loss": 0.1157,
      "step": 32490
    },
    {
      "epoch": 10.324015247776366,
      "grad_norm": 2.9668376445770264,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 32500
    },
    {
      "epoch": 10.327191867852605,
      "grad_norm": 2.908139228820801,
      "learning_rate": 2e-05,
      "loss": 0.1092,
      "step": 32510
    },
    {
      "epoch": 10.330368487928844,
      "grad_norm": 3.305955648422241,
      "learning_rate": 2e-05,
      "loss": 0.1084,
      "step": 32520
    },
    {
      "epoch": 10.333545108005083,
      "grad_norm": 2.7157835960388184,
      "learning_rate": 2e-05,
      "loss": 0.1082,
      "step": 32530
    },
    {
      "epoch": 10.336721728081322,
      "grad_norm": 4.690793514251709,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 32540
    },
    {
      "epoch": 10.339898348157561,
      "grad_norm": 2.935931444168091,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 32550
    },
    {
      "epoch": 10.343074968233799,
      "grad_norm": 2.9103682041168213,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 32560
    },
    {
      "epoch": 10.346251588310038,
      "grad_norm": 2.667065382003784,
      "learning_rate": 2e-05,
      "loss": 0.1164,
      "step": 32570
    },
    {
      "epoch": 10.349428208386277,
      "grad_norm": 1.7646093368530273,
      "learning_rate": 2e-05,
      "loss": 0.1046,
      "step": 32580
    },
    {
      "epoch": 10.352604828462516,
      "grad_norm": 3.3276925086975098,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 32590
    },
    {
      "epoch": 10.355781448538755,
      "grad_norm": 3.504940986633301,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 32600
    },
    {
      "epoch": 10.358958068614994,
      "grad_norm": 5.943277359008789,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 32610
    },
    {
      "epoch": 10.362134688691233,
      "grad_norm": 2.282909631729126,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 32620
    },
    {
      "epoch": 10.365311308767472,
      "grad_norm": 3.2017643451690674,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 32630
    },
    {
      "epoch": 10.36848792884371,
      "grad_norm": 2.8335931301116943,
      "learning_rate": 2e-05,
      "loss": 0.1025,
      "step": 32640
    },
    {
      "epoch": 10.371664548919949,
      "grad_norm": 2.8192083835601807,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 32650
    },
    {
      "epoch": 10.374841168996188,
      "grad_norm": 4.476767063140869,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 32660
    },
    {
      "epoch": 10.378017789072427,
      "grad_norm": 4.179440975189209,
      "learning_rate": 2e-05,
      "loss": 0.1071,
      "step": 32670
    },
    {
      "epoch": 10.381194409148666,
      "grad_norm": 4.113686561584473,
      "learning_rate": 2e-05,
      "loss": 0.0973,
      "step": 32680
    },
    {
      "epoch": 10.384371029224905,
      "grad_norm": 5.9718146324157715,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 32690
    },
    {
      "epoch": 10.387547649301144,
      "grad_norm": 3.58259654045105,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 32700
    },
    {
      "epoch": 10.390724269377383,
      "grad_norm": 2.348351240158081,
      "learning_rate": 2e-05,
      "loss": 0.119,
      "step": 32710
    },
    {
      "epoch": 10.393900889453622,
      "grad_norm": 3.9967336654663086,
      "learning_rate": 2e-05,
      "loss": 0.11,
      "step": 32720
    },
    {
      "epoch": 10.39707750952986,
      "grad_norm": 2.9004602432250977,
      "learning_rate": 2e-05,
      "loss": 0.106,
      "step": 32730
    },
    {
      "epoch": 10.400254129606099,
      "grad_norm": 3.7328855991363525,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 32740
    },
    {
      "epoch": 10.403430749682338,
      "grad_norm": 2.7046310901641846,
      "learning_rate": 2e-05,
      "loss": 0.1081,
      "step": 32750
    },
    {
      "epoch": 10.406607369758577,
      "grad_norm": 2.602555990219116,
      "learning_rate": 2e-05,
      "loss": 0.1048,
      "step": 32760
    },
    {
      "epoch": 10.406607369758577,
      "eval_loss": 1.862220287322998,
      "eval_mse": 1.8608567267213991,
      "eval_pearson": 0.3979862790282978,
      "eval_runtime": 7.5774,
      "eval_samples_per_second": 2845.317,
      "eval_spearmanr": 0.4072944559774084,
      "eval_steps_per_second": 11.218,
      "step": 32760
    },
    {
      "epoch": 10.409783989834816,
      "grad_norm": 2.3552935123443604,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 32770
    },
    {
      "epoch": 10.412960609911055,
      "grad_norm": 2.0539989471435547,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 32780
    },
    {
      "epoch": 10.416137229987294,
      "grad_norm": 3.658625841140747,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 32790
    },
    {
      "epoch": 10.419313850063533,
      "grad_norm": 3.308359384536743,
      "learning_rate": 2e-05,
      "loss": 0.1049,
      "step": 32800
    },
    {
      "epoch": 10.42249047013977,
      "grad_norm": 3.2764806747436523,
      "learning_rate": 2e-05,
      "loss": 0.1055,
      "step": 32810
    },
    {
      "epoch": 10.42566709021601,
      "grad_norm": 2.3533682823181152,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 32820
    },
    {
      "epoch": 10.428843710292249,
      "grad_norm": 2.228908061981201,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 32830
    },
    {
      "epoch": 10.432020330368488,
      "grad_norm": 2.551907777786255,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 32840
    },
    {
      "epoch": 10.435196950444727,
      "grad_norm": 11.364213943481445,
      "learning_rate": 2e-05,
      "loss": 0.1021,
      "step": 32850
    },
    {
      "epoch": 10.438373570520966,
      "grad_norm": 4.186465263366699,
      "learning_rate": 2e-05,
      "loss": 0.1025,
      "step": 32860
    },
    {
      "epoch": 10.441550190597205,
      "grad_norm": 2.360116958618164,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 32870
    },
    {
      "epoch": 10.444726810673444,
      "grad_norm": 3.1419286727905273,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 32880
    },
    {
      "epoch": 10.447903430749683,
      "grad_norm": 2.642275333404541,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 32890
    },
    {
      "epoch": 10.45108005082592,
      "grad_norm": 3.30180287361145,
      "learning_rate": 2e-05,
      "loss": 0.1114,
      "step": 32900
    },
    {
      "epoch": 10.45425667090216,
      "grad_norm": 3.0635082721710205,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 32910
    },
    {
      "epoch": 10.457433290978399,
      "grad_norm": 2.0237274169921875,
      "learning_rate": 2e-05,
      "loss": 0.1116,
      "step": 32920
    },
    {
      "epoch": 10.460609911054638,
      "grad_norm": 2.999779462814331,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 32930
    },
    {
      "epoch": 10.463786531130877,
      "grad_norm": 4.177590847015381,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 32940
    },
    {
      "epoch": 10.466963151207116,
      "grad_norm": 2.3897221088409424,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 32950
    },
    {
      "epoch": 10.470139771283355,
      "grad_norm": 2.118075370788574,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 32960
    },
    {
      "epoch": 10.473316391359594,
      "grad_norm": 2.9075136184692383,
      "learning_rate": 2e-05,
      "loss": 0.1073,
      "step": 32970
    },
    {
      "epoch": 10.476493011435831,
      "grad_norm": 2.2543790340423584,
      "learning_rate": 2e-05,
      "loss": 0.1081,
      "step": 32980
    },
    {
      "epoch": 10.47966963151207,
      "grad_norm": 2.3139803409576416,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 32990
    },
    {
      "epoch": 10.48284625158831,
      "grad_norm": 3.52242112159729,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 33000
    },
    {
      "epoch": 10.486022871664549,
      "grad_norm": 2.6604819297790527,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 33010
    },
    {
      "epoch": 10.489199491740788,
      "grad_norm": 3.8958120346069336,
      "learning_rate": 2e-05,
      "loss": 0.1178,
      "step": 33020
    },
    {
      "epoch": 10.492376111817027,
      "grad_norm": 2.5005311965942383,
      "learning_rate": 2e-05,
      "loss": 0.1074,
      "step": 33030
    },
    {
      "epoch": 10.495552731893266,
      "grad_norm": 2.6002111434936523,
      "learning_rate": 2e-05,
      "loss": 0.1092,
      "step": 33040
    },
    {
      "epoch": 10.498729351969505,
      "grad_norm": 2.710981845855713,
      "learning_rate": 2e-05,
      "loss": 0.1012,
      "step": 33050
    },
    {
      "epoch": 10.501905972045744,
      "grad_norm": 4.013445854187012,
      "learning_rate": 2e-05,
      "loss": 0.0998,
      "step": 33060
    },
    {
      "epoch": 10.505082592121981,
      "grad_norm": 2.909619092941284,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 33070
    },
    {
      "epoch": 10.506670902160101,
      "eval_loss": 1.8883254528045654,
      "eval_mse": 1.8873254517898488,
      "eval_pearson": 0.3723162179854931,
      "eval_runtime": 7.4734,
      "eval_samples_per_second": 2884.911,
      "eval_spearmanr": 0.3791297264281207,
      "eval_steps_per_second": 11.374,
      "step": 33075
    },
    {
      "epoch": 10.50825921219822,
      "grad_norm": 4.852637767791748,
      "learning_rate": 2e-05,
      "loss": 0.1084,
      "step": 33080
    },
    {
      "epoch": 10.51143583227446,
      "grad_norm": 4.284929275512695,
      "learning_rate": 2e-05,
      "loss": 0.1173,
      "step": 33090
    },
    {
      "epoch": 10.514612452350699,
      "grad_norm": 4.24368143081665,
      "learning_rate": 2e-05,
      "loss": 0.1066,
      "step": 33100
    },
    {
      "epoch": 10.517789072426938,
      "grad_norm": 4.718623161315918,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 33110
    },
    {
      "epoch": 10.520965692503177,
      "grad_norm": 2.7009613513946533,
      "learning_rate": 2e-05,
      "loss": 0.1095,
      "step": 33120
    },
    {
      "epoch": 10.524142312579416,
      "grad_norm": 2.464420795440674,
      "learning_rate": 2e-05,
      "loss": 0.1029,
      "step": 33130
    },
    {
      "epoch": 10.527318932655655,
      "grad_norm": 3.8780882358551025,
      "learning_rate": 2e-05,
      "loss": 0.0961,
      "step": 33140
    },
    {
      "epoch": 10.530495552731892,
      "grad_norm": 3.843501329421997,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 33150
    },
    {
      "epoch": 10.533672172808132,
      "grad_norm": 3.061279296875,
      "learning_rate": 2e-05,
      "loss": 0.108,
      "step": 33160
    },
    {
      "epoch": 10.53684879288437,
      "grad_norm": 2.8700907230377197,
      "learning_rate": 2e-05,
      "loss": 0.1147,
      "step": 33170
    },
    {
      "epoch": 10.54002541296061,
      "grad_norm": 3.467053174972534,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 33180
    },
    {
      "epoch": 10.543202033036849,
      "grad_norm": 5.223871231079102,
      "learning_rate": 2e-05,
      "loss": 0.1055,
      "step": 33190
    },
    {
      "epoch": 10.546378653113088,
      "grad_norm": 5.410841464996338,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 33200
    },
    {
      "epoch": 10.549555273189327,
      "grad_norm": 3.3795547485351562,
      "learning_rate": 2e-05,
      "loss": 0.1062,
      "step": 33210
    },
    {
      "epoch": 10.552731893265566,
      "grad_norm": 3.3931901454925537,
      "learning_rate": 2e-05,
      "loss": 0.1121,
      "step": 33220
    },
    {
      "epoch": 10.555908513341805,
      "grad_norm": 2.846184730529785,
      "learning_rate": 2e-05,
      "loss": 0.1074,
      "step": 33230
    },
    {
      "epoch": 10.559085133418042,
      "grad_norm": 4.252307891845703,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 33240
    },
    {
      "epoch": 10.562261753494282,
      "grad_norm": 3.927725076675415,
      "learning_rate": 2e-05,
      "loss": 0.1159,
      "step": 33250
    },
    {
      "epoch": 10.56543837357052,
      "grad_norm": 2.560758352279663,
      "learning_rate": 2e-05,
      "loss": 0.1049,
      "step": 33260
    },
    {
      "epoch": 10.56861499364676,
      "grad_norm": 3.328190565109253,
      "learning_rate": 2e-05,
      "loss": 0.1168,
      "step": 33270
    },
    {
      "epoch": 10.571791613722999,
      "grad_norm": 3.5767412185668945,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 33280
    },
    {
      "epoch": 10.574968233799238,
      "grad_norm": 4.087185382843018,
      "learning_rate": 2e-05,
      "loss": 0.1053,
      "step": 33290
    },
    {
      "epoch": 10.578144853875477,
      "grad_norm": 2.7329652309417725,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 33300
    },
    {
      "epoch": 10.581321473951716,
      "grad_norm": 3.5406997203826904,
      "learning_rate": 2e-05,
      "loss": 0.1032,
      "step": 33310
    },
    {
      "epoch": 10.584498094027953,
      "grad_norm": 2.910386562347412,
      "learning_rate": 2e-05,
      "loss": 0.1105,
      "step": 33320
    },
    {
      "epoch": 10.587674714104192,
      "grad_norm": 2.6946804523468018,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 33330
    },
    {
      "epoch": 10.590851334180432,
      "grad_norm": 3.297851324081421,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 33340
    },
    {
      "epoch": 10.59402795425667,
      "grad_norm": 2.9156150817871094,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 33350
    },
    {
      "epoch": 10.59720457433291,
      "grad_norm": 2.6050376892089844,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 33360
    },
    {
      "epoch": 10.600381194409149,
      "grad_norm": 1.8970786333084106,
      "learning_rate": 2e-05,
      "loss": 0.1118,
      "step": 33370
    },
    {
      "epoch": 10.603557814485388,
      "grad_norm": 4.6497111320495605,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 33380
    },
    {
      "epoch": 10.606734434561627,
      "grad_norm": 3.6567063331604004,
      "learning_rate": 2e-05,
      "loss": 0.1091,
      "step": 33390
    },
    {
      "epoch": 10.606734434561627,
      "eval_loss": 1.8686304092407227,
      "eval_mse": 1.8673916628251819,
      "eval_pearson": 0.39993309606433003,
      "eval_runtime": 7.2811,
      "eval_samples_per_second": 2961.088,
      "eval_spearmanr": 0.40823937530466325,
      "eval_steps_per_second": 11.674,
      "step": 33390
    },
    {
      "epoch": 10.609911054637866,
      "grad_norm": 3.670647621154785,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 33400
    },
    {
      "epoch": 10.613087674714103,
      "grad_norm": 3.0896668434143066,
      "learning_rate": 2e-05,
      "loss": 0.1039,
      "step": 33410
    },
    {
      "epoch": 10.616264294790343,
      "grad_norm": 2.3583455085754395,
      "learning_rate": 2e-05,
      "loss": 0.1126,
      "step": 33420
    },
    {
      "epoch": 10.619440914866582,
      "grad_norm": 2.6181304454803467,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 33430
    },
    {
      "epoch": 10.62261753494282,
      "grad_norm": 4.787382125854492,
      "learning_rate": 2e-05,
      "loss": 0.105,
      "step": 33440
    },
    {
      "epoch": 10.62579415501906,
      "grad_norm": 5.048530101776123,
      "learning_rate": 2e-05,
      "loss": 0.1135,
      "step": 33450
    },
    {
      "epoch": 10.628970775095299,
      "grad_norm": 4.253851890563965,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 33460
    },
    {
      "epoch": 10.632147395171538,
      "grad_norm": 2.8108866214752197,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 33470
    },
    {
      "epoch": 10.635324015247777,
      "grad_norm": 3.2568700313568115,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 33480
    },
    {
      "epoch": 10.638500635324014,
      "grad_norm": 2.7698283195495605,
      "learning_rate": 2e-05,
      "loss": 0.1063,
      "step": 33490
    },
    {
      "epoch": 10.641677255400253,
      "grad_norm": 2.9372658729553223,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 33500
    },
    {
      "epoch": 10.644853875476493,
      "grad_norm": 2.236917734146118,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 33510
    },
    {
      "epoch": 10.648030495552732,
      "grad_norm": 4.415950775146484,
      "learning_rate": 2e-05,
      "loss": 0.1109,
      "step": 33520
    },
    {
      "epoch": 10.65120711562897,
      "grad_norm": 3.131934404373169,
      "learning_rate": 2e-05,
      "loss": 0.1032,
      "step": 33530
    },
    {
      "epoch": 10.65438373570521,
      "grad_norm": 3.1897523403167725,
      "learning_rate": 2e-05,
      "loss": 0.1081,
      "step": 33540
    },
    {
      "epoch": 10.657560355781449,
      "grad_norm": 5.304138660430908,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 33550
    },
    {
      "epoch": 10.660736975857688,
      "grad_norm": 5.223209857940674,
      "learning_rate": 2e-05,
      "loss": 0.1062,
      "step": 33560
    },
    {
      "epoch": 10.663913595933927,
      "grad_norm": 3.7869162559509277,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 33570
    },
    {
      "epoch": 10.667090216010164,
      "grad_norm": 2.5468430519104004,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 33580
    },
    {
      "epoch": 10.670266836086403,
      "grad_norm": 3.3187272548675537,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 33590
    },
    {
      "epoch": 10.673443456162643,
      "grad_norm": 4.1745758056640625,
      "learning_rate": 2e-05,
      "loss": 0.1043,
      "step": 33600
    },
    {
      "epoch": 10.676620076238882,
      "grad_norm": 2.8146536350250244,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 33610
    },
    {
      "epoch": 10.67979669631512,
      "grad_norm": 4.647152423858643,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 33620
    },
    {
      "epoch": 10.68297331639136,
      "grad_norm": 3.1835315227508545,
      "learning_rate": 2e-05,
      "loss": 0.1107,
      "step": 33630
    },
    {
      "epoch": 10.686149936467599,
      "grad_norm": 3.049598217010498,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 33640
    },
    {
      "epoch": 10.689326556543838,
      "grad_norm": 3.431847333908081,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 33650
    },
    {
      "epoch": 10.692503176620075,
      "grad_norm": 3.972816228866577,
      "learning_rate": 2e-05,
      "loss": 0.1062,
      "step": 33660
    },
    {
      "epoch": 10.695679796696314,
      "grad_norm": 2.2035207748413086,
      "learning_rate": 2e-05,
      "loss": 0.1097,
      "step": 33670
    },
    {
      "epoch": 10.698856416772554,
      "grad_norm": 3.239948272705078,
      "learning_rate": 2e-05,
      "loss": 0.1069,
      "step": 33680
    },
    {
      "epoch": 10.702033036848793,
      "grad_norm": 2.7631185054779053,
      "learning_rate": 2e-05,
      "loss": 0.1029,
      "step": 33690
    },
    {
      "epoch": 10.705209656925032,
      "grad_norm": 2.434256076812744,
      "learning_rate": 2e-05,
      "loss": 0.1113,
      "step": 33700
    },
    {
      "epoch": 10.706797966963151,
      "eval_loss": 1.8395049571990967,
      "eval_mse": 1.8380248698653008,
      "eval_pearson": 0.383644375760219,
      "eval_runtime": 7.4797,
      "eval_samples_per_second": 2882.467,
      "eval_spearmanr": 0.3944554781982155,
      "eval_steps_per_second": 11.364,
      "step": 33705
    },
    {
      "epoch": 10.70838627700127,
      "grad_norm": 3.423076868057251,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 33710
    },
    {
      "epoch": 10.71156289707751,
      "grad_norm": 2.687999963760376,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 33720
    },
    {
      "epoch": 10.714739517153749,
      "grad_norm": 4.233133792877197,
      "learning_rate": 2e-05,
      "loss": 0.1115,
      "step": 33730
    },
    {
      "epoch": 10.717916137229988,
      "grad_norm": 3.6952764987945557,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 33740
    },
    {
      "epoch": 10.721092757306225,
      "grad_norm": 2.0805962085723877,
      "learning_rate": 2e-05,
      "loss": 0.1167,
      "step": 33750
    },
    {
      "epoch": 10.724269377382464,
      "grad_norm": 2.195761203765869,
      "learning_rate": 2e-05,
      "loss": 0.1071,
      "step": 33760
    },
    {
      "epoch": 10.727445997458704,
      "grad_norm": 4.005809783935547,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 33770
    },
    {
      "epoch": 10.730622617534943,
      "grad_norm": 4.267279624938965,
      "learning_rate": 2e-05,
      "loss": 0.121,
      "step": 33780
    },
    {
      "epoch": 10.733799237611182,
      "grad_norm": 5.574039459228516,
      "learning_rate": 2e-05,
      "loss": 0.1178,
      "step": 33790
    },
    {
      "epoch": 10.73697585768742,
      "grad_norm": 3.8711516857147217,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 33800
    },
    {
      "epoch": 10.74015247776366,
      "grad_norm": 2.178058624267578,
      "learning_rate": 2e-05,
      "loss": 0.113,
      "step": 33810
    },
    {
      "epoch": 10.743329097839899,
      "grad_norm": 2.154386520385742,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 33820
    },
    {
      "epoch": 10.746505717916136,
      "grad_norm": 2.712977170944214,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 33830
    },
    {
      "epoch": 10.749682337992375,
      "grad_norm": 2.4152028560638428,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 33840
    },
    {
      "epoch": 10.752858958068614,
      "grad_norm": 3.8888964653015137,
      "learning_rate": 2e-05,
      "loss": 0.1214,
      "step": 33850
    },
    {
      "epoch": 10.756035578144854,
      "grad_norm": 7.823637008666992,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 33860
    },
    {
      "epoch": 10.759212198221093,
      "grad_norm": 2.7042055130004883,
      "learning_rate": 2e-05,
      "loss": 0.1096,
      "step": 33870
    },
    {
      "epoch": 10.762388818297332,
      "grad_norm": 2.553318500518799,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 33880
    },
    {
      "epoch": 10.76556543837357,
      "grad_norm": 2.1820626258850098,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 33890
    },
    {
      "epoch": 10.76874205844981,
      "grad_norm": 2.995359182357788,
      "learning_rate": 2e-05,
      "loss": 0.1156,
      "step": 33900
    },
    {
      "epoch": 10.771918678526049,
      "grad_norm": 2.1614091396331787,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 33910
    },
    {
      "epoch": 10.775095298602288,
      "grad_norm": 2.3821537494659424,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 33920
    },
    {
      "epoch": 10.778271918678525,
      "grad_norm": 2.162046194076538,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 33930
    },
    {
      "epoch": 10.781448538754764,
      "grad_norm": 3.9249987602233887,
      "learning_rate": 2e-05,
      "loss": 0.1046,
      "step": 33940
    },
    {
      "epoch": 10.784625158831004,
      "grad_norm": 2.229039430618286,
      "learning_rate": 2e-05,
      "loss": 0.115,
      "step": 33950
    },
    {
      "epoch": 10.787801778907243,
      "grad_norm": 3.2481231689453125,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 33960
    },
    {
      "epoch": 10.790978398983482,
      "grad_norm": 2.989020586013794,
      "learning_rate": 2e-05,
      "loss": 0.1162,
      "step": 33970
    },
    {
      "epoch": 10.79415501905972,
      "grad_norm": 2.313910484313965,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 33980
    },
    {
      "epoch": 10.79733163913596,
      "grad_norm": 4.135939121246338,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 33990
    },
    {
      "epoch": 10.800508259212199,
      "grad_norm": 2.2778878211975098,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 34000
    },
    {
      "epoch": 10.803684879288436,
      "grad_norm": 1.9266328811645508,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 34010
    },
    {
      "epoch": 10.806861499364675,
      "grad_norm": 3.437398910522461,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 34020
    },
    {
      "epoch": 10.806861499364675,
      "eval_loss": 1.88064706325531,
      "eval_mse": 1.8789422084537641,
      "eval_pearson": 0.39513995459099926,
      "eval_runtime": 7.5878,
      "eval_samples_per_second": 2841.387,
      "eval_spearmanr": 0.4074544742967335,
      "eval_steps_per_second": 11.202,
      "step": 34020
    },
    {
      "epoch": 10.810038119440915,
      "grad_norm": 2.91501784324646,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 34030
    },
    {
      "epoch": 10.813214739517154,
      "grad_norm": 1.8411685228347778,
      "learning_rate": 2e-05,
      "loss": 0.1027,
      "step": 34040
    },
    {
      "epoch": 10.816391359593393,
      "grad_norm": 3.1304821968078613,
      "learning_rate": 2e-05,
      "loss": 0.1185,
      "step": 34050
    },
    {
      "epoch": 10.819567979669632,
      "grad_norm": 5.971736907958984,
      "learning_rate": 2e-05,
      "loss": 0.1072,
      "step": 34060
    },
    {
      "epoch": 10.82274459974587,
      "grad_norm": 3.6490228176116943,
      "learning_rate": 2e-05,
      "loss": 0.1074,
      "step": 34070
    },
    {
      "epoch": 10.82592121982211,
      "grad_norm": 2.248307943344116,
      "learning_rate": 2e-05,
      "loss": 0.1025,
      "step": 34080
    },
    {
      "epoch": 10.829097839898349,
      "grad_norm": 2.1858303546905518,
      "learning_rate": 2e-05,
      "loss": 0.1004,
      "step": 34090
    },
    {
      "epoch": 10.832274459974586,
      "grad_norm": 1.9844586849212646,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 34100
    },
    {
      "epoch": 10.835451080050825,
      "grad_norm": 4.626306533813477,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 34110
    },
    {
      "epoch": 10.838627700127065,
      "grad_norm": 3.416733741760254,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 34120
    },
    {
      "epoch": 10.841804320203304,
      "grad_norm": 2.338575601577759,
      "learning_rate": 2e-05,
      "loss": 0.1079,
      "step": 34130
    },
    {
      "epoch": 10.844980940279543,
      "grad_norm": 2.3686795234680176,
      "learning_rate": 2e-05,
      "loss": 0.0977,
      "step": 34140
    },
    {
      "epoch": 10.848157560355782,
      "grad_norm": 3.8644514083862305,
      "learning_rate": 2e-05,
      "loss": 0.1094,
      "step": 34150
    },
    {
      "epoch": 10.851334180432021,
      "grad_norm": 1.6956384181976318,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 34160
    },
    {
      "epoch": 10.85451080050826,
      "grad_norm": 4.120748043060303,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 34170
    },
    {
      "epoch": 10.857687420584497,
      "grad_norm": 4.363735198974609,
      "learning_rate": 2e-05,
      "loss": 0.1045,
      "step": 34180
    },
    {
      "epoch": 10.860864040660736,
      "grad_norm": 2.9333605766296387,
      "learning_rate": 2e-05,
      "loss": 0.1165,
      "step": 34190
    },
    {
      "epoch": 10.864040660736975,
      "grad_norm": 2.463531017303467,
      "learning_rate": 2e-05,
      "loss": 0.1044,
      "step": 34200
    },
    {
      "epoch": 10.867217280813215,
      "grad_norm": 2.877392530441284,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 34210
    },
    {
      "epoch": 10.870393900889454,
      "grad_norm": 2.307856559753418,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 34220
    },
    {
      "epoch": 10.873570520965693,
      "grad_norm": 2.633066177368164,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 34230
    },
    {
      "epoch": 10.876747141041932,
      "grad_norm": 4.436201095581055,
      "learning_rate": 2e-05,
      "loss": 0.1119,
      "step": 34240
    },
    {
      "epoch": 10.879923761118171,
      "grad_norm": 2.733599901199341,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 34250
    },
    {
      "epoch": 10.88310038119441,
      "grad_norm": 2.4686455726623535,
      "learning_rate": 2e-05,
      "loss": 0.1111,
      "step": 34260
    },
    {
      "epoch": 10.886277001270647,
      "grad_norm": 6.9416399002075195,
      "learning_rate": 2e-05,
      "loss": 0.1115,
      "step": 34270
    },
    {
      "epoch": 10.889453621346886,
      "grad_norm": 12.513123512268066,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 34280
    },
    {
      "epoch": 10.892630241423126,
      "grad_norm": 2.7833476066589355,
      "learning_rate": 2e-05,
      "loss": 0.109,
      "step": 34290
    },
    {
      "epoch": 10.895806861499365,
      "grad_norm": 2.8872737884521484,
      "learning_rate": 2e-05,
      "loss": 0.1032,
      "step": 34300
    },
    {
      "epoch": 10.898983481575604,
      "grad_norm": 3.949347972869873,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 34310
    },
    {
      "epoch": 10.902160101651843,
      "grad_norm": 4.023618698120117,
      "learning_rate": 2e-05,
      "loss": 0.1106,
      "step": 34320
    },
    {
      "epoch": 10.905336721728082,
      "grad_norm": 2.2635951042175293,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 34330
    },
    {
      "epoch": 10.906925031766201,
      "eval_loss": 1.841751217842102,
      "eval_mse": 1.840180252433996,
      "eval_pearson": 0.3910571748322121,
      "eval_runtime": 7.3982,
      "eval_samples_per_second": 2914.204,
      "eval_spearmanr": 0.3951397271847759,
      "eval_steps_per_second": 11.489,
      "step": 34335
    },
    {
      "epoch": 10.908513341804321,
      "grad_norm": 2.49153995513916,
      "learning_rate": 2e-05,
      "loss": 0.0937,
      "step": 34340
    },
    {
      "epoch": 10.911689961880558,
      "grad_norm": 2.7425882816314697,
      "learning_rate": 2e-05,
      "loss": 0.1107,
      "step": 34350
    },
    {
      "epoch": 10.914866581956797,
      "grad_norm": 4.50364875793457,
      "learning_rate": 2e-05,
      "loss": 0.1078,
      "step": 34360
    },
    {
      "epoch": 10.918043202033036,
      "grad_norm": 3.0348854064941406,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 34370
    },
    {
      "epoch": 10.921219822109276,
      "grad_norm": 3.7392311096191406,
      "learning_rate": 2e-05,
      "loss": 0.1102,
      "step": 34380
    },
    {
      "epoch": 10.924396442185515,
      "grad_norm": 3.418550729751587,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 34390
    },
    {
      "epoch": 10.927573062261754,
      "grad_norm": 2.7106995582580566,
      "learning_rate": 2e-05,
      "loss": 0.1077,
      "step": 34400
    },
    {
      "epoch": 10.930749682337993,
      "grad_norm": 2.175947904586792,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 34410
    },
    {
      "epoch": 10.933926302414232,
      "grad_norm": 2.4633545875549316,
      "learning_rate": 2e-05,
      "loss": 0.1047,
      "step": 34420
    },
    {
      "epoch": 10.937102922490471,
      "grad_norm": 2.0066306591033936,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 34430
    },
    {
      "epoch": 10.940279542566708,
      "grad_norm": 2.9633145332336426,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 34440
    },
    {
      "epoch": 10.943456162642947,
      "grad_norm": 2.3393471240997314,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 34450
    },
    {
      "epoch": 10.946632782719186,
      "grad_norm": 2.9943270683288574,
      "learning_rate": 2e-05,
      "loss": 0.1019,
      "step": 34460
    },
    {
      "epoch": 10.949809402795426,
      "grad_norm": 2.2069289684295654,
      "learning_rate": 2e-05,
      "loss": 0.1158,
      "step": 34470
    },
    {
      "epoch": 10.952986022871665,
      "grad_norm": 3.1613235473632812,
      "learning_rate": 2e-05,
      "loss": 0.1014,
      "step": 34480
    },
    {
      "epoch": 10.956162642947904,
      "grad_norm": 1.9031935930252075,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 34490
    },
    {
      "epoch": 10.959339263024143,
      "grad_norm": 3.1325864791870117,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 34500
    },
    {
      "epoch": 10.962515883100382,
      "grad_norm": 3.264453887939453,
      "learning_rate": 2e-05,
      "loss": 0.104,
      "step": 34510
    },
    {
      "epoch": 10.96569250317662,
      "grad_norm": 5.750155925750732,
      "learning_rate": 2e-05,
      "loss": 0.1141,
      "step": 34520
    },
    {
      "epoch": 10.968869123252858,
      "grad_norm": 3.4569318294525146,
      "learning_rate": 2e-05,
      "loss": 0.1067,
      "step": 34530
    },
    {
      "epoch": 10.972045743329097,
      "grad_norm": 3.67686128616333,
      "learning_rate": 2e-05,
      "loss": 0.1056,
      "step": 34540
    },
    {
      "epoch": 10.975222363405337,
      "grad_norm": 2.459996223449707,
      "learning_rate": 2e-05,
      "loss": 0.1045,
      "step": 34550
    },
    {
      "epoch": 10.978398983481576,
      "grad_norm": 3.5346765518188477,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 34560
    },
    {
      "epoch": 10.981575603557815,
      "grad_norm": 3.48895525932312,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 34570
    },
    {
      "epoch": 10.984752223634054,
      "grad_norm": 2.890232801437378,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 34580
    },
    {
      "epoch": 10.987928843710293,
      "grad_norm": 2.8179879188537598,
      "learning_rate": 2e-05,
      "loss": 0.1189,
      "step": 34590
    },
    {
      "epoch": 10.991105463786532,
      "grad_norm": 2.1048479080200195,
      "learning_rate": 2e-05,
      "loss": 0.1071,
      "step": 34600
    },
    {
      "epoch": 10.99428208386277,
      "grad_norm": 2.800159454345703,
      "learning_rate": 2e-05,
      "loss": 0.1,
      "step": 34610
    },
    {
      "epoch": 10.997458703939008,
      "grad_norm": 4.515599250793457,
      "learning_rate": 2e-05,
      "loss": 0.1112,
      "step": 34620
    },
    {
      "epoch": 11.000635324015247,
      "grad_norm": 2.945457935333252,
      "learning_rate": 2e-05,
      "loss": 0.1025,
      "step": 34630
    },
    {
      "epoch": 11.003811944091487,
      "grad_norm": 2.300663948059082,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 34640
    },
    {
      "epoch": 11.006988564167726,
      "grad_norm": 3.64141583442688,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 34650
    },
    {
      "epoch": 11.006988564167726,
      "eval_loss": 1.8592513799667358,
      "eval_mse": 1.8579886245192272,
      "eval_pearson": 0.35493595355991214,
      "eval_runtime": 7.5027,
      "eval_samples_per_second": 2873.648,
      "eval_spearmanr": 0.36105988144015266,
      "eval_steps_per_second": 11.329,
      "step": 34650
    },
    {
      "epoch": 11.010165184243965,
      "grad_norm": 1.8743661642074585,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 34660
    },
    {
      "epoch": 11.013341804320204,
      "grad_norm": 2.4123013019561768,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 34670
    },
    {
      "epoch": 11.016518424396443,
      "grad_norm": 2.0141115188598633,
      "learning_rate": 2e-05,
      "loss": 0.0941,
      "step": 34680
    },
    {
      "epoch": 11.01969504447268,
      "grad_norm": 3.1853504180908203,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 34690
    },
    {
      "epoch": 11.02287166454892,
      "grad_norm": 3.073847770690918,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 34700
    },
    {
      "epoch": 11.026048284625158,
      "grad_norm": 2.894895076751709,
      "learning_rate": 2e-05,
      "loss": 0.0925,
      "step": 34710
    },
    {
      "epoch": 11.029224904701397,
      "grad_norm": 4.210459232330322,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 34720
    },
    {
      "epoch": 11.032401524777637,
      "grad_norm": 3.2909109592437744,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 34730
    },
    {
      "epoch": 11.035578144853876,
      "grad_norm": 2.891496419906616,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 34740
    },
    {
      "epoch": 11.038754764930115,
      "grad_norm": 2.690718412399292,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 34750
    },
    {
      "epoch": 11.041931385006354,
      "grad_norm": 4.025425910949707,
      "learning_rate": 2e-05,
      "loss": 0.0998,
      "step": 34760
    },
    {
      "epoch": 11.045108005082593,
      "grad_norm": 3.012596607208252,
      "learning_rate": 2e-05,
      "loss": 0.094,
      "step": 34770
    },
    {
      "epoch": 11.04828462515883,
      "grad_norm": 2.643200635910034,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 34780
    },
    {
      "epoch": 11.05146124523507,
      "grad_norm": 2.6065282821655273,
      "learning_rate": 2e-05,
      "loss": 0.102,
      "step": 34790
    },
    {
      "epoch": 11.054637865311308,
      "grad_norm": 7.174772262573242,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 34800
    },
    {
      "epoch": 11.057814485387548,
      "grad_norm": 3.7039310932159424,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 34810
    },
    {
      "epoch": 11.060991105463787,
      "grad_norm": 2.7148444652557373,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 34820
    },
    {
      "epoch": 11.064167725540026,
      "grad_norm": 3.2557122707366943,
      "learning_rate": 2e-05,
      "loss": 0.1059,
      "step": 34830
    },
    {
      "epoch": 11.067344345616265,
      "grad_norm": 3.7137162685394287,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 34840
    },
    {
      "epoch": 11.070520965692504,
      "grad_norm": 2.5858840942382812,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 34850
    },
    {
      "epoch": 11.073697585768741,
      "grad_norm": 3.2391395568847656,
      "learning_rate": 2e-05,
      "loss": 0.0963,
      "step": 34860
    },
    {
      "epoch": 11.07687420584498,
      "grad_norm": 3.6077418327331543,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 34870
    },
    {
      "epoch": 11.08005082592122,
      "grad_norm": 2.1285126209259033,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 34880
    },
    {
      "epoch": 11.083227445997458,
      "grad_norm": 2.6063153743743896,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 34890
    },
    {
      "epoch": 11.086404066073698,
      "grad_norm": 3.0374507904052734,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 34900
    },
    {
      "epoch": 11.089580686149937,
      "grad_norm": 2.9675583839416504,
      "learning_rate": 2e-05,
      "loss": 0.0955,
      "step": 34910
    },
    {
      "epoch": 11.092757306226176,
      "grad_norm": 2.715770959854126,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 34920
    },
    {
      "epoch": 11.095933926302415,
      "grad_norm": 3.523292303085327,
      "learning_rate": 2e-05,
      "loss": 0.1046,
      "step": 34930
    },
    {
      "epoch": 11.099110546378654,
      "grad_norm": 2.096526622772217,
      "learning_rate": 2e-05,
      "loss": 0.1008,
      "step": 34940
    },
    {
      "epoch": 11.102287166454891,
      "grad_norm": 1.9672460556030273,
      "learning_rate": 2e-05,
      "loss": 0.101,
      "step": 34950
    },
    {
      "epoch": 11.10546378653113,
      "grad_norm": 2.1426193714141846,
      "learning_rate": 2e-05,
      "loss": 0.0944,
      "step": 34960
    },
    {
      "epoch": 11.10705209656925,
      "eval_loss": 1.8722076416015625,
      "eval_mse": 1.8710506257851625,
      "eval_pearson": 0.3855929425184979,
      "eval_runtime": 7.5033,
      "eval_samples_per_second": 2873.383,
      "eval_spearmanr": 0.395782191947813,
      "eval_steps_per_second": 11.328,
      "step": 34965
    },
    {
      "epoch": 11.10864040660737,
      "grad_norm": 2.257911443710327,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 34970
    },
    {
      "epoch": 11.111817026683608,
      "grad_norm": 4.1479339599609375,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 34980
    },
    {
      "epoch": 11.114993646759848,
      "grad_norm": 7.0363593101501465,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 34990
    },
    {
      "epoch": 11.118170266836087,
      "grad_norm": 2.327604293823242,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 35000
    },
    {
      "epoch": 11.121346886912326,
      "grad_norm": 6.247128486633301,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 35010
    },
    {
      "epoch": 11.124523506988565,
      "grad_norm": 3.2269062995910645,
      "learning_rate": 2e-05,
      "loss": 0.1039,
      "step": 35020
    },
    {
      "epoch": 11.127700127064802,
      "grad_norm": 2.4272031784057617,
      "learning_rate": 2e-05,
      "loss": 0.1042,
      "step": 35030
    },
    {
      "epoch": 11.130876747141041,
      "grad_norm": 3.409494161605835,
      "learning_rate": 2e-05,
      "loss": 0.1039,
      "step": 35040
    },
    {
      "epoch": 11.13405336721728,
      "grad_norm": 2.745795965194702,
      "learning_rate": 2e-05,
      "loss": 0.0907,
      "step": 35050
    },
    {
      "epoch": 11.13722998729352,
      "grad_norm": 1.7690081596374512,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 35060
    },
    {
      "epoch": 11.140406607369759,
      "grad_norm": 1.89761483669281,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 35070
    },
    {
      "epoch": 11.143583227445998,
      "grad_norm": 3.1836423873901367,
      "learning_rate": 2e-05,
      "loss": 0.0903,
      "step": 35080
    },
    {
      "epoch": 11.146759847522237,
      "grad_norm": 1.8041431903839111,
      "learning_rate": 2e-05,
      "loss": 0.0928,
      "step": 35090
    },
    {
      "epoch": 11.149936467598476,
      "grad_norm": 3.092801332473755,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 35100
    },
    {
      "epoch": 11.153113087674715,
      "grad_norm": 2.3454174995422363,
      "learning_rate": 2e-05,
      "loss": 0.1007,
      "step": 35110
    },
    {
      "epoch": 11.156289707750952,
      "grad_norm": 2.643430471420288,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 35120
    },
    {
      "epoch": 11.159466327827191,
      "grad_norm": 2.053666591644287,
      "learning_rate": 2e-05,
      "loss": 0.0979,
      "step": 35130
    },
    {
      "epoch": 11.16264294790343,
      "grad_norm": 2.893326759338379,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 35140
    },
    {
      "epoch": 11.16581956797967,
      "grad_norm": 2.1024985313415527,
      "learning_rate": 2e-05,
      "loss": 0.1041,
      "step": 35150
    },
    {
      "epoch": 11.168996188055909,
      "grad_norm": 3.8736088275909424,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 35160
    },
    {
      "epoch": 11.172172808132148,
      "grad_norm": 3.5709311962127686,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 35170
    },
    {
      "epoch": 11.175349428208387,
      "grad_norm": 3.760561943054199,
      "learning_rate": 2e-05,
      "loss": 0.0963,
      "step": 35180
    },
    {
      "epoch": 11.178526048284626,
      "grad_norm": 2.799180030822754,
      "learning_rate": 2e-05,
      "loss": 0.0963,
      "step": 35190
    },
    {
      "epoch": 11.181702668360865,
      "grad_norm": 1.937398910522461,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 35200
    },
    {
      "epoch": 11.184879288437102,
      "grad_norm": 2.3031680583953857,
      "learning_rate": 2e-05,
      "loss": 0.1076,
      "step": 35210
    },
    {
      "epoch": 11.188055908513341,
      "grad_norm": 2.565101146697998,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 35220
    },
    {
      "epoch": 11.19123252858958,
      "grad_norm": 3.4278361797332764,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 35230
    },
    {
      "epoch": 11.19440914866582,
      "grad_norm": 3.9461750984191895,
      "learning_rate": 2e-05,
      "loss": 0.096,
      "step": 35240
    },
    {
      "epoch": 11.197585768742059,
      "grad_norm": 1.8105247020721436,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 35250
    },
    {
      "epoch": 11.200762388818298,
      "grad_norm": 1.7846356630325317,
      "learning_rate": 2e-05,
      "loss": 0.1073,
      "step": 35260
    },
    {
      "epoch": 11.203939008894537,
      "grad_norm": 2.0934934616088867,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 35270
    },
    {
      "epoch": 11.207115628970776,
      "grad_norm": 2.610295534133911,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 35280
    },
    {
      "epoch": 11.207115628970776,
      "eval_loss": 1.8723703622817993,
      "eval_mse": 1.8712398638945342,
      "eval_pearson": 0.38842840635529713,
      "eval_runtime": 7.3263,
      "eval_samples_per_second": 2942.827,
      "eval_spearmanr": 0.39767435657071015,
      "eval_steps_per_second": 11.602,
      "step": 35280
    },
    {
      "epoch": 11.210292249047013,
      "grad_norm": 2.612396478652954,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 35290
    },
    {
      "epoch": 11.213468869123252,
      "grad_norm": 2.5078883171081543,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 35300
    },
    {
      "epoch": 11.216645489199491,
      "grad_norm": 2.096989393234253,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 35310
    },
    {
      "epoch": 11.21982210927573,
      "grad_norm": 7.62582540512085,
      "learning_rate": 2e-05,
      "loss": 0.0907,
      "step": 35320
    },
    {
      "epoch": 11.22299872935197,
      "grad_norm": 2.6040167808532715,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 35330
    },
    {
      "epoch": 11.226175349428209,
      "grad_norm": 2.4210596084594727,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 35340
    },
    {
      "epoch": 11.229351969504448,
      "grad_norm": 1.6991761922836304,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 35350
    },
    {
      "epoch": 11.232528589580687,
      "grad_norm": 2.946706771850586,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 35360
    },
    {
      "epoch": 11.235705209656926,
      "grad_norm": 3.853586196899414,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 35370
    },
    {
      "epoch": 11.238881829733163,
      "grad_norm": 3.289903402328491,
      "learning_rate": 2e-05,
      "loss": 0.0916,
      "step": 35380
    },
    {
      "epoch": 11.242058449809402,
      "grad_norm": 2.491412401199341,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 35390
    },
    {
      "epoch": 11.245235069885641,
      "grad_norm": 2.3416192531585693,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 35400
    },
    {
      "epoch": 11.24841168996188,
      "grad_norm": 3.0475034713745117,
      "learning_rate": 2e-05,
      "loss": 0.0996,
      "step": 35410
    },
    {
      "epoch": 11.25158831003812,
      "grad_norm": 2.4696452617645264,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 35420
    },
    {
      "epoch": 11.254764930114359,
      "grad_norm": 6.260926246643066,
      "learning_rate": 2e-05,
      "loss": 0.0973,
      "step": 35430
    },
    {
      "epoch": 11.257941550190598,
      "grad_norm": 3.315192222595215,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 35440
    },
    {
      "epoch": 11.261118170266837,
      "grad_norm": 3.3506088256835938,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 35450
    },
    {
      "epoch": 11.264294790343074,
      "grad_norm": 3.069827079772949,
      "learning_rate": 2e-05,
      "loss": 0.1075,
      "step": 35460
    },
    {
      "epoch": 11.267471410419313,
      "grad_norm": 2.270050287246704,
      "learning_rate": 2e-05,
      "loss": 0.1043,
      "step": 35470
    },
    {
      "epoch": 11.270648030495552,
      "grad_norm": 3.1350975036621094,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 35480
    },
    {
      "epoch": 11.273824650571791,
      "grad_norm": 2.26468563079834,
      "learning_rate": 2e-05,
      "loss": 0.1014,
      "step": 35490
    },
    {
      "epoch": 11.27700127064803,
      "grad_norm": 2.9689383506774902,
      "learning_rate": 2e-05,
      "loss": 0.0999,
      "step": 35500
    },
    {
      "epoch": 11.28017789072427,
      "grad_norm": 2.7284867763519287,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 35510
    },
    {
      "epoch": 11.283354510800509,
      "grad_norm": 1.971252202987671,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 35520
    },
    {
      "epoch": 11.286531130876748,
      "grad_norm": 2.331516742706299,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 35530
    },
    {
      "epoch": 11.289707750952987,
      "grad_norm": 1.8358166217803955,
      "learning_rate": 2e-05,
      "loss": 0.0973,
      "step": 35540
    },
    {
      "epoch": 11.292884371029224,
      "grad_norm": 2.657947063446045,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 35550
    },
    {
      "epoch": 11.296060991105463,
      "grad_norm": 2.3121373653411865,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 35560
    },
    {
      "epoch": 11.299237611181702,
      "grad_norm": 2.368652820587158,
      "learning_rate": 2e-05,
      "loss": 0.102,
      "step": 35570
    },
    {
      "epoch": 11.302414231257941,
      "grad_norm": 2.352543830871582,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 35580
    },
    {
      "epoch": 11.30559085133418,
      "grad_norm": 2.604640245437622,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 35590
    },
    {
      "epoch": 11.3071791613723,
      "eval_loss": 1.9266552925109863,
      "eval_mse": 1.9252418283573551,
      "eval_pearson": 0.39848533302175637,
      "eval_runtime": 7.513,
      "eval_samples_per_second": 2869.701,
      "eval_spearmanr": 0.40574639132835943,
      "eval_steps_per_second": 11.314,
      "step": 35595
    },
    {
      "epoch": 11.30876747141042,
      "grad_norm": 2.734302282333374,
      "learning_rate": 2e-05,
      "loss": 0.1005,
      "step": 35600
    },
    {
      "epoch": 11.311944091486659,
      "grad_norm": 2.558297872543335,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 35610
    },
    {
      "epoch": 11.315120711562898,
      "grad_norm": 3.5222339630126953,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 35620
    },
    {
      "epoch": 11.318297331639137,
      "grad_norm": 3.9819350242614746,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 35630
    },
    {
      "epoch": 11.321473951715374,
      "grad_norm": 2.673171043395996,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 35640
    },
    {
      "epoch": 11.324650571791613,
      "grad_norm": 2.6974618434906006,
      "learning_rate": 2e-05,
      "loss": 0.1026,
      "step": 35650
    },
    {
      "epoch": 11.327827191867852,
      "grad_norm": 1.6305097341537476,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 35660
    },
    {
      "epoch": 11.331003811944091,
      "grad_norm": 2.2975971698760986,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 35670
    },
    {
      "epoch": 11.33418043202033,
      "grad_norm": 2.8290061950683594,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 35680
    },
    {
      "epoch": 11.33735705209657,
      "grad_norm": 2.4126076698303223,
      "learning_rate": 2e-05,
      "loss": 0.0906,
      "step": 35690
    },
    {
      "epoch": 11.340533672172809,
      "grad_norm": 3.3740897178649902,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 35700
    },
    {
      "epoch": 11.343710292249048,
      "grad_norm": 2.1170995235443115,
      "learning_rate": 2e-05,
      "loss": 0.0976,
      "step": 35710
    },
    {
      "epoch": 11.346886912325285,
      "grad_norm": 2.417484998703003,
      "learning_rate": 2e-05,
      "loss": 0.0932,
      "step": 35720
    },
    {
      "epoch": 11.350063532401524,
      "grad_norm": 2.353492498397827,
      "learning_rate": 2e-05,
      "loss": 0.0905,
      "step": 35730
    },
    {
      "epoch": 11.353240152477763,
      "grad_norm": 2.872185230255127,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 35740
    },
    {
      "epoch": 11.356416772554002,
      "grad_norm": 3.28879451751709,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 35750
    },
    {
      "epoch": 11.359593392630241,
      "grad_norm": 2.93235182762146,
      "learning_rate": 2e-05,
      "loss": 0.0999,
      "step": 35760
    },
    {
      "epoch": 11.36277001270648,
      "grad_norm": 2.747066020965576,
      "learning_rate": 2e-05,
      "loss": 0.0967,
      "step": 35770
    },
    {
      "epoch": 11.36594663278272,
      "grad_norm": 2.846127510070801,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 35780
    },
    {
      "epoch": 11.369123252858959,
      "grad_norm": 2.739271879196167,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 35790
    },
    {
      "epoch": 11.372299872935198,
      "grad_norm": 3.903386354446411,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 35800
    },
    {
      "epoch": 11.375476493011435,
      "grad_norm": 3.1648032665252686,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 35810
    },
    {
      "epoch": 11.378653113087674,
      "grad_norm": 2.2955920696258545,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 35820
    },
    {
      "epoch": 11.381829733163913,
      "grad_norm": 2.2280144691467285,
      "learning_rate": 2e-05,
      "loss": 0.0939,
      "step": 35830
    },
    {
      "epoch": 11.385006353240152,
      "grad_norm": 2.5496907234191895,
      "learning_rate": 2e-05,
      "loss": 0.0976,
      "step": 35840
    },
    {
      "epoch": 11.388182973316392,
      "grad_norm": 2.38985013961792,
      "learning_rate": 2e-05,
      "loss": 0.1008,
      "step": 35850
    },
    {
      "epoch": 11.39135959339263,
      "grad_norm": 3.138385534286499,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 35860
    },
    {
      "epoch": 11.39453621346887,
      "grad_norm": 4.198342323303223,
      "learning_rate": 2e-05,
      "loss": 0.0996,
      "step": 35870
    },
    {
      "epoch": 11.397712833545109,
      "grad_norm": 3.586878538131714,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 35880
    },
    {
      "epoch": 11.400889453621346,
      "grad_norm": 2.0240724086761475,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 35890
    },
    {
      "epoch": 11.404066073697585,
      "grad_norm": 1.8927704095840454,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 35900
    },
    {
      "epoch": 11.407242693773824,
      "grad_norm": 2.281442642211914,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 35910
    },
    {
      "epoch": 11.407242693773824,
      "eval_loss": 1.811055302619934,
      "eval_mse": 1.8098642520095067,
      "eval_pearson": 0.4004893655681369,
      "eval_runtime": 7.4928,
      "eval_samples_per_second": 2877.418,
      "eval_spearmanr": 0.4067307007947209,
      "eval_steps_per_second": 11.344,
      "step": 35910
    },
    {
      "epoch": 11.410419313850063,
      "grad_norm": 1.9248247146606445,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 35920
    },
    {
      "epoch": 11.413595933926302,
      "grad_norm": 2.526348352432251,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 35930
    },
    {
      "epoch": 11.416772554002542,
      "grad_norm": 2.2037501335144043,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 35940
    },
    {
      "epoch": 11.41994917407878,
      "grad_norm": 1.9970406293869019,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 35950
    },
    {
      "epoch": 11.42312579415502,
      "grad_norm": 2.4522809982299805,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 35960
    },
    {
      "epoch": 11.426302414231259,
      "grad_norm": 4.181057929992676,
      "learning_rate": 2e-05,
      "loss": 0.1055,
      "step": 35970
    },
    {
      "epoch": 11.429479034307496,
      "grad_norm": 4.432849884033203,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 35980
    },
    {
      "epoch": 11.432655654383735,
      "grad_norm": 2.6837151050567627,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 35990
    },
    {
      "epoch": 11.435832274459974,
      "grad_norm": 2.8890953063964844,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 36000
    },
    {
      "epoch": 11.439008894536213,
      "grad_norm": 6.9718170166015625,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 36010
    },
    {
      "epoch": 11.442185514612452,
      "grad_norm": 2.479156494140625,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 36020
    },
    {
      "epoch": 11.445362134688692,
      "grad_norm": 5.4361772537231445,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 36030
    },
    {
      "epoch": 11.44853875476493,
      "grad_norm": 3.8131179809570312,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 36040
    },
    {
      "epoch": 11.45171537484117,
      "grad_norm": 2.1684772968292236,
      "learning_rate": 2e-05,
      "loss": 0.1058,
      "step": 36050
    },
    {
      "epoch": 11.454891994917407,
      "grad_norm": 4.417057514190674,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 36060
    },
    {
      "epoch": 11.458068614993646,
      "grad_norm": 5.107370376586914,
      "learning_rate": 2e-05,
      "loss": 0.0975,
      "step": 36070
    },
    {
      "epoch": 11.461245235069885,
      "grad_norm": 2.525216579437256,
      "learning_rate": 2e-05,
      "loss": 0.0926,
      "step": 36080
    },
    {
      "epoch": 11.464421855146124,
      "grad_norm": 3.91753888130188,
      "learning_rate": 2e-05,
      "loss": 0.1006,
      "step": 36090
    },
    {
      "epoch": 11.467598475222363,
      "grad_norm": 5.597957611083984,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 36100
    },
    {
      "epoch": 11.470775095298603,
      "grad_norm": 3.899306535720825,
      "learning_rate": 2e-05,
      "loss": 0.1001,
      "step": 36110
    },
    {
      "epoch": 11.473951715374842,
      "grad_norm": 3.3486850261688232,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 36120
    },
    {
      "epoch": 11.47712833545108,
      "grad_norm": 4.473634243011475,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 36130
    },
    {
      "epoch": 11.48030495552732,
      "grad_norm": 4.0293869972229,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 36140
    },
    {
      "epoch": 11.483481575603557,
      "grad_norm": 3.2108609676361084,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 36150
    },
    {
      "epoch": 11.486658195679796,
      "grad_norm": 2.6487877368927,
      "learning_rate": 2e-05,
      "loss": 0.0907,
      "step": 36160
    },
    {
      "epoch": 11.489834815756035,
      "grad_norm": 8.0406494140625,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 36170
    },
    {
      "epoch": 11.493011435832274,
      "grad_norm": 4.256712913513184,
      "learning_rate": 2e-05,
      "loss": 0.1034,
      "step": 36180
    },
    {
      "epoch": 11.496188055908513,
      "grad_norm": 3.780956983566284,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 36190
    },
    {
      "epoch": 11.499364675984753,
      "grad_norm": 2.2980964183807373,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 36200
    },
    {
      "epoch": 11.502541296060992,
      "grad_norm": 3.012134075164795,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 36210
    },
    {
      "epoch": 11.50571791613723,
      "grad_norm": 2.732666254043579,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 36220
    },
    {
      "epoch": 11.507306226175348,
      "eval_loss": 1.8407402038574219,
      "eval_mse": 1.8396109738216109,
      "eval_pearson": 0.38432237027586375,
      "eval_runtime": 7.3929,
      "eval_samples_per_second": 2916.318,
      "eval_spearmanr": 0.39241431347876665,
      "eval_steps_per_second": 11.498,
      "step": 36225
    },
    {
      "epoch": 11.508894536213468,
      "grad_norm": 3.6291391849517822,
      "learning_rate": 2e-05,
      "loss": 0.1022,
      "step": 36230
    },
    {
      "epoch": 11.512071156289707,
      "grad_norm": 4.866255760192871,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 36240
    },
    {
      "epoch": 11.515247776365946,
      "grad_norm": 2.775728702545166,
      "learning_rate": 2e-05,
      "loss": 0.1043,
      "step": 36250
    },
    {
      "epoch": 11.518424396442185,
      "grad_norm": 4.669625282287598,
      "learning_rate": 2e-05,
      "loss": 0.0937,
      "step": 36260
    },
    {
      "epoch": 11.521601016518424,
      "grad_norm": 2.1738357543945312,
      "learning_rate": 2e-05,
      "loss": 0.107,
      "step": 36270
    },
    {
      "epoch": 11.524777636594663,
      "grad_norm": 2.353847026824951,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 36280
    },
    {
      "epoch": 11.527954256670903,
      "grad_norm": 2.264730215072632,
      "learning_rate": 2e-05,
      "loss": 0.093,
      "step": 36290
    },
    {
      "epoch": 11.531130876747142,
      "grad_norm": 3.264068365097046,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 36300
    },
    {
      "epoch": 11.53430749682338,
      "grad_norm": 3.692650318145752,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 36310
    },
    {
      "epoch": 11.537484116899618,
      "grad_norm": 2.3955576419830322,
      "learning_rate": 2e-05,
      "loss": 0.1028,
      "step": 36320
    },
    {
      "epoch": 11.540660736975857,
      "grad_norm": 2.204692840576172,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 36330
    },
    {
      "epoch": 11.543837357052096,
      "grad_norm": 1.8659080266952515,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 36340
    },
    {
      "epoch": 11.547013977128335,
      "grad_norm": 2.515948534011841,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 36350
    },
    {
      "epoch": 11.550190597204574,
      "grad_norm": 2.6685476303100586,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 36360
    },
    {
      "epoch": 11.553367217280814,
      "grad_norm": 5.179006576538086,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 36370
    },
    {
      "epoch": 11.556543837357053,
      "grad_norm": 3.8305444717407227,
      "learning_rate": 2e-05,
      "loss": 0.0941,
      "step": 36380
    },
    {
      "epoch": 11.559720457433292,
      "grad_norm": 2.4085140228271484,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 36390
    },
    {
      "epoch": 11.562897077509529,
      "grad_norm": 10.682320594787598,
      "learning_rate": 2e-05,
      "loss": 0.0907,
      "step": 36400
    },
    {
      "epoch": 11.566073697585768,
      "grad_norm": 2.6098203659057617,
      "learning_rate": 2e-05,
      "loss": 0.1,
      "step": 36410
    },
    {
      "epoch": 11.569250317662007,
      "grad_norm": 2.501256227493286,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 36420
    },
    {
      "epoch": 11.572426937738246,
      "grad_norm": 4.12458610534668,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 36430
    },
    {
      "epoch": 11.575603557814485,
      "grad_norm": 2.792868137359619,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 36440
    },
    {
      "epoch": 11.578780177890724,
      "grad_norm": 2.145968437194824,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 36450
    },
    {
      "epoch": 11.581956797966964,
      "grad_norm": 2.4138965606689453,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 36460
    },
    {
      "epoch": 11.585133418043203,
      "grad_norm": 3.086981773376465,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 36470
    },
    {
      "epoch": 11.588310038119442,
      "grad_norm": 1.9012818336486816,
      "learning_rate": 2e-05,
      "loss": 0.0994,
      "step": 36480
    },
    {
      "epoch": 11.591486658195679,
      "grad_norm": 2.0939369201660156,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 36490
    },
    {
      "epoch": 11.594663278271918,
      "grad_norm": 1.9880969524383545,
      "learning_rate": 2e-05,
      "loss": 0.0912,
      "step": 36500
    },
    {
      "epoch": 11.597839898348157,
      "grad_norm": 2.9241175651550293,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 36510
    },
    {
      "epoch": 11.601016518424396,
      "grad_norm": 1.9433759450912476,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 36520
    },
    {
      "epoch": 11.604193138500635,
      "grad_norm": 2.687910795211792,
      "learning_rate": 2e-05,
      "loss": 0.0968,
      "step": 36530
    },
    {
      "epoch": 11.607369758576874,
      "grad_norm": 1.997700572013855,
      "learning_rate": 2e-05,
      "loss": 0.1011,
      "step": 36540
    },
    {
      "epoch": 11.607369758576874,
      "eval_loss": 1.8237208127975464,
      "eval_mse": 1.8226507428131946,
      "eval_pearson": 0.39142377010107887,
      "eval_runtime": 7.2866,
      "eval_samples_per_second": 2958.858,
      "eval_spearmanr": 0.39679286612750864,
      "eval_steps_per_second": 11.665,
      "step": 36540
    },
    {
      "epoch": 11.610546378653114,
      "grad_norm": 2.929434061050415,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 36550
    },
    {
      "epoch": 11.613722998729353,
      "grad_norm": 2.0464389324188232,
      "learning_rate": 2e-05,
      "loss": 0.1036,
      "step": 36560
    },
    {
      "epoch": 11.61689961880559,
      "grad_norm": 5.026037693023682,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 36570
    },
    {
      "epoch": 11.620076238881829,
      "grad_norm": 2.7681846618652344,
      "learning_rate": 2e-05,
      "loss": 0.0955,
      "step": 36580
    },
    {
      "epoch": 11.623252858958068,
      "grad_norm": 2.8450849056243896,
      "learning_rate": 2e-05,
      "loss": 0.0912,
      "step": 36590
    },
    {
      "epoch": 11.626429479034307,
      "grad_norm": 4.224026679992676,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 36600
    },
    {
      "epoch": 11.629606099110546,
      "grad_norm": 1.9844638109207153,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 36610
    },
    {
      "epoch": 11.632782719186785,
      "grad_norm": 2.352656841278076,
      "learning_rate": 2e-05,
      "loss": 0.1001,
      "step": 36620
    },
    {
      "epoch": 11.635959339263025,
      "grad_norm": 2.8478784561157227,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 36630
    },
    {
      "epoch": 11.639135959339264,
      "grad_norm": 2.720452070236206,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 36640
    },
    {
      "epoch": 11.642312579415503,
      "grad_norm": 3.5406882762908936,
      "learning_rate": 2e-05,
      "loss": 0.1087,
      "step": 36650
    },
    {
      "epoch": 11.64548919949174,
      "grad_norm": 3.11460542678833,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 36660
    },
    {
      "epoch": 11.648665819567979,
      "grad_norm": 2.2685470581054688,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 36670
    },
    {
      "epoch": 11.651842439644218,
      "grad_norm": 2.4922845363616943,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 36680
    },
    {
      "epoch": 11.655019059720457,
      "grad_norm": 4.262623310089111,
      "learning_rate": 2e-05,
      "loss": 0.0995,
      "step": 36690
    },
    {
      "epoch": 11.658195679796696,
      "grad_norm": 3.0376057624816895,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 36700
    },
    {
      "epoch": 11.661372299872935,
      "grad_norm": 1.9730360507965088,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 36710
    },
    {
      "epoch": 11.664548919949175,
      "grad_norm": 3.102489948272705,
      "learning_rate": 2e-05,
      "loss": 0.098,
      "step": 36720
    },
    {
      "epoch": 11.667725540025414,
      "grad_norm": 2.108687162399292,
      "learning_rate": 2e-05,
      "loss": 0.1003,
      "step": 36730
    },
    {
      "epoch": 11.670902160101651,
      "grad_norm": 2.813143491744995,
      "learning_rate": 2e-05,
      "loss": 0.097,
      "step": 36740
    },
    {
      "epoch": 11.67407878017789,
      "grad_norm": 2.353400707244873,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 36750
    },
    {
      "epoch": 11.67725540025413,
      "grad_norm": 2.934013843536377,
      "learning_rate": 2e-05,
      "loss": 0.0973,
      "step": 36760
    },
    {
      "epoch": 11.680432020330368,
      "grad_norm": 3.130810022354126,
      "learning_rate": 2e-05,
      "loss": 0.1089,
      "step": 36770
    },
    {
      "epoch": 11.683608640406607,
      "grad_norm": 4.717886924743652,
      "learning_rate": 2e-05,
      "loss": 0.0999,
      "step": 36780
    },
    {
      "epoch": 11.686785260482846,
      "grad_norm": 3.9029252529144287,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 36790
    },
    {
      "epoch": 11.689961880559085,
      "grad_norm": 2.947173833847046,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 36800
    },
    {
      "epoch": 11.693138500635325,
      "grad_norm": 2.122501850128174,
      "learning_rate": 2e-05,
      "loss": 0.1002,
      "step": 36810
    },
    {
      "epoch": 11.696315120711564,
      "grad_norm": 3.7638370990753174,
      "learning_rate": 2e-05,
      "loss": 0.0946,
      "step": 36820
    },
    {
      "epoch": 11.699491740787801,
      "grad_norm": 2.1583924293518066,
      "learning_rate": 2e-05,
      "loss": 0.0992,
      "step": 36830
    },
    {
      "epoch": 11.70266836086404,
      "grad_norm": 1.8147079944610596,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 36840
    },
    {
      "epoch": 11.70584498094028,
      "grad_norm": 3.0575926303863525,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 36850
    },
    {
      "epoch": 11.707433290978399,
      "eval_loss": 1.8351212739944458,
      "eval_mse": 1.833656164898722,
      "eval_pearson": 0.40278019736451054,
      "eval_runtime": 7.5023,
      "eval_samples_per_second": 2873.8,
      "eval_spearmanr": 0.4087968985035351,
      "eval_steps_per_second": 11.33,
      "step": 36855
    },
    {
      "epoch": 11.709021601016518,
      "grad_norm": 2.3799545764923096,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 36860
    },
    {
      "epoch": 11.712198221092757,
      "grad_norm": 3.248483419418335,
      "learning_rate": 2e-05,
      "loss": 0.1016,
      "step": 36870
    },
    {
      "epoch": 11.715374841168996,
      "grad_norm": 2.5615906715393066,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 36880
    },
    {
      "epoch": 11.718551461245236,
      "grad_norm": 2.3000173568725586,
      "learning_rate": 2e-05,
      "loss": 0.094,
      "step": 36890
    },
    {
      "epoch": 11.721728081321475,
      "grad_norm": 4.48017692565918,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 36900
    },
    {
      "epoch": 11.724904701397712,
      "grad_norm": 2.3108041286468506,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 36910
    },
    {
      "epoch": 11.728081321473951,
      "grad_norm": 2.4112439155578613,
      "learning_rate": 2e-05,
      "loss": 0.0977,
      "step": 36920
    },
    {
      "epoch": 11.73125794155019,
      "grad_norm": 2.42560076713562,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 36930
    },
    {
      "epoch": 11.73443456162643,
      "grad_norm": 2.197288751602173,
      "learning_rate": 2e-05,
      "loss": 0.0981,
      "step": 36940
    },
    {
      "epoch": 11.737611181702668,
      "grad_norm": 2.844697952270508,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 36950
    },
    {
      "epoch": 11.740787801778907,
      "grad_norm": 5.545475006103516,
      "learning_rate": 2e-05,
      "loss": 0.1008,
      "step": 36960
    },
    {
      "epoch": 11.743964421855146,
      "grad_norm": 3.740537405014038,
      "learning_rate": 2e-05,
      "loss": 0.1011,
      "step": 36970
    },
    {
      "epoch": 11.747141041931386,
      "grad_norm": 2.277876377105713,
      "learning_rate": 2e-05,
      "loss": 0.087,
      "step": 36980
    },
    {
      "epoch": 11.750317662007625,
      "grad_norm": 2.340677261352539,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 36990
    },
    {
      "epoch": 11.753494282083864,
      "grad_norm": 3.1023027896881104,
      "learning_rate": 2e-05,
      "loss": 0.097,
      "step": 37000
    },
    {
      "epoch": 11.756670902160101,
      "grad_norm": 3.0032882690429688,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 37010
    },
    {
      "epoch": 11.75984752223634,
      "grad_norm": 1.9980965852737427,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 37020
    },
    {
      "epoch": 11.76302414231258,
      "grad_norm": 1.6948738098144531,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 37030
    },
    {
      "epoch": 11.766200762388818,
      "grad_norm": 3.092247247695923,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 37040
    },
    {
      "epoch": 11.769377382465057,
      "grad_norm": 4.443461894989014,
      "learning_rate": 2e-05,
      "loss": 0.0973,
      "step": 37050
    },
    {
      "epoch": 11.772554002541296,
      "grad_norm": 2.527172803878784,
      "learning_rate": 2e-05,
      "loss": 0.101,
      "step": 37060
    },
    {
      "epoch": 11.775730622617536,
      "grad_norm": 2.3596067428588867,
      "learning_rate": 2e-05,
      "loss": 0.102,
      "step": 37070
    },
    {
      "epoch": 11.778907242693775,
      "grad_norm": 3.048757791519165,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 37080
    },
    {
      "epoch": 11.782083862770012,
      "grad_norm": 4.45214319229126,
      "learning_rate": 2e-05,
      "loss": 0.0993,
      "step": 37090
    },
    {
      "epoch": 11.785260482846251,
      "grad_norm": 3.3988027572631836,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 37100
    },
    {
      "epoch": 11.78843710292249,
      "grad_norm": 2.007547616958618,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 37110
    },
    {
      "epoch": 11.79161372299873,
      "grad_norm": 2.4435722827911377,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 37120
    },
    {
      "epoch": 11.794790343074968,
      "grad_norm": 6.899364948272705,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 37130
    },
    {
      "epoch": 11.797966963151207,
      "grad_norm": 3.30106258392334,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 37140
    },
    {
      "epoch": 11.801143583227446,
      "grad_norm": 3.1502645015716553,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 37150
    },
    {
      "epoch": 11.804320203303686,
      "grad_norm": 2.8907740116119385,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 37160
    },
    {
      "epoch": 11.807496823379925,
      "grad_norm": 3.1531853675842285,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 37170
    },
    {
      "epoch": 11.807496823379925,
      "eval_loss": 1.8034101724624634,
      "eval_mse": 1.8022582104979066,
      "eval_pearson": 0.3971473166507607,
      "eval_runtime": 7.3795,
      "eval_samples_per_second": 2921.589,
      "eval_spearmanr": 0.4056391602511703,
      "eval_steps_per_second": 11.518,
      "step": 37170
    },
    {
      "epoch": 11.810673443456162,
      "grad_norm": 4.015343189239502,
      "learning_rate": 2e-05,
      "loss": 0.0995,
      "step": 37180
    },
    {
      "epoch": 11.813850063532401,
      "grad_norm": 1.8983210325241089,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 37190
    },
    {
      "epoch": 11.81702668360864,
      "grad_norm": 3.501725196838379,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 37200
    },
    {
      "epoch": 11.82020330368488,
      "grad_norm": 12.91720962524414,
      "learning_rate": 2e-05,
      "loss": 0.0986,
      "step": 37210
    },
    {
      "epoch": 11.823379923761118,
      "grad_norm": 2.363023042678833,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 37220
    },
    {
      "epoch": 11.826556543837357,
      "grad_norm": 3.0868656635284424,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 37230
    },
    {
      "epoch": 11.829733163913597,
      "grad_norm": 2.128910779953003,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 37240
    },
    {
      "epoch": 11.832909783989836,
      "grad_norm": 2.7178549766540527,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 37250
    },
    {
      "epoch": 11.836086404066073,
      "grad_norm": 2.013633966445923,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 37260
    },
    {
      "epoch": 11.839263024142312,
      "grad_norm": 2.065855026245117,
      "learning_rate": 2e-05,
      "loss": 0.1017,
      "step": 37270
    },
    {
      "epoch": 11.842439644218551,
      "grad_norm": 8.169017791748047,
      "learning_rate": 2e-05,
      "loss": 0.0989,
      "step": 37280
    },
    {
      "epoch": 11.84561626429479,
      "grad_norm": 2.5570895671844482,
      "learning_rate": 2e-05,
      "loss": 0.1035,
      "step": 37290
    },
    {
      "epoch": 11.84879288437103,
      "grad_norm": 19.148326873779297,
      "learning_rate": 2e-05,
      "loss": 0.0988,
      "step": 37300
    },
    {
      "epoch": 11.851969504447268,
      "grad_norm": 2.599376916885376,
      "learning_rate": 2e-05,
      "loss": 0.1024,
      "step": 37310
    },
    {
      "epoch": 11.855146124523507,
      "grad_norm": 1.8357195854187012,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 37320
    },
    {
      "epoch": 11.858322744599747,
      "grad_norm": 2.8348209857940674,
      "learning_rate": 2e-05,
      "loss": 0.1037,
      "step": 37330
    },
    {
      "epoch": 11.861499364675986,
      "grad_norm": 3.4721903800964355,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 37340
    },
    {
      "epoch": 11.864675984752223,
      "grad_norm": 3.32191801071167,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 37350
    },
    {
      "epoch": 11.867852604828462,
      "grad_norm": 2.6407370567321777,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 37360
    },
    {
      "epoch": 11.871029224904701,
      "grad_norm": 2.0391881465911865,
      "learning_rate": 2e-05,
      "loss": 0.0975,
      "step": 37370
    },
    {
      "epoch": 11.87420584498094,
      "grad_norm": 3.033461809158325,
      "learning_rate": 2e-05,
      "loss": 0.0982,
      "step": 37380
    },
    {
      "epoch": 11.87738246505718,
      "grad_norm": 3.9079580307006836,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 37390
    },
    {
      "epoch": 11.880559085133418,
      "grad_norm": 2.5253751277923584,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 37400
    },
    {
      "epoch": 11.883735705209657,
      "grad_norm": 2.7994496822357178,
      "learning_rate": 2e-05,
      "loss": 0.096,
      "step": 37410
    },
    {
      "epoch": 11.886912325285897,
      "grad_norm": 2.0926992893218994,
      "learning_rate": 2e-05,
      "loss": 0.0957,
      "step": 37420
    },
    {
      "epoch": 11.890088945362134,
      "grad_norm": 5.330587863922119,
      "learning_rate": 2e-05,
      "loss": 0.1054,
      "step": 37430
    },
    {
      "epoch": 11.893265565438373,
      "grad_norm": 3.3556811809539795,
      "learning_rate": 2e-05,
      "loss": 0.097,
      "step": 37440
    },
    {
      "epoch": 11.896442185514612,
      "grad_norm": 2.114793062210083,
      "learning_rate": 2e-05,
      "loss": 0.1031,
      "step": 37450
    },
    {
      "epoch": 11.899618805590851,
      "grad_norm": 2.3051671981811523,
      "learning_rate": 2e-05,
      "loss": 0.0984,
      "step": 37460
    },
    {
      "epoch": 11.90279542566709,
      "grad_norm": 3.0062077045440674,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 37470
    },
    {
      "epoch": 11.90597204574333,
      "grad_norm": 2.5134737491607666,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 37480
    },
    {
      "epoch": 11.907560355781449,
      "eval_loss": 2.039097547531128,
      "eval_mse": 2.037484143736577,
      "eval_pearson": 0.38039506170760384,
      "eval_runtime": 7.4802,
      "eval_samples_per_second": 2882.267,
      "eval_spearmanr": 0.39357319007478114,
      "eval_steps_per_second": 11.363,
      "step": 37485
    },
    {
      "epoch": 11.909148665819568,
      "grad_norm": 2.517887592315674,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 37490
    },
    {
      "epoch": 11.912325285895808,
      "grad_norm": 2.3379435539245605,
      "learning_rate": 2e-05,
      "loss": 0.096,
      "step": 37500
    },
    {
      "epoch": 11.915501905972047,
      "grad_norm": 6.597736835479736,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 37510
    },
    {
      "epoch": 11.918678526048284,
      "grad_norm": 3.2054872512817383,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 37520
    },
    {
      "epoch": 11.921855146124523,
      "grad_norm": 4.659989356994629,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 37530
    },
    {
      "epoch": 11.925031766200762,
      "grad_norm": 3.0743143558502197,
      "learning_rate": 2e-05,
      "loss": 0.0947,
      "step": 37540
    },
    {
      "epoch": 11.928208386277001,
      "grad_norm": 4.592683792114258,
      "learning_rate": 2e-05,
      "loss": 0.1033,
      "step": 37550
    },
    {
      "epoch": 11.93138500635324,
      "grad_norm": 2.9262585639953613,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 37560
    },
    {
      "epoch": 11.93456162642948,
      "grad_norm": 2.7611632347106934,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 37570
    },
    {
      "epoch": 11.937738246505718,
      "grad_norm": 2.6895477771759033,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 37580
    },
    {
      "epoch": 11.940914866581958,
      "grad_norm": 2.094242811203003,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 37590
    },
    {
      "epoch": 11.944091486658195,
      "grad_norm": 1.916332483291626,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 37600
    },
    {
      "epoch": 11.947268106734434,
      "grad_norm": 2.9741973876953125,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 37610
    },
    {
      "epoch": 11.950444726810673,
      "grad_norm": 3.005107879638672,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 37620
    },
    {
      "epoch": 11.953621346886912,
      "grad_norm": 2.6823508739471436,
      "learning_rate": 2e-05,
      "loss": 0.1009,
      "step": 37630
    },
    {
      "epoch": 11.956797966963151,
      "grad_norm": 3.5817739963531494,
      "learning_rate": 2e-05,
      "loss": 0.0976,
      "step": 37640
    },
    {
      "epoch": 11.95997458703939,
      "grad_norm": 3.023186206817627,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 37650
    },
    {
      "epoch": 11.96315120711563,
      "grad_norm": 2.58724308013916,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 37660
    },
    {
      "epoch": 11.966327827191868,
      "grad_norm": 2.2765989303588867,
      "learning_rate": 2e-05,
      "loss": 0.0977,
      "step": 37670
    },
    {
      "epoch": 11.969504447268108,
      "grad_norm": 3.4840304851531982,
      "learning_rate": 2e-05,
      "loss": 0.0964,
      "step": 37680
    },
    {
      "epoch": 11.972681067344345,
      "grad_norm": 2.4060869216918945,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 37690
    },
    {
      "epoch": 11.975857687420584,
      "grad_norm": 2.024914026260376,
      "learning_rate": 2e-05,
      "loss": 0.0983,
      "step": 37700
    },
    {
      "epoch": 11.979034307496823,
      "grad_norm": 3.907111167907715,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 37710
    },
    {
      "epoch": 11.982210927573062,
      "grad_norm": 5.782017707824707,
      "learning_rate": 2e-05,
      "loss": 0.0988,
      "step": 37720
    },
    {
      "epoch": 11.985387547649301,
      "grad_norm": 5.155880451202393,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 37730
    },
    {
      "epoch": 11.98856416772554,
      "grad_norm": 3.55908203125,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 37740
    },
    {
      "epoch": 11.99174078780178,
      "grad_norm": 2.691772937774658,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 37750
    },
    {
      "epoch": 11.994917407878019,
      "grad_norm": 3.8305556774139404,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 37760
    },
    {
      "epoch": 11.998094027954256,
      "grad_norm": 3.355360746383667,
      "learning_rate": 2e-05,
      "loss": 0.096,
      "step": 37770
    },
    {
      "epoch": 12.001270648030495,
      "grad_norm": 2.372433662414551,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 37780
    },
    {
      "epoch": 12.004447268106734,
      "grad_norm": 3.1477274894714355,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 37790
    },
    {
      "epoch": 12.007623888182973,
      "grad_norm": 5.752991676330566,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 37800
    },
    {
      "epoch": 12.007623888182973,
      "eval_loss": 1.8581241369247437,
      "eval_mse": 1.8569876801547403,
      "eval_pearson": 0.37209352458297423,
      "eval_runtime": 7.305,
      "eval_samples_per_second": 2951.419,
      "eval_spearmanr": 0.38236184663543826,
      "eval_steps_per_second": 11.636,
      "step": 37800
    },
    {
      "epoch": 12.010800508259212,
      "grad_norm": 2.6533889770507812,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 37810
    },
    {
      "epoch": 12.013977128335451,
      "grad_norm": 5.057065486907959,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 37820
    },
    {
      "epoch": 12.01715374841169,
      "grad_norm": 2.6475274562835693,
      "learning_rate": 2e-05,
      "loss": 0.0845,
      "step": 37830
    },
    {
      "epoch": 12.02033036848793,
      "grad_norm": 2.2258615493774414,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 37840
    },
    {
      "epoch": 12.023506988564169,
      "grad_norm": 4.819363117218018,
      "learning_rate": 2e-05,
      "loss": 0.0954,
      "step": 37850
    },
    {
      "epoch": 12.026683608640406,
      "grad_norm": 2.5688629150390625,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 37860
    },
    {
      "epoch": 12.029860228716645,
      "grad_norm": 3.419584035873413,
      "learning_rate": 2e-05,
      "loss": 0.0866,
      "step": 37870
    },
    {
      "epoch": 12.033036848792884,
      "grad_norm": 2.291332483291626,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 37880
    },
    {
      "epoch": 12.036213468869123,
      "grad_norm": 2.9985291957855225,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 37890
    },
    {
      "epoch": 12.039390088945362,
      "grad_norm": 3.2850522994995117,
      "learning_rate": 2e-05,
      "loss": 0.09,
      "step": 37900
    },
    {
      "epoch": 12.042566709021601,
      "grad_norm": 1.8650481700897217,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 37910
    },
    {
      "epoch": 12.04574332909784,
      "grad_norm": 2.2610912322998047,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 37920
    },
    {
      "epoch": 12.04891994917408,
      "grad_norm": 1.9158087968826294,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 37930
    },
    {
      "epoch": 12.052096569250317,
      "grad_norm": 2.573693037033081,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 37940
    },
    {
      "epoch": 12.055273189326556,
      "grad_norm": 18.24966049194336,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 37950
    },
    {
      "epoch": 12.058449809402795,
      "grad_norm": 1.95958411693573,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 37960
    },
    {
      "epoch": 12.061626429479034,
      "grad_norm": 2.149346351623535,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 37970
    },
    {
      "epoch": 12.064803049555273,
      "grad_norm": 1.6692496538162231,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 37980
    },
    {
      "epoch": 12.067979669631512,
      "grad_norm": 1.9048328399658203,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 37990
    },
    {
      "epoch": 12.071156289707751,
      "grad_norm": 4.611881256103516,
      "learning_rate": 2e-05,
      "loss": 0.0997,
      "step": 38000
    },
    {
      "epoch": 12.07433290978399,
      "grad_norm": 2.8579933643341064,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 38010
    },
    {
      "epoch": 12.07750952986023,
      "grad_norm": 2.617199420928955,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 38020
    },
    {
      "epoch": 12.080686149936467,
      "grad_norm": 1.8321555852890015,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 38030
    },
    {
      "epoch": 12.083862770012706,
      "grad_norm": 2.0819265842437744,
      "learning_rate": 2e-05,
      "loss": 0.0898,
      "step": 38040
    },
    {
      "epoch": 12.087039390088945,
      "grad_norm": 2.1053762435913086,
      "learning_rate": 2e-05,
      "loss": 0.0898,
      "step": 38050
    },
    {
      "epoch": 12.090216010165184,
      "grad_norm": 2.809572219848633,
      "learning_rate": 2e-05,
      "loss": 0.0936,
      "step": 38060
    },
    {
      "epoch": 12.093392630241423,
      "grad_norm": 2.5925960540771484,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 38070
    },
    {
      "epoch": 12.096569250317662,
      "grad_norm": 1.7787988185882568,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 38080
    },
    {
      "epoch": 12.099745870393901,
      "grad_norm": 1.9542584419250488,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 38090
    },
    {
      "epoch": 12.10292249047014,
      "grad_norm": 2.1744771003723145,
      "learning_rate": 2e-05,
      "loss": 0.0935,
      "step": 38100
    },
    {
      "epoch": 12.106099110546378,
      "grad_norm": 2.0863168239593506,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 38110
    },
    {
      "epoch": 12.107687420584497,
      "eval_loss": 1.841788411140442,
      "eval_mse": 1.840893606230589,
      "eval_pearson": 0.39496342740784474,
      "eval_runtime": 7.3918,
      "eval_samples_per_second": 2916.738,
      "eval_spearmanr": 0.4052984385643171,
      "eval_steps_per_second": 11.499,
      "step": 38115
    },
    {
      "epoch": 12.109275730622617,
      "grad_norm": 2.9447214603424072,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 38120
    },
    {
      "epoch": 12.112452350698856,
      "grad_norm": 3.6809487342834473,
      "learning_rate": 2e-05,
      "loss": 0.0898,
      "step": 38130
    },
    {
      "epoch": 12.115628970775095,
      "grad_norm": 2.356299638748169,
      "learning_rate": 2e-05,
      "loss": 0.094,
      "step": 38140
    },
    {
      "epoch": 12.118805590851334,
      "grad_norm": 2.988461494445801,
      "learning_rate": 2e-05,
      "loss": 0.0898,
      "step": 38150
    },
    {
      "epoch": 12.121982210927573,
      "grad_norm": 1.9841499328613281,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 38160
    },
    {
      "epoch": 12.125158831003812,
      "grad_norm": 2.2765274047851562,
      "learning_rate": 2e-05,
      "loss": 0.087,
      "step": 38170
    },
    {
      "epoch": 12.128335451080051,
      "grad_norm": 2.5990984439849854,
      "learning_rate": 2e-05,
      "loss": 0.091,
      "step": 38180
    },
    {
      "epoch": 12.13151207115629,
      "grad_norm": 2.658499240875244,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 38190
    },
    {
      "epoch": 12.134688691232528,
      "grad_norm": 4.545513153076172,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 38200
    },
    {
      "epoch": 12.137865311308767,
      "grad_norm": 3.1439902782440186,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 38210
    },
    {
      "epoch": 12.141041931385006,
      "grad_norm": 5.092403411865234,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 38220
    },
    {
      "epoch": 12.144218551461245,
      "grad_norm": 3.3048698902130127,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 38230
    },
    {
      "epoch": 12.147395171537484,
      "grad_norm": 5.493799209594727,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 38240
    },
    {
      "epoch": 12.150571791613723,
      "grad_norm": 2.2303338050842285,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 38250
    },
    {
      "epoch": 12.153748411689962,
      "grad_norm": 3.5208961963653564,
      "learning_rate": 2e-05,
      "loss": 0.0798,
      "step": 38260
    },
    {
      "epoch": 12.156925031766201,
      "grad_norm": 3.3990767002105713,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 38270
    },
    {
      "epoch": 12.16010165184244,
      "grad_norm": 3.3528711795806885,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 38280
    },
    {
      "epoch": 12.163278271918678,
      "grad_norm": 2.978884696960449,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 38290
    },
    {
      "epoch": 12.166454891994917,
      "grad_norm": 2.7485837936401367,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 38300
    },
    {
      "epoch": 12.169631512071156,
      "grad_norm": 2.7898740768432617,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 38310
    },
    {
      "epoch": 12.172808132147395,
      "grad_norm": 2.438290596008301,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 38320
    },
    {
      "epoch": 12.175984752223634,
      "grad_norm": 3.1470139026641846,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 38330
    },
    {
      "epoch": 12.179161372299873,
      "grad_norm": 2.107257604598999,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 38340
    },
    {
      "epoch": 12.182337992376112,
      "grad_norm": 1.6952850818634033,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 38350
    },
    {
      "epoch": 12.185514612452351,
      "grad_norm": 2.846529006958008,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 38360
    },
    {
      "epoch": 12.188691232528589,
      "grad_norm": 2.002445936203003,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 38370
    },
    {
      "epoch": 12.191867852604828,
      "grad_norm": 2.7769358158111572,
      "learning_rate": 2e-05,
      "loss": 0.0795,
      "step": 38380
    },
    {
      "epoch": 12.195044472681067,
      "grad_norm": 2.614555597305298,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 38390
    },
    {
      "epoch": 12.198221092757306,
      "grad_norm": 2.333932399749756,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 38400
    },
    {
      "epoch": 12.201397712833545,
      "grad_norm": 2.4249989986419678,
      "learning_rate": 2e-05,
      "loss": 0.0919,
      "step": 38410
    },
    {
      "epoch": 12.204574332909784,
      "grad_norm": 4.539488792419434,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 38420
    },
    {
      "epoch": 12.207750952986023,
      "grad_norm": 3.04095458984375,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 38430
    },
    {
      "epoch": 12.207750952986023,
      "eval_loss": 1.9058550596237183,
      "eval_mse": 1.9042417697124667,
      "eval_pearson": 0.3800390194289273,
      "eval_runtime": 7.4955,
      "eval_samples_per_second": 2876.398,
      "eval_spearmanr": 0.3917222915917817,
      "eval_steps_per_second": 11.34,
      "step": 38430
    },
    {
      "epoch": 12.210927573062262,
      "grad_norm": 2.291943311691284,
      "learning_rate": 2e-05,
      "loss": 0.0879,
      "step": 38440
    },
    {
      "epoch": 12.214104193138501,
      "grad_norm": 2.6429646015167236,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 38450
    },
    {
      "epoch": 12.217280813214739,
      "grad_norm": 6.691506385803223,
      "learning_rate": 2e-05,
      "loss": 0.0825,
      "step": 38460
    },
    {
      "epoch": 12.220457433290978,
      "grad_norm": 2.2838547229766846,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 38470
    },
    {
      "epoch": 12.223634053367217,
      "grad_norm": 2.2531065940856934,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 38480
    },
    {
      "epoch": 12.226810673443456,
      "grad_norm": 2.8850314617156982,
      "learning_rate": 2e-05,
      "loss": 0.094,
      "step": 38490
    },
    {
      "epoch": 12.229987293519695,
      "grad_norm": 2.868727207183838,
      "learning_rate": 2e-05,
      "loss": 0.0967,
      "step": 38500
    },
    {
      "epoch": 12.233163913595934,
      "grad_norm": 2.9108338356018066,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 38510
    },
    {
      "epoch": 12.236340533672173,
      "grad_norm": 2.591606616973877,
      "learning_rate": 2e-05,
      "loss": 0.0951,
      "step": 38520
    },
    {
      "epoch": 12.239517153748412,
      "grad_norm": 42.05965042114258,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 38530
    },
    {
      "epoch": 12.24269377382465,
      "grad_norm": 5.571779727935791,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 38540
    },
    {
      "epoch": 12.245870393900889,
      "grad_norm": 2.2713046073913574,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 38550
    },
    {
      "epoch": 12.249047013977128,
      "grad_norm": 2.9748928546905518,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 38560
    },
    {
      "epoch": 12.252223634053367,
      "grad_norm": 2.067136526107788,
      "learning_rate": 2e-05,
      "loss": 0.087,
      "step": 38570
    },
    {
      "epoch": 12.255400254129606,
      "grad_norm": 2.3261730670928955,
      "learning_rate": 2e-05,
      "loss": 0.0925,
      "step": 38580
    },
    {
      "epoch": 12.258576874205845,
      "grad_norm": 3.6701295375823975,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 38590
    },
    {
      "epoch": 12.261753494282084,
      "grad_norm": 1.6037272214889526,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 38600
    },
    {
      "epoch": 12.264930114358323,
      "grad_norm": 2.6085267066955566,
      "learning_rate": 2e-05,
      "loss": 0.0915,
      "step": 38610
    },
    {
      "epoch": 12.268106734434562,
      "grad_norm": 1.878473162651062,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 38620
    },
    {
      "epoch": 12.2712833545108,
      "grad_norm": 2.1599998474121094,
      "learning_rate": 2e-05,
      "loss": 0.0866,
      "step": 38630
    },
    {
      "epoch": 12.274459974587039,
      "grad_norm": 3.991547107696533,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 38640
    },
    {
      "epoch": 12.277636594663278,
      "grad_norm": 2.4653122425079346,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 38650
    },
    {
      "epoch": 12.280813214739517,
      "grad_norm": 2.321369171142578,
      "learning_rate": 2e-05,
      "loss": 0.099,
      "step": 38660
    },
    {
      "epoch": 12.283989834815756,
      "grad_norm": 3.8281402587890625,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 38670
    },
    {
      "epoch": 12.287166454891995,
      "grad_norm": 4.534133434295654,
      "learning_rate": 2e-05,
      "loss": 0.0866,
      "step": 38680
    },
    {
      "epoch": 12.290343074968234,
      "grad_norm": 2.681135416030884,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 38690
    },
    {
      "epoch": 12.293519695044473,
      "grad_norm": 3.391861915588379,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 38700
    },
    {
      "epoch": 12.29669631512071,
      "grad_norm": 2.489846706390381,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 38710
    },
    {
      "epoch": 12.29987293519695,
      "grad_norm": 3.4536778926849365,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 38720
    },
    {
      "epoch": 12.303049555273189,
      "grad_norm": 5.462790489196777,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 38730
    },
    {
      "epoch": 12.306226175349428,
      "grad_norm": 1.6298718452453613,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 38740
    },
    {
      "epoch": 12.307814485387548,
      "eval_loss": 1.903420329093933,
      "eval_mse": 1.9020460019195677,
      "eval_pearson": 0.38067904023409393,
      "eval_runtime": 7.2957,
      "eval_samples_per_second": 2955.175,
      "eval_spearmanr": 0.39114520894293214,
      "eval_steps_per_second": 11.651,
      "step": 38745
    },
    {
      "epoch": 12.309402795425667,
      "grad_norm": 2.3981006145477295,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 38750
    },
    {
      "epoch": 12.312579415501906,
      "grad_norm": 2.17771053314209,
      "learning_rate": 2e-05,
      "loss": 0.09,
      "step": 38760
    },
    {
      "epoch": 12.315756035578145,
      "grad_norm": 5.258417129516602,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 38770
    },
    {
      "epoch": 12.318932655654384,
      "grad_norm": 2.1895675659179688,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 38780
    },
    {
      "epoch": 12.322109275730623,
      "grad_norm": 2.938204288482666,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 38790
    },
    {
      "epoch": 12.32528589580686,
      "grad_norm": 2.6223158836364746,
      "learning_rate": 2e-05,
      "loss": 0.0855,
      "step": 38800
    },
    {
      "epoch": 12.3284625158831,
      "grad_norm": 1.8145794868469238,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 38810
    },
    {
      "epoch": 12.331639135959339,
      "grad_norm": 3.6293036937713623,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 38820
    },
    {
      "epoch": 12.334815756035578,
      "grad_norm": 3.1696321964263916,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 38830
    },
    {
      "epoch": 12.337992376111817,
      "grad_norm": 3.4029078483581543,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 38840
    },
    {
      "epoch": 12.341168996188056,
      "grad_norm": 2.943390369415283,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 38850
    },
    {
      "epoch": 12.344345616264295,
      "grad_norm": 2.016848564147949,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 38860
    },
    {
      "epoch": 12.347522236340534,
      "grad_norm": 2.898634672164917,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 38870
    },
    {
      "epoch": 12.350698856416773,
      "grad_norm": 1.7371917963027954,
      "learning_rate": 2e-05,
      "loss": 0.0855,
      "step": 38880
    },
    {
      "epoch": 12.35387547649301,
      "grad_norm": 3.6962831020355225,
      "learning_rate": 2e-05,
      "loss": 0.0874,
      "step": 38890
    },
    {
      "epoch": 12.35705209656925,
      "grad_norm": 2.3510243892669678,
      "learning_rate": 2e-05,
      "loss": 0.0945,
      "step": 38900
    },
    {
      "epoch": 12.360228716645489,
      "grad_norm": 2.2669732570648193,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 38910
    },
    {
      "epoch": 12.363405336721728,
      "grad_norm": 2.8684778213500977,
      "learning_rate": 2e-05,
      "loss": 0.0944,
      "step": 38920
    },
    {
      "epoch": 12.366581956797967,
      "grad_norm": 2.144340991973877,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 38930
    },
    {
      "epoch": 12.369758576874206,
      "grad_norm": 1.8925002813339233,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 38940
    },
    {
      "epoch": 12.372935196950445,
      "grad_norm": 1.8107011318206787,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 38950
    },
    {
      "epoch": 12.376111817026684,
      "grad_norm": 3.467543125152588,
      "learning_rate": 2e-05,
      "loss": 0.0971,
      "step": 38960
    },
    {
      "epoch": 12.379288437102922,
      "grad_norm": 2.0999536514282227,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 38970
    },
    {
      "epoch": 12.38246505717916,
      "grad_norm": 2.444951295852661,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 38980
    },
    {
      "epoch": 12.3856416772554,
      "grad_norm": 2.0280933380126953,
      "learning_rate": 2e-05,
      "loss": 0.0948,
      "step": 38990
    },
    {
      "epoch": 12.388818297331639,
      "grad_norm": 3.780932903289795,
      "learning_rate": 2e-05,
      "loss": 0.0802,
      "step": 39000
    },
    {
      "epoch": 12.391994917407878,
      "grad_norm": 2.637702703475952,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 39010
    },
    {
      "epoch": 12.395171537484117,
      "grad_norm": 3.972874164581299,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 39020
    },
    {
      "epoch": 12.398348157560356,
      "grad_norm": 2.5890121459960938,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 39030
    },
    {
      "epoch": 12.401524777636595,
      "grad_norm": 3.985018014907837,
      "learning_rate": 2e-05,
      "loss": 0.0959,
      "step": 39040
    },
    {
      "epoch": 12.404701397712834,
      "grad_norm": 5.245904445648193,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 39050
    },
    {
      "epoch": 12.407878017789072,
      "grad_norm": 1.9299302101135254,
      "learning_rate": 2e-05,
      "loss": 0.0917,
      "step": 39060
    },
    {
      "epoch": 12.407878017789072,
      "eval_loss": 1.9056622982025146,
      "eval_mse": 1.9040976545833923,
      "eval_pearson": 0.3878957053330457,
      "eval_runtime": 7.4119,
      "eval_samples_per_second": 2908.831,
      "eval_spearmanr": 0.39802794753404014,
      "eval_steps_per_second": 11.468,
      "step": 39060
    },
    {
      "epoch": 12.41105463786531,
      "grad_norm": 1.9139500856399536,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 39070
    },
    {
      "epoch": 12.41423125794155,
      "grad_norm": 2.7397327423095703,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 39080
    },
    {
      "epoch": 12.417407878017789,
      "grad_norm": 2.132242202758789,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 39090
    },
    {
      "epoch": 12.420584498094028,
      "grad_norm": 5.378198146820068,
      "learning_rate": 2e-05,
      "loss": 0.0958,
      "step": 39100
    },
    {
      "epoch": 12.423761118170267,
      "grad_norm": 1.8748658895492554,
      "learning_rate": 2e-05,
      "loss": 0.0988,
      "step": 39110
    },
    {
      "epoch": 12.426937738246506,
      "grad_norm": 2.2671501636505127,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 39120
    },
    {
      "epoch": 12.430114358322745,
      "grad_norm": 4.342686653137207,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 39130
    },
    {
      "epoch": 12.433290978398983,
      "grad_norm": 3.3518002033233643,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 39140
    },
    {
      "epoch": 12.436467598475222,
      "grad_norm": 3.7345237731933594,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 39150
    },
    {
      "epoch": 12.43964421855146,
      "grad_norm": 2.174762725830078,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 39160
    },
    {
      "epoch": 12.4428208386277,
      "grad_norm": 2.6237471103668213,
      "learning_rate": 2e-05,
      "loss": 0.0991,
      "step": 39170
    },
    {
      "epoch": 12.445997458703939,
      "grad_norm": 2.750953197479248,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 39180
    },
    {
      "epoch": 12.449174078780178,
      "grad_norm": 2.4712164402008057,
      "learning_rate": 2e-05,
      "loss": 0.087,
      "step": 39190
    },
    {
      "epoch": 12.452350698856417,
      "grad_norm": 2.3358707427978516,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 39200
    },
    {
      "epoch": 12.455527318932656,
      "grad_norm": 1.7221423387527466,
      "learning_rate": 2e-05,
      "loss": 0.094,
      "step": 39210
    },
    {
      "epoch": 12.458703939008895,
      "grad_norm": 1.7765443325042725,
      "learning_rate": 2e-05,
      "loss": 0.0874,
      "step": 39220
    },
    {
      "epoch": 12.461880559085133,
      "grad_norm": 2.008784532546997,
      "learning_rate": 2e-05,
      "loss": 0.0893,
      "step": 39230
    },
    {
      "epoch": 12.465057179161372,
      "grad_norm": 3.1614606380462646,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 39240
    },
    {
      "epoch": 12.46823379923761,
      "grad_norm": 3.7402536869049072,
      "learning_rate": 2e-05,
      "loss": 0.0943,
      "step": 39250
    },
    {
      "epoch": 12.47141041931385,
      "grad_norm": 1.7755345106124878,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 39260
    },
    {
      "epoch": 12.474587039390089,
      "grad_norm": 3.5809600353240967,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 39270
    },
    {
      "epoch": 12.477763659466328,
      "grad_norm": 3.87052059173584,
      "learning_rate": 2e-05,
      "loss": 0.0952,
      "step": 39280
    },
    {
      "epoch": 12.480940279542567,
      "grad_norm": 2.6923792362213135,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 39290
    },
    {
      "epoch": 12.484116899618806,
      "grad_norm": 4.776761531829834,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 39300
    },
    {
      "epoch": 12.487293519695044,
      "grad_norm": 2.5088908672332764,
      "learning_rate": 2e-05,
      "loss": 0.0853,
      "step": 39310
    },
    {
      "epoch": 12.490470139771283,
      "grad_norm": 2.3028223514556885,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 39320
    },
    {
      "epoch": 12.493646759847522,
      "grad_norm": 2.4912705421447754,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 39330
    },
    {
      "epoch": 12.496823379923761,
      "grad_norm": 2.4392740726470947,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 39340
    },
    {
      "epoch": 12.5,
      "grad_norm": 2.2532660961151123,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 39350
    },
    {
      "epoch": 12.503176620076239,
      "grad_norm": 2.3440024852752686,
      "learning_rate": 2e-05,
      "loss": 0.0998,
      "step": 39360
    },
    {
      "epoch": 12.506353240152478,
      "grad_norm": 1.766575813293457,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 39370
    },
    {
      "epoch": 12.507941550190598,
      "eval_loss": 1.7915314435958862,
      "eval_mse": 1.7903803914564218,
      "eval_pearson": 0.4161095265696679,
      "eval_runtime": 7.4102,
      "eval_samples_per_second": 2909.508,
      "eval_spearmanr": 0.4227987173995727,
      "eval_steps_per_second": 11.471,
      "step": 39375
    },
    {
      "epoch": 12.509529860228717,
      "grad_norm": 1.9746534824371338,
      "learning_rate": 2e-05,
      "loss": 0.0874,
      "step": 39380
    },
    {
      "epoch": 12.512706480304956,
      "grad_norm": 2.6742098331451416,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 39390
    },
    {
      "epoch": 12.515883100381194,
      "grad_norm": 2.054910659790039,
      "learning_rate": 2e-05,
      "loss": 0.0916,
      "step": 39400
    },
    {
      "epoch": 12.519059720457433,
      "grad_norm": 2.0673301219940186,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 39410
    },
    {
      "epoch": 12.522236340533672,
      "grad_norm": 3.502476692199707,
      "learning_rate": 2e-05,
      "loss": 0.0871,
      "step": 39420
    },
    {
      "epoch": 12.525412960609911,
      "grad_norm": 2.134429931640625,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 39430
    },
    {
      "epoch": 12.52858958068615,
      "grad_norm": 2.018068790435791,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 39440
    },
    {
      "epoch": 12.53176620076239,
      "grad_norm": 2.0763139724731445,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 39450
    },
    {
      "epoch": 12.534942820838628,
      "grad_norm": 2.3607046604156494,
      "learning_rate": 2e-05,
      "loss": 0.0901,
      "step": 39460
    },
    {
      "epoch": 12.538119440914867,
      "grad_norm": 1.9709855318069458,
      "learning_rate": 2e-05,
      "loss": 0.0762,
      "step": 39470
    },
    {
      "epoch": 12.541296060991105,
      "grad_norm": 3.7050864696502686,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 39480
    },
    {
      "epoch": 12.544472681067344,
      "grad_norm": 1.956143856048584,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 39490
    },
    {
      "epoch": 12.547649301143583,
      "grad_norm": 2.069579839706421,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 39500
    },
    {
      "epoch": 12.550825921219822,
      "grad_norm": 2.5657896995544434,
      "learning_rate": 2e-05,
      "loss": 0.0995,
      "step": 39510
    },
    {
      "epoch": 12.554002541296061,
      "grad_norm": 2.3433754444122314,
      "learning_rate": 2e-05,
      "loss": 0.0956,
      "step": 39520
    },
    {
      "epoch": 12.5571791613723,
      "grad_norm": 2.978991746902466,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 39530
    },
    {
      "epoch": 12.56035578144854,
      "grad_norm": 3.0493545532226562,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 39540
    },
    {
      "epoch": 12.563532401524778,
      "grad_norm": 1.7518088817596436,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 39550
    },
    {
      "epoch": 12.566709021601017,
      "grad_norm": 1.9600070714950562,
      "learning_rate": 2e-05,
      "loss": 0.0933,
      "step": 39560
    },
    {
      "epoch": 12.569885641677255,
      "grad_norm": 3.3075437545776367,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 39570
    },
    {
      "epoch": 12.573062261753494,
      "grad_norm": 2.639514207839966,
      "learning_rate": 2e-05,
      "loss": 0.0927,
      "step": 39580
    },
    {
      "epoch": 12.576238881829733,
      "grad_norm": 1.679298996925354,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 39590
    },
    {
      "epoch": 12.579415501905972,
      "grad_norm": 1.741077184677124,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 39600
    },
    {
      "epoch": 12.582592121982211,
      "grad_norm": 1.9061322212219238,
      "learning_rate": 2e-05,
      "loss": 0.0833,
      "step": 39610
    },
    {
      "epoch": 12.58576874205845,
      "grad_norm": 2.9589507579803467,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 39620
    },
    {
      "epoch": 12.58894536213469,
      "grad_norm": 1.6151381731033325,
      "learning_rate": 2e-05,
      "loss": 0.0905,
      "step": 39630
    },
    {
      "epoch": 12.592121982210928,
      "grad_norm": 2.229332447052002,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 39640
    },
    {
      "epoch": 12.595298602287166,
      "grad_norm": 6.435570240020752,
      "learning_rate": 2e-05,
      "loss": 0.0926,
      "step": 39650
    },
    {
      "epoch": 12.598475222363405,
      "grad_norm": 2.461679220199585,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 39660
    },
    {
      "epoch": 12.601651842439644,
      "grad_norm": 5.719533920288086,
      "learning_rate": 2e-05,
      "loss": 0.0815,
      "step": 39670
    },
    {
      "epoch": 12.604828462515883,
      "grad_norm": 2.3009819984436035,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 39680
    },
    {
      "epoch": 12.608005082592122,
      "grad_norm": 6.959132194519043,
      "learning_rate": 2e-05,
      "loss": 0.0869,
      "step": 39690
    },
    {
      "epoch": 12.608005082592122,
      "eval_loss": 1.850799322128296,
      "eval_mse": 1.8495729494183315,
      "eval_pearson": 0.4044789950909593,
      "eval_runtime": 7.5894,
      "eval_samples_per_second": 2840.797,
      "eval_spearmanr": 0.41251050576598647,
      "eval_steps_per_second": 11.2,
      "step": 39690
    },
    {
      "epoch": 12.611181702668361,
      "grad_norm": 2.5758795738220215,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 39700
    },
    {
      "epoch": 12.6143583227446,
      "grad_norm": 2.111159324645996,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 39710
    },
    {
      "epoch": 12.61753494282084,
      "grad_norm": 1.8530081510543823,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 39720
    },
    {
      "epoch": 12.620711562897078,
      "grad_norm": 4.382394313812256,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 39730
    },
    {
      "epoch": 12.623888182973316,
      "grad_norm": 2.68294358253479,
      "learning_rate": 2e-05,
      "loss": 0.09,
      "step": 39740
    },
    {
      "epoch": 12.627064803049555,
      "grad_norm": 2.7133708000183105,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 39750
    },
    {
      "epoch": 12.630241423125794,
      "grad_norm": 2.441157102584839,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 39760
    },
    {
      "epoch": 12.633418043202033,
      "grad_norm": 1.8044586181640625,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 39770
    },
    {
      "epoch": 12.636594663278272,
      "grad_norm": 2.553513288497925,
      "learning_rate": 2e-05,
      "loss": 0.0837,
      "step": 39780
    },
    {
      "epoch": 12.639771283354511,
      "grad_norm": 2.7564897537231445,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 39790
    },
    {
      "epoch": 12.64294790343075,
      "grad_norm": 2.1018502712249756,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 39800
    },
    {
      "epoch": 12.64612452350699,
      "grad_norm": 2.5133588314056396,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 39810
    },
    {
      "epoch": 12.649301143583227,
      "grad_norm": 5.045520782470703,
      "learning_rate": 2e-05,
      "loss": 0.0908,
      "step": 39820
    },
    {
      "epoch": 12.652477763659466,
      "grad_norm": 3.335350513458252,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 39830
    },
    {
      "epoch": 12.655654383735705,
      "grad_norm": 2.8369126319885254,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 39840
    },
    {
      "epoch": 12.658831003811944,
      "grad_norm": 5.259079456329346,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 39850
    },
    {
      "epoch": 12.662007623888183,
      "grad_norm": 2.3721795082092285,
      "learning_rate": 2e-05,
      "loss": 0.0939,
      "step": 39860
    },
    {
      "epoch": 12.665184243964422,
      "grad_norm": 5.6285080909729,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 39870
    },
    {
      "epoch": 12.668360864040661,
      "grad_norm": 1.8858234882354736,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 39880
    },
    {
      "epoch": 12.6715374841169,
      "grad_norm": 3.861482620239258,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 39890
    },
    {
      "epoch": 12.67471410419314,
      "grad_norm": 2.037262439727783,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 39900
    },
    {
      "epoch": 12.677890724269377,
      "grad_norm": 3.1454713344573975,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 39910
    },
    {
      "epoch": 12.681067344345616,
      "grad_norm": 1.6340007781982422,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 39920
    },
    {
      "epoch": 12.684243964421855,
      "grad_norm": 2.092355251312256,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 39930
    },
    {
      "epoch": 12.687420584498094,
      "grad_norm": 2.8465309143066406,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 39940
    },
    {
      "epoch": 12.690597204574333,
      "grad_norm": 2.139920949935913,
      "learning_rate": 2e-05,
      "loss": 0.0849,
      "step": 39950
    },
    {
      "epoch": 12.693773824650572,
      "grad_norm": 4.188043117523193,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 39960
    },
    {
      "epoch": 12.696950444726811,
      "grad_norm": 1.9638954401016235,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 39970
    },
    {
      "epoch": 12.70012706480305,
      "grad_norm": 8.408963203430176,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 39980
    },
    {
      "epoch": 12.703303684879288,
      "grad_norm": 1.8506889343261719,
      "learning_rate": 2e-05,
      "loss": 0.0903,
      "step": 39990
    },
    {
      "epoch": 12.706480304955527,
      "grad_norm": 3.308964490890503,
      "learning_rate": 2e-05,
      "loss": 0.0877,
      "step": 40000
    },
    {
      "epoch": 12.708068614993646,
      "eval_loss": 1.9106682538986206,
      "eval_mse": 1.9093310551216796,
      "eval_pearson": 0.3734496945268207,
      "eval_runtime": 7.4953,
      "eval_samples_per_second": 2876.468,
      "eval_spearmanr": 0.3885992057643716,
      "eval_steps_per_second": 11.34,
      "step": 40005
    },
    {
      "epoch": 12.709656925031766,
      "grad_norm": 2.3034846782684326,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 40010
    },
    {
      "epoch": 12.712833545108005,
      "grad_norm": 1.8738874197006226,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 40020
    },
    {
      "epoch": 12.716010165184244,
      "grad_norm": 3.958611011505127,
      "learning_rate": 2e-05,
      "loss": 0.0918,
      "step": 40030
    },
    {
      "epoch": 12.719186785260483,
      "grad_norm": 4.791311264038086,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 40040
    },
    {
      "epoch": 12.722363405336722,
      "grad_norm": 3.1797702312469482,
      "learning_rate": 2e-05,
      "loss": 0.09,
      "step": 40050
    },
    {
      "epoch": 12.725540025412961,
      "grad_norm": 3.119816303253174,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 40060
    },
    {
      "epoch": 12.7287166454892,
      "grad_norm": 3.036365270614624,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 40070
    },
    {
      "epoch": 12.731893265565438,
      "grad_norm": 2.751361608505249,
      "learning_rate": 2e-05,
      "loss": 0.0958,
      "step": 40080
    },
    {
      "epoch": 12.735069885641677,
      "grad_norm": 2.4465036392211914,
      "learning_rate": 2e-05,
      "loss": 0.0873,
      "step": 40090
    },
    {
      "epoch": 12.738246505717916,
      "grad_norm": 1.907428503036499,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 40100
    },
    {
      "epoch": 12.741423125794155,
      "grad_norm": 2.1639485359191895,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 40110
    },
    {
      "epoch": 12.744599745870394,
      "grad_norm": 1.9073560237884521,
      "learning_rate": 2e-05,
      "loss": 0.0885,
      "step": 40120
    },
    {
      "epoch": 12.747776365946633,
      "grad_norm": 1.8643574714660645,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 40130
    },
    {
      "epoch": 12.750952986022872,
      "grad_norm": 2.437469720840454,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 40140
    },
    {
      "epoch": 12.754129606099111,
      "grad_norm": 2.9142372608184814,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 40150
    },
    {
      "epoch": 12.757306226175348,
      "grad_norm": 2.000905990600586,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 40160
    },
    {
      "epoch": 12.760482846251588,
      "grad_norm": 1.8937751054763794,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 40170
    },
    {
      "epoch": 12.763659466327827,
      "grad_norm": 1.9248584508895874,
      "learning_rate": 2e-05,
      "loss": 0.0985,
      "step": 40180
    },
    {
      "epoch": 12.766836086404066,
      "grad_norm": 3.275334358215332,
      "learning_rate": 2e-05,
      "loss": 0.0974,
      "step": 40190
    },
    {
      "epoch": 12.770012706480305,
      "grad_norm": 2.0430967807769775,
      "learning_rate": 2e-05,
      "loss": 0.0888,
      "step": 40200
    },
    {
      "epoch": 12.773189326556544,
      "grad_norm": 3.2374463081359863,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 40210
    },
    {
      "epoch": 12.776365946632783,
      "grad_norm": 2.354376792907715,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 40220
    },
    {
      "epoch": 12.779542566709022,
      "grad_norm": 5.600395679473877,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 40230
    },
    {
      "epoch": 12.782719186785261,
      "grad_norm": 10.59752082824707,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 40240
    },
    {
      "epoch": 12.7858958068615,
      "grad_norm": 3.0652034282684326,
      "learning_rate": 2e-05,
      "loss": 0.0921,
      "step": 40250
    },
    {
      "epoch": 12.789072426937738,
      "grad_norm": 1.9167689085006714,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 40260
    },
    {
      "epoch": 12.792249047013977,
      "grad_norm": 2.4565322399139404,
      "learning_rate": 2e-05,
      "loss": 0.0861,
      "step": 40270
    },
    {
      "epoch": 12.795425667090216,
      "grad_norm": 1.789919137954712,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 40280
    },
    {
      "epoch": 12.798602287166455,
      "grad_norm": 2.81252121925354,
      "learning_rate": 2e-05,
      "loss": 0.0938,
      "step": 40290
    },
    {
      "epoch": 12.801778907242694,
      "grad_norm": 4.8145575523376465,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 40300
    },
    {
      "epoch": 12.804955527318933,
      "grad_norm": 2.3659420013427734,
      "learning_rate": 2e-05,
      "loss": 0.093,
      "step": 40310
    },
    {
      "epoch": 12.808132147395172,
      "grad_norm": 7.032491683959961,
      "learning_rate": 2e-05,
      "loss": 0.0875,
      "step": 40320
    },
    {
      "epoch": 12.808132147395172,
      "eval_loss": 1.9078257083892822,
      "eval_mse": 1.9068090503174875,
      "eval_pearson": 0.36895162552972693,
      "eval_runtime": 7.5297,
      "eval_samples_per_second": 2863.327,
      "eval_spearmanr": 0.38149697941840555,
      "eval_steps_per_second": 11.289,
      "step": 40320
    },
    {
      "epoch": 12.811308767471411,
      "grad_norm": 2.112889051437378,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 40330
    },
    {
      "epoch": 12.814485387547649,
      "grad_norm": 2.0485622882843018,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 40340
    },
    {
      "epoch": 12.817662007623888,
      "grad_norm": 2.3358747959136963,
      "learning_rate": 2e-05,
      "loss": 0.0899,
      "step": 40350
    },
    {
      "epoch": 12.820838627700127,
      "grad_norm": 3.726332187652588,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 40360
    },
    {
      "epoch": 12.824015247776366,
      "grad_norm": 2.3863847255706787,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 40370
    },
    {
      "epoch": 12.827191867852605,
      "grad_norm": 2.4324793815612793,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 40380
    },
    {
      "epoch": 12.830368487928844,
      "grad_norm": 1.8743650913238525,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 40390
    },
    {
      "epoch": 12.833545108005083,
      "grad_norm": 2.4425179958343506,
      "learning_rate": 2e-05,
      "loss": 0.095,
      "step": 40400
    },
    {
      "epoch": 12.836721728081322,
      "grad_norm": 2.4758458137512207,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 40410
    },
    {
      "epoch": 12.839898348157561,
      "grad_norm": 2.80837082862854,
      "learning_rate": 2e-05,
      "loss": 0.0934,
      "step": 40420
    },
    {
      "epoch": 12.843074968233799,
      "grad_norm": 3.5866379737854004,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 40430
    },
    {
      "epoch": 12.846251588310038,
      "grad_norm": 7.247672080993652,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 40440
    },
    {
      "epoch": 12.849428208386277,
      "grad_norm": 1.9547334909439087,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 40450
    },
    {
      "epoch": 12.852604828462516,
      "grad_norm": 2.1129302978515625,
      "learning_rate": 2e-05,
      "loss": 0.0904,
      "step": 40460
    },
    {
      "epoch": 12.855781448538755,
      "grad_norm": 2.6653144359588623,
      "learning_rate": 2e-05,
      "loss": 0.0816,
      "step": 40470
    },
    {
      "epoch": 12.858958068614994,
      "grad_norm": 3.2239930629730225,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 40480
    },
    {
      "epoch": 12.862134688691233,
      "grad_norm": 2.578777551651001,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 40490
    },
    {
      "epoch": 12.865311308767472,
      "grad_norm": 2.8633341789245605,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 40500
    },
    {
      "epoch": 12.86848792884371,
      "grad_norm": 2.2964720726013184,
      "learning_rate": 2e-05,
      "loss": 0.0913,
      "step": 40510
    },
    {
      "epoch": 12.871664548919949,
      "grad_norm": 3.7241153717041016,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 40520
    },
    {
      "epoch": 12.874841168996188,
      "grad_norm": 2.023405075073242,
      "learning_rate": 2e-05,
      "loss": 0.1009,
      "step": 40530
    },
    {
      "epoch": 12.878017789072427,
      "grad_norm": 2.434272289276123,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 40540
    },
    {
      "epoch": 12.881194409148666,
      "grad_norm": 2.888500928878784,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 40550
    },
    {
      "epoch": 12.884371029224905,
      "grad_norm": 2.4475932121276855,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 40560
    },
    {
      "epoch": 12.887547649301144,
      "grad_norm": 1.9181283712387085,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 40570
    },
    {
      "epoch": 12.890724269377383,
      "grad_norm": 2.483492136001587,
      "learning_rate": 2e-05,
      "loss": 0.0875,
      "step": 40580
    },
    {
      "epoch": 12.893900889453622,
      "grad_norm": 1.9828441143035889,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 40590
    },
    {
      "epoch": 12.89707750952986,
      "grad_norm": 2.811497926712036,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 40600
    },
    {
      "epoch": 12.900254129606099,
      "grad_norm": 2.507627248764038,
      "learning_rate": 2e-05,
      "loss": 0.0838,
      "step": 40610
    },
    {
      "epoch": 12.903430749682338,
      "grad_norm": 2.2070610523223877,
      "learning_rate": 2e-05,
      "loss": 0.0978,
      "step": 40620
    },
    {
      "epoch": 12.906607369758577,
      "grad_norm": 2.179985761642456,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 40630
    },
    {
      "epoch": 12.908195679796696,
      "eval_loss": 1.9690090417861938,
      "eval_mse": 1.9675752049059727,
      "eval_pearson": 0.36680211005465063,
      "eval_runtime": 7.5794,
      "eval_samples_per_second": 2844.569,
      "eval_spearmanr": 0.3796348673023835,
      "eval_steps_per_second": 11.215,
      "step": 40635
    },
    {
      "epoch": 12.909783989834816,
      "grad_norm": 1.6775919198989868,
      "learning_rate": 2e-05,
      "loss": 0.0831,
      "step": 40640
    },
    {
      "epoch": 12.912960609911055,
      "grad_norm": 2.045564889907837,
      "learning_rate": 2e-05,
      "loss": 0.0896,
      "step": 40650
    },
    {
      "epoch": 12.916137229987294,
      "grad_norm": 2.720620632171631,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 40660
    },
    {
      "epoch": 12.919313850063533,
      "grad_norm": 1.9148212671279907,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 40670
    },
    {
      "epoch": 12.92249047013977,
      "grad_norm": 2.211275339126587,
      "learning_rate": 2e-05,
      "loss": 0.091,
      "step": 40680
    },
    {
      "epoch": 12.92566709021601,
      "grad_norm": 3.275729179382324,
      "learning_rate": 2e-05,
      "loss": 0.0924,
      "step": 40690
    },
    {
      "epoch": 12.928843710292249,
      "grad_norm": 2.1784772872924805,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 40700
    },
    {
      "epoch": 12.932020330368488,
      "grad_norm": 3.0575923919677734,
      "learning_rate": 2e-05,
      "loss": 0.0903,
      "step": 40710
    },
    {
      "epoch": 12.935196950444727,
      "grad_norm": 1.5542511940002441,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 40720
    },
    {
      "epoch": 12.938373570520966,
      "grad_norm": 2.6525332927703857,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 40730
    },
    {
      "epoch": 12.941550190597205,
      "grad_norm": 1.7068670988082886,
      "learning_rate": 2e-05,
      "loss": 0.0855,
      "step": 40740
    },
    {
      "epoch": 12.944726810673444,
      "grad_norm": 1.779704213142395,
      "learning_rate": 2e-05,
      "loss": 0.0837,
      "step": 40750
    },
    {
      "epoch": 12.947903430749683,
      "grad_norm": 1.6013891696929932,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 40760
    },
    {
      "epoch": 12.95108005082592,
      "grad_norm": 1.7587461471557617,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 40770
    },
    {
      "epoch": 12.95425667090216,
      "grad_norm": 2.3976147174835205,
      "learning_rate": 2e-05,
      "loss": 0.0903,
      "step": 40780
    },
    {
      "epoch": 12.957433290978399,
      "grad_norm": 4.186550140380859,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 40790
    },
    {
      "epoch": 12.960609911054638,
      "grad_norm": 2.987588405609131,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 40800
    },
    {
      "epoch": 12.963786531130877,
      "grad_norm": 2.7492868900299072,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 40810
    },
    {
      "epoch": 12.966963151207116,
      "grad_norm": 1.9030832052230835,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 40820
    },
    {
      "epoch": 12.970139771283355,
      "grad_norm": 2.300095796585083,
      "learning_rate": 2e-05,
      "loss": 0.0867,
      "step": 40830
    },
    {
      "epoch": 12.973316391359594,
      "grad_norm": 3.962585926055908,
      "learning_rate": 2e-05,
      "loss": 0.0815,
      "step": 40840
    },
    {
      "epoch": 12.976493011435831,
      "grad_norm": 1.9660991430282593,
      "learning_rate": 2e-05,
      "loss": 0.0847,
      "step": 40850
    },
    {
      "epoch": 12.97966963151207,
      "grad_norm": 2.8884341716766357,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 40860
    },
    {
      "epoch": 12.98284625158831,
      "grad_norm": 2.076463222503662,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 40870
    },
    {
      "epoch": 12.986022871664549,
      "grad_norm": 1.9712326526641846,
      "learning_rate": 2e-05,
      "loss": 0.0898,
      "step": 40880
    },
    {
      "epoch": 12.989199491740788,
      "grad_norm": 2.339142084121704,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 40890
    },
    {
      "epoch": 12.992376111817027,
      "grad_norm": 1.506783366203308,
      "learning_rate": 2e-05,
      "loss": 0.0897,
      "step": 40900
    },
    {
      "epoch": 12.995552731893266,
      "grad_norm": 3.9141552448272705,
      "learning_rate": 2e-05,
      "loss": 0.078,
      "step": 40910
    },
    {
      "epoch": 12.998729351969505,
      "grad_norm": 3.3753151893615723,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 40920
    },
    {
      "epoch": 13.001905972045744,
      "grad_norm": 2.37962007522583,
      "learning_rate": 2e-05,
      "loss": 0.0875,
      "step": 40930
    },
    {
      "epoch": 13.005082592121981,
      "grad_norm": 3.8294425010681152,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 40940
    },
    {
      "epoch": 13.00825921219822,
      "grad_norm": 2.1726574897766113,
      "learning_rate": 2e-05,
      "loss": 0.0787,
      "step": 40950
    },
    {
      "epoch": 13.00825921219822,
      "eval_loss": 1.873641014099121,
      "eval_mse": 1.8717675025870328,
      "eval_pearson": 0.39316126174738364,
      "eval_runtime": 7.6063,
      "eval_samples_per_second": 2834.498,
      "eval_spearmanr": 0.40466764602364813,
      "eval_steps_per_second": 11.175,
      "step": 40950
    },
    {
      "epoch": 13.01143583227446,
      "grad_norm": 2.3669328689575195,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 40960
    },
    {
      "epoch": 13.014612452350699,
      "grad_norm": 3.5597288608551025,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 40970
    },
    {
      "epoch": 13.017789072426938,
      "grad_norm": 1.3729757070541382,
      "learning_rate": 2e-05,
      "loss": 0.0787,
      "step": 40980
    },
    {
      "epoch": 13.020965692503177,
      "grad_norm": 2.839526414871216,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 40990
    },
    {
      "epoch": 13.024142312579416,
      "grad_norm": 2.9693098068237305,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 41000
    },
    {
      "epoch": 13.027318932655655,
      "grad_norm": 2.00058913230896,
      "learning_rate": 2e-05,
      "loss": 0.0759,
      "step": 41010
    },
    {
      "epoch": 13.030495552731892,
      "grad_norm": 2.3522424697875977,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 41020
    },
    {
      "epoch": 13.033672172808132,
      "grad_norm": 1.4879215955734253,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 41030
    },
    {
      "epoch": 13.03684879288437,
      "grad_norm": 1.8887590169906616,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 41040
    },
    {
      "epoch": 13.04002541296061,
      "grad_norm": 1.2850475311279297,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 41050
    },
    {
      "epoch": 13.043202033036849,
      "grad_norm": 3.289715528488159,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 41060
    },
    {
      "epoch": 13.046378653113088,
      "grad_norm": 2.380192756652832,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 41070
    },
    {
      "epoch": 13.049555273189327,
      "grad_norm": 1.8933676481246948,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 41080
    },
    {
      "epoch": 13.052731893265566,
      "grad_norm": 6.18718147277832,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 41090
    },
    {
      "epoch": 13.055908513341805,
      "grad_norm": 2.478487014770508,
      "learning_rate": 2e-05,
      "loss": 0.0949,
      "step": 41100
    },
    {
      "epoch": 13.059085133418042,
      "grad_norm": 10.029857635498047,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 41110
    },
    {
      "epoch": 13.062261753494282,
      "grad_norm": 2.3592288494110107,
      "learning_rate": 2e-05,
      "loss": 0.0891,
      "step": 41120
    },
    {
      "epoch": 13.06543837357052,
      "grad_norm": 2.1151013374328613,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 41130
    },
    {
      "epoch": 13.06861499364676,
      "grad_norm": 1.702091932296753,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 41140
    },
    {
      "epoch": 13.071791613722999,
      "grad_norm": 2.7958004474639893,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 41150
    },
    {
      "epoch": 13.074968233799238,
      "grad_norm": 2.5832507610321045,
      "learning_rate": 2e-05,
      "loss": 0.081,
      "step": 41160
    },
    {
      "epoch": 13.078144853875477,
      "grad_norm": 2.719310998916626,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 41170
    },
    {
      "epoch": 13.081321473951716,
      "grad_norm": 3.3714520931243896,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 41180
    },
    {
      "epoch": 13.084498094027953,
      "grad_norm": 1.8287070989608765,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 41190
    },
    {
      "epoch": 13.087674714104192,
      "grad_norm": 2.6044180393218994,
      "learning_rate": 2e-05,
      "loss": 0.0838,
      "step": 41200
    },
    {
      "epoch": 13.090851334180432,
      "grad_norm": 3.187206745147705,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 41210
    },
    {
      "epoch": 13.09402795425667,
      "grad_norm": 1.964709997177124,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 41220
    },
    {
      "epoch": 13.09720457433291,
      "grad_norm": 4.127906799316406,
      "learning_rate": 2e-05,
      "loss": 0.0914,
      "step": 41230
    },
    {
      "epoch": 13.100381194409149,
      "grad_norm": 2.7687196731567383,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 41240
    },
    {
      "epoch": 13.103557814485388,
      "grad_norm": 4.37141227722168,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 41250
    },
    {
      "epoch": 13.106734434561627,
      "grad_norm": 2.1545963287353516,
      "learning_rate": 2e-05,
      "loss": 0.0838,
      "step": 41260
    },
    {
      "epoch": 13.108322744599747,
      "eval_loss": 1.8700460195541382,
      "eval_mse": 1.8688205625440282,
      "eval_pearson": 0.3889510730856484,
      "eval_runtime": 7.4033,
      "eval_samples_per_second": 2912.208,
      "eval_spearmanr": 0.39493932895414274,
      "eval_steps_per_second": 11.481,
      "step": 41265
    },
    {
      "epoch": 13.109911054637866,
      "grad_norm": 2.8897008895874023,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 41270
    },
    {
      "epoch": 13.113087674714103,
      "grad_norm": 1.7222919464111328,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 41280
    },
    {
      "epoch": 13.116264294790343,
      "grad_norm": 3.8478779792785645,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 41290
    },
    {
      "epoch": 13.119440914866582,
      "grad_norm": 2.89778995513916,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 41300
    },
    {
      "epoch": 13.12261753494282,
      "grad_norm": 1.9985660314559937,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 41310
    },
    {
      "epoch": 13.12579415501906,
      "grad_norm": 8.300142288208008,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 41320
    },
    {
      "epoch": 13.128970775095299,
      "grad_norm": 1.9879604578018188,
      "learning_rate": 2e-05,
      "loss": 0.0791,
      "step": 41330
    },
    {
      "epoch": 13.132147395171538,
      "grad_norm": 8.542352676391602,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 41340
    },
    {
      "epoch": 13.135324015247777,
      "grad_norm": 1.9757689237594604,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 41350
    },
    {
      "epoch": 13.138500635324014,
      "grad_norm": 1.6589521169662476,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 41360
    },
    {
      "epoch": 13.141677255400253,
      "grad_norm": 2.345942497253418,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 41370
    },
    {
      "epoch": 13.144853875476493,
      "grad_norm": 2.1096084117889404,
      "learning_rate": 2e-05,
      "loss": 0.0796,
      "step": 41380
    },
    {
      "epoch": 13.148030495552732,
      "grad_norm": 3.6791019439697266,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 41390
    },
    {
      "epoch": 13.15120711562897,
      "grad_norm": 1.8886350393295288,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 41400
    },
    {
      "epoch": 13.15438373570521,
      "grad_norm": 2.5306992530822754,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 41410
    },
    {
      "epoch": 13.157560355781449,
      "grad_norm": 2.8874824047088623,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 41420
    },
    {
      "epoch": 13.160736975857688,
      "grad_norm": 2.8144989013671875,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 41430
    },
    {
      "epoch": 13.163913595933927,
      "grad_norm": 1.7291386127471924,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 41440
    },
    {
      "epoch": 13.167090216010164,
      "grad_norm": 2.4163999557495117,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 41450
    },
    {
      "epoch": 13.170266836086403,
      "grad_norm": 2.621326446533203,
      "learning_rate": 2e-05,
      "loss": 0.0852,
      "step": 41460
    },
    {
      "epoch": 13.173443456162643,
      "grad_norm": 2.0369975566864014,
      "learning_rate": 2e-05,
      "loss": 0.0789,
      "step": 41470
    },
    {
      "epoch": 13.176620076238882,
      "grad_norm": 2.018496036529541,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 41480
    },
    {
      "epoch": 13.17979669631512,
      "grad_norm": 2.8114657402038574,
      "learning_rate": 2e-05,
      "loss": 0.0759,
      "step": 41490
    },
    {
      "epoch": 13.18297331639136,
      "grad_norm": 1.5029264688491821,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 41500
    },
    {
      "epoch": 13.186149936467599,
      "grad_norm": 3.4579808712005615,
      "learning_rate": 2e-05,
      "loss": 0.0815,
      "step": 41510
    },
    {
      "epoch": 13.189326556543838,
      "grad_norm": 3.5410358905792236,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 41520
    },
    {
      "epoch": 13.192503176620077,
      "grad_norm": 3.062028408050537,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 41530
    },
    {
      "epoch": 13.195679796696314,
      "grad_norm": 2.1205673217773438,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 41540
    },
    {
      "epoch": 13.198856416772554,
      "grad_norm": 2.092893123626709,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 41550
    },
    {
      "epoch": 13.202033036848793,
      "grad_norm": 3.2407500743865967,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 41560
    },
    {
      "epoch": 13.205209656925032,
      "grad_norm": 2.6469027996063232,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 41570
    },
    {
      "epoch": 13.20838627700127,
      "grad_norm": 3.442133903503418,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 41580
    },
    {
      "epoch": 13.20838627700127,
      "eval_loss": 1.9275368452072144,
      "eval_mse": 1.9264713942009355,
      "eval_pearson": 0.3566196816773728,
      "eval_runtime": 7.3639,
      "eval_samples_per_second": 2927.805,
      "eval_spearmanr": 0.366355900191037,
      "eval_steps_per_second": 11.543,
      "step": 41580
    },
    {
      "epoch": 13.21156289707751,
      "grad_norm": 3.278377056121826,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 41590
    },
    {
      "epoch": 13.214739517153749,
      "grad_norm": 1.626531720161438,
      "learning_rate": 2e-05,
      "loss": 0.0799,
      "step": 41600
    },
    {
      "epoch": 13.217916137229988,
      "grad_norm": 2.31544828414917,
      "learning_rate": 2e-05,
      "loss": 0.0815,
      "step": 41610
    },
    {
      "epoch": 13.221092757306225,
      "grad_norm": 2.6248180866241455,
      "learning_rate": 2e-05,
      "loss": 0.0744,
      "step": 41620
    },
    {
      "epoch": 13.224269377382464,
      "grad_norm": 2.275986433029175,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 41630
    },
    {
      "epoch": 13.227445997458704,
      "grad_norm": 2.0147290229797363,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 41640
    },
    {
      "epoch": 13.230622617534943,
      "grad_norm": 2.9599618911743164,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 41650
    },
    {
      "epoch": 13.233799237611182,
      "grad_norm": 2.0092766284942627,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 41660
    },
    {
      "epoch": 13.23697585768742,
      "grad_norm": 2.6169228553771973,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 41670
    },
    {
      "epoch": 13.24015247776366,
      "grad_norm": 2.7411088943481445,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 41680
    },
    {
      "epoch": 13.243329097839899,
      "grad_norm": 2.0571959018707275,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 41690
    },
    {
      "epoch": 13.246505717916138,
      "grad_norm": 1.6642518043518066,
      "learning_rate": 2e-05,
      "loss": 0.0849,
      "step": 41700
    },
    {
      "epoch": 13.249682337992375,
      "grad_norm": 3.9261696338653564,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 41710
    },
    {
      "epoch": 13.252858958068614,
      "grad_norm": 3.057382106781006,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 41720
    },
    {
      "epoch": 13.256035578144854,
      "grad_norm": 2.4352450370788574,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 41730
    },
    {
      "epoch": 13.259212198221093,
      "grad_norm": 3.5201964378356934,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 41740
    },
    {
      "epoch": 13.262388818297332,
      "grad_norm": 1.8668794631958008,
      "learning_rate": 2e-05,
      "loss": 0.0786,
      "step": 41750
    },
    {
      "epoch": 13.26556543837357,
      "grad_norm": 1.9557238817214966,
      "learning_rate": 2e-05,
      "loss": 0.0785,
      "step": 41760
    },
    {
      "epoch": 13.26874205844981,
      "grad_norm": 1.8877426385879517,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 41770
    },
    {
      "epoch": 13.271918678526049,
      "grad_norm": 1.8197362422943115,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 41780
    },
    {
      "epoch": 13.275095298602286,
      "grad_norm": 2.781298875808716,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 41790
    },
    {
      "epoch": 13.278271918678525,
      "grad_norm": 2.055218458175659,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 41800
    },
    {
      "epoch": 13.281448538754764,
      "grad_norm": 2.787707805633545,
      "learning_rate": 2e-05,
      "loss": 0.088,
      "step": 41810
    },
    {
      "epoch": 13.284625158831004,
      "grad_norm": 3.1965718269348145,
      "learning_rate": 2e-05,
      "loss": 0.0791,
      "step": 41820
    },
    {
      "epoch": 13.287801778907243,
      "grad_norm": 1.8682361841201782,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 41830
    },
    {
      "epoch": 13.290978398983482,
      "grad_norm": 3.282582998275757,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 41840
    },
    {
      "epoch": 13.29415501905972,
      "grad_norm": 1.7810226678848267,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 41850
    },
    {
      "epoch": 13.29733163913596,
      "grad_norm": 5.677526950836182,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 41860
    },
    {
      "epoch": 13.300508259212199,
      "grad_norm": 1.6864522695541382,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 41870
    },
    {
      "epoch": 13.303684879288436,
      "grad_norm": 2.239635705947876,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 41880
    },
    {
      "epoch": 13.306861499364675,
      "grad_norm": 3.0062668323516846,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 41890
    },
    {
      "epoch": 13.308449809402795,
      "eval_loss": 1.9490714073181152,
      "eval_mse": 1.9480688806865123,
      "eval_pearson": 0.3846779527245233,
      "eval_runtime": 7.4226,
      "eval_samples_per_second": 2904.625,
      "eval_spearmanr": 0.3943506300255183,
      "eval_steps_per_second": 11.451,
      "step": 41895
    },
    {
      "epoch": 13.310038119440915,
      "grad_norm": 1.6084768772125244,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 41900
    },
    {
      "epoch": 13.313214739517154,
      "grad_norm": 1.9971376657485962,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 41910
    },
    {
      "epoch": 13.316391359593393,
      "grad_norm": 5.279058456420898,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 41920
    },
    {
      "epoch": 13.319567979669632,
      "grad_norm": 1.877078652381897,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 41930
    },
    {
      "epoch": 13.32274459974587,
      "grad_norm": 2.6425814628601074,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 41940
    },
    {
      "epoch": 13.32592121982211,
      "grad_norm": 1.719499111175537,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 41950
    },
    {
      "epoch": 13.329097839898349,
      "grad_norm": 1.8661813735961914,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 41960
    },
    {
      "epoch": 13.332274459974586,
      "grad_norm": 3.19720196723938,
      "learning_rate": 2e-05,
      "loss": 0.0795,
      "step": 41970
    },
    {
      "epoch": 13.335451080050825,
      "grad_norm": 1.9518773555755615,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 41980
    },
    {
      "epoch": 13.338627700127065,
      "grad_norm": 2.290670156478882,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 41990
    },
    {
      "epoch": 13.341804320203304,
      "grad_norm": 2.9013965129852295,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 42000
    },
    {
      "epoch": 13.344980940279543,
      "grad_norm": 2.965135097503662,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 42010
    },
    {
      "epoch": 13.348157560355782,
      "grad_norm": 2.600486993789673,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 42020
    },
    {
      "epoch": 13.351334180432021,
      "grad_norm": 2.627168655395508,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 42030
    },
    {
      "epoch": 13.35451080050826,
      "grad_norm": 1.4617094993591309,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 42040
    },
    {
      "epoch": 13.357687420584497,
      "grad_norm": 2.0710716247558594,
      "learning_rate": 2e-05,
      "loss": 0.0895,
      "step": 42050
    },
    {
      "epoch": 13.360864040660736,
      "grad_norm": 2.1114203929901123,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 42060
    },
    {
      "epoch": 13.364040660736975,
      "grad_norm": 2.961501121520996,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 42070
    },
    {
      "epoch": 13.367217280813215,
      "grad_norm": 1.701311707496643,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 42080
    },
    {
      "epoch": 13.370393900889454,
      "grad_norm": 1.8541839122772217,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 42090
    },
    {
      "epoch": 13.373570520965693,
      "grad_norm": 7.285867691040039,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 42100
    },
    {
      "epoch": 13.376747141041932,
      "grad_norm": 3.764094114303589,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 42110
    },
    {
      "epoch": 13.379923761118171,
      "grad_norm": 1.7321304082870483,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 42120
    },
    {
      "epoch": 13.38310038119441,
      "grad_norm": 2.352644443511963,
      "learning_rate": 2e-05,
      "loss": 0.0744,
      "step": 42130
    },
    {
      "epoch": 13.386277001270647,
      "grad_norm": 1.6956145763397217,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 42140
    },
    {
      "epoch": 13.389453621346886,
      "grad_norm": 3.6341283321380615,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 42150
    },
    {
      "epoch": 13.392630241423126,
      "grad_norm": 2.3986175060272217,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 42160
    },
    {
      "epoch": 13.395806861499365,
      "grad_norm": 1.848497748374939,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 42170
    },
    {
      "epoch": 13.398983481575604,
      "grad_norm": 1.9760693311691284,
      "learning_rate": 2e-05,
      "loss": 0.0765,
      "step": 42180
    },
    {
      "epoch": 13.402160101651843,
      "grad_norm": 21.07830238342285,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 42190
    },
    {
      "epoch": 13.405336721728082,
      "grad_norm": 3.1550774574279785,
      "learning_rate": 2e-05,
      "loss": 0.0864,
      "step": 42200
    },
    {
      "epoch": 13.408513341804321,
      "grad_norm": 3.9903831481933594,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 42210
    },
    {
      "epoch": 13.408513341804321,
      "eval_loss": 1.9401609897613525,
      "eval_mse": 1.9385455676632655,
      "eval_pearson": 0.39724488447862366,
      "eval_runtime": 7.2989,
      "eval_samples_per_second": 2953.871,
      "eval_spearmanr": 0.40906849024129066,
      "eval_steps_per_second": 11.646,
      "step": 42210
    },
    {
      "epoch": 13.411689961880558,
      "grad_norm": 3.113828659057617,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 42220
    },
    {
      "epoch": 13.414866581956797,
      "grad_norm": 1.888710856437683,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 42230
    },
    {
      "epoch": 13.418043202033036,
      "grad_norm": 6.730861663818359,
      "learning_rate": 2e-05,
      "loss": 0.077,
      "step": 42240
    },
    {
      "epoch": 13.421219822109276,
      "grad_norm": 1.8601365089416504,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 42250
    },
    {
      "epoch": 13.424396442185515,
      "grad_norm": 3.9004499912261963,
      "learning_rate": 2e-05,
      "loss": 0.0832,
      "step": 42260
    },
    {
      "epoch": 13.427573062261754,
      "grad_norm": 2.4223456382751465,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 42270
    },
    {
      "epoch": 13.430749682337993,
      "grad_norm": 2.7245278358459473,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 42280
    },
    {
      "epoch": 13.433926302414232,
      "grad_norm": 3.8578169345855713,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 42290
    },
    {
      "epoch": 13.437102922490471,
      "grad_norm": 2.0230369567871094,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 42300
    },
    {
      "epoch": 13.440279542566708,
      "grad_norm": 2.1153323650360107,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 42310
    },
    {
      "epoch": 13.443456162642947,
      "grad_norm": 1.9492809772491455,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 42320
    },
    {
      "epoch": 13.446632782719186,
      "grad_norm": 1.5833666324615479,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 42330
    },
    {
      "epoch": 13.449809402795426,
      "grad_norm": 3.2651119232177734,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 42340
    },
    {
      "epoch": 13.452986022871665,
      "grad_norm": 2.0432534217834473,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 42350
    },
    {
      "epoch": 13.456162642947904,
      "grad_norm": 2.15006685256958,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 42360
    },
    {
      "epoch": 13.459339263024143,
      "grad_norm": 1.6814358234405518,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 42370
    },
    {
      "epoch": 13.462515883100382,
      "grad_norm": 2.7561604976654053,
      "learning_rate": 2e-05,
      "loss": 0.0867,
      "step": 42380
    },
    {
      "epoch": 13.46569250317662,
      "grad_norm": 1.8943560123443604,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 42390
    },
    {
      "epoch": 13.468869123252858,
      "grad_norm": 2.311082363128662,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 42400
    },
    {
      "epoch": 13.472045743329097,
      "grad_norm": 1.957900881767273,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 42410
    },
    {
      "epoch": 13.475222363405337,
      "grad_norm": 4.239512920379639,
      "learning_rate": 2e-05,
      "loss": 0.0761,
      "step": 42420
    },
    {
      "epoch": 13.478398983481576,
      "grad_norm": 2.1238417625427246,
      "learning_rate": 2e-05,
      "loss": 0.0789,
      "step": 42430
    },
    {
      "epoch": 13.481575603557815,
      "grad_norm": 3.0920584201812744,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 42440
    },
    {
      "epoch": 13.484752223634054,
      "grad_norm": 3.135627269744873,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 42450
    },
    {
      "epoch": 13.487928843710293,
      "grad_norm": 2.5219428539276123,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 42460
    },
    {
      "epoch": 13.491105463786532,
      "grad_norm": 2.795072317123413,
      "learning_rate": 2e-05,
      "loss": 0.0876,
      "step": 42470
    },
    {
      "epoch": 13.49428208386277,
      "grad_norm": 2.1086206436157227,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 42480
    },
    {
      "epoch": 13.497458703939008,
      "grad_norm": 1.890663743019104,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 42490
    },
    {
      "epoch": 13.500635324015247,
      "grad_norm": 2.385676622390747,
      "learning_rate": 2e-05,
      "loss": 0.0778,
      "step": 42500
    },
    {
      "epoch": 13.503811944091487,
      "grad_norm": 1.9083917140960693,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 42510
    },
    {
      "epoch": 13.506988564167726,
      "grad_norm": 2.241661787033081,
      "learning_rate": 2e-05,
      "loss": 0.0787,
      "step": 42520
    },
    {
      "epoch": 13.508576874205845,
      "eval_loss": 2.034501552581787,
      "eval_mse": 2.033620356703402,
      "eval_pearson": 0.3487843023127524,
      "eval_runtime": 7.2783,
      "eval_samples_per_second": 2962.233,
      "eval_spearmanr": 0.36335104698045334,
      "eval_steps_per_second": 11.679,
      "step": 42525
    },
    {
      "epoch": 13.510165184243965,
      "grad_norm": 2.0111470222473145,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 42530
    },
    {
      "epoch": 13.513341804320204,
      "grad_norm": 2.688504457473755,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 42540
    },
    {
      "epoch": 13.516518424396443,
      "grad_norm": 5.993739128112793,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 42550
    },
    {
      "epoch": 13.51969504447268,
      "grad_norm": 2.361483573913574,
      "learning_rate": 2e-05,
      "loss": 0.0889,
      "step": 42560
    },
    {
      "epoch": 13.52287166454892,
      "grad_norm": 2.365399122238159,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 42570
    },
    {
      "epoch": 13.526048284625158,
      "grad_norm": 1.92428719997406,
      "learning_rate": 2e-05,
      "loss": 0.0821,
      "step": 42580
    },
    {
      "epoch": 13.529224904701397,
      "grad_norm": 2.3597710132598877,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 42590
    },
    {
      "epoch": 13.532401524777637,
      "grad_norm": 1.9782336950302124,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 42600
    },
    {
      "epoch": 13.535578144853876,
      "grad_norm": 1.6218585968017578,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 42610
    },
    {
      "epoch": 13.538754764930115,
      "grad_norm": 2.0434043407440186,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 42620
    },
    {
      "epoch": 13.541931385006354,
      "grad_norm": 4.361180305480957,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 42630
    },
    {
      "epoch": 13.545108005082593,
      "grad_norm": 1.3983262777328491,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 42640
    },
    {
      "epoch": 13.54828462515883,
      "grad_norm": 2.3657402992248535,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 42650
    },
    {
      "epoch": 13.55146124523507,
      "grad_norm": 1.854849100112915,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 42660
    },
    {
      "epoch": 13.554637865311308,
      "grad_norm": 2.1345269680023193,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 42670
    },
    {
      "epoch": 13.557814485387548,
      "grad_norm": 1.929181456565857,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 42680
    },
    {
      "epoch": 13.560991105463787,
      "grad_norm": 2.7095577716827393,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 42690
    },
    {
      "epoch": 13.564167725540026,
      "grad_norm": 2.5141751766204834,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 42700
    },
    {
      "epoch": 13.567344345616265,
      "grad_norm": 2.040609359741211,
      "learning_rate": 2e-05,
      "loss": 0.0822,
      "step": 42710
    },
    {
      "epoch": 13.570520965692504,
      "grad_norm": 5.976899147033691,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 42720
    },
    {
      "epoch": 13.573697585768741,
      "grad_norm": 2.0678069591522217,
      "learning_rate": 2e-05,
      "loss": 0.0791,
      "step": 42730
    },
    {
      "epoch": 13.57687420584498,
      "grad_norm": 2.8079419136047363,
      "learning_rate": 2e-05,
      "loss": 0.0834,
      "step": 42740
    },
    {
      "epoch": 13.58005082592122,
      "grad_norm": 2.276789665222168,
      "learning_rate": 2e-05,
      "loss": 0.0785,
      "step": 42750
    },
    {
      "epoch": 13.583227445997458,
      "grad_norm": 2.2863407135009766,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 42760
    },
    {
      "epoch": 13.586404066073698,
      "grad_norm": 3.059131622314453,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 42770
    },
    {
      "epoch": 13.589580686149937,
      "grad_norm": 1.9245491027832031,
      "learning_rate": 2e-05,
      "loss": 0.08,
      "step": 42780
    },
    {
      "epoch": 13.592757306226176,
      "grad_norm": 2.0716259479522705,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 42790
    },
    {
      "epoch": 13.595933926302415,
      "grad_norm": 1.86076819896698,
      "learning_rate": 2e-05,
      "loss": 0.0843,
      "step": 42800
    },
    {
      "epoch": 13.599110546378654,
      "grad_norm": 1.677625298500061,
      "learning_rate": 2e-05,
      "loss": 0.0793,
      "step": 42810
    },
    {
      "epoch": 13.602287166454891,
      "grad_norm": 3.7403652667999268,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 42820
    },
    {
      "epoch": 13.60546378653113,
      "grad_norm": 2.5245039463043213,
      "learning_rate": 2e-05,
      "loss": 0.0886,
      "step": 42830
    },
    {
      "epoch": 13.60864040660737,
      "grad_norm": 2.925869941711426,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 42840
    },
    {
      "epoch": 13.60864040660737,
      "eval_loss": 1.9899322986602783,
      "eval_mse": 1.9881338598915066,
      "eval_pearson": 0.35950305885598777,
      "eval_runtime": 7.374,
      "eval_samples_per_second": 2923.769,
      "eval_spearmanr": 0.3706687837284008,
      "eval_steps_per_second": 11.527,
      "step": 42840
    },
    {
      "epoch": 13.611817026683608,
      "grad_norm": 3.5443429946899414,
      "learning_rate": 2e-05,
      "loss": 0.0798,
      "step": 42850
    },
    {
      "epoch": 13.614993646759848,
      "grad_norm": 2.0112361907958984,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 42860
    },
    {
      "epoch": 13.618170266836087,
      "grad_norm": 2.8794021606445312,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 42870
    },
    {
      "epoch": 13.621346886912326,
      "grad_norm": 1.9810823202133179,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 42880
    },
    {
      "epoch": 13.624523506988565,
      "grad_norm": 2.308051347732544,
      "learning_rate": 2e-05,
      "loss": 0.082,
      "step": 42890
    },
    {
      "epoch": 13.627700127064802,
      "grad_norm": 2.4869565963745117,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 42900
    },
    {
      "epoch": 13.630876747141041,
      "grad_norm": 1.9788267612457275,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 42910
    },
    {
      "epoch": 13.63405336721728,
      "grad_norm": 3.529147148132324,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 42920
    },
    {
      "epoch": 13.63722998729352,
      "grad_norm": 2.207831859588623,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 42930
    },
    {
      "epoch": 13.640406607369759,
      "grad_norm": 1.9284284114837646,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 42940
    },
    {
      "epoch": 13.643583227445998,
      "grad_norm": 2.1844749450683594,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 42950
    },
    {
      "epoch": 13.646759847522237,
      "grad_norm": 1.846123456954956,
      "learning_rate": 2e-05,
      "loss": 0.0772,
      "step": 42960
    },
    {
      "epoch": 13.649936467598476,
      "grad_norm": 2.3778669834136963,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 42970
    },
    {
      "epoch": 13.653113087674715,
      "grad_norm": 3.513953685760498,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 42980
    },
    {
      "epoch": 13.656289707750952,
      "grad_norm": 2.2863306999206543,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 42990
    },
    {
      "epoch": 13.659466327827191,
      "grad_norm": 3.147306442260742,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 43000
    },
    {
      "epoch": 13.66264294790343,
      "grad_norm": 1.7751888036727905,
      "learning_rate": 2e-05,
      "loss": 0.0794,
      "step": 43010
    },
    {
      "epoch": 13.66581956797967,
      "grad_norm": 4.2477498054504395,
      "learning_rate": 2e-05,
      "loss": 0.0772,
      "step": 43020
    },
    {
      "epoch": 13.668996188055909,
      "grad_norm": 1.8068326711654663,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 43030
    },
    {
      "epoch": 13.672172808132148,
      "grad_norm": 2.6651458740234375,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 43040
    },
    {
      "epoch": 13.675349428208387,
      "grad_norm": 1.9024202823638916,
      "learning_rate": 2e-05,
      "loss": 0.0796,
      "step": 43050
    },
    {
      "epoch": 13.678526048284626,
      "grad_norm": 2.1693434715270996,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 43060
    },
    {
      "epoch": 13.681702668360863,
      "grad_norm": 2.2391164302825928,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 43070
    },
    {
      "epoch": 13.684879288437102,
      "grad_norm": 2.693014621734619,
      "learning_rate": 2e-05,
      "loss": 0.0794,
      "step": 43080
    },
    {
      "epoch": 13.688055908513341,
      "grad_norm": 3.719900369644165,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 43090
    },
    {
      "epoch": 13.69123252858958,
      "grad_norm": 2.5527920722961426,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 43100
    },
    {
      "epoch": 13.69440914866582,
      "grad_norm": 1.9581001996994019,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 43110
    },
    {
      "epoch": 13.697585768742059,
      "grad_norm": 2.3903865814208984,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 43120
    },
    {
      "epoch": 13.700762388818298,
      "grad_norm": 3.146310806274414,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 43130
    },
    {
      "epoch": 13.703939008894537,
      "grad_norm": 1.387094497680664,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 43140
    },
    {
      "epoch": 13.707115628970776,
      "grad_norm": 2.0241520404815674,
      "learning_rate": 2e-05,
      "loss": 0.0894,
      "step": 43150
    },
    {
      "epoch": 13.708703939008895,
      "eval_loss": 1.842012882232666,
      "eval_mse": 1.8406392137725631,
      "eval_pearson": 0.3945621085398756,
      "eval_runtime": 7.4824,
      "eval_samples_per_second": 2881.423,
      "eval_spearmanr": 0.402910993385896,
      "eval_steps_per_second": 11.36,
      "step": 43155
    },
    {
      "epoch": 13.710292249047013,
      "grad_norm": 2.0628066062927246,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 43160
    },
    {
      "epoch": 13.713468869123252,
      "grad_norm": 3.2874646186828613,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 43170
    },
    {
      "epoch": 13.716645489199491,
      "grad_norm": 5.288384437561035,
      "learning_rate": 2e-05,
      "loss": 0.0871,
      "step": 43180
    },
    {
      "epoch": 13.71982210927573,
      "grad_norm": 3.9145307540893555,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 43190
    },
    {
      "epoch": 13.72299872935197,
      "grad_norm": 2.5980234146118164,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 43200
    },
    {
      "epoch": 13.726175349428209,
      "grad_norm": 4.1863484382629395,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 43210
    },
    {
      "epoch": 13.729351969504448,
      "grad_norm": 2.1614487171173096,
      "learning_rate": 2e-05,
      "loss": 0.0765,
      "step": 43220
    },
    {
      "epoch": 13.732528589580687,
      "grad_norm": 1.6743683815002441,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 43230
    },
    {
      "epoch": 13.735705209656924,
      "grad_norm": 2.8989944458007812,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 43240
    },
    {
      "epoch": 13.738881829733163,
      "grad_norm": 4.126217842102051,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 43250
    },
    {
      "epoch": 13.742058449809402,
      "grad_norm": 3.443756580352783,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 43260
    },
    {
      "epoch": 13.745235069885641,
      "grad_norm": 2.599745035171509,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 43270
    },
    {
      "epoch": 13.74841168996188,
      "grad_norm": 3.802703619003296,
      "learning_rate": 2e-05,
      "loss": 0.0872,
      "step": 43280
    },
    {
      "epoch": 13.75158831003812,
      "grad_norm": 2.8406553268432617,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 43290
    },
    {
      "epoch": 13.754764930114359,
      "grad_norm": 2.83546781539917,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 43300
    },
    {
      "epoch": 13.757941550190598,
      "grad_norm": 2.036046028137207,
      "learning_rate": 2e-05,
      "loss": 0.0816,
      "step": 43310
    },
    {
      "epoch": 13.761118170266837,
      "grad_norm": 2.8516793251037598,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 43320
    },
    {
      "epoch": 13.764294790343076,
      "grad_norm": 2.2963428497314453,
      "learning_rate": 2e-05,
      "loss": 0.0816,
      "step": 43330
    },
    {
      "epoch": 13.767471410419313,
      "grad_norm": 1.5113935470581055,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 43340
    },
    {
      "epoch": 13.770648030495552,
      "grad_norm": 2.5312559604644775,
      "learning_rate": 2e-05,
      "loss": 0.083,
      "step": 43350
    },
    {
      "epoch": 13.773824650571791,
      "grad_norm": 2.225421905517578,
      "learning_rate": 2e-05,
      "loss": 0.09,
      "step": 43360
    },
    {
      "epoch": 13.77700127064803,
      "grad_norm": 2.144949197769165,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 43370
    },
    {
      "epoch": 13.78017789072427,
      "grad_norm": 2.365541934967041,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 43380
    },
    {
      "epoch": 13.783354510800509,
      "grad_norm": 3.5892601013183594,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 43390
    },
    {
      "epoch": 13.786531130876748,
      "grad_norm": 2.582035779953003,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 43400
    },
    {
      "epoch": 13.789707750952987,
      "grad_norm": 2.8976902961730957,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 43410
    },
    {
      "epoch": 13.792884371029224,
      "grad_norm": 2.480156421661377,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 43420
    },
    {
      "epoch": 13.796060991105463,
      "grad_norm": 3.0242245197296143,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 43430
    },
    {
      "epoch": 13.799237611181702,
      "grad_norm": 2.851154088973999,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 43440
    },
    {
      "epoch": 13.802414231257941,
      "grad_norm": 1.7397013902664185,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 43450
    },
    {
      "epoch": 13.80559085133418,
      "grad_norm": 2.03511905670166,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 43460
    },
    {
      "epoch": 13.80876747141042,
      "grad_norm": 2.008000135421753,
      "learning_rate": 2e-05,
      "loss": 0.0732,
      "step": 43470
    },
    {
      "epoch": 13.80876747141042,
      "eval_loss": 1.9171448945999146,
      "eval_mse": 1.915739480518012,
      "eval_pearson": 0.38664594334222263,
      "eval_runtime": 7.4753,
      "eval_samples_per_second": 2884.15,
      "eval_spearmanr": 0.39851599733399623,
      "eval_steps_per_second": 11.371,
      "step": 43470
    },
    {
      "epoch": 13.811944091486659,
      "grad_norm": 2.09059739112854,
      "learning_rate": 2e-05,
      "loss": 0.0883,
      "step": 43480
    },
    {
      "epoch": 13.815120711562898,
      "grad_norm": 3.5533132553100586,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 43490
    },
    {
      "epoch": 13.818297331639137,
      "grad_norm": 3.1617066860198975,
      "learning_rate": 2e-05,
      "loss": 0.0862,
      "step": 43500
    },
    {
      "epoch": 13.821473951715374,
      "grad_norm": 2.200148344039917,
      "learning_rate": 2e-05,
      "loss": 0.086,
      "step": 43510
    },
    {
      "epoch": 13.824650571791613,
      "grad_norm": 2.9399328231811523,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 43520
    },
    {
      "epoch": 13.827827191867852,
      "grad_norm": 2.497483015060425,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 43530
    },
    {
      "epoch": 13.831003811944091,
      "grad_norm": 1.6726256608963013,
      "learning_rate": 2e-05,
      "loss": 0.0815,
      "step": 43540
    },
    {
      "epoch": 13.83418043202033,
      "grad_norm": 1.4249902963638306,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 43550
    },
    {
      "epoch": 13.83735705209657,
      "grad_norm": 2.529850959777832,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 43560
    },
    {
      "epoch": 13.840533672172809,
      "grad_norm": 2.148339033126831,
      "learning_rate": 2e-05,
      "loss": 0.0799,
      "step": 43570
    },
    {
      "epoch": 13.843710292249048,
      "grad_norm": 1.9353526830673218,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 43580
    },
    {
      "epoch": 13.846886912325285,
      "grad_norm": 2.2173633575439453,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 43590
    },
    {
      "epoch": 13.850063532401524,
      "grad_norm": 2.2490615844726562,
      "learning_rate": 2e-05,
      "loss": 0.0867,
      "step": 43600
    },
    {
      "epoch": 13.853240152477763,
      "grad_norm": 1.933561086654663,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 43610
    },
    {
      "epoch": 13.856416772554002,
      "grad_norm": 1.728305459022522,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 43620
    },
    {
      "epoch": 13.859593392630241,
      "grad_norm": 2.5241334438323975,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 43630
    },
    {
      "epoch": 13.86277001270648,
      "grad_norm": 3.759917974472046,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 43640
    },
    {
      "epoch": 13.86594663278272,
      "grad_norm": 6.248685359954834,
      "learning_rate": 2e-05,
      "loss": 0.0732,
      "step": 43650
    },
    {
      "epoch": 13.869123252858959,
      "grad_norm": 2.008887529373169,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 43660
    },
    {
      "epoch": 13.872299872935198,
      "grad_norm": 1.7697218656539917,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 43670
    },
    {
      "epoch": 13.875476493011435,
      "grad_norm": 4.0780205726623535,
      "learning_rate": 2e-05,
      "loss": 0.0833,
      "step": 43680
    },
    {
      "epoch": 13.878653113087674,
      "grad_norm": 2.9043476581573486,
      "learning_rate": 2e-05,
      "loss": 0.0853,
      "step": 43690
    },
    {
      "epoch": 13.881829733163913,
      "grad_norm": 2.196572780609131,
      "learning_rate": 2e-05,
      "loss": 0.0841,
      "step": 43700
    },
    {
      "epoch": 13.885006353240152,
      "grad_norm": 4.002687454223633,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 43710
    },
    {
      "epoch": 13.888182973316392,
      "grad_norm": 2.0424892902374268,
      "learning_rate": 2e-05,
      "loss": 0.0839,
      "step": 43720
    },
    {
      "epoch": 13.89135959339263,
      "grad_norm": 3.2349398136138916,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 43730
    },
    {
      "epoch": 13.89453621346887,
      "grad_norm": 1.7990880012512207,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 43740
    },
    {
      "epoch": 13.897712833545109,
      "grad_norm": 1.9460854530334473,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 43750
    },
    {
      "epoch": 13.900889453621346,
      "grad_norm": 2.203932046890259,
      "learning_rate": 2e-05,
      "loss": 0.0845,
      "step": 43760
    },
    {
      "epoch": 13.904066073697585,
      "grad_norm": 2.4902191162109375,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 43770
    },
    {
      "epoch": 13.907242693773824,
      "grad_norm": 2.5592925548553467,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 43780
    },
    {
      "epoch": 13.908831003811944,
      "eval_loss": 1.9523276090621948,
      "eval_mse": 1.950333727628048,
      "eval_pearson": 0.37883133241438516,
      "eval_runtime": 7.5973,
      "eval_samples_per_second": 2837.866,
      "eval_spearmanr": 0.39102046059287654,
      "eval_steps_per_second": 11.188,
      "step": 43785
    },
    {
      "epoch": 13.910419313850063,
      "grad_norm": 2.3946990966796875,
      "learning_rate": 2e-05,
      "loss": 0.0799,
      "step": 43790
    },
    {
      "epoch": 13.913595933926302,
      "grad_norm": 1.975266695022583,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 43800
    },
    {
      "epoch": 13.916772554002542,
      "grad_norm": 2.5605549812316895,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 43810
    },
    {
      "epoch": 13.91994917407878,
      "grad_norm": 3.203517436981201,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 43820
    },
    {
      "epoch": 13.92312579415502,
      "grad_norm": 3.056121349334717,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 43830
    },
    {
      "epoch": 13.926302414231259,
      "grad_norm": 3.5276806354522705,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 43840
    },
    {
      "epoch": 13.929479034307496,
      "grad_norm": 2.333005905151367,
      "learning_rate": 2e-05,
      "loss": 0.0882,
      "step": 43850
    },
    {
      "epoch": 13.932655654383735,
      "grad_norm": 2.5854885578155518,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 43860
    },
    {
      "epoch": 13.935832274459974,
      "grad_norm": 4.669917583465576,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 43870
    },
    {
      "epoch": 13.939008894536213,
      "grad_norm": 1.865736484527588,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 43880
    },
    {
      "epoch": 13.942185514612452,
      "grad_norm": 1.597063422203064,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 43890
    },
    {
      "epoch": 13.945362134688692,
      "grad_norm": 2.7816429138183594,
      "learning_rate": 2e-05,
      "loss": 0.0763,
      "step": 43900
    },
    {
      "epoch": 13.94853875476493,
      "grad_norm": 1.7940068244934082,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 43910
    },
    {
      "epoch": 13.95171537484117,
      "grad_norm": 2.281562566757202,
      "learning_rate": 2e-05,
      "loss": 0.0851,
      "step": 43920
    },
    {
      "epoch": 13.954891994917407,
      "grad_norm": 4.351764678955078,
      "learning_rate": 2e-05,
      "loss": 0.0834,
      "step": 43930
    },
    {
      "epoch": 13.958068614993646,
      "grad_norm": 3.190854072570801,
      "learning_rate": 2e-05,
      "loss": 0.0871,
      "step": 43940
    },
    {
      "epoch": 13.961245235069885,
      "grad_norm": 1.6901201009750366,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 43950
    },
    {
      "epoch": 13.964421855146124,
      "grad_norm": 3.9128031730651855,
      "learning_rate": 2e-05,
      "loss": 0.0892,
      "step": 43960
    },
    {
      "epoch": 13.967598475222363,
      "grad_norm": 2.4105355739593506,
      "learning_rate": 2e-05,
      "loss": 0.0821,
      "step": 43970
    },
    {
      "epoch": 13.970775095298603,
      "grad_norm": 2.7529239654541016,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 43980
    },
    {
      "epoch": 13.973951715374842,
      "grad_norm": 6.149559020996094,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 43990
    },
    {
      "epoch": 13.97712833545108,
      "grad_norm": 3.160752534866333,
      "learning_rate": 2e-05,
      "loss": 0.0856,
      "step": 44000
    },
    {
      "epoch": 13.98030495552732,
      "grad_norm": 1.716569185256958,
      "learning_rate": 2e-05,
      "loss": 0.0759,
      "step": 44010
    },
    {
      "epoch": 13.983481575603557,
      "grad_norm": 2.0076985359191895,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 44020
    },
    {
      "epoch": 13.986658195679796,
      "grad_norm": 3.6115121841430664,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 44030
    },
    {
      "epoch": 13.989834815756035,
      "grad_norm": 2.8814151287078857,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 44040
    },
    {
      "epoch": 13.993011435832274,
      "grad_norm": 2.229030132293701,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 44050
    },
    {
      "epoch": 13.996188055908513,
      "grad_norm": 2.098404884338379,
      "learning_rate": 2e-05,
      "loss": 0.077,
      "step": 44060
    },
    {
      "epoch": 13.999364675984753,
      "grad_norm": 1.766778588294983,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 44070
    },
    {
      "epoch": 14.002541296060992,
      "grad_norm": 9.49251937866211,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 44080
    },
    {
      "epoch": 14.00571791613723,
      "grad_norm": 2.2391746044158936,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 44090
    },
    {
      "epoch": 14.008894536213468,
      "grad_norm": 3.0860462188720703,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 44100
    },
    {
      "epoch": 14.008894536213468,
      "eval_loss": 1.790218472480774,
      "eval_mse": 1.7887319450033397,
      "eval_pearson": 0.40014097262025955,
      "eval_runtime": 7.49,
      "eval_samples_per_second": 2878.523,
      "eval_spearmanr": 0.4088759443194929,
      "eval_steps_per_second": 11.349,
      "step": 44100
    },
    {
      "epoch": 14.012071156289707,
      "grad_norm": 1.717825174331665,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 44110
    },
    {
      "epoch": 14.015247776365946,
      "grad_norm": 3.2206215858459473,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 44120
    },
    {
      "epoch": 14.018424396442185,
      "grad_norm": 1.8199676275253296,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 44130
    },
    {
      "epoch": 14.021601016518424,
      "grad_norm": 1.7234281301498413,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 44140
    },
    {
      "epoch": 14.024777636594663,
      "grad_norm": 2.702965497970581,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 44150
    },
    {
      "epoch": 14.027954256670903,
      "grad_norm": 2.8805599212646484,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 44160
    },
    {
      "epoch": 14.031130876747142,
      "grad_norm": 1.7015740871429443,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 44170
    },
    {
      "epoch": 14.03430749682338,
      "grad_norm": 1.6174421310424805,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 44180
    },
    {
      "epoch": 14.037484116899618,
      "grad_norm": 1.4816036224365234,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 44190
    },
    {
      "epoch": 14.040660736975857,
      "grad_norm": 1.659826397895813,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 44200
    },
    {
      "epoch": 14.043837357052096,
      "grad_norm": 4.31050968170166,
      "learning_rate": 2e-05,
      "loss": 0.0789,
      "step": 44210
    },
    {
      "epoch": 14.047013977128335,
      "grad_norm": 2.266918897628784,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 44220
    },
    {
      "epoch": 14.050190597204574,
      "grad_norm": 2.472829818725586,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 44230
    },
    {
      "epoch": 14.053367217280814,
      "grad_norm": 4.648229598999023,
      "learning_rate": 2e-05,
      "loss": 0.0831,
      "step": 44240
    },
    {
      "epoch": 14.056543837357053,
      "grad_norm": 1.5560914278030396,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 44250
    },
    {
      "epoch": 14.059720457433292,
      "grad_norm": 3.5032570362091064,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 44260
    },
    {
      "epoch": 14.062897077509529,
      "grad_norm": 3.580164909362793,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 44270
    },
    {
      "epoch": 14.066073697585768,
      "grad_norm": 2.1957013607025146,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 44280
    },
    {
      "epoch": 14.069250317662007,
      "grad_norm": 2.165761709213257,
      "learning_rate": 2e-05,
      "loss": 0.0774,
      "step": 44290
    },
    {
      "epoch": 14.072426937738246,
      "grad_norm": 2.7532336711883545,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 44300
    },
    {
      "epoch": 14.075603557814485,
      "grad_norm": 1.6711796522140503,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 44310
    },
    {
      "epoch": 14.078780177890724,
      "grad_norm": 2.9942171573638916,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 44320
    },
    {
      "epoch": 14.081956797966964,
      "grad_norm": 1.6071853637695312,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 44330
    },
    {
      "epoch": 14.085133418043203,
      "grad_norm": 2.2054178714752197,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 44340
    },
    {
      "epoch": 14.088310038119442,
      "grad_norm": 1.818649172782898,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 44350
    },
    {
      "epoch": 14.091486658195679,
      "grad_norm": 1.5687507390975952,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 44360
    },
    {
      "epoch": 14.094663278271918,
      "grad_norm": 2.828115224838257,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 44370
    },
    {
      "epoch": 14.097839898348157,
      "grad_norm": 1.8421733379364014,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 44380
    },
    {
      "epoch": 14.101016518424396,
      "grad_norm": 1.9570162296295166,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 44390
    },
    {
      "epoch": 14.104193138500635,
      "grad_norm": 2.042098045349121,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 44400
    },
    {
      "epoch": 14.107369758576874,
      "grad_norm": 1.8915797472000122,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 44410
    },
    {
      "epoch": 14.108958068614994,
      "eval_loss": 1.7656482458114624,
      "eval_mse": 1.7644951542476555,
      "eval_pearson": 0.41703667442309855,
      "eval_runtime": 7.607,
      "eval_samples_per_second": 2834.238,
      "eval_spearmanr": 0.4259572323472727,
      "eval_steps_per_second": 11.174,
      "step": 44415
    },
    {
      "epoch": 14.110546378653114,
      "grad_norm": 2.294904947280884,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 44420
    },
    {
      "epoch": 14.113722998729353,
      "grad_norm": 2.252499580383301,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 44430
    },
    {
      "epoch": 14.11689961880559,
      "grad_norm": 2.9332544803619385,
      "learning_rate": 2e-05,
      "loss": 0.0778,
      "step": 44440
    },
    {
      "epoch": 14.120076238881829,
      "grad_norm": 1.2897406816482544,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 44450
    },
    {
      "epoch": 14.123252858958068,
      "grad_norm": 2.375811815261841,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 44460
    },
    {
      "epoch": 14.126429479034307,
      "grad_norm": 8.6399564743042,
      "learning_rate": 2e-05,
      "loss": 0.0728,
      "step": 44470
    },
    {
      "epoch": 14.129606099110546,
      "grad_norm": 2.281344175338745,
      "learning_rate": 2e-05,
      "loss": 0.0762,
      "step": 44480
    },
    {
      "epoch": 14.132782719186785,
      "grad_norm": 1.8315420150756836,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 44490
    },
    {
      "epoch": 14.135959339263025,
      "grad_norm": 1.4180712699890137,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 44500
    },
    {
      "epoch": 14.139135959339264,
      "grad_norm": 1.9284695386886597,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 44510
    },
    {
      "epoch": 14.142312579415503,
      "grad_norm": 2.9345366954803467,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 44520
    },
    {
      "epoch": 14.14548919949174,
      "grad_norm": 2.3677449226379395,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 44530
    },
    {
      "epoch": 14.148665819567979,
      "grad_norm": 3.289278984069824,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 44540
    },
    {
      "epoch": 14.151842439644218,
      "grad_norm": 2.7828729152679443,
      "learning_rate": 2e-05,
      "loss": 0.0811,
      "step": 44550
    },
    {
      "epoch": 14.155019059720457,
      "grad_norm": 2.4020376205444336,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 44560
    },
    {
      "epoch": 14.158195679796696,
      "grad_norm": 2.179842948913574,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 44570
    },
    {
      "epoch": 14.161372299872935,
      "grad_norm": 1.9451618194580078,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 44580
    },
    {
      "epoch": 14.164548919949175,
      "grad_norm": 1.900080919265747,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 44590
    },
    {
      "epoch": 14.167725540025414,
      "grad_norm": 2.3616836071014404,
      "learning_rate": 2e-05,
      "loss": 0.072,
      "step": 44600
    },
    {
      "epoch": 14.170902160101653,
      "grad_norm": 6.793559551239014,
      "learning_rate": 2e-05,
      "loss": 0.0842,
      "step": 44610
    },
    {
      "epoch": 14.17407878017789,
      "grad_norm": 2.1012096405029297,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 44620
    },
    {
      "epoch": 14.17725540025413,
      "grad_norm": 2.0966100692749023,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 44630
    },
    {
      "epoch": 14.180432020330368,
      "grad_norm": 2.8937294483184814,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 44640
    },
    {
      "epoch": 14.183608640406607,
      "grad_norm": 2.490198850631714,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 44650
    },
    {
      "epoch": 14.186785260482846,
      "grad_norm": 1.3955830335617065,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 44660
    },
    {
      "epoch": 14.189961880559085,
      "grad_norm": 2.180359363555908,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 44670
    },
    {
      "epoch": 14.193138500635325,
      "grad_norm": 2.2366483211517334,
      "learning_rate": 2e-05,
      "loss": 0.0772,
      "step": 44680
    },
    {
      "epoch": 14.196315120711564,
      "grad_norm": 2.0733397006988525,
      "learning_rate": 2e-05,
      "loss": 0.0757,
      "step": 44690
    },
    {
      "epoch": 14.199491740787801,
      "grad_norm": 2.6307384967803955,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 44700
    },
    {
      "epoch": 14.20266836086404,
      "grad_norm": 1.9256572723388672,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 44710
    },
    {
      "epoch": 14.20584498094028,
      "grad_norm": 14.524486541748047,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 44720
    },
    {
      "epoch": 14.209021601016518,
      "grad_norm": 2.8482441902160645,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 44730
    },
    {
      "epoch": 14.209021601016518,
      "eval_loss": 1.8554084300994873,
      "eval_mse": 1.8539544027789405,
      "eval_pearson": 0.4113732386052922,
      "eval_runtime": 7.5023,
      "eval_samples_per_second": 2873.794,
      "eval_spearmanr": 0.41981682155525674,
      "eval_steps_per_second": 11.33,
      "step": 44730
    },
    {
      "epoch": 14.212198221092757,
      "grad_norm": 5.115640640258789,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 44740
    },
    {
      "epoch": 14.215374841168996,
      "grad_norm": 3.181567907333374,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 44750
    },
    {
      "epoch": 14.218551461245236,
      "grad_norm": 2.2909414768218994,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 44760
    },
    {
      "epoch": 14.221728081321475,
      "grad_norm": 1.7799586057662964,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 44770
    },
    {
      "epoch": 14.224904701397714,
      "grad_norm": 2.878117322921753,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 44780
    },
    {
      "epoch": 14.228081321473951,
      "grad_norm": 1.5725630521774292,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 44790
    },
    {
      "epoch": 14.23125794155019,
      "grad_norm": 2.248227596282959,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 44800
    },
    {
      "epoch": 14.23443456162643,
      "grad_norm": 2.6845014095306396,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 44810
    },
    {
      "epoch": 14.237611181702668,
      "grad_norm": 1.8824225664138794,
      "learning_rate": 2e-05,
      "loss": 0.0817,
      "step": 44820
    },
    {
      "epoch": 14.240787801778907,
      "grad_norm": 3.295760154724121,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 44830
    },
    {
      "epoch": 14.243964421855146,
      "grad_norm": 4.147715091705322,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 44840
    },
    {
      "epoch": 14.247141041931386,
      "grad_norm": 2.6186351776123047,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 44850
    },
    {
      "epoch": 14.250317662007625,
      "grad_norm": 1.6216919422149658,
      "learning_rate": 2e-05,
      "loss": 0.0794,
      "step": 44860
    },
    {
      "epoch": 14.253494282083862,
      "grad_norm": 3.660256862640381,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 44870
    },
    {
      "epoch": 14.256670902160101,
      "grad_norm": 1.467347264289856,
      "learning_rate": 2e-05,
      "loss": 0.072,
      "step": 44880
    },
    {
      "epoch": 14.25984752223634,
      "grad_norm": 2.211688756942749,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 44890
    },
    {
      "epoch": 14.26302414231258,
      "grad_norm": 1.7261779308319092,
      "learning_rate": 2e-05,
      "loss": 0.0709,
      "step": 44900
    },
    {
      "epoch": 14.266200762388818,
      "grad_norm": 3.4964332580566406,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 44910
    },
    {
      "epoch": 14.269377382465057,
      "grad_norm": 1.8460966348648071,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 44920
    },
    {
      "epoch": 14.272554002541296,
      "grad_norm": 2.630702257156372,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 44930
    },
    {
      "epoch": 14.275730622617536,
      "grad_norm": 2.0069355964660645,
      "learning_rate": 2e-05,
      "loss": 0.0722,
      "step": 44940
    },
    {
      "epoch": 14.278907242693775,
      "grad_norm": 1.918455719947815,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 44950
    },
    {
      "epoch": 14.282083862770012,
      "grad_norm": 2.8349921703338623,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 44960
    },
    {
      "epoch": 14.285260482846251,
      "grad_norm": 2.3649370670318604,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 44970
    },
    {
      "epoch": 14.28843710292249,
      "grad_norm": 2.7723584175109863,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 44980
    },
    {
      "epoch": 14.29161372299873,
      "grad_norm": 2.1671817302703857,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 44990
    },
    {
      "epoch": 14.294790343074968,
      "grad_norm": 2.206326961517334,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 45000
    },
    {
      "epoch": 14.297966963151207,
      "grad_norm": 1.7347277402877808,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 45010
    },
    {
      "epoch": 14.301143583227446,
      "grad_norm": 2.0854499340057373,
      "learning_rate": 2e-05,
      "loss": 0.0765,
      "step": 45020
    },
    {
      "epoch": 14.304320203303686,
      "grad_norm": 1.492751121520996,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 45030
    },
    {
      "epoch": 14.307496823379923,
      "grad_norm": 2.5675344467163086,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 45040
    },
    {
      "epoch": 14.309085133418042,
      "eval_loss": 1.9469492435455322,
      "eval_mse": 1.9454713956793297,
      "eval_pearson": 0.3550409909291273,
      "eval_runtime": 7.4538,
      "eval_samples_per_second": 2892.487,
      "eval_spearmanr": 0.3649904759669358,
      "eval_steps_per_second": 11.404,
      "step": 45045
    },
    {
      "epoch": 14.310673443456162,
      "grad_norm": 1.941362977027893,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 45050
    },
    {
      "epoch": 14.313850063532401,
      "grad_norm": 2.051333427429199,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 45060
    },
    {
      "epoch": 14.31702668360864,
      "grad_norm": 1.8536038398742676,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 45070
    },
    {
      "epoch": 14.32020330368488,
      "grad_norm": 2.3568263053894043,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 45080
    },
    {
      "epoch": 14.323379923761118,
      "grad_norm": 1.718971848487854,
      "learning_rate": 2e-05,
      "loss": 0.0761,
      "step": 45090
    },
    {
      "epoch": 14.326556543837357,
      "grad_norm": 1.3594543933868408,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 45100
    },
    {
      "epoch": 14.329733163913597,
      "grad_norm": 3.417200803756714,
      "learning_rate": 2e-05,
      "loss": 0.0737,
      "step": 45110
    },
    {
      "epoch": 14.332909783989836,
      "grad_norm": 2.8459577560424805,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 45120
    },
    {
      "epoch": 14.336086404066073,
      "grad_norm": 2.244093179702759,
      "learning_rate": 2e-05,
      "loss": 0.0789,
      "step": 45130
    },
    {
      "epoch": 14.339263024142312,
      "grad_norm": 1.8213502168655396,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 45140
    },
    {
      "epoch": 14.342439644218551,
      "grad_norm": 2.628600597381592,
      "learning_rate": 2e-05,
      "loss": 0.0789,
      "step": 45150
    },
    {
      "epoch": 14.34561626429479,
      "grad_norm": 2.7664055824279785,
      "learning_rate": 2e-05,
      "loss": 0.0759,
      "step": 45160
    },
    {
      "epoch": 14.34879288437103,
      "grad_norm": 2.5338613986968994,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 45170
    },
    {
      "epoch": 14.351969504447268,
      "grad_norm": 1.542521595954895,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 45180
    },
    {
      "epoch": 14.355146124523507,
      "grad_norm": 3.3122875690460205,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 45190
    },
    {
      "epoch": 14.358322744599747,
      "grad_norm": 3.851789712905884,
      "learning_rate": 2e-05,
      "loss": 0.0778,
      "step": 45200
    },
    {
      "epoch": 14.361499364675986,
      "grad_norm": 3.547874927520752,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 45210
    },
    {
      "epoch": 14.364675984752223,
      "grad_norm": 2.0061309337615967,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 45220
    },
    {
      "epoch": 14.367852604828462,
      "grad_norm": 3.4642255306243896,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 45230
    },
    {
      "epoch": 14.371029224904701,
      "grad_norm": 4.144218921661377,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 45240
    },
    {
      "epoch": 14.37420584498094,
      "grad_norm": 6.746362209320068,
      "learning_rate": 2e-05,
      "loss": 0.0757,
      "step": 45250
    },
    {
      "epoch": 14.37738246505718,
      "grad_norm": 1.6083296537399292,
      "learning_rate": 2e-05,
      "loss": 0.0737,
      "step": 45260
    },
    {
      "epoch": 14.380559085133418,
      "grad_norm": 1.9096089601516724,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 45270
    },
    {
      "epoch": 14.383735705209657,
      "grad_norm": 2.3670873641967773,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 45280
    },
    {
      "epoch": 14.386912325285897,
      "grad_norm": 1.6049597263336182,
      "learning_rate": 2e-05,
      "loss": 0.0729,
      "step": 45290
    },
    {
      "epoch": 14.390088945362134,
      "grad_norm": 2.2827043533325195,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 45300
    },
    {
      "epoch": 14.393265565438373,
      "grad_norm": 4.101850986480713,
      "learning_rate": 2e-05,
      "loss": 0.085,
      "step": 45310
    },
    {
      "epoch": 14.396442185514612,
      "grad_norm": 2.1071877479553223,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 45320
    },
    {
      "epoch": 14.399618805590851,
      "grad_norm": 1.7411748170852661,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 45330
    },
    {
      "epoch": 14.40279542566709,
      "grad_norm": 3.9910600185394287,
      "learning_rate": 2e-05,
      "loss": 0.0774,
      "step": 45340
    },
    {
      "epoch": 14.40597204574333,
      "grad_norm": 3.392643451690674,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 45350
    },
    {
      "epoch": 14.409148665819568,
      "grad_norm": 1.8227410316467285,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 45360
    },
    {
      "epoch": 14.409148665819568,
      "eval_loss": 1.8121013641357422,
      "eval_mse": 1.8107597733700929,
      "eval_pearson": 0.3990850755251888,
      "eval_runtime": 7.489,
      "eval_samples_per_second": 2878.871,
      "eval_spearmanr": 0.4037509129322791,
      "eval_steps_per_second": 11.35,
      "step": 45360
    },
    {
      "epoch": 14.412325285895808,
      "grad_norm": 2.4832816123962402,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 45370
    },
    {
      "epoch": 14.415501905972047,
      "grad_norm": 2.020393133163452,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 45380
    },
    {
      "epoch": 14.418678526048284,
      "grad_norm": 2.915245771408081,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 45390
    },
    {
      "epoch": 14.421855146124523,
      "grad_norm": 2.3538191318511963,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 45400
    },
    {
      "epoch": 14.425031766200762,
      "grad_norm": 2.154568672180176,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 45410
    },
    {
      "epoch": 14.428208386277001,
      "grad_norm": 3.0719563961029053,
      "learning_rate": 2e-05,
      "loss": 0.0707,
      "step": 45420
    },
    {
      "epoch": 14.43138500635324,
      "grad_norm": 3.1589667797088623,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 45430
    },
    {
      "epoch": 14.43456162642948,
      "grad_norm": 1.9322086572647095,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 45440
    },
    {
      "epoch": 14.437738246505718,
      "grad_norm": 1.713274359703064,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 45450
    },
    {
      "epoch": 14.440914866581958,
      "grad_norm": 2.1719393730163574,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 45460
    },
    {
      "epoch": 14.444091486658195,
      "grad_norm": 1.6652233600616455,
      "learning_rate": 2e-05,
      "loss": 0.0834,
      "step": 45470
    },
    {
      "epoch": 14.447268106734434,
      "grad_norm": 2.2998976707458496,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 45480
    },
    {
      "epoch": 14.450444726810673,
      "grad_norm": 1.2602012157440186,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 45490
    },
    {
      "epoch": 14.453621346886912,
      "grad_norm": 1.8588058948516846,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 45500
    },
    {
      "epoch": 14.456797966963151,
      "grad_norm": 2.222673177719116,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 45510
    },
    {
      "epoch": 14.45997458703939,
      "grad_norm": 2.082263231277466,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 45520
    },
    {
      "epoch": 14.46315120711563,
      "grad_norm": 2.4670820236206055,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 45530
    },
    {
      "epoch": 14.466327827191868,
      "grad_norm": 1.888577938079834,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 45540
    },
    {
      "epoch": 14.469504447268108,
      "grad_norm": 19.596176147460938,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 45550
    },
    {
      "epoch": 14.472681067344345,
      "grad_norm": 2.3566203117370605,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 45560
    },
    {
      "epoch": 14.475857687420584,
      "grad_norm": 1.6881603002548218,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 45570
    },
    {
      "epoch": 14.479034307496823,
      "grad_norm": 1.9887664318084717,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 45580
    },
    {
      "epoch": 14.482210927573062,
      "grad_norm": 2.317866086959839,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 45590
    },
    {
      "epoch": 14.485387547649301,
      "grad_norm": 3.5067741870880127,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 45600
    },
    {
      "epoch": 14.48856416772554,
      "grad_norm": 3.0664052963256836,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 45610
    },
    {
      "epoch": 14.49174078780178,
      "grad_norm": 2.1212215423583984,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 45620
    },
    {
      "epoch": 14.494917407878019,
      "grad_norm": 1.8520700931549072,
      "learning_rate": 2e-05,
      "loss": 0.0763,
      "step": 45630
    },
    {
      "epoch": 14.498094027954256,
      "grad_norm": 2.0334243774414062,
      "learning_rate": 2e-05,
      "loss": 0.072,
      "step": 45640
    },
    {
      "epoch": 14.501270648030495,
      "grad_norm": 2.3632543087005615,
      "learning_rate": 2e-05,
      "loss": 0.0729,
      "step": 45650
    },
    {
      "epoch": 14.504447268106734,
      "grad_norm": 2.10479998588562,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 45660
    },
    {
      "epoch": 14.507623888182973,
      "grad_norm": 1.5905877351760864,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 45670
    },
    {
      "epoch": 14.509212198221093,
      "eval_loss": 1.7990585565567017,
      "eval_mse": 1.797771028433028,
      "eval_pearson": 0.4257878611454516,
      "eval_runtime": 7.3715,
      "eval_samples_per_second": 2924.785,
      "eval_spearmanr": 0.43233438000452595,
      "eval_steps_per_second": 11.531,
      "step": 45675
    },
    {
      "epoch": 14.510800508259212,
      "grad_norm": 2.7506773471832275,
      "learning_rate": 2e-05,
      "loss": 0.072,
      "step": 45680
    },
    {
      "epoch": 14.513977128335451,
      "grad_norm": 1.739977240562439,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 45690
    },
    {
      "epoch": 14.51715374841169,
      "grad_norm": 2.27555251121521,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 45700
    },
    {
      "epoch": 14.52033036848793,
      "grad_norm": 1.537719488143921,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 45710
    },
    {
      "epoch": 14.523506988564169,
      "grad_norm": 1.8285319805145264,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 45720
    },
    {
      "epoch": 14.526683608640406,
      "grad_norm": 2.285074234008789,
      "learning_rate": 2e-05,
      "loss": 0.0733,
      "step": 45730
    },
    {
      "epoch": 14.529860228716645,
      "grad_norm": 3.205376625061035,
      "learning_rate": 2e-05,
      "loss": 0.0761,
      "step": 45740
    },
    {
      "epoch": 14.533036848792884,
      "grad_norm": 1.7814843654632568,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 45750
    },
    {
      "epoch": 14.536213468869123,
      "grad_norm": 2.33880877494812,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 45760
    },
    {
      "epoch": 14.539390088945362,
      "grad_norm": 6.664092063903809,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 45770
    },
    {
      "epoch": 14.542566709021601,
      "grad_norm": 3.1525633335113525,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 45780
    },
    {
      "epoch": 14.54574332909784,
      "grad_norm": 2.0806174278259277,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 45790
    },
    {
      "epoch": 14.54891994917408,
      "grad_norm": 2.1348049640655518,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 45800
    },
    {
      "epoch": 14.552096569250317,
      "grad_norm": 1.9041311740875244,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 45810
    },
    {
      "epoch": 14.555273189326556,
      "grad_norm": 2.578665256500244,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 45820
    },
    {
      "epoch": 14.558449809402795,
      "grad_norm": 1.8424876928329468,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 45830
    },
    {
      "epoch": 14.561626429479034,
      "grad_norm": 2.2531001567840576,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 45840
    },
    {
      "epoch": 14.564803049555273,
      "grad_norm": 1.4935847520828247,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 45850
    },
    {
      "epoch": 14.567979669631512,
      "grad_norm": 2.241609811782837,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 45860
    },
    {
      "epoch": 14.571156289707751,
      "grad_norm": 3.6210389137268066,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 45870
    },
    {
      "epoch": 14.57433290978399,
      "grad_norm": 3.7957253456115723,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 45880
    },
    {
      "epoch": 14.57750952986023,
      "grad_norm": 1.4596683979034424,
      "learning_rate": 2e-05,
      "loss": 0.0802,
      "step": 45890
    },
    {
      "epoch": 14.580686149936467,
      "grad_norm": 2.3812379837036133,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 45900
    },
    {
      "epoch": 14.583862770012706,
      "grad_norm": 2.1193904876708984,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 45910
    },
    {
      "epoch": 14.587039390088945,
      "grad_norm": 2.1756298542022705,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 45920
    },
    {
      "epoch": 14.590216010165184,
      "grad_norm": 1.9673444032669067,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 45930
    },
    {
      "epoch": 14.593392630241423,
      "grad_norm": 2.2703192234039307,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 45940
    },
    {
      "epoch": 14.596569250317662,
      "grad_norm": 1.9981006383895874,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 45950
    },
    {
      "epoch": 14.599745870393901,
      "grad_norm": 2.394232749938965,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 45960
    },
    {
      "epoch": 14.60292249047014,
      "grad_norm": 2.096372604370117,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 45970
    },
    {
      "epoch": 14.606099110546378,
      "grad_norm": 2.37764048576355,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 45980
    },
    {
      "epoch": 14.609275730622617,
      "grad_norm": 1.7796180248260498,
      "learning_rate": 2e-05,
      "loss": 0.0772,
      "step": 45990
    },
    {
      "epoch": 14.609275730622617,
      "eval_loss": 1.8304914236068726,
      "eval_mse": 1.8284791748383271,
      "eval_pearson": 0.39199304913622,
      "eval_runtime": 7.4948,
      "eval_samples_per_second": 2876.67,
      "eval_spearmanr": 0.3976566978783667,
      "eval_steps_per_second": 11.341,
      "step": 45990
    },
    {
      "epoch": 14.612452350698856,
      "grad_norm": 6.658493518829346,
      "learning_rate": 2e-05,
      "loss": 0.081,
      "step": 46000
    },
    {
      "epoch": 14.615628970775095,
      "grad_norm": 2.6698052883148193,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 46010
    },
    {
      "epoch": 14.618805590851334,
      "grad_norm": 2.0438404083251953,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 46020
    },
    {
      "epoch": 14.621982210927573,
      "grad_norm": 2.0068166255950928,
      "learning_rate": 2e-05,
      "loss": 0.0709,
      "step": 46030
    },
    {
      "epoch": 14.625158831003812,
      "grad_norm": 3.0244572162628174,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 46040
    },
    {
      "epoch": 14.628335451080051,
      "grad_norm": 1.750057578086853,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 46050
    },
    {
      "epoch": 14.63151207115629,
      "grad_norm": 2.375854015350342,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 46060
    },
    {
      "epoch": 14.634688691232528,
      "grad_norm": 2.8098671436309814,
      "learning_rate": 2e-05,
      "loss": 0.0763,
      "step": 46070
    },
    {
      "epoch": 14.637865311308767,
      "grad_norm": 2.2216227054595947,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 46080
    },
    {
      "epoch": 14.641041931385006,
      "grad_norm": 2.2529845237731934,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 46090
    },
    {
      "epoch": 14.644218551461245,
      "grad_norm": 2.2220401763916016,
      "learning_rate": 2e-05,
      "loss": 0.0797,
      "step": 46100
    },
    {
      "epoch": 14.647395171537484,
      "grad_norm": 1.9303319454193115,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 46110
    },
    {
      "epoch": 14.650571791613723,
      "grad_norm": 1.8709971904754639,
      "learning_rate": 2e-05,
      "loss": 0.0733,
      "step": 46120
    },
    {
      "epoch": 14.653748411689962,
      "grad_norm": 1.7243831157684326,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 46130
    },
    {
      "epoch": 14.656925031766201,
      "grad_norm": 3.4308323860168457,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 46140
    },
    {
      "epoch": 14.660101651842439,
      "grad_norm": 1.922727346420288,
      "learning_rate": 2e-05,
      "loss": 0.0774,
      "step": 46150
    },
    {
      "epoch": 14.663278271918678,
      "grad_norm": 2.5504026412963867,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 46160
    },
    {
      "epoch": 14.666454891994917,
      "grad_norm": 1.920279622077942,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 46170
    },
    {
      "epoch": 14.669631512071156,
      "grad_norm": 3.9245169162750244,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 46180
    },
    {
      "epoch": 14.672808132147395,
      "grad_norm": 1.704637050628662,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 46190
    },
    {
      "epoch": 14.675984752223634,
      "grad_norm": 2.033162832260132,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 46200
    },
    {
      "epoch": 14.679161372299873,
      "grad_norm": 1.625190258026123,
      "learning_rate": 2e-05,
      "loss": 0.0707,
      "step": 46210
    },
    {
      "epoch": 14.682337992376112,
      "grad_norm": 3.401188611984253,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 46220
    },
    {
      "epoch": 14.685514612452351,
      "grad_norm": 1.9977694749832153,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 46230
    },
    {
      "epoch": 14.688691232528589,
      "grad_norm": 1.446268916130066,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 46240
    },
    {
      "epoch": 14.691867852604828,
      "grad_norm": 1.6165927648544312,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 46250
    },
    {
      "epoch": 14.695044472681067,
      "grad_norm": 2.154125928878784,
      "learning_rate": 2e-05,
      "loss": 0.0835,
      "step": 46260
    },
    {
      "epoch": 14.698221092757306,
      "grad_norm": 6.010066032409668,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 46270
    },
    {
      "epoch": 14.701397712833545,
      "grad_norm": 3.673387289047241,
      "learning_rate": 2e-05,
      "loss": 0.0857,
      "step": 46280
    },
    {
      "epoch": 14.704574332909784,
      "grad_norm": 2.0423858165740967,
      "learning_rate": 2e-05,
      "loss": 0.0707,
      "step": 46290
    },
    {
      "epoch": 14.707750952986023,
      "grad_norm": 2.33058500289917,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 46300
    },
    {
      "epoch": 14.709339263024143,
      "eval_loss": 1.7856122255325317,
      "eval_mse": 1.783698350702566,
      "eval_pearson": 0.39615935203650504,
      "eval_runtime": 7.4911,
      "eval_samples_per_second": 2878.091,
      "eval_spearmanr": 0.4038676975128569,
      "eval_steps_per_second": 11.347,
      "step": 46305
    },
    {
      "epoch": 14.710927573062262,
      "grad_norm": 1.6177664995193481,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 46310
    },
    {
      "epoch": 14.7141041931385,
      "grad_norm": 1.9076077938079834,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 46320
    },
    {
      "epoch": 14.717280813214739,
      "grad_norm": 2.6007916927337646,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 46330
    },
    {
      "epoch": 14.720457433290978,
      "grad_norm": 2.4170711040496826,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 46340
    },
    {
      "epoch": 14.723634053367217,
      "grad_norm": 2.223710060119629,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 46350
    },
    {
      "epoch": 14.726810673443456,
      "grad_norm": 1.4920512437820435,
      "learning_rate": 2e-05,
      "loss": 0.0824,
      "step": 46360
    },
    {
      "epoch": 14.729987293519695,
      "grad_norm": 1.945960283279419,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 46370
    },
    {
      "epoch": 14.733163913595934,
      "grad_norm": 3.264592170715332,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 46380
    },
    {
      "epoch": 14.736340533672173,
      "grad_norm": 1.8536908626556396,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 46390
    },
    {
      "epoch": 14.739517153748412,
      "grad_norm": 2.6282665729522705,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 46400
    },
    {
      "epoch": 14.742693773824652,
      "grad_norm": 1.977880597114563,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 46410
    },
    {
      "epoch": 14.745870393900889,
      "grad_norm": 1.9680043458938599,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 46420
    },
    {
      "epoch": 14.749047013977128,
      "grad_norm": 1.7946184873580933,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 46430
    },
    {
      "epoch": 14.752223634053367,
      "grad_norm": 1.635546326637268,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 46440
    },
    {
      "epoch": 14.755400254129606,
      "grad_norm": 2.093278169631958,
      "learning_rate": 2e-05,
      "loss": 0.078,
      "step": 46450
    },
    {
      "epoch": 14.758576874205845,
      "grad_norm": 2.64884352684021,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 46460
    },
    {
      "epoch": 14.761753494282084,
      "grad_norm": 2.4076128005981445,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 46470
    },
    {
      "epoch": 14.764930114358323,
      "grad_norm": 2.870720863342285,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 46480
    },
    {
      "epoch": 14.768106734434562,
      "grad_norm": 2.889714002609253,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 46490
    },
    {
      "epoch": 14.7712833545108,
      "grad_norm": 1.63492751121521,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 46500
    },
    {
      "epoch": 14.774459974587039,
      "grad_norm": 1.6268596649169922,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 46510
    },
    {
      "epoch": 14.777636594663278,
      "grad_norm": 2.453517436981201,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 46520
    },
    {
      "epoch": 14.780813214739517,
      "grad_norm": 4.623254299163818,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 46530
    },
    {
      "epoch": 14.783989834815756,
      "grad_norm": 2.960272789001465,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 46540
    },
    {
      "epoch": 14.787166454891995,
      "grad_norm": 2.7436342239379883,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 46550
    },
    {
      "epoch": 14.790343074968234,
      "grad_norm": 1.877852201461792,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 46560
    },
    {
      "epoch": 14.793519695044473,
      "grad_norm": 1.456824541091919,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 46570
    },
    {
      "epoch": 14.796696315120712,
      "grad_norm": 3.6235952377319336,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 46580
    },
    {
      "epoch": 14.79987293519695,
      "grad_norm": 3.962930202484131,
      "learning_rate": 2e-05,
      "loss": 0.087,
      "step": 46590
    },
    {
      "epoch": 14.803049555273189,
      "grad_norm": 2.4690003395080566,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 46600
    },
    {
      "epoch": 14.806226175349428,
      "grad_norm": 3.2230899333953857,
      "learning_rate": 2e-05,
      "loss": 0.0809,
      "step": 46610
    },
    {
      "epoch": 14.809402795425667,
      "grad_norm": 8.31330680847168,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 46620
    },
    {
      "epoch": 14.809402795425667,
      "eval_loss": 1.8072850704193115,
      "eval_mse": 1.8059685661974112,
      "eval_pearson": 0.4196594577024261,
      "eval_runtime": 7.4993,
      "eval_samples_per_second": 2874.952,
      "eval_spearmanr": 0.4237725662608894,
      "eval_steps_per_second": 11.334,
      "step": 46620
    },
    {
      "epoch": 14.812579415501906,
      "grad_norm": 1.8517364263534546,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 46630
    },
    {
      "epoch": 14.815756035578145,
      "grad_norm": 6.385802268981934,
      "learning_rate": 2e-05,
      "loss": 0.0853,
      "step": 46640
    },
    {
      "epoch": 14.818932655654384,
      "grad_norm": 2.0234620571136475,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 46650
    },
    {
      "epoch": 14.822109275730623,
      "grad_norm": 3.2945969104766846,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 46660
    },
    {
      "epoch": 14.82528589580686,
      "grad_norm": 5.08837890625,
      "learning_rate": 2e-05,
      "loss": 0.0848,
      "step": 46670
    },
    {
      "epoch": 14.8284625158831,
      "grad_norm": 1.4177374839782715,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 46680
    },
    {
      "epoch": 14.831639135959339,
      "grad_norm": 3.0605735778808594,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 46690
    },
    {
      "epoch": 14.834815756035578,
      "grad_norm": 1.9726588726043701,
      "learning_rate": 2e-05,
      "loss": 0.0785,
      "step": 46700
    },
    {
      "epoch": 14.837992376111817,
      "grad_norm": 1.8401942253112793,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 46710
    },
    {
      "epoch": 14.841168996188056,
      "grad_norm": 1.6960185766220093,
      "learning_rate": 2e-05,
      "loss": 0.0792,
      "step": 46720
    },
    {
      "epoch": 14.844345616264295,
      "grad_norm": 2.0726962089538574,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 46730
    },
    {
      "epoch": 14.847522236340534,
      "grad_norm": 2.0573954582214355,
      "learning_rate": 2e-05,
      "loss": 0.0772,
      "step": 46740
    },
    {
      "epoch": 14.850698856416773,
      "grad_norm": 1.7405027151107788,
      "learning_rate": 2e-05,
      "loss": 0.0796,
      "step": 46750
    },
    {
      "epoch": 14.85387547649301,
      "grad_norm": 2.0031354427337646,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 46760
    },
    {
      "epoch": 14.85705209656925,
      "grad_norm": 1.8150025606155396,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 46770
    },
    {
      "epoch": 14.860228716645489,
      "grad_norm": 3.0498409271240234,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 46780
    },
    {
      "epoch": 14.863405336721728,
      "grad_norm": 1.9733725786209106,
      "learning_rate": 2e-05,
      "loss": 0.0854,
      "step": 46790
    },
    {
      "epoch": 14.866581956797967,
      "grad_norm": 2.397860288619995,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 46800
    },
    {
      "epoch": 14.869758576874206,
      "grad_norm": 2.654681921005249,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 46810
    },
    {
      "epoch": 14.872935196950445,
      "grad_norm": 2.790482759475708,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 46820
    },
    {
      "epoch": 14.876111817026684,
      "grad_norm": 2.926494836807251,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 46830
    },
    {
      "epoch": 14.879288437102922,
      "grad_norm": 6.855254173278809,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 46840
    },
    {
      "epoch": 14.88246505717916,
      "grad_norm": 2.2921481132507324,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 46850
    },
    {
      "epoch": 14.8856416772554,
      "grad_norm": 2.2803499698638916,
      "learning_rate": 2e-05,
      "loss": 0.0825,
      "step": 46860
    },
    {
      "epoch": 14.888818297331639,
      "grad_norm": 1.6280723810195923,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 46870
    },
    {
      "epoch": 14.891994917407878,
      "grad_norm": 2.1671574115753174,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 46880
    },
    {
      "epoch": 14.895171537484117,
      "grad_norm": 1.8580623865127563,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 46890
    },
    {
      "epoch": 14.898348157560356,
      "grad_norm": 2.08984375,
      "learning_rate": 2e-05,
      "loss": 0.0759,
      "step": 46900
    },
    {
      "epoch": 14.901524777636595,
      "grad_norm": 1.6005207300186157,
      "learning_rate": 2e-05,
      "loss": 0.0757,
      "step": 46910
    },
    {
      "epoch": 14.904701397712834,
      "grad_norm": 3.6946938037872314,
      "learning_rate": 2e-05,
      "loss": 0.0793,
      "step": 46920
    },
    {
      "epoch": 14.907878017789072,
      "grad_norm": 2.5883896350860596,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 46930
    },
    {
      "epoch": 14.909466327827191,
      "eval_loss": 1.8634312152862549,
      "eval_mse": 1.8619221718467496,
      "eval_pearson": 0.40960945320393677,
      "eval_runtime": 7.6061,
      "eval_samples_per_second": 2834.55,
      "eval_spearmanr": 0.417761101071763,
      "eval_steps_per_second": 11.175,
      "step": 46935
    },
    {
      "epoch": 14.91105463786531,
      "grad_norm": 1.6824860572814941,
      "learning_rate": 2e-05,
      "loss": 0.0722,
      "step": 46940
    },
    {
      "epoch": 14.91423125794155,
      "grad_norm": 2.5718791484832764,
      "learning_rate": 2e-05,
      "loss": 0.0763,
      "step": 46950
    },
    {
      "epoch": 14.917407878017789,
      "grad_norm": 6.667241096496582,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 46960
    },
    {
      "epoch": 14.920584498094028,
      "grad_norm": 3.485841751098633,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 46970
    },
    {
      "epoch": 14.923761118170267,
      "grad_norm": 4.421009540557861,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 46980
    },
    {
      "epoch": 14.926937738246506,
      "grad_norm": 1.645696997642517,
      "learning_rate": 2e-05,
      "loss": 0.0744,
      "step": 46990
    },
    {
      "epoch": 14.930114358322745,
      "grad_norm": 2.9220333099365234,
      "learning_rate": 2e-05,
      "loss": 0.0695,
      "step": 47000
    },
    {
      "epoch": 14.933290978398983,
      "grad_norm": 1.6858919858932495,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 47010
    },
    {
      "epoch": 14.936467598475222,
      "grad_norm": 1.6057387590408325,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 47020
    },
    {
      "epoch": 14.93964421855146,
      "grad_norm": 1.8016349077224731,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 47030
    },
    {
      "epoch": 14.9428208386277,
      "grad_norm": 3.1606075763702393,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 47040
    },
    {
      "epoch": 14.945997458703939,
      "grad_norm": 3.050248146057129,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 47050
    },
    {
      "epoch": 14.949174078780178,
      "grad_norm": 10.938036918640137,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 47060
    },
    {
      "epoch": 14.952350698856417,
      "grad_norm": 2.2681751251220703,
      "learning_rate": 2e-05,
      "loss": 0.0829,
      "step": 47070
    },
    {
      "epoch": 14.955527318932656,
      "grad_norm": 2.1589858531951904,
      "learning_rate": 2e-05,
      "loss": 0.0805,
      "step": 47080
    },
    {
      "epoch": 14.958703939008895,
      "grad_norm": 2.6359703540802,
      "learning_rate": 2e-05,
      "loss": 0.0794,
      "step": 47090
    },
    {
      "epoch": 14.961880559085133,
      "grad_norm": 3.29608154296875,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 47100
    },
    {
      "epoch": 14.965057179161372,
      "grad_norm": 1.8853102922439575,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 47110
    },
    {
      "epoch": 14.96823379923761,
      "grad_norm": 5.252787113189697,
      "learning_rate": 2e-05,
      "loss": 0.0766,
      "step": 47120
    },
    {
      "epoch": 14.97141041931385,
      "grad_norm": 3.101663112640381,
      "learning_rate": 2e-05,
      "loss": 0.0828,
      "step": 47130
    },
    {
      "epoch": 14.974587039390089,
      "grad_norm": 3.7721993923187256,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 47140
    },
    {
      "epoch": 14.977763659466328,
      "grad_norm": 2.6175522804260254,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 47150
    },
    {
      "epoch": 14.980940279542567,
      "grad_norm": 2.830024242401123,
      "learning_rate": 2e-05,
      "loss": 0.0825,
      "step": 47160
    },
    {
      "epoch": 14.984116899618806,
      "grad_norm": 1.5390613079071045,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 47170
    },
    {
      "epoch": 14.987293519695044,
      "grad_norm": 1.8641127347946167,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 47180
    },
    {
      "epoch": 14.990470139771283,
      "grad_norm": 2.535382032394409,
      "learning_rate": 2e-05,
      "loss": 0.0738,
      "step": 47190
    },
    {
      "epoch": 14.993646759847522,
      "grad_norm": 2.341029405593872,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 47200
    },
    {
      "epoch": 14.996823379923761,
      "grad_norm": 1.8187633752822876,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 47210
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.7346272468566895,
      "learning_rate": 2e-05,
      "loss": 0.0746,
      "step": 47220
    },
    {
      "epoch": 15.003176620076239,
      "grad_norm": 2.5258171558380127,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 47230
    },
    {
      "epoch": 15.006353240152478,
      "grad_norm": 5.05096435546875,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 47240
    },
    {
      "epoch": 15.009529860228717,
      "grad_norm": 1.7563223838806152,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 47250
    },
    {
      "epoch": 15.009529860228717,
      "eval_loss": 1.7828326225280762,
      "eval_mse": 1.7816904886202378,
      "eval_pearson": 0.42200371374990575,
      "eval_runtime": 7.3992,
      "eval_samples_per_second": 2913.826,
      "eval_spearmanr": 0.4256698238823816,
      "eval_steps_per_second": 11.488,
      "step": 47250
    },
    {
      "epoch": 15.012706480304956,
      "grad_norm": 2.087618112564087,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 47260
    },
    {
      "epoch": 15.015883100381194,
      "grad_norm": 1.9324029684066772,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 47270
    },
    {
      "epoch": 15.019059720457433,
      "grad_norm": 1.5106041431427002,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 47280
    },
    {
      "epoch": 15.022236340533672,
      "grad_norm": 2.385509490966797,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 47290
    },
    {
      "epoch": 15.025412960609911,
      "grad_norm": 2.0243706703186035,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 47300
    },
    {
      "epoch": 15.02858958068615,
      "grad_norm": 1.646774411201477,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 47310
    },
    {
      "epoch": 15.03176620076239,
      "grad_norm": 2.9984238147735596,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 47320
    },
    {
      "epoch": 15.034942820838628,
      "grad_norm": 2.632688045501709,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 47330
    },
    {
      "epoch": 15.038119440914867,
      "grad_norm": 10.717144012451172,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 47340
    },
    {
      "epoch": 15.041296060991105,
      "grad_norm": 2.447114944458008,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 47350
    },
    {
      "epoch": 15.044472681067344,
      "grad_norm": 3.089895486831665,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 47360
    },
    {
      "epoch": 15.047649301143583,
      "grad_norm": 3.0231845378875732,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 47370
    },
    {
      "epoch": 15.050825921219822,
      "grad_norm": 2.508974313735962,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 47380
    },
    {
      "epoch": 15.054002541296061,
      "grad_norm": 2.9867076873779297,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 47390
    },
    {
      "epoch": 15.0571791613723,
      "grad_norm": 2.6933774948120117,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 47400
    },
    {
      "epoch": 15.06035578144854,
      "grad_norm": 4.278092384338379,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 47410
    },
    {
      "epoch": 15.063532401524778,
      "grad_norm": 2.441427707672119,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 47420
    },
    {
      "epoch": 15.066709021601017,
      "grad_norm": 2.8994650840759277,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 47430
    },
    {
      "epoch": 15.069885641677255,
      "grad_norm": 2.6057403087615967,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 47440
    },
    {
      "epoch": 15.073062261753494,
      "grad_norm": 2.6941030025482178,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 47450
    },
    {
      "epoch": 15.076238881829733,
      "grad_norm": 4.246239185333252,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 47460
    },
    {
      "epoch": 15.079415501905972,
      "grad_norm": 3.6882691383361816,
      "learning_rate": 2e-05,
      "loss": 0.0733,
      "step": 47470
    },
    {
      "epoch": 15.082592121982211,
      "grad_norm": 2.3029091358184814,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 47480
    },
    {
      "epoch": 15.08576874205845,
      "grad_norm": 1.2660611867904663,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 47490
    },
    {
      "epoch": 15.08894536213469,
      "grad_norm": 1.9874613285064697,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 47500
    },
    {
      "epoch": 15.092121982210928,
      "grad_norm": 2.7218642234802246,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 47510
    },
    {
      "epoch": 15.095298602287166,
      "grad_norm": 2.6935243606567383,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 47520
    },
    {
      "epoch": 15.098475222363405,
      "grad_norm": 2.7877655029296875,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 47530
    },
    {
      "epoch": 15.101651842439644,
      "grad_norm": 1.9244252443313599,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 47540
    },
    {
      "epoch": 15.104828462515883,
      "grad_norm": 2.5492491722106934,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 47550
    },
    {
      "epoch": 15.108005082592122,
      "grad_norm": 1.8705682754516602,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 47560
    },
    {
      "epoch": 15.109593392630241,
      "eval_loss": 1.7652785778045654,
      "eval_mse": 1.763956872917062,
      "eval_pearson": 0.4197759557538882,
      "eval_runtime": 7.5986,
      "eval_samples_per_second": 2837.348,
      "eval_spearmanr": 0.42697251887531584,
      "eval_steps_per_second": 11.186,
      "step": 47565
    },
    {
      "epoch": 15.111181702668361,
      "grad_norm": 2.433884620666504,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 47570
    },
    {
      "epoch": 15.1143583227446,
      "grad_norm": 2.015167713165283,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 47580
    },
    {
      "epoch": 15.11753494282084,
      "grad_norm": 2.4733245372772217,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 47590
    },
    {
      "epoch": 15.120711562897078,
      "grad_norm": 1.8578014373779297,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 47600
    },
    {
      "epoch": 15.123888182973316,
      "grad_norm": 2.703035831451416,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 47610
    },
    {
      "epoch": 15.127064803049555,
      "grad_norm": 2.793050527572632,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 47620
    },
    {
      "epoch": 15.130241423125794,
      "grad_norm": 4.453670501708984,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 47630
    },
    {
      "epoch": 15.133418043202033,
      "grad_norm": 3.162842273712158,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 47640
    },
    {
      "epoch": 15.136594663278272,
      "grad_norm": 2.1659493446350098,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 47650
    },
    {
      "epoch": 15.139771283354511,
      "grad_norm": 1.4911904335021973,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 47660
    },
    {
      "epoch": 15.14294790343075,
      "grad_norm": 1.8144394159317017,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 47670
    },
    {
      "epoch": 15.14612452350699,
      "grad_norm": 2.185108184814453,
      "learning_rate": 2e-05,
      "loss": 0.0728,
      "step": 47680
    },
    {
      "epoch": 15.149301143583227,
      "grad_norm": 4.53479528427124,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 47690
    },
    {
      "epoch": 15.152477763659466,
      "grad_norm": 3.0429136753082275,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 47700
    },
    {
      "epoch": 15.155654383735705,
      "grad_norm": 2.1640396118164062,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 47710
    },
    {
      "epoch": 15.158831003811944,
      "grad_norm": 2.893470525741577,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 47720
    },
    {
      "epoch": 15.162007623888183,
      "grad_norm": 3.9285523891448975,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 47730
    },
    {
      "epoch": 15.165184243964422,
      "grad_norm": 1.7802027463912964,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 47740
    },
    {
      "epoch": 15.168360864040661,
      "grad_norm": 5.453904628753662,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 47750
    },
    {
      "epoch": 15.1715374841169,
      "grad_norm": 1.8833565711975098,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 47760
    },
    {
      "epoch": 15.17471410419314,
      "grad_norm": 1.4412645101547241,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 47770
    },
    {
      "epoch": 15.177890724269377,
      "grad_norm": 2.992034912109375,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 47780
    },
    {
      "epoch": 15.181067344345616,
      "grad_norm": 2.589647054672241,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 47790
    },
    {
      "epoch": 15.184243964421855,
      "grad_norm": 1.8227808475494385,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 47800
    },
    {
      "epoch": 15.187420584498094,
      "grad_norm": 1.9993890523910522,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 47810
    },
    {
      "epoch": 15.190597204574333,
      "grad_norm": 3.8824000358581543,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 47820
    },
    {
      "epoch": 15.193773824650572,
      "grad_norm": 3.590763568878174,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 47830
    },
    {
      "epoch": 15.196950444726811,
      "grad_norm": 3.706662178039551,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 47840
    },
    {
      "epoch": 15.20012706480305,
      "grad_norm": 1.6114064455032349,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 47850
    },
    {
      "epoch": 15.20330368487929,
      "grad_norm": 3.1592016220092773,
      "learning_rate": 2e-05,
      "loss": 0.0665,
      "step": 47860
    },
    {
      "epoch": 15.206480304955527,
      "grad_norm": 3.2659032344818115,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 47870
    },
    {
      "epoch": 15.209656925031766,
      "grad_norm": 2.971909523010254,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 47880
    },
    {
      "epoch": 15.209656925031766,
      "eval_loss": 1.830969214439392,
      "eval_mse": 1.8295417554828586,
      "eval_pearson": 0.38721406292783495,
      "eval_runtime": 7.6075,
      "eval_samples_per_second": 2834.027,
      "eval_spearmanr": 0.3938997895062151,
      "eval_steps_per_second": 11.173,
      "step": 47880
    },
    {
      "epoch": 15.212833545108005,
      "grad_norm": 3.771831750869751,
      "learning_rate": 2e-05,
      "loss": 0.0746,
      "step": 47890
    },
    {
      "epoch": 15.216010165184244,
      "grad_norm": 2.292300224304199,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 47900
    },
    {
      "epoch": 15.219186785260483,
      "grad_norm": 2.440387010574341,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 47910
    },
    {
      "epoch": 15.222363405336722,
      "grad_norm": 1.7122864723205566,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 47920
    },
    {
      "epoch": 15.225540025412961,
      "grad_norm": 3.25826096534729,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 47930
    },
    {
      "epoch": 15.2287166454892,
      "grad_norm": 1.8213447332382202,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 47940
    },
    {
      "epoch": 15.231893265565438,
      "grad_norm": 1.5811190605163574,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 47950
    },
    {
      "epoch": 15.235069885641677,
      "grad_norm": 1.552909016609192,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 47960
    },
    {
      "epoch": 15.238246505717916,
      "grad_norm": 1.8632986545562744,
      "learning_rate": 2e-05,
      "loss": 0.0722,
      "step": 47970
    },
    {
      "epoch": 15.241423125794155,
      "grad_norm": 2.3392930030822754,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 47980
    },
    {
      "epoch": 15.244599745870394,
      "grad_norm": 1.6810864210128784,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 47990
    },
    {
      "epoch": 15.247776365946633,
      "grad_norm": 1.6175535917282104,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 48000
    },
    {
      "epoch": 15.250952986022872,
      "grad_norm": 1.24813711643219,
      "learning_rate": 2e-05,
      "loss": 0.075,
      "step": 48010
    },
    {
      "epoch": 15.254129606099111,
      "grad_norm": 4.13193941116333,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 48020
    },
    {
      "epoch": 15.25730622617535,
      "grad_norm": 1.8869307041168213,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 48030
    },
    {
      "epoch": 15.260482846251588,
      "grad_norm": 3.1147050857543945,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 48040
    },
    {
      "epoch": 15.263659466327827,
      "grad_norm": 3.014270067214966,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 48050
    },
    {
      "epoch": 15.266836086404066,
      "grad_norm": 1.2790101766586304,
      "learning_rate": 2e-05,
      "loss": 0.0784,
      "step": 48060
    },
    {
      "epoch": 15.270012706480305,
      "grad_norm": 1.584459900856018,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 48070
    },
    {
      "epoch": 15.273189326556544,
      "grad_norm": 2.374480724334717,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 48080
    },
    {
      "epoch": 15.276365946632783,
      "grad_norm": 2.5909416675567627,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 48090
    },
    {
      "epoch": 15.279542566709022,
      "grad_norm": 2.3548173904418945,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 48100
    },
    {
      "epoch": 15.282719186785261,
      "grad_norm": 1.8006821870803833,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 48110
    },
    {
      "epoch": 15.285895806861499,
      "grad_norm": 2.499922275543213,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 48120
    },
    {
      "epoch": 15.289072426937738,
      "grad_norm": 2.1020774841308594,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 48130
    },
    {
      "epoch": 15.292249047013977,
      "grad_norm": 2.3372609615325928,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 48140
    },
    {
      "epoch": 15.295425667090216,
      "grad_norm": 1.4249449968338013,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 48150
    },
    {
      "epoch": 15.298602287166455,
      "grad_norm": 2.446605920791626,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 48160
    },
    {
      "epoch": 15.301778907242694,
      "grad_norm": 1.6896032094955444,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 48170
    },
    {
      "epoch": 15.304955527318933,
      "grad_norm": 2.038550615310669,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 48180
    },
    {
      "epoch": 15.308132147395172,
      "grad_norm": 2.948662281036377,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 48190
    },
    {
      "epoch": 15.309720457433292,
      "eval_loss": 1.7240078449249268,
      "eval_mse": 1.7223791587966493,
      "eval_pearson": 0.419740244990314,
      "eval_runtime": 7.2862,
      "eval_samples_per_second": 2959.036,
      "eval_spearmanr": 0.42189041728374027,
      "eval_steps_per_second": 11.666,
      "step": 48195
    },
    {
      "epoch": 15.311308767471411,
      "grad_norm": 2.4132816791534424,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 48200
    },
    {
      "epoch": 15.314485387547649,
      "grad_norm": 3.873663902282715,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 48210
    },
    {
      "epoch": 15.317662007623888,
      "grad_norm": 3.1796603202819824,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 48220
    },
    {
      "epoch": 15.320838627700127,
      "grad_norm": 2.2478439807891846,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 48230
    },
    {
      "epoch": 15.324015247776366,
      "grad_norm": 2.943131446838379,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 48240
    },
    {
      "epoch": 15.327191867852605,
      "grad_norm": 1.9090989828109741,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 48250
    },
    {
      "epoch": 15.330368487928844,
      "grad_norm": 5.84456205368042,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 48260
    },
    {
      "epoch": 15.333545108005083,
      "grad_norm": 2.191061496734619,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 48270
    },
    {
      "epoch": 15.336721728081322,
      "grad_norm": 1.7924251556396484,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 48280
    },
    {
      "epoch": 15.339898348157561,
      "grad_norm": 3.624919891357422,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 48290
    },
    {
      "epoch": 15.343074968233799,
      "grad_norm": 1.9197465181350708,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 48300
    },
    {
      "epoch": 15.346251588310038,
      "grad_norm": 2.138219118118286,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 48310
    },
    {
      "epoch": 15.349428208386277,
      "grad_norm": 2.0709645748138428,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 48320
    },
    {
      "epoch": 15.352604828462516,
      "grad_norm": 2.364492416381836,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 48330
    },
    {
      "epoch": 15.355781448538755,
      "grad_norm": 2.3332159519195557,
      "learning_rate": 2e-05,
      "loss": 0.0666,
      "step": 48340
    },
    {
      "epoch": 15.358958068614994,
      "grad_norm": 1.8254815340042114,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 48350
    },
    {
      "epoch": 15.362134688691233,
      "grad_norm": 2.7063183784484863,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 48360
    },
    {
      "epoch": 15.365311308767472,
      "grad_norm": 2.3984858989715576,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 48370
    },
    {
      "epoch": 15.36848792884371,
      "grad_norm": 2.2101712226867676,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 48380
    },
    {
      "epoch": 15.371664548919949,
      "grad_norm": 3.9498257637023926,
      "learning_rate": 2e-05,
      "loss": 0.0693,
      "step": 48390
    },
    {
      "epoch": 15.374841168996188,
      "grad_norm": 2.1375179290771484,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 48400
    },
    {
      "epoch": 15.378017789072427,
      "grad_norm": 3.5383989810943604,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 48410
    },
    {
      "epoch": 15.381194409148666,
      "grad_norm": 1.3389015197753906,
      "learning_rate": 2e-05,
      "loss": 0.0746,
      "step": 48420
    },
    {
      "epoch": 15.384371029224905,
      "grad_norm": 1.4132159948349,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 48430
    },
    {
      "epoch": 15.387547649301144,
      "grad_norm": 2.3136348724365234,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 48440
    },
    {
      "epoch": 15.390724269377383,
      "grad_norm": 2.6695520877838135,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 48450
    },
    {
      "epoch": 15.393900889453622,
      "grad_norm": 1.7303978204727173,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 48460
    },
    {
      "epoch": 15.39707750952986,
      "grad_norm": 2.0863664150238037,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 48470
    },
    {
      "epoch": 15.400254129606099,
      "grad_norm": 2.8569021224975586,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 48480
    },
    {
      "epoch": 15.403430749682338,
      "grad_norm": 5.256704807281494,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 48490
    },
    {
      "epoch": 15.406607369758577,
      "grad_norm": 2.87845516204834,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 48500
    },
    {
      "epoch": 15.409783989834816,
      "grad_norm": 8.359064102172852,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 48510
    },
    {
      "epoch": 15.409783989834816,
      "eval_loss": 1.7807660102844238,
      "eval_mse": 1.7798316160026861,
      "eval_pearson": 0.413129248532051,
      "eval_runtime": 7.6015,
      "eval_samples_per_second": 2836.3,
      "eval_spearmanr": 0.4207320620253919,
      "eval_steps_per_second": 11.182,
      "step": 48510
    },
    {
      "epoch": 15.412960609911055,
      "grad_norm": 1.509182333946228,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 48520
    },
    {
      "epoch": 15.416137229987294,
      "grad_norm": 3.8259217739105225,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 48530
    },
    {
      "epoch": 15.419313850063533,
      "grad_norm": 1.7089625597000122,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 48540
    },
    {
      "epoch": 15.42249047013977,
      "grad_norm": 3.6751465797424316,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 48550
    },
    {
      "epoch": 15.42566709021601,
      "grad_norm": 1.7116292715072632,
      "learning_rate": 2e-05,
      "loss": 0.0787,
      "step": 48560
    },
    {
      "epoch": 15.428843710292249,
      "grad_norm": 1.590127944946289,
      "learning_rate": 2e-05,
      "loss": 0.0826,
      "step": 48570
    },
    {
      "epoch": 15.432020330368488,
      "grad_norm": 1.7648687362670898,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 48580
    },
    {
      "epoch": 15.435196950444727,
      "grad_norm": 2.0510809421539307,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 48590
    },
    {
      "epoch": 15.438373570520966,
      "grad_norm": 1.970123291015625,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 48600
    },
    {
      "epoch": 15.441550190597205,
      "grad_norm": 2.8134677410125732,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 48610
    },
    {
      "epoch": 15.444726810673444,
      "grad_norm": 1.578524112701416,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 48620
    },
    {
      "epoch": 15.447903430749683,
      "grad_norm": 1.7025187015533447,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 48630
    },
    {
      "epoch": 15.45108005082592,
      "grad_norm": 2.358891010284424,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 48640
    },
    {
      "epoch": 15.45425667090216,
      "grad_norm": 2.87056565284729,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 48650
    },
    {
      "epoch": 15.457433290978399,
      "grad_norm": 3.1888046264648438,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 48660
    },
    {
      "epoch": 15.460609911054638,
      "grad_norm": 1.7278690338134766,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 48670
    },
    {
      "epoch": 15.463786531130877,
      "grad_norm": 1.5491009950637817,
      "learning_rate": 2e-05,
      "loss": 0.0689,
      "step": 48680
    },
    {
      "epoch": 15.466963151207116,
      "grad_norm": 2.887688636779785,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 48690
    },
    {
      "epoch": 15.470139771283355,
      "grad_norm": 2.9706504344940186,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 48700
    },
    {
      "epoch": 15.473316391359594,
      "grad_norm": 2.1081044673919678,
      "learning_rate": 2e-05,
      "loss": 0.0646,
      "step": 48710
    },
    {
      "epoch": 15.476493011435831,
      "grad_norm": 2.1865270137786865,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 48720
    },
    {
      "epoch": 15.47966963151207,
      "grad_norm": 1.4871888160705566,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 48730
    },
    {
      "epoch": 15.48284625158831,
      "grad_norm": 1.5049980878829956,
      "learning_rate": 2e-05,
      "loss": 0.0669,
      "step": 48740
    },
    {
      "epoch": 15.486022871664549,
      "grad_norm": 2.6293675899505615,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 48750
    },
    {
      "epoch": 15.489199491740788,
      "grad_norm": 2.531235933303833,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 48760
    },
    {
      "epoch": 15.492376111817027,
      "grad_norm": 2.0409116744995117,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 48770
    },
    {
      "epoch": 15.495552731893266,
      "grad_norm": 1.8624051809310913,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 48780
    },
    {
      "epoch": 15.498729351969505,
      "grad_norm": 1.5846118927001953,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 48790
    },
    {
      "epoch": 15.501905972045744,
      "grad_norm": 3.4015376567840576,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 48800
    },
    {
      "epoch": 15.505082592121981,
      "grad_norm": 2.1502933502197266,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 48810
    },
    {
      "epoch": 15.50825921219822,
      "grad_norm": 2.3184866905212402,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 48820
    },
    {
      "epoch": 15.50984752223634,
      "eval_loss": 1.875920057296753,
      "eval_mse": 1.87402259767719,
      "eval_pearson": 0.41638623958465115,
      "eval_runtime": 7.5013,
      "eval_samples_per_second": 2874.179,
      "eval_spearmanr": 0.4262840268775766,
      "eval_steps_per_second": 11.331,
      "step": 48825
    },
    {
      "epoch": 15.51143583227446,
      "grad_norm": 4.652573585510254,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 48830
    },
    {
      "epoch": 15.514612452350699,
      "grad_norm": 3.4677999019622803,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 48840
    },
    {
      "epoch": 15.517789072426938,
      "grad_norm": 2.0250773429870605,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 48850
    },
    {
      "epoch": 15.520965692503177,
      "grad_norm": 2.0560057163238525,
      "learning_rate": 2e-05,
      "loss": 0.0646,
      "step": 48860
    },
    {
      "epoch": 15.524142312579416,
      "grad_norm": 2.5750679969787598,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 48870
    },
    {
      "epoch": 15.527318932655655,
      "grad_norm": 1.6833665370941162,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 48880
    },
    {
      "epoch": 15.530495552731892,
      "grad_norm": 2.9358904361724854,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 48890
    },
    {
      "epoch": 15.533672172808132,
      "grad_norm": 3.370537281036377,
      "learning_rate": 2e-05,
      "loss": 0.0688,
      "step": 48900
    },
    {
      "epoch": 15.53684879288437,
      "grad_norm": 3.100862503051758,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 48910
    },
    {
      "epoch": 15.54002541296061,
      "grad_norm": 2.361876964569092,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 48920
    },
    {
      "epoch": 15.543202033036849,
      "grad_norm": 2.1024441719055176,
      "learning_rate": 2e-05,
      "loss": 0.0752,
      "step": 48930
    },
    {
      "epoch": 15.546378653113088,
      "grad_norm": 1.830081820487976,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 48940
    },
    {
      "epoch": 15.549555273189327,
      "grad_norm": 1.7992122173309326,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 48950
    },
    {
      "epoch": 15.552731893265566,
      "grad_norm": 2.4798576831817627,
      "learning_rate": 2e-05,
      "loss": 0.0709,
      "step": 48960
    },
    {
      "epoch": 15.555908513341805,
      "grad_norm": 1.8532651662826538,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 48970
    },
    {
      "epoch": 15.559085133418042,
      "grad_norm": 2.872709035873413,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 48980
    },
    {
      "epoch": 15.562261753494282,
      "grad_norm": 4.352262020111084,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 48990
    },
    {
      "epoch": 15.56543837357052,
      "grad_norm": 1.8464409112930298,
      "learning_rate": 2e-05,
      "loss": 0.0665,
      "step": 49000
    },
    {
      "epoch": 15.56861499364676,
      "grad_norm": 1.5147370100021362,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 49010
    },
    {
      "epoch": 15.571791613722999,
      "grad_norm": 2.2556912899017334,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 49020
    },
    {
      "epoch": 15.574968233799238,
      "grad_norm": 1.7670574188232422,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 49030
    },
    {
      "epoch": 15.578144853875477,
      "grad_norm": 3.753631114959717,
      "learning_rate": 2e-05,
      "loss": 0.0785,
      "step": 49040
    },
    {
      "epoch": 15.581321473951716,
      "grad_norm": 2.545015573501587,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 49050
    },
    {
      "epoch": 15.584498094027953,
      "grad_norm": 4.413838863372803,
      "learning_rate": 2e-05,
      "loss": 0.0813,
      "step": 49060
    },
    {
      "epoch": 15.587674714104192,
      "grad_norm": 1.497723937034607,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 49070
    },
    {
      "epoch": 15.590851334180432,
      "grad_norm": 2.0868303775787354,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 49080
    },
    {
      "epoch": 15.59402795425667,
      "grad_norm": 3.526158332824707,
      "learning_rate": 2e-05,
      "loss": 0.0767,
      "step": 49090
    },
    {
      "epoch": 15.59720457433291,
      "grad_norm": 2.1402111053466797,
      "learning_rate": 2e-05,
      "loss": 0.0688,
      "step": 49100
    },
    {
      "epoch": 15.600381194409149,
      "grad_norm": 2.7413065433502197,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 49110
    },
    {
      "epoch": 15.603557814485388,
      "grad_norm": 2.67374849319458,
      "learning_rate": 2e-05,
      "loss": 0.0733,
      "step": 49120
    },
    {
      "epoch": 15.606734434561627,
      "grad_norm": 1.930618405342102,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 49130
    },
    {
      "epoch": 15.609911054637866,
      "grad_norm": 2.6888632774353027,
      "learning_rate": 2e-05,
      "loss": 0.0737,
      "step": 49140
    },
    {
      "epoch": 15.609911054637866,
      "eval_loss": 1.8157639503479004,
      "eval_mse": 1.8145220911348021,
      "eval_pearson": 0.3924639607445289,
      "eval_runtime": 7.3785,
      "eval_samples_per_second": 2922.008,
      "eval_spearmanr": 0.39995872349971123,
      "eval_steps_per_second": 11.52,
      "step": 49140
    },
    {
      "epoch": 15.613087674714103,
      "grad_norm": 2.1199917793273926,
      "learning_rate": 2e-05,
      "loss": 0.0695,
      "step": 49150
    },
    {
      "epoch": 15.616264294790343,
      "grad_norm": 2.884546995162964,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 49160
    },
    {
      "epoch": 15.619440914866582,
      "grad_norm": 1.509296178817749,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 49170
    },
    {
      "epoch": 15.62261753494282,
      "grad_norm": 2.242812156677246,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 49180
    },
    {
      "epoch": 15.62579415501906,
      "grad_norm": 2.257965564727783,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 49190
    },
    {
      "epoch": 15.628970775095299,
      "grad_norm": 2.337148666381836,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 49200
    },
    {
      "epoch": 15.632147395171538,
      "grad_norm": 2.094207286834717,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 49210
    },
    {
      "epoch": 15.635324015247777,
      "grad_norm": 4.897932052612305,
      "learning_rate": 2e-05,
      "loss": 0.0747,
      "step": 49220
    },
    {
      "epoch": 15.638500635324014,
      "grad_norm": 3.189850091934204,
      "learning_rate": 2e-05,
      "loss": 0.0814,
      "step": 49230
    },
    {
      "epoch": 15.641677255400253,
      "grad_norm": 1.5265384912490845,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 49240
    },
    {
      "epoch": 15.644853875476493,
      "grad_norm": 2.163954734802246,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 49250
    },
    {
      "epoch": 15.648030495552732,
      "grad_norm": 2.608635187149048,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 49260
    },
    {
      "epoch": 15.65120711562897,
      "grad_norm": 1.819762945175171,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 49270
    },
    {
      "epoch": 15.65438373570521,
      "grad_norm": 1.4640138149261475,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 49280
    },
    {
      "epoch": 15.657560355781449,
      "grad_norm": 1.5242600440979004,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 49290
    },
    {
      "epoch": 15.660736975857688,
      "grad_norm": 1.9612579345703125,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 49300
    },
    {
      "epoch": 15.663913595933927,
      "grad_norm": 3.669517755508423,
      "learning_rate": 2e-05,
      "loss": 0.0812,
      "step": 49310
    },
    {
      "epoch": 15.667090216010164,
      "grad_norm": 4.961787223815918,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 49320
    },
    {
      "epoch": 15.670266836086403,
      "grad_norm": 1.6273906230926514,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 49330
    },
    {
      "epoch": 15.673443456162643,
      "grad_norm": 3.021629571914673,
      "learning_rate": 2e-05,
      "loss": 0.0816,
      "step": 49340
    },
    {
      "epoch": 15.676620076238882,
      "grad_norm": 2.1004140377044678,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 49350
    },
    {
      "epoch": 15.67979669631512,
      "grad_norm": 1.7599765062332153,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 49360
    },
    {
      "epoch": 15.68297331639136,
      "grad_norm": 1.9390952587127686,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 49370
    },
    {
      "epoch": 15.686149936467599,
      "grad_norm": 1.7160876989364624,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 49380
    },
    {
      "epoch": 15.689326556543838,
      "grad_norm": 1.5287169218063354,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 49390
    },
    {
      "epoch": 15.692503176620075,
      "grad_norm": 1.5303786993026733,
      "learning_rate": 2e-05,
      "loss": 0.0665,
      "step": 49400
    },
    {
      "epoch": 15.695679796696314,
      "grad_norm": 1.7491652965545654,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 49410
    },
    {
      "epoch": 15.698856416772554,
      "grad_norm": 8.488829612731934,
      "learning_rate": 2e-05,
      "loss": 0.0785,
      "step": 49420
    },
    {
      "epoch": 15.702033036848793,
      "grad_norm": 1.4625060558319092,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 49430
    },
    {
      "epoch": 15.705209656925032,
      "grad_norm": 1.6183784008026123,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 49440
    },
    {
      "epoch": 15.70838627700127,
      "grad_norm": 2.5346505641937256,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 49450
    },
    {
      "epoch": 15.70997458703939,
      "eval_loss": 1.7878409624099731,
      "eval_mse": 1.7862705836490709,
      "eval_pearson": 0.40426645004955186,
      "eval_runtime": 7.5755,
      "eval_samples_per_second": 2846.009,
      "eval_spearmanr": 0.40966570698477506,
      "eval_steps_per_second": 11.22,
      "step": 49455
    },
    {
      "epoch": 15.71156289707751,
      "grad_norm": 1.6670541763305664,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 49460
    },
    {
      "epoch": 15.714739517153749,
      "grad_norm": 2.901609182357788,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 49470
    },
    {
      "epoch": 15.717916137229988,
      "grad_norm": 1.5351961851119995,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 49480
    },
    {
      "epoch": 15.721092757306225,
      "grad_norm": 1.6724958419799805,
      "learning_rate": 2e-05,
      "loss": 0.0739,
      "step": 49490
    },
    {
      "epoch": 15.724269377382464,
      "grad_norm": 1.590510368347168,
      "learning_rate": 2e-05,
      "loss": 0.0722,
      "step": 49500
    },
    {
      "epoch": 15.727445997458704,
      "grad_norm": 2.700838565826416,
      "learning_rate": 2e-05,
      "loss": 0.0771,
      "step": 49510
    },
    {
      "epoch": 15.730622617534943,
      "grad_norm": 2.366076946258545,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 49520
    },
    {
      "epoch": 15.733799237611182,
      "grad_norm": 1.7938919067382812,
      "learning_rate": 2e-05,
      "loss": 0.0756,
      "step": 49530
    },
    {
      "epoch": 15.73697585768742,
      "grad_norm": 2.245495080947876,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 49540
    },
    {
      "epoch": 15.74015247776366,
      "grad_norm": 2.95585036277771,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 49550
    },
    {
      "epoch": 15.743329097839899,
      "grad_norm": 1.6098695993423462,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 49560
    },
    {
      "epoch": 15.746505717916136,
      "grad_norm": 1.8385437726974487,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 49570
    },
    {
      "epoch": 15.749682337992375,
      "grad_norm": 1.4286442995071411,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 49580
    },
    {
      "epoch": 15.752858958068614,
      "grad_norm": 1.8692737817764282,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 49590
    },
    {
      "epoch": 15.756035578144854,
      "grad_norm": 2.5640017986297607,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 49600
    },
    {
      "epoch": 15.759212198221093,
      "grad_norm": 2.1835708618164062,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 49610
    },
    {
      "epoch": 15.762388818297332,
      "grad_norm": 2.016432523727417,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 49620
    },
    {
      "epoch": 15.76556543837357,
      "grad_norm": 1.5742402076721191,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 49630
    },
    {
      "epoch": 15.76874205844981,
      "grad_norm": 1.4811482429504395,
      "learning_rate": 2e-05,
      "loss": 0.0632,
      "step": 49640
    },
    {
      "epoch": 15.771918678526049,
      "grad_norm": 6.316410541534424,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 49650
    },
    {
      "epoch": 15.775095298602288,
      "grad_norm": 1.6591562032699585,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 49660
    },
    {
      "epoch": 15.778271918678525,
      "grad_norm": 2.557926893234253,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 49670
    },
    {
      "epoch": 15.781448538754764,
      "grad_norm": 1.6297473907470703,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 49680
    },
    {
      "epoch": 15.784625158831004,
      "grad_norm": 2.9850637912750244,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 49690
    },
    {
      "epoch": 15.787801778907243,
      "grad_norm": 2.272742986679077,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 49700
    },
    {
      "epoch": 15.790978398983482,
      "grad_norm": 2.663712501525879,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 49710
    },
    {
      "epoch": 15.79415501905972,
      "grad_norm": 3.121267557144165,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 49720
    },
    {
      "epoch": 15.79733163913596,
      "grad_norm": 1.689671516418457,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 49730
    },
    {
      "epoch": 15.800508259212199,
      "grad_norm": 1.7376694679260254,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 49740
    },
    {
      "epoch": 15.803684879288436,
      "grad_norm": 2.119300365447998,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 49750
    },
    {
      "epoch": 15.806861499364675,
      "grad_norm": 1.3367244005203247,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 49760
    },
    {
      "epoch": 15.810038119440915,
      "grad_norm": 2.5591561794281006,
      "learning_rate": 2e-05,
      "loss": 0.0728,
      "step": 49770
    },
    {
      "epoch": 15.810038119440915,
      "eval_loss": 1.8207266330718994,
      "eval_mse": 1.819418343310011,
      "eval_pearson": 0.3888966844031486,
      "eval_runtime": 7.5863,
      "eval_samples_per_second": 2841.956,
      "eval_spearmanr": 0.3931149197459285,
      "eval_steps_per_second": 11.204,
      "step": 49770
    },
    {
      "epoch": 15.813214739517154,
      "grad_norm": 1.9182900190353394,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 49780
    },
    {
      "epoch": 15.816391359593393,
      "grad_norm": 1.7568432092666626,
      "learning_rate": 2e-05,
      "loss": 0.0852,
      "step": 49790
    },
    {
      "epoch": 15.819567979669632,
      "grad_norm": 2.4438788890838623,
      "learning_rate": 2e-05,
      "loss": 0.0774,
      "step": 49800
    },
    {
      "epoch": 15.82274459974587,
      "grad_norm": 3.1970579624176025,
      "learning_rate": 2e-05,
      "loss": 0.0726,
      "step": 49810
    },
    {
      "epoch": 15.82592121982211,
      "grad_norm": 2.1249969005584717,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 49820
    },
    {
      "epoch": 15.829097839898349,
      "grad_norm": 1.9323605298995972,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 49830
    },
    {
      "epoch": 15.832274459974586,
      "grad_norm": 3.1165151596069336,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 49840
    },
    {
      "epoch": 15.835451080050825,
      "grad_norm": 2.7589409351348877,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 49850
    },
    {
      "epoch": 15.838627700127065,
      "grad_norm": 2.563788414001465,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 49860
    },
    {
      "epoch": 15.841804320203304,
      "grad_norm": 1.7402369976043701,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 49870
    },
    {
      "epoch": 15.844980940279543,
      "grad_norm": 2.447655200958252,
      "learning_rate": 2e-05,
      "loss": 0.0853,
      "step": 49880
    },
    {
      "epoch": 15.848157560355782,
      "grad_norm": 4.735329627990723,
      "learning_rate": 2e-05,
      "loss": 0.0744,
      "step": 49890
    },
    {
      "epoch": 15.851334180432021,
      "grad_norm": 2.613992214202881,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 49900
    },
    {
      "epoch": 15.85451080050826,
      "grad_norm": 1.2792420387268066,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 49910
    },
    {
      "epoch": 15.857687420584497,
      "grad_norm": 2.413456916809082,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 49920
    },
    {
      "epoch": 15.860864040660736,
      "grad_norm": 2.968855857849121,
      "learning_rate": 2e-05,
      "loss": 0.0666,
      "step": 49930
    },
    {
      "epoch": 15.864040660736975,
      "grad_norm": 3.3080601692199707,
      "learning_rate": 2e-05,
      "loss": 0.0731,
      "step": 49940
    },
    {
      "epoch": 15.867217280813215,
      "grad_norm": 3.452131509780884,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 49950
    },
    {
      "epoch": 15.870393900889454,
      "grad_norm": 1.9829814434051514,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 49960
    },
    {
      "epoch": 15.873570520965693,
      "grad_norm": 3.5187978744506836,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 49970
    },
    {
      "epoch": 15.876747141041932,
      "grad_norm": 2.9872357845306396,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 49980
    },
    {
      "epoch": 15.879923761118171,
      "grad_norm": 2.139338970184326,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 49990
    },
    {
      "epoch": 15.88310038119441,
      "grad_norm": 2.6147046089172363,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 50000
    },
    {
      "epoch": 15.886277001270647,
      "grad_norm": 2.083725690841675,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 50010
    },
    {
      "epoch": 15.889453621346886,
      "grad_norm": 2.1812431812286377,
      "learning_rate": 2e-05,
      "loss": 0.0757,
      "step": 50020
    },
    {
      "epoch": 15.892630241423126,
      "grad_norm": 1.3001168966293335,
      "learning_rate": 2e-05,
      "loss": 0.0758,
      "step": 50030
    },
    {
      "epoch": 15.895806861499365,
      "grad_norm": 3.597446918487549,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 50040
    },
    {
      "epoch": 15.898983481575604,
      "grad_norm": 3.5763356685638428,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 50050
    },
    {
      "epoch": 15.902160101651843,
      "grad_norm": 1.4570392370224,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 50060
    },
    {
      "epoch": 15.905336721728082,
      "grad_norm": 3.214054822921753,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 50070
    },
    {
      "epoch": 15.908513341804321,
      "grad_norm": 1.9868295192718506,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 50080
    },
    {
      "epoch": 15.910101651842439,
      "eval_loss": 1.875032663345337,
      "eval_mse": 1.8735317957008704,
      "eval_pearson": 0.3799511534453506,
      "eval_runtime": 7.3844,
      "eval_samples_per_second": 2919.657,
      "eval_spearmanr": 0.3866267054154118,
      "eval_steps_per_second": 11.511,
      "step": 50085
    },
    {
      "epoch": 15.911689961880558,
      "grad_norm": 2.075789213180542,
      "learning_rate": 2e-05,
      "loss": 0.0729,
      "step": 50090
    },
    {
      "epoch": 15.914866581956797,
      "grad_norm": 1.5265262126922607,
      "learning_rate": 2e-05,
      "loss": 0.0728,
      "step": 50100
    },
    {
      "epoch": 15.918043202033036,
      "grad_norm": 1.8777227401733398,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 50110
    },
    {
      "epoch": 15.921219822109276,
      "grad_norm": 2.1363744735717773,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 50120
    },
    {
      "epoch": 15.924396442185515,
      "grad_norm": 1.6288681030273438,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 50130
    },
    {
      "epoch": 15.927573062261754,
      "grad_norm": 2.1336355209350586,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 50140
    },
    {
      "epoch": 15.930749682337993,
      "grad_norm": 2.4166369438171387,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 50150
    },
    {
      "epoch": 15.933926302414232,
      "grad_norm": 2.053762674331665,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 50160
    },
    {
      "epoch": 15.937102922490471,
      "grad_norm": 1.819475531578064,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 50170
    },
    {
      "epoch": 15.940279542566708,
      "grad_norm": 1.4251352548599243,
      "learning_rate": 2e-05,
      "loss": 0.0757,
      "step": 50180
    },
    {
      "epoch": 15.943456162642947,
      "grad_norm": 2.2734429836273193,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 50190
    },
    {
      "epoch": 15.946632782719186,
      "grad_norm": 2.7105984687805176,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 50200
    },
    {
      "epoch": 15.949809402795426,
      "grad_norm": 2.3315980434417725,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 50210
    },
    {
      "epoch": 15.952986022871665,
      "grad_norm": 1.9824820756912231,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 50220
    },
    {
      "epoch": 15.956162642947904,
      "grad_norm": 3.3391783237457275,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 50230
    },
    {
      "epoch": 15.959339263024143,
      "grad_norm": 2.461007833480835,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 50240
    },
    {
      "epoch": 15.962515883100382,
      "grad_norm": 3.150308847427368,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 50250
    },
    {
      "epoch": 15.96569250317662,
      "grad_norm": 1.8067848682403564,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 50260
    },
    {
      "epoch": 15.968869123252858,
      "grad_norm": 3.2608628273010254,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 50270
    },
    {
      "epoch": 15.972045743329097,
      "grad_norm": 2.6859686374664307,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 50280
    },
    {
      "epoch": 15.975222363405337,
      "grad_norm": 3.484780788421631,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 50290
    },
    {
      "epoch": 15.978398983481576,
      "grad_norm": 5.324681758880615,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 50300
    },
    {
      "epoch": 15.981575603557815,
      "grad_norm": 2.2189183235168457,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 50310
    },
    {
      "epoch": 15.984752223634054,
      "grad_norm": 4.782855033874512,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 50320
    },
    {
      "epoch": 15.987928843710293,
      "grad_norm": 1.7455910444259644,
      "learning_rate": 2e-05,
      "loss": 0.0737,
      "step": 50330
    },
    {
      "epoch": 15.991105463786532,
      "grad_norm": 1.985369086265564,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 50340
    },
    {
      "epoch": 15.99428208386277,
      "grad_norm": 3.127471685409546,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 50350
    },
    {
      "epoch": 15.997458703939008,
      "grad_norm": 2.240797519683838,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 50360
    },
    {
      "epoch": 16.00063532401525,
      "grad_norm": 1.6364203691482544,
      "learning_rate": 2e-05,
      "loss": 0.0778,
      "step": 50370
    },
    {
      "epoch": 16.00381194409149,
      "grad_norm": 1.7789077758789062,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 50380
    },
    {
      "epoch": 16.006988564167724,
      "grad_norm": 1.6903140544891357,
      "learning_rate": 2e-05,
      "loss": 0.0634,
      "step": 50390
    },
    {
      "epoch": 16.010165184243963,
      "grad_norm": 1.9507941007614136,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 50400
    },
    {
      "epoch": 16.010165184243963,
      "eval_loss": 1.913893461227417,
      "eval_mse": 1.9124926780725455,
      "eval_pearson": 0.36034701885244774,
      "eval_runtime": 7.4327,
      "eval_samples_per_second": 2900.703,
      "eval_spearmanr": 0.36712862082488623,
      "eval_steps_per_second": 11.436,
      "step": 50400
    },
    {
      "epoch": 16.013341804320202,
      "grad_norm": 1.969264268875122,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 50410
    },
    {
      "epoch": 16.01651842439644,
      "grad_norm": 1.4548252820968628,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 50420
    },
    {
      "epoch": 16.01969504447268,
      "grad_norm": 1.8021891117095947,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 50430
    },
    {
      "epoch": 16.02287166454892,
      "grad_norm": 1.5351183414459229,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 50440
    },
    {
      "epoch": 16.02604828462516,
      "grad_norm": 2.931123733520508,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 50450
    },
    {
      "epoch": 16.029224904701397,
      "grad_norm": 1.785810112953186,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 50460
    },
    {
      "epoch": 16.032401524777637,
      "grad_norm": 3.7131214141845703,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 50470
    },
    {
      "epoch": 16.035578144853876,
      "grad_norm": 1.9380244016647339,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 50480
    },
    {
      "epoch": 16.038754764930115,
      "grad_norm": 2.0413999557495117,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 50490
    },
    {
      "epoch": 16.041931385006354,
      "grad_norm": 1.5406322479248047,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 50500
    },
    {
      "epoch": 16.045108005082593,
      "grad_norm": 2.61187744140625,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 50510
    },
    {
      "epoch": 16.048284625158832,
      "grad_norm": 1.7727386951446533,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 50520
    },
    {
      "epoch": 16.05146124523507,
      "grad_norm": 3.392331838607788,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 50530
    },
    {
      "epoch": 16.05463786531131,
      "grad_norm": 3.592541217803955,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 50540
    },
    {
      "epoch": 16.05781448538755,
      "grad_norm": 1.5262480974197388,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 50550
    },
    {
      "epoch": 16.060991105463785,
      "grad_norm": 2.7543396949768066,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 50560
    },
    {
      "epoch": 16.064167725540024,
      "grad_norm": 1.7795733213424683,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 50570
    },
    {
      "epoch": 16.067344345616263,
      "grad_norm": 1.6884878873825073,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 50580
    },
    {
      "epoch": 16.070520965692502,
      "grad_norm": 2.7604076862335205,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 50590
    },
    {
      "epoch": 16.07369758576874,
      "grad_norm": 2.475220203399658,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 50600
    },
    {
      "epoch": 16.07687420584498,
      "grad_norm": 8.985699653625488,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 50610
    },
    {
      "epoch": 16.08005082592122,
      "grad_norm": 2.7184560298919678,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 50620
    },
    {
      "epoch": 16.08322744599746,
      "grad_norm": 2.154904842376709,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 50630
    },
    {
      "epoch": 16.086404066073698,
      "grad_norm": 1.409199595451355,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 50640
    },
    {
      "epoch": 16.089580686149937,
      "grad_norm": 2.4661076068878174,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 50650
    },
    {
      "epoch": 16.092757306226176,
      "grad_norm": 1.830939531326294,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 50660
    },
    {
      "epoch": 16.095933926302415,
      "grad_norm": 3.5483598709106445,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 50670
    },
    {
      "epoch": 16.099110546378654,
      "grad_norm": 1.3367139101028442,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 50680
    },
    {
      "epoch": 16.102287166454893,
      "grad_norm": 1.351844310760498,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 50690
    },
    {
      "epoch": 16.105463786531132,
      "grad_norm": 2.046597480773926,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 50700
    },
    {
      "epoch": 16.10864040660737,
      "grad_norm": 3.344655990600586,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 50710
    },
    {
      "epoch": 16.11022871664549,
      "eval_loss": 1.8529659509658813,
      "eval_mse": 1.8512837409834915,
      "eval_pearson": 0.4091591409744138,
      "eval_runtime": 7.497,
      "eval_samples_per_second": 2875.813,
      "eval_spearmanr": 0.41451216784287204,
      "eval_steps_per_second": 11.338,
      "step": 50715
    },
    {
      "epoch": 16.11181702668361,
      "grad_norm": 2.277373790740967,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 50720
    },
    {
      "epoch": 16.114993646759846,
      "grad_norm": 2.29127836227417,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 50730
    },
    {
      "epoch": 16.118170266836085,
      "grad_norm": 1.4418927431106567,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 50740
    },
    {
      "epoch": 16.121346886912324,
      "grad_norm": 2.0810394287109375,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 50750
    },
    {
      "epoch": 16.124523506988563,
      "grad_norm": 2.3558313846588135,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 50760
    },
    {
      "epoch": 16.127700127064802,
      "grad_norm": 4.13189172744751,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 50770
    },
    {
      "epoch": 16.13087674714104,
      "grad_norm": 2.2360634803771973,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 50780
    },
    {
      "epoch": 16.13405336721728,
      "grad_norm": 2.3815016746520996,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 50790
    },
    {
      "epoch": 16.13722998729352,
      "grad_norm": 1.7186977863311768,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 50800
    },
    {
      "epoch": 16.14040660736976,
      "grad_norm": 1.3100436925888062,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 50810
    },
    {
      "epoch": 16.143583227445998,
      "grad_norm": 2.710383176803589,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 50820
    },
    {
      "epoch": 16.146759847522237,
      "grad_norm": 3.387418031692505,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 50830
    },
    {
      "epoch": 16.149936467598476,
      "grad_norm": 3.243291139602661,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 50840
    },
    {
      "epoch": 16.153113087674715,
      "grad_norm": 1.5524922609329224,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 50850
    },
    {
      "epoch": 16.156289707750954,
      "grad_norm": 1.4252262115478516,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 50860
    },
    {
      "epoch": 16.159466327827193,
      "grad_norm": 1.651949167251587,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 50870
    },
    {
      "epoch": 16.162642947903432,
      "grad_norm": 2.494852066040039,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 50880
    },
    {
      "epoch": 16.16581956797967,
      "grad_norm": 3.8639163970947266,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 50890
    },
    {
      "epoch": 16.168996188055907,
      "grad_norm": 1.401865005493164,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 50900
    },
    {
      "epoch": 16.172172808132146,
      "grad_norm": 1.814682126045227,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 50910
    },
    {
      "epoch": 16.175349428208385,
      "grad_norm": 1.2916666269302368,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 50920
    },
    {
      "epoch": 16.178526048284624,
      "grad_norm": 3.069571018218994,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 50930
    },
    {
      "epoch": 16.181702668360863,
      "grad_norm": 7.203607082366943,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 50940
    },
    {
      "epoch": 16.184879288437102,
      "grad_norm": 2.22745418548584,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 50950
    },
    {
      "epoch": 16.18805590851334,
      "grad_norm": 2.3869664669036865,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 50960
    },
    {
      "epoch": 16.19123252858958,
      "grad_norm": 1.7384778261184692,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 50970
    },
    {
      "epoch": 16.19440914866582,
      "grad_norm": 4.811707496643066,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 50980
    },
    {
      "epoch": 16.19758576874206,
      "grad_norm": 2.0588245391845703,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 50990
    },
    {
      "epoch": 16.200762388818298,
      "grad_norm": 3.2028160095214844,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 51000
    },
    {
      "epoch": 16.203939008894537,
      "grad_norm": 2.732154130935669,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 51010
    },
    {
      "epoch": 16.207115628970776,
      "grad_norm": 3.999803304672241,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 51020
    },
    {
      "epoch": 16.210292249047015,
      "grad_norm": 1.33541738986969,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 51030
    },
    {
      "epoch": 16.210292249047015,
      "eval_loss": 1.910302996635437,
      "eval_mse": 1.90895726256114,
      "eval_pearson": 0.3686743796489897,
      "eval_runtime": 7.494,
      "eval_samples_per_second": 2876.957,
      "eval_spearmanr": 0.37717944796375535,
      "eval_steps_per_second": 11.342,
      "step": 51030
    },
    {
      "epoch": 16.213468869123254,
      "grad_norm": 1.6353726387023926,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 51040
    },
    {
      "epoch": 16.216645489199493,
      "grad_norm": 1.991805911064148,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 51050
    },
    {
      "epoch": 16.219822109275732,
      "grad_norm": 1.571757435798645,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 51060
    },
    {
      "epoch": 16.22299872935197,
      "grad_norm": 1.970697045326233,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 51070
    },
    {
      "epoch": 16.226175349428207,
      "grad_norm": 2.364567756652832,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 51080
    },
    {
      "epoch": 16.229351969504446,
      "grad_norm": 1.7655702829360962,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 51090
    },
    {
      "epoch": 16.232528589580685,
      "grad_norm": 1.8023589849472046,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 51100
    },
    {
      "epoch": 16.235705209656924,
      "grad_norm": 2.114680051803589,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 51110
    },
    {
      "epoch": 16.238881829733163,
      "grad_norm": 1.8735215663909912,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 51120
    },
    {
      "epoch": 16.242058449809402,
      "grad_norm": 2.622795343399048,
      "learning_rate": 2e-05,
      "loss": 0.065,
      "step": 51130
    },
    {
      "epoch": 16.24523506988564,
      "grad_norm": 2.0263047218322754,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 51140
    },
    {
      "epoch": 16.24841168996188,
      "grad_norm": 2.49367618560791,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 51150
    },
    {
      "epoch": 16.25158831003812,
      "grad_norm": 1.2217154502868652,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 51160
    },
    {
      "epoch": 16.25476493011436,
      "grad_norm": 1.9454892873764038,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 51170
    },
    {
      "epoch": 16.257941550190598,
      "grad_norm": 2.86547589302063,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 51180
    },
    {
      "epoch": 16.261118170266837,
      "grad_norm": 1.8502284288406372,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 51190
    },
    {
      "epoch": 16.264294790343076,
      "grad_norm": 2.5790505409240723,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 51200
    },
    {
      "epoch": 16.267471410419315,
      "grad_norm": 3.5938854217529297,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 51210
    },
    {
      "epoch": 16.270648030495554,
      "grad_norm": 3.2051808834075928,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 51220
    },
    {
      "epoch": 16.273824650571793,
      "grad_norm": 1.257200837135315,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 51230
    },
    {
      "epoch": 16.27700127064803,
      "grad_norm": 1.987102746963501,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 51240
    },
    {
      "epoch": 16.280177890724268,
      "grad_norm": 1.6745132207870483,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 51250
    },
    {
      "epoch": 16.283354510800507,
      "grad_norm": 1.4421409368515015,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 51260
    },
    {
      "epoch": 16.286531130876746,
      "grad_norm": 2.073082447052002,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 51270
    },
    {
      "epoch": 16.289707750952985,
      "grad_norm": 1.3438621759414673,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 51280
    },
    {
      "epoch": 16.292884371029224,
      "grad_norm": 2.976741075515747,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 51290
    },
    {
      "epoch": 16.296060991105463,
      "grad_norm": 1.7048355340957642,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 51300
    },
    {
      "epoch": 16.299237611181702,
      "grad_norm": 1.5583913326263428,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 51310
    },
    {
      "epoch": 16.30241423125794,
      "grad_norm": 1.901214361190796,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 51320
    },
    {
      "epoch": 16.30559085133418,
      "grad_norm": 1.6340984106063843,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 51330
    },
    {
      "epoch": 16.30876747141042,
      "grad_norm": 1.1567572355270386,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 51340
    },
    {
      "epoch": 16.31035578144854,
      "eval_loss": 1.933061122894287,
      "eval_mse": 1.9317782383239335,
      "eval_pearson": 0.36661333434582816,
      "eval_runtime": 7.379,
      "eval_samples_per_second": 2921.81,
      "eval_spearmanr": 0.3731704648041542,
      "eval_steps_per_second": 11.519,
      "step": 51345
    },
    {
      "epoch": 16.31194409148666,
      "grad_norm": 2.8053882122039795,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 51350
    },
    {
      "epoch": 16.315120711562898,
      "grad_norm": 1.5693494081497192,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 51360
    },
    {
      "epoch": 16.318297331639137,
      "grad_norm": 1.6431721448898315,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 51370
    },
    {
      "epoch": 16.321473951715376,
      "grad_norm": 2.097963809967041,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 51380
    },
    {
      "epoch": 16.324650571791615,
      "grad_norm": 1.4805548191070557,
      "learning_rate": 2e-05,
      "loss": 0.0779,
      "step": 51390
    },
    {
      "epoch": 16.327827191867854,
      "grad_norm": 2.0874969959259033,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 51400
    },
    {
      "epoch": 16.331003811944093,
      "grad_norm": 1.3247383832931519,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 51410
    },
    {
      "epoch": 16.33418043202033,
      "grad_norm": 2.3586184978485107,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 51420
    },
    {
      "epoch": 16.337357052096568,
      "grad_norm": 1.6065829992294312,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 51430
    },
    {
      "epoch": 16.340533672172807,
      "grad_norm": 6.437698841094971,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 51440
    },
    {
      "epoch": 16.343710292249046,
      "grad_norm": 3.766948699951172,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 51450
    },
    {
      "epoch": 16.346886912325285,
      "grad_norm": 2.420586109161377,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 51460
    },
    {
      "epoch": 16.350063532401524,
      "grad_norm": 1.5993895530700684,
      "learning_rate": 2e-05,
      "loss": 0.0665,
      "step": 51470
    },
    {
      "epoch": 16.353240152477763,
      "grad_norm": 5.1015095710754395,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 51480
    },
    {
      "epoch": 16.356416772554002,
      "grad_norm": 1.4465152025222778,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 51490
    },
    {
      "epoch": 16.35959339263024,
      "grad_norm": 1.6882392168045044,
      "learning_rate": 2e-05,
      "loss": 0.0689,
      "step": 51500
    },
    {
      "epoch": 16.36277001270648,
      "grad_norm": 1.6693267822265625,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 51510
    },
    {
      "epoch": 16.36594663278272,
      "grad_norm": 1.9861533641815186,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 51520
    },
    {
      "epoch": 16.36912325285896,
      "grad_norm": 1.5017026662826538,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 51530
    },
    {
      "epoch": 16.372299872935198,
      "grad_norm": 1.4202628135681152,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 51540
    },
    {
      "epoch": 16.375476493011437,
      "grad_norm": 2.1555590629577637,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 51550
    },
    {
      "epoch": 16.378653113087676,
      "grad_norm": 1.664417028427124,
      "learning_rate": 2e-05,
      "loss": 0.0646,
      "step": 51560
    },
    {
      "epoch": 16.381829733163915,
      "grad_norm": 2.051732301712036,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 51570
    },
    {
      "epoch": 16.385006353240154,
      "grad_norm": 2.151106119155884,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 51580
    },
    {
      "epoch": 16.38818297331639,
      "grad_norm": 1.9076732397079468,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 51590
    },
    {
      "epoch": 16.39135959339263,
      "grad_norm": 8.950393676757812,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 51600
    },
    {
      "epoch": 16.394536213468868,
      "grad_norm": 2.4284656047821045,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 51610
    },
    {
      "epoch": 16.397712833545107,
      "grad_norm": 1.6683933734893799,
      "learning_rate": 2e-05,
      "loss": 0.0644,
      "step": 51620
    },
    {
      "epoch": 16.400889453621346,
      "grad_norm": 3.0556013584136963,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 51630
    },
    {
      "epoch": 16.404066073697585,
      "grad_norm": 1.4004814624786377,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 51640
    },
    {
      "epoch": 16.407242693773824,
      "grad_norm": 2.1231305599212646,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 51650
    },
    {
      "epoch": 16.410419313850063,
      "grad_norm": 2.1887171268463135,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 51660
    },
    {
      "epoch": 16.410419313850063,
      "eval_loss": 1.7341439723968506,
      "eval_mse": 1.7330582129933174,
      "eval_pearson": 0.4322192993816213,
      "eval_runtime": 7.4806,
      "eval_samples_per_second": 2882.122,
      "eval_spearmanr": 0.43495845811284917,
      "eval_steps_per_second": 11.363,
      "step": 51660
    },
    {
      "epoch": 16.413595933926302,
      "grad_norm": 1.770486831665039,
      "learning_rate": 2e-05,
      "loss": 0.0745,
      "step": 51670
    },
    {
      "epoch": 16.41677255400254,
      "grad_norm": 1.8962119817733765,
      "learning_rate": 2e-05,
      "loss": 0.0746,
      "step": 51680
    },
    {
      "epoch": 16.41994917407878,
      "grad_norm": 2.497636079788208,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 51690
    },
    {
      "epoch": 16.42312579415502,
      "grad_norm": 1.7378426790237427,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 51700
    },
    {
      "epoch": 16.42630241423126,
      "grad_norm": 1.5625444650650024,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 51710
    },
    {
      "epoch": 16.429479034307498,
      "grad_norm": 1.9511579275131226,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 51720
    },
    {
      "epoch": 16.432655654383737,
      "grad_norm": 1.429713487625122,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 51730
    },
    {
      "epoch": 16.435832274459976,
      "grad_norm": 1.6717840433120728,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 51740
    },
    {
      "epoch": 16.439008894536215,
      "grad_norm": 2.397425651550293,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 51750
    },
    {
      "epoch": 16.44218551461245,
      "grad_norm": 1.4220023155212402,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 51760
    },
    {
      "epoch": 16.44536213468869,
      "grad_norm": 1.836935043334961,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 51770
    },
    {
      "epoch": 16.44853875476493,
      "grad_norm": 1.192201018333435,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 51780
    },
    {
      "epoch": 16.451715374841168,
      "grad_norm": 2.531510591506958,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 51790
    },
    {
      "epoch": 16.454891994917407,
      "grad_norm": 1.416219711303711,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 51800
    },
    {
      "epoch": 16.458068614993646,
      "grad_norm": 2.0181503295898438,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 51810
    },
    {
      "epoch": 16.461245235069885,
      "grad_norm": 1.5196950435638428,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 51820
    },
    {
      "epoch": 16.464421855146124,
      "grad_norm": 2.594507932662964,
      "learning_rate": 2e-05,
      "loss": 0.0704,
      "step": 51830
    },
    {
      "epoch": 16.467598475222363,
      "grad_norm": 1.7557661533355713,
      "learning_rate": 2e-05,
      "loss": 0.0693,
      "step": 51840
    },
    {
      "epoch": 16.470775095298603,
      "grad_norm": 2.339580774307251,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 51850
    },
    {
      "epoch": 16.47395171537484,
      "grad_norm": 1.4524513483047485,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 51860
    },
    {
      "epoch": 16.47712833545108,
      "grad_norm": 2.7289516925811768,
      "learning_rate": 2e-05,
      "loss": 0.0695,
      "step": 51870
    },
    {
      "epoch": 16.48030495552732,
      "grad_norm": 1.888644814491272,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 51880
    },
    {
      "epoch": 16.48348157560356,
      "grad_norm": 2.2099993228912354,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 51890
    },
    {
      "epoch": 16.486658195679798,
      "grad_norm": 2.216474771499634,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 51900
    },
    {
      "epoch": 16.489834815756037,
      "grad_norm": 1.993884801864624,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 51910
    },
    {
      "epoch": 16.493011435832276,
      "grad_norm": 1.5879273414611816,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 51920
    },
    {
      "epoch": 16.49618805590851,
      "grad_norm": 1.6118236780166626,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 51930
    },
    {
      "epoch": 16.49936467598475,
      "grad_norm": 1.3082878589630127,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 51940
    },
    {
      "epoch": 16.50254129606099,
      "grad_norm": 2.332885503768921,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 51950
    },
    {
      "epoch": 16.50571791613723,
      "grad_norm": 3.683873176574707,
      "learning_rate": 2e-05,
      "loss": 0.0646,
      "step": 51960
    },
    {
      "epoch": 16.508894536213468,
      "grad_norm": 3.31667160987854,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 51970
    },
    {
      "epoch": 16.510482846251588,
      "eval_loss": 1.8679612874984741,
      "eval_mse": 1.8665089159098096,
      "eval_pearson": 0.3825230798730738,
      "eval_runtime": 7.3718,
      "eval_samples_per_second": 2924.66,
      "eval_spearmanr": 0.39259448808375164,
      "eval_steps_per_second": 11.53,
      "step": 51975
    },
    {
      "epoch": 16.512071156289707,
      "grad_norm": 1.4185121059417725,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 51980
    },
    {
      "epoch": 16.515247776365946,
      "grad_norm": 3.170330762863159,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 51990
    },
    {
      "epoch": 16.518424396442185,
      "grad_norm": 3.702785015106201,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 52000
    },
    {
      "epoch": 16.521601016518424,
      "grad_norm": 1.2379640340805054,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 52010
    },
    {
      "epoch": 16.524777636594663,
      "grad_norm": 2.1348040103912354,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 52020
    },
    {
      "epoch": 16.527954256670903,
      "grad_norm": 1.2812914848327637,
      "learning_rate": 2e-05,
      "loss": 0.0734,
      "step": 52030
    },
    {
      "epoch": 16.53113087674714,
      "grad_norm": 1.619234323501587,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 52040
    },
    {
      "epoch": 16.53430749682338,
      "grad_norm": 2.194977283477783,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 52050
    },
    {
      "epoch": 16.53748411689962,
      "grad_norm": 1.8597588539123535,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 52060
    },
    {
      "epoch": 16.54066073697586,
      "grad_norm": 3.2673492431640625,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 52070
    },
    {
      "epoch": 16.543837357052098,
      "grad_norm": 3.208256244659424,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 52080
    },
    {
      "epoch": 16.547013977128337,
      "grad_norm": 1.9552710056304932,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 52090
    },
    {
      "epoch": 16.550190597204573,
      "grad_norm": 2.3995869159698486,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 52100
    },
    {
      "epoch": 16.55336721728081,
      "grad_norm": 1.8696179389953613,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 52110
    },
    {
      "epoch": 16.55654383735705,
      "grad_norm": 2.3770551681518555,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 52120
    },
    {
      "epoch": 16.55972045743329,
      "grad_norm": 1.8008309602737427,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 52130
    },
    {
      "epoch": 16.56289707750953,
      "grad_norm": 3.1887638568878174,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 52140
    },
    {
      "epoch": 16.566073697585768,
      "grad_norm": 6.8893842697143555,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 52150
    },
    {
      "epoch": 16.569250317662007,
      "grad_norm": 4.029172420501709,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 52160
    },
    {
      "epoch": 16.572426937738246,
      "grad_norm": 1.6606817245483398,
      "learning_rate": 2e-05,
      "loss": 0.0632,
      "step": 52170
    },
    {
      "epoch": 16.575603557814485,
      "grad_norm": 2.215017795562744,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 52180
    },
    {
      "epoch": 16.578780177890724,
      "grad_norm": 2.889962673187256,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 52190
    },
    {
      "epoch": 16.581956797966964,
      "grad_norm": 5.4279303550720215,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 52200
    },
    {
      "epoch": 16.585133418043203,
      "grad_norm": 1.2587089538574219,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 52210
    },
    {
      "epoch": 16.58831003811944,
      "grad_norm": 2.694004535675049,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 52220
    },
    {
      "epoch": 16.59148665819568,
      "grad_norm": 2.4032442569732666,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 52230
    },
    {
      "epoch": 16.59466327827192,
      "grad_norm": 1.822214961051941,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 52240
    },
    {
      "epoch": 16.59783989834816,
      "grad_norm": 3.419121265411377,
      "learning_rate": 2e-05,
      "loss": 0.0689,
      "step": 52250
    },
    {
      "epoch": 16.601016518424398,
      "grad_norm": 3.055326223373413,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 52260
    },
    {
      "epoch": 16.604193138500634,
      "grad_norm": 1.4548531770706177,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 52270
    },
    {
      "epoch": 16.607369758576873,
      "grad_norm": 3.747933864593506,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 52280
    },
    {
      "epoch": 16.610546378653112,
      "grad_norm": 1.8848400115966797,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 52290
    },
    {
      "epoch": 16.610546378653112,
      "eval_loss": 1.91376793384552,
      "eval_mse": 1.912771075796025,
      "eval_pearson": 0.36630668599902,
      "eval_runtime": 7.5866,
      "eval_samples_per_second": 2841.869,
      "eval_spearmanr": 0.375931613141331,
      "eval_steps_per_second": 11.204,
      "step": 52290
    },
    {
      "epoch": 16.61372299872935,
      "grad_norm": 1.524479627609253,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 52300
    },
    {
      "epoch": 16.61689961880559,
      "grad_norm": 1.3969733715057373,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 52310
    },
    {
      "epoch": 16.62007623888183,
      "grad_norm": 2.3972654342651367,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 52320
    },
    {
      "epoch": 16.623252858958068,
      "grad_norm": 1.6768686771392822,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 52330
    },
    {
      "epoch": 16.626429479034307,
      "grad_norm": 2.200561046600342,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 52340
    },
    {
      "epoch": 16.629606099110546,
      "grad_norm": 1.3140641450881958,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 52350
    },
    {
      "epoch": 16.632782719186785,
      "grad_norm": 1.2967301607131958,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 52360
    },
    {
      "epoch": 16.635959339263025,
      "grad_norm": 3.4260144233703613,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 52370
    },
    {
      "epoch": 16.639135959339264,
      "grad_norm": 1.7904887199401855,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 52380
    },
    {
      "epoch": 16.642312579415503,
      "grad_norm": 2.1281626224517822,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 52390
    },
    {
      "epoch": 16.64548919949174,
      "grad_norm": 2.485185384750366,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 52400
    },
    {
      "epoch": 16.64866581956798,
      "grad_norm": 1.527518391609192,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 52410
    },
    {
      "epoch": 16.65184243964422,
      "grad_norm": 1.4192012548446655,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 52420
    },
    {
      "epoch": 16.65501905972046,
      "grad_norm": 1.8729811906814575,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 52430
    },
    {
      "epoch": 16.658195679796698,
      "grad_norm": 4.455744743347168,
      "learning_rate": 2e-05,
      "loss": 0.0735,
      "step": 52440
    },
    {
      "epoch": 16.661372299872934,
      "grad_norm": 5.1106276512146,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 52450
    },
    {
      "epoch": 16.664548919949173,
      "grad_norm": 1.5904635190963745,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 52460
    },
    {
      "epoch": 16.667725540025412,
      "grad_norm": 3.8765532970428467,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 52470
    },
    {
      "epoch": 16.67090216010165,
      "grad_norm": 2.965632677078247,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 52480
    },
    {
      "epoch": 16.67407878017789,
      "grad_norm": 1.3229933977127075,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 52490
    },
    {
      "epoch": 16.67725540025413,
      "grad_norm": 1.728365421295166,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 52500
    },
    {
      "epoch": 16.680432020330368,
      "grad_norm": 1.5108939409255981,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 52510
    },
    {
      "epoch": 16.683608640406607,
      "grad_norm": 2.8376357555389404,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 52520
    },
    {
      "epoch": 16.686785260482846,
      "grad_norm": 2.810295581817627,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 52530
    },
    {
      "epoch": 16.689961880559085,
      "grad_norm": 2.6962170600891113,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 52540
    },
    {
      "epoch": 16.693138500635325,
      "grad_norm": 1.605997085571289,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 52550
    },
    {
      "epoch": 16.696315120711564,
      "grad_norm": 1.9454090595245361,
      "learning_rate": 2e-05,
      "loss": 0.0712,
      "step": 52560
    },
    {
      "epoch": 16.699491740787803,
      "grad_norm": 3.8002803325653076,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 52570
    },
    {
      "epoch": 16.702668360864042,
      "grad_norm": 2.936917304992676,
      "learning_rate": 2e-05,
      "loss": 0.0808,
      "step": 52580
    },
    {
      "epoch": 16.70584498094028,
      "grad_norm": 1.4958714246749878,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 52590
    },
    {
      "epoch": 16.70902160101652,
      "grad_norm": 2.637284517288208,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 52600
    },
    {
      "epoch": 16.71060991105464,
      "eval_loss": 1.9483673572540283,
      "eval_mse": 1.9467805756902208,
      "eval_pearson": 0.37210534822432034,
      "eval_runtime": 7.465,
      "eval_samples_per_second": 2888.129,
      "eval_spearmanr": 0.38277315365334796,
      "eval_steps_per_second": 11.386,
      "step": 52605
    },
    {
      "epoch": 16.712198221092756,
      "grad_norm": 2.140389919281006,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 52610
    },
    {
      "epoch": 16.715374841168995,
      "grad_norm": 2.446012020111084,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 52620
    },
    {
      "epoch": 16.718551461245234,
      "grad_norm": 1.259712815284729,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 52630
    },
    {
      "epoch": 16.721728081321473,
      "grad_norm": 2.0309767723083496,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 52640
    },
    {
      "epoch": 16.724904701397712,
      "grad_norm": 2.2929039001464844,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 52650
    },
    {
      "epoch": 16.72808132147395,
      "grad_norm": 2.5137202739715576,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 52660
    },
    {
      "epoch": 16.73125794155019,
      "grad_norm": 2.2585840225219727,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 52670
    },
    {
      "epoch": 16.73443456162643,
      "grad_norm": 6.994805812835693,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 52680
    },
    {
      "epoch": 16.73761118170267,
      "grad_norm": 1.430769920349121,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 52690
    },
    {
      "epoch": 16.740787801778907,
      "grad_norm": 2.1445441246032715,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 52700
    },
    {
      "epoch": 16.743964421855146,
      "grad_norm": 1.7747021913528442,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 52710
    },
    {
      "epoch": 16.747141041931386,
      "grad_norm": 2.6668686866760254,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 52720
    },
    {
      "epoch": 16.750317662007625,
      "grad_norm": 2.4106996059417725,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 52730
    },
    {
      "epoch": 16.753494282083864,
      "grad_norm": 1.875857949256897,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 52740
    },
    {
      "epoch": 16.756670902160103,
      "grad_norm": 3.5279457569122314,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 52750
    },
    {
      "epoch": 16.759847522236342,
      "grad_norm": 1.9302393198013306,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 52760
    },
    {
      "epoch": 16.76302414231258,
      "grad_norm": 2.1393816471099854,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 52770
    },
    {
      "epoch": 16.76620076238882,
      "grad_norm": 3.0782761573791504,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 52780
    },
    {
      "epoch": 16.769377382465056,
      "grad_norm": 2.3965630531311035,
      "learning_rate": 2e-05,
      "loss": 0.0666,
      "step": 52790
    },
    {
      "epoch": 16.772554002541295,
      "grad_norm": 1.9185285568237305,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 52800
    },
    {
      "epoch": 16.775730622617534,
      "grad_norm": 3.0486533641815186,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 52810
    },
    {
      "epoch": 16.778907242693773,
      "grad_norm": 3.2172420024871826,
      "learning_rate": 2e-05,
      "loss": 0.0632,
      "step": 52820
    },
    {
      "epoch": 16.782083862770012,
      "grad_norm": 1.6962530612945557,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 52830
    },
    {
      "epoch": 16.78526048284625,
      "grad_norm": 1.6295268535614014,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 52840
    },
    {
      "epoch": 16.78843710292249,
      "grad_norm": 3.1238064765930176,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 52850
    },
    {
      "epoch": 16.79161372299873,
      "grad_norm": 3.3523223400115967,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 52860
    },
    {
      "epoch": 16.79479034307497,
      "grad_norm": 2.366150140762329,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 52870
    },
    {
      "epoch": 16.797966963151207,
      "grad_norm": 2.3350605964660645,
      "learning_rate": 2e-05,
      "loss": 0.0707,
      "step": 52880
    },
    {
      "epoch": 16.801143583227446,
      "grad_norm": 1.747243046760559,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 52890
    },
    {
      "epoch": 16.804320203303686,
      "grad_norm": 8.32445240020752,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 52900
    },
    {
      "epoch": 16.807496823379925,
      "grad_norm": 2.5736489295959473,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 52910
    },
    {
      "epoch": 16.810673443456164,
      "grad_norm": 1.6246612071990967,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 52920
    },
    {
      "epoch": 16.810673443456164,
      "eval_loss": 1.9002296924591064,
      "eval_mse": 1.8987912289729145,
      "eval_pearson": 0.371029104379525,
      "eval_runtime": 7.4873,
      "eval_samples_per_second": 2879.525,
      "eval_spearmanr": 0.37523435599595184,
      "eval_steps_per_second": 11.352,
      "step": 52920
    },
    {
      "epoch": 16.813850063532403,
      "grad_norm": 1.537519097328186,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 52930
    },
    {
      "epoch": 16.817026683608642,
      "grad_norm": 1.443477749824524,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 52940
    },
    {
      "epoch": 16.820203303684877,
      "grad_norm": 2.891874074935913,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 52950
    },
    {
      "epoch": 16.823379923761117,
      "grad_norm": 1.7369974851608276,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 52960
    },
    {
      "epoch": 16.826556543837356,
      "grad_norm": 2.1170942783355713,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 52970
    },
    {
      "epoch": 16.829733163913595,
      "grad_norm": 1.3349554538726807,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 52980
    },
    {
      "epoch": 16.832909783989834,
      "grad_norm": 1.4502724409103394,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 52990
    },
    {
      "epoch": 16.836086404066073,
      "grad_norm": 1.5028902292251587,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 53000
    },
    {
      "epoch": 16.839263024142312,
      "grad_norm": 2.775993585586548,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 53010
    },
    {
      "epoch": 16.84243964421855,
      "grad_norm": 4.182693004608154,
      "learning_rate": 2e-05,
      "loss": 0.073,
      "step": 53020
    },
    {
      "epoch": 16.84561626429479,
      "grad_norm": 2.8540992736816406,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 53030
    },
    {
      "epoch": 16.84879288437103,
      "grad_norm": 4.2040181159973145,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 53040
    },
    {
      "epoch": 16.85196950444727,
      "grad_norm": 1.5988341569900513,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 53050
    },
    {
      "epoch": 16.855146124523507,
      "grad_norm": 1.7604104280471802,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 53060
    },
    {
      "epoch": 16.858322744599747,
      "grad_norm": 2.9384050369262695,
      "learning_rate": 2e-05,
      "loss": 0.071,
      "step": 53070
    },
    {
      "epoch": 16.861499364675986,
      "grad_norm": 2.620347499847412,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 53080
    },
    {
      "epoch": 16.864675984752225,
      "grad_norm": 2.63922381401062,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 53090
    },
    {
      "epoch": 16.867852604828464,
      "grad_norm": 2.7918906211853027,
      "learning_rate": 2e-05,
      "loss": 0.0781,
      "step": 53100
    },
    {
      "epoch": 16.871029224904703,
      "grad_norm": 1.551776647567749,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 53110
    },
    {
      "epoch": 16.874205844980942,
      "grad_norm": 30.809886932373047,
      "learning_rate": 2e-05,
      "loss": 0.0727,
      "step": 53120
    },
    {
      "epoch": 16.877382465057178,
      "grad_norm": 2.6442337036132812,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 53130
    },
    {
      "epoch": 16.880559085133417,
      "grad_norm": 1.8175987005233765,
      "learning_rate": 2e-05,
      "loss": 0.0666,
      "step": 53140
    },
    {
      "epoch": 16.883735705209656,
      "grad_norm": 1.5705639123916626,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 53150
    },
    {
      "epoch": 16.886912325285895,
      "grad_norm": 1.9530494213104248,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 53160
    },
    {
      "epoch": 16.890088945362134,
      "grad_norm": 2.4389307498931885,
      "learning_rate": 2e-05,
      "loss": 0.0634,
      "step": 53170
    },
    {
      "epoch": 16.893265565438373,
      "grad_norm": 1.6171821355819702,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 53180
    },
    {
      "epoch": 16.896442185514612,
      "grad_norm": 1.8960576057434082,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 53190
    },
    {
      "epoch": 16.89961880559085,
      "grad_norm": 3.4602787494659424,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 53200
    },
    {
      "epoch": 16.90279542566709,
      "grad_norm": 3.108412742614746,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 53210
    },
    {
      "epoch": 16.90597204574333,
      "grad_norm": 5.802037715911865,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 53220
    },
    {
      "epoch": 16.90914866581957,
      "grad_norm": 1.5831444263458252,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 53230
    },
    {
      "epoch": 16.910736975857688,
      "eval_loss": 1.8451489210128784,
      "eval_mse": 1.844036821683727,
      "eval_pearson": 0.3897448661199998,
      "eval_runtime": 7.5171,
      "eval_samples_per_second": 2868.131,
      "eval_spearmanr": 0.3973988591153189,
      "eval_steps_per_second": 11.308,
      "step": 53235
    },
    {
      "epoch": 16.912325285895808,
      "grad_norm": 2.0352964401245117,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 53240
    },
    {
      "epoch": 16.915501905972047,
      "grad_norm": 1.394861102104187,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 53250
    },
    {
      "epoch": 16.918678526048286,
      "grad_norm": 1.7056330442428589,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 53260
    },
    {
      "epoch": 16.921855146124525,
      "grad_norm": 2.4212422370910645,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 53270
    },
    {
      "epoch": 16.925031766200764,
      "grad_norm": 2.511582136154175,
      "learning_rate": 2e-05,
      "loss": 0.0819,
      "step": 53280
    },
    {
      "epoch": 16.928208386277003,
      "grad_norm": 1.3673570156097412,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 53290
    },
    {
      "epoch": 16.93138500635324,
      "grad_norm": 7.6982598304748535,
      "learning_rate": 2e-05,
      "loss": 0.072,
      "step": 53300
    },
    {
      "epoch": 16.934561626429478,
      "grad_norm": 1.8216888904571533,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 53310
    },
    {
      "epoch": 16.937738246505717,
      "grad_norm": 2.288733720779419,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 53320
    },
    {
      "epoch": 16.940914866581956,
      "grad_norm": 1.9108527898788452,
      "learning_rate": 2e-05,
      "loss": 0.0736,
      "step": 53330
    },
    {
      "epoch": 16.944091486658195,
      "grad_norm": 2.484264850616455,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 53340
    },
    {
      "epoch": 16.947268106734434,
      "grad_norm": 1.704679012298584,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 53350
    },
    {
      "epoch": 16.950444726810673,
      "grad_norm": 3.214101791381836,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 53360
    },
    {
      "epoch": 16.953621346886912,
      "grad_norm": 3.0219638347625732,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 53370
    },
    {
      "epoch": 16.95679796696315,
      "grad_norm": 2.262495517730713,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 53380
    },
    {
      "epoch": 16.95997458703939,
      "grad_norm": 1.8630733489990234,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 53390
    },
    {
      "epoch": 16.96315120711563,
      "grad_norm": 1.3486953973770142,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 53400
    },
    {
      "epoch": 16.96632782719187,
      "grad_norm": 1.5437442064285278,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 53410
    },
    {
      "epoch": 16.969504447268108,
      "grad_norm": 2.1059582233428955,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 53420
    },
    {
      "epoch": 16.972681067344347,
      "grad_norm": 2.042691946029663,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 53430
    },
    {
      "epoch": 16.975857687420586,
      "grad_norm": 2.0168044567108154,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 53440
    },
    {
      "epoch": 16.979034307496825,
      "grad_norm": 1.4357571601867676,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 53450
    },
    {
      "epoch": 16.982210927573064,
      "grad_norm": 1.550581455230713,
      "learning_rate": 2e-05,
      "loss": 0.0689,
      "step": 53460
    },
    {
      "epoch": 16.9853875476493,
      "grad_norm": 1.4813419580459595,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 53470
    },
    {
      "epoch": 16.98856416772554,
      "grad_norm": 3.0118274688720703,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 53480
    },
    {
      "epoch": 16.991740787801778,
      "grad_norm": 2.5393857955932617,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 53490
    },
    {
      "epoch": 16.994917407878017,
      "grad_norm": 2.0415899753570557,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 53500
    },
    {
      "epoch": 16.998094027954256,
      "grad_norm": 3.267751455307007,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 53510
    },
    {
      "epoch": 17.001270648030495,
      "grad_norm": 2.057049512863159,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 53520
    },
    {
      "epoch": 17.004447268106734,
      "grad_norm": 1.859559416770935,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 53530
    },
    {
      "epoch": 17.007623888182973,
      "grad_norm": 2.0829429626464844,
      "learning_rate": 2e-05,
      "loss": 0.065,
      "step": 53540
    },
    {
      "epoch": 17.010800508259212,
      "grad_norm": 2.0801405906677246,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 53550
    },
    {
      "epoch": 17.010800508259212,
      "eval_loss": 1.8855640888214111,
      "eval_mse": 1.8842710335339818,
      "eval_pearson": 0.39362983800510043,
      "eval_runtime": 7.594,
      "eval_samples_per_second": 2839.073,
      "eval_spearmanr": 0.4044869330155194,
      "eval_steps_per_second": 11.193,
      "step": 53550
    },
    {
      "epoch": 17.01397712833545,
      "grad_norm": 1.5334012508392334,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 53560
    },
    {
      "epoch": 17.01715374841169,
      "grad_norm": 1.6630557775497437,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 53570
    },
    {
      "epoch": 17.02033036848793,
      "grad_norm": 1.3151540756225586,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 53580
    },
    {
      "epoch": 17.02350698856417,
      "grad_norm": 1.3591103553771973,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 53590
    },
    {
      "epoch": 17.026683608640408,
      "grad_norm": 1.516982078552246,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 53600
    },
    {
      "epoch": 17.029860228716647,
      "grad_norm": 2.0890843868255615,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 53610
    },
    {
      "epoch": 17.033036848792886,
      "grad_norm": 2.5296201705932617,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 53620
    },
    {
      "epoch": 17.036213468869125,
      "grad_norm": 1.9669548273086548,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 53630
    },
    {
      "epoch": 17.03939008894536,
      "grad_norm": 1.624497413635254,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 53640
    },
    {
      "epoch": 17.0425667090216,
      "grad_norm": 2.6731841564178467,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 53650
    },
    {
      "epoch": 17.04574332909784,
      "grad_norm": 1.974327564239502,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 53660
    },
    {
      "epoch": 17.048919949174078,
      "grad_norm": 2.440840005874634,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 53670
    },
    {
      "epoch": 17.052096569250317,
      "grad_norm": 1.2679779529571533,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 53680
    },
    {
      "epoch": 17.055273189326556,
      "grad_norm": 2.405693292617798,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 53690
    },
    {
      "epoch": 17.058449809402795,
      "grad_norm": 2.292365074157715,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 53700
    },
    {
      "epoch": 17.061626429479034,
      "grad_norm": 2.9742698669433594,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 53710
    },
    {
      "epoch": 17.064803049555273,
      "grad_norm": 2.9746296405792236,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 53720
    },
    {
      "epoch": 17.067979669631512,
      "grad_norm": 2.7003207206726074,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 53730
    },
    {
      "epoch": 17.07115628970775,
      "grad_norm": 1.4060702323913574,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 53740
    },
    {
      "epoch": 17.07433290978399,
      "grad_norm": 1.7217347621917725,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 53750
    },
    {
      "epoch": 17.07750952986023,
      "grad_norm": 2.681593418121338,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 53760
    },
    {
      "epoch": 17.08068614993647,
      "grad_norm": 1.852132797241211,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 53770
    },
    {
      "epoch": 17.083862770012708,
      "grad_norm": 1.556679368019104,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 53780
    },
    {
      "epoch": 17.087039390088947,
      "grad_norm": 1.9101747274398804,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 53790
    },
    {
      "epoch": 17.090216010165186,
      "grad_norm": 1.4407496452331543,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 53800
    },
    {
      "epoch": 17.09339263024142,
      "grad_norm": 1.817886471748352,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 53810
    },
    {
      "epoch": 17.09656925031766,
      "grad_norm": 7.621386528015137,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 53820
    },
    {
      "epoch": 17.0997458703939,
      "grad_norm": 1.5880608558654785,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 53830
    },
    {
      "epoch": 17.10292249047014,
      "grad_norm": 1.923018455505371,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 53840
    },
    {
      "epoch": 17.106099110546378,
      "grad_norm": 2.0255699157714844,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 53850
    },
    {
      "epoch": 17.109275730622617,
      "grad_norm": 2.737696647644043,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 53860
    },
    {
      "epoch": 17.110864040660736,
      "eval_loss": 1.9205548763275146,
      "eval_mse": 1.9192884618530468,
      "eval_pearson": 0.37353376901070373,
      "eval_runtime": 7.3652,
      "eval_samples_per_second": 2927.293,
      "eval_spearmanr": 0.38332751550259064,
      "eval_steps_per_second": 11.541,
      "step": 53865
    },
    {
      "epoch": 17.112452350698856,
      "grad_norm": 1.8917734622955322,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 53870
    },
    {
      "epoch": 17.115628970775095,
      "grad_norm": 1.5193346738815308,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 53880
    },
    {
      "epoch": 17.118805590851334,
      "grad_norm": 2.967996120452881,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 53890
    },
    {
      "epoch": 17.121982210927573,
      "grad_norm": 3.061474323272705,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 53900
    },
    {
      "epoch": 17.125158831003812,
      "grad_norm": 1.281898021697998,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 53910
    },
    {
      "epoch": 17.12833545108005,
      "grad_norm": 1.8462800979614258,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 53920
    },
    {
      "epoch": 17.13151207115629,
      "grad_norm": 1.433175802230835,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 53930
    },
    {
      "epoch": 17.13468869123253,
      "grad_norm": 2.182784080505371,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 53940
    },
    {
      "epoch": 17.13786531130877,
      "grad_norm": 3.832839012145996,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 53950
    },
    {
      "epoch": 17.141041931385008,
      "grad_norm": 1.5427982807159424,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 53960
    },
    {
      "epoch": 17.144218551461247,
      "grad_norm": 2.0376601219177246,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 53970
    },
    {
      "epoch": 17.147395171537482,
      "grad_norm": 1.6347953081130981,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 53980
    },
    {
      "epoch": 17.15057179161372,
      "grad_norm": 1.9385695457458496,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 53990
    },
    {
      "epoch": 17.15374841168996,
      "grad_norm": 2.14908766746521,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 54000
    },
    {
      "epoch": 17.1569250317662,
      "grad_norm": 1.516098141670227,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 54010
    },
    {
      "epoch": 17.16010165184244,
      "grad_norm": 1.4468770027160645,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 54020
    },
    {
      "epoch": 17.163278271918678,
      "grad_norm": 2.3658533096313477,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 54030
    },
    {
      "epoch": 17.166454891994917,
      "grad_norm": 2.429255247116089,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 54040
    },
    {
      "epoch": 17.169631512071156,
      "grad_norm": 1.7760926485061646,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 54050
    },
    {
      "epoch": 17.172808132147395,
      "grad_norm": 2.5280086994171143,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 54060
    },
    {
      "epoch": 17.175984752223634,
      "grad_norm": 1.2932180166244507,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 54070
    },
    {
      "epoch": 17.179161372299873,
      "grad_norm": 2.300139904022217,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 54080
    },
    {
      "epoch": 17.182337992376112,
      "grad_norm": 1.5914878845214844,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 54090
    },
    {
      "epoch": 17.18551461245235,
      "grad_norm": 2.5125131607055664,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 54100
    },
    {
      "epoch": 17.18869123252859,
      "grad_norm": 2.571650505065918,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 54110
    },
    {
      "epoch": 17.19186785260483,
      "grad_norm": 1.8245196342468262,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 54120
    },
    {
      "epoch": 17.19504447268107,
      "grad_norm": 3.6218488216400146,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 54130
    },
    {
      "epoch": 17.198221092757308,
      "grad_norm": 1.5120823383331299,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 54140
    },
    {
      "epoch": 17.201397712833543,
      "grad_norm": 2.514004707336426,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 54150
    },
    {
      "epoch": 17.204574332909782,
      "grad_norm": 2.119746446609497,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 54160
    },
    {
      "epoch": 17.20775095298602,
      "grad_norm": 3.439943552017212,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 54170
    },
    {
      "epoch": 17.21092757306226,
      "grad_norm": 3.003612995147705,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 54180
    },
    {
      "epoch": 17.21092757306226,
      "eval_loss": 1.898336410522461,
      "eval_mse": 1.8974615216642232,
      "eval_pearson": 0.3628559668307313,
      "eval_runtime": 7.4767,
      "eval_samples_per_second": 2883.616,
      "eval_spearmanr": 0.3735808689838763,
      "eval_steps_per_second": 11.369,
      "step": 54180
    },
    {
      "epoch": 17.2141041931385,
      "grad_norm": 1.5876191854476929,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 54190
    },
    {
      "epoch": 17.21728081321474,
      "grad_norm": 3.7215092182159424,
      "learning_rate": 2e-05,
      "loss": 0.0597,
      "step": 54200
    },
    {
      "epoch": 17.220457433290978,
      "grad_norm": 2.187502384185791,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 54210
    },
    {
      "epoch": 17.223634053367217,
      "grad_norm": 4.9192118644714355,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 54220
    },
    {
      "epoch": 17.226810673443456,
      "grad_norm": 3.264617443084717,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 54230
    },
    {
      "epoch": 17.229987293519695,
      "grad_norm": 1.8535652160644531,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 54240
    },
    {
      "epoch": 17.233163913595934,
      "grad_norm": 1.9569737911224365,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 54250
    },
    {
      "epoch": 17.236340533672173,
      "grad_norm": 7.09051513671875,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 54260
    },
    {
      "epoch": 17.239517153748412,
      "grad_norm": 2.4357450008392334,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 54270
    },
    {
      "epoch": 17.24269377382465,
      "grad_norm": 1.2852219343185425,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 54280
    },
    {
      "epoch": 17.24587039390089,
      "grad_norm": 8.432173728942871,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 54290
    },
    {
      "epoch": 17.24904701397713,
      "grad_norm": 1.186377763748169,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 54300
    },
    {
      "epoch": 17.25222363405337,
      "grad_norm": 1.5150424242019653,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 54310
    },
    {
      "epoch": 17.255400254129604,
      "grad_norm": 1.5885982513427734,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 54320
    },
    {
      "epoch": 17.258576874205843,
      "grad_norm": 1.7199909687042236,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 54330
    },
    {
      "epoch": 17.261753494282082,
      "grad_norm": 1.416420817375183,
      "learning_rate": 2e-05,
      "loss": 0.0644,
      "step": 54340
    },
    {
      "epoch": 17.26493011435832,
      "grad_norm": 2.055938482284546,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 54350
    },
    {
      "epoch": 17.26810673443456,
      "grad_norm": 2.2811813354492188,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 54360
    },
    {
      "epoch": 17.2712833545108,
      "grad_norm": 2.8253493309020996,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 54370
    },
    {
      "epoch": 17.27445997458704,
      "grad_norm": 1.3833088874816895,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 54380
    },
    {
      "epoch": 17.277636594663278,
      "grad_norm": 1.3672479391098022,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 54390
    },
    {
      "epoch": 17.280813214739517,
      "grad_norm": 1.8964353799819946,
      "learning_rate": 2e-05,
      "loss": 0.0681,
      "step": 54400
    },
    {
      "epoch": 17.283989834815756,
      "grad_norm": 2.8940112590789795,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 54410
    },
    {
      "epoch": 17.287166454891995,
      "grad_norm": 2.01230525970459,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 54420
    },
    {
      "epoch": 17.290343074968234,
      "grad_norm": 2.179115056991577,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 54430
    },
    {
      "epoch": 17.293519695044473,
      "grad_norm": 3.479308605194092,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 54440
    },
    {
      "epoch": 17.296696315120712,
      "grad_norm": 2.6378426551818848,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 54450
    },
    {
      "epoch": 17.29987293519695,
      "grad_norm": 1.5587412118911743,
      "learning_rate": 2e-05,
      "loss": 0.0688,
      "step": 54460
    },
    {
      "epoch": 17.30304955527319,
      "grad_norm": 1.0100356340408325,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 54470
    },
    {
      "epoch": 17.30622617534943,
      "grad_norm": 3.9745047092437744,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 54480
    },
    {
      "epoch": 17.30940279542567,
      "grad_norm": 1.6302673816680908,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 54490
    },
    {
      "epoch": 17.310991105463785,
      "eval_loss": 1.9004662036895752,
      "eval_mse": 1.8991715967351077,
      "eval_pearson": 0.3837506798315517,
      "eval_runtime": 7.2713,
      "eval_samples_per_second": 2965.063,
      "eval_spearmanr": 0.3936077745051004,
      "eval_steps_per_second": 11.69,
      "step": 54495
    },
    {
      "epoch": 17.312579415501904,
      "grad_norm": 3.509823799133301,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 54500
    },
    {
      "epoch": 17.315756035578143,
      "grad_norm": 2.0938243865966797,
      "learning_rate": 2e-05,
      "loss": 0.0646,
      "step": 54510
    },
    {
      "epoch": 17.318932655654383,
      "grad_norm": 1.8536714315414429,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 54520
    },
    {
      "epoch": 17.32210927573062,
      "grad_norm": 1.9915566444396973,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 54530
    },
    {
      "epoch": 17.32528589580686,
      "grad_norm": 3.8997511863708496,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 54540
    },
    {
      "epoch": 17.3284625158831,
      "grad_norm": 1.4104031324386597,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 54550
    },
    {
      "epoch": 17.33163913595934,
      "grad_norm": 3.3030807971954346,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 54560
    },
    {
      "epoch": 17.334815756035578,
      "grad_norm": 2.0232741832733154,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 54570
    },
    {
      "epoch": 17.337992376111817,
      "grad_norm": 2.920588254928589,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 54580
    },
    {
      "epoch": 17.341168996188056,
      "grad_norm": 2.903205394744873,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 54590
    },
    {
      "epoch": 17.344345616264295,
      "grad_norm": 1.6242034435272217,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 54600
    },
    {
      "epoch": 17.347522236340534,
      "grad_norm": 2.153003454208374,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 54610
    },
    {
      "epoch": 17.350698856416773,
      "grad_norm": 1.718385100364685,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 54620
    },
    {
      "epoch": 17.353875476493013,
      "grad_norm": 1.6246603727340698,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 54630
    },
    {
      "epoch": 17.35705209656925,
      "grad_norm": 2.5533030033111572,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 54640
    },
    {
      "epoch": 17.36022871664549,
      "grad_norm": 2.9052658081054688,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 54650
    },
    {
      "epoch": 17.36340533672173,
      "grad_norm": 1.8938634395599365,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 54660
    },
    {
      "epoch": 17.366581956797965,
      "grad_norm": 1.4871817827224731,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 54670
    },
    {
      "epoch": 17.369758576874204,
      "grad_norm": 1.9491946697235107,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 54680
    },
    {
      "epoch": 17.372935196950444,
      "grad_norm": 1.9031953811645508,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 54690
    },
    {
      "epoch": 17.376111817026683,
      "grad_norm": 1.5787948369979858,
      "learning_rate": 2e-05,
      "loss": 0.0666,
      "step": 54700
    },
    {
      "epoch": 17.37928843710292,
      "grad_norm": 3.5088424682617188,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 54710
    },
    {
      "epoch": 17.38246505717916,
      "grad_norm": 2.7505245208740234,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 54720
    },
    {
      "epoch": 17.3856416772554,
      "grad_norm": 1.7112013101577759,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 54730
    },
    {
      "epoch": 17.38881829733164,
      "grad_norm": 1.7033673524856567,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 54740
    },
    {
      "epoch": 17.391994917407878,
      "grad_norm": 1.8110647201538086,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 54750
    },
    {
      "epoch": 17.395171537484117,
      "grad_norm": 4.062097072601318,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 54760
    },
    {
      "epoch": 17.398348157560356,
      "grad_norm": 3.0055220127105713,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 54770
    },
    {
      "epoch": 17.401524777636595,
      "grad_norm": 1.996309518814087,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 54780
    },
    {
      "epoch": 17.404701397712834,
      "grad_norm": 3.1022706031799316,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 54790
    },
    {
      "epoch": 17.407878017789074,
      "grad_norm": 1.8498485088348389,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 54800
    },
    {
      "epoch": 17.411054637865313,
      "grad_norm": 3.8787901401519775,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 54810
    },
    {
      "epoch": 17.411054637865313,
      "eval_loss": 1.8240982294082642,
      "eval_mse": 1.8224716225657993,
      "eval_pearson": 0.3887194238844922,
      "eval_runtime": 7.4584,
      "eval_samples_per_second": 2890.697,
      "eval_spearmanr": 0.3993025668101365,
      "eval_steps_per_second": 11.397,
      "step": 54810
    },
    {
      "epoch": 17.41423125794155,
      "grad_norm": 1.5753226280212402,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 54820
    },
    {
      "epoch": 17.41740787801779,
      "grad_norm": 1.7511694431304932,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 54830
    },
    {
      "epoch": 17.420584498094026,
      "grad_norm": 1.7498728036880493,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 54840
    },
    {
      "epoch": 17.423761118170265,
      "grad_norm": 6.085260391235352,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 54850
    },
    {
      "epoch": 17.426937738246504,
      "grad_norm": 1.9805861711502075,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 54860
    },
    {
      "epoch": 17.430114358322744,
      "grad_norm": 2.0319881439208984,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 54870
    },
    {
      "epoch": 17.433290978398983,
      "grad_norm": 3.1413280963897705,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 54880
    },
    {
      "epoch": 17.43646759847522,
      "grad_norm": 2.53275728225708,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 54890
    },
    {
      "epoch": 17.43964421855146,
      "grad_norm": 1.263790488243103,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 54900
    },
    {
      "epoch": 17.4428208386277,
      "grad_norm": 2.3577423095703125,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 54910
    },
    {
      "epoch": 17.44599745870394,
      "grad_norm": 2.485914945602417,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 54920
    },
    {
      "epoch": 17.449174078780178,
      "grad_norm": 1.5967144966125488,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 54930
    },
    {
      "epoch": 17.452350698856417,
      "grad_norm": 1.544209599494934,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 54940
    },
    {
      "epoch": 17.455527318932656,
      "grad_norm": 2.8729937076568604,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 54950
    },
    {
      "epoch": 17.458703939008895,
      "grad_norm": 4.522315502166748,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 54960
    },
    {
      "epoch": 17.461880559085134,
      "grad_norm": 2.181323528289795,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 54970
    },
    {
      "epoch": 17.465057179161374,
      "grad_norm": 3.806108236312866,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 54980
    },
    {
      "epoch": 17.468233799237613,
      "grad_norm": 1.8456201553344727,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 54990
    },
    {
      "epoch": 17.47141041931385,
      "grad_norm": 2.153681755065918,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 55000
    },
    {
      "epoch": 17.474587039390087,
      "grad_norm": 1.247682809829712,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 55010
    },
    {
      "epoch": 17.477763659466326,
      "grad_norm": 2.293614625930786,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 55020
    },
    {
      "epoch": 17.480940279542565,
      "grad_norm": 1.868579387664795,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 55030
    },
    {
      "epoch": 17.484116899618805,
      "grad_norm": 2.460963726043701,
      "learning_rate": 2e-05,
      "loss": 0.0653,
      "step": 55040
    },
    {
      "epoch": 17.487293519695044,
      "grad_norm": 1.4779648780822754,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 55050
    },
    {
      "epoch": 17.490470139771283,
      "grad_norm": 1.9833242893218994,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 55060
    },
    {
      "epoch": 17.493646759847522,
      "grad_norm": 1.7812942266464233,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 55070
    },
    {
      "epoch": 17.49682337992376,
      "grad_norm": 1.174674153327942,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 55080
    },
    {
      "epoch": 17.5,
      "grad_norm": 2.0956835746765137,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 55090
    },
    {
      "epoch": 17.50317662007624,
      "grad_norm": 1.3359419107437134,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 55100
    },
    {
      "epoch": 17.506353240152478,
      "grad_norm": 3.941516399383545,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 55110
    },
    {
      "epoch": 17.509529860228717,
      "grad_norm": 3.859321117401123,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 55120
    },
    {
      "epoch": 17.511118170266837,
      "eval_loss": 1.8260384798049927,
      "eval_mse": 1.8246323720311852,
      "eval_pearson": 0.39196349355391835,
      "eval_runtime": 7.3697,
      "eval_samples_per_second": 2925.476,
      "eval_spearmanr": 0.40175087738572995,
      "eval_steps_per_second": 11.534,
      "step": 55125
    },
    {
      "epoch": 17.512706480304956,
      "grad_norm": 2.65545392036438,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 55130
    },
    {
      "epoch": 17.515883100381195,
      "grad_norm": 2.0036802291870117,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 55140
    },
    {
      "epoch": 17.519059720457435,
      "grad_norm": 1.3875598907470703,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 55150
    },
    {
      "epoch": 17.522236340533674,
      "grad_norm": 2.8114421367645264,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 55160
    },
    {
      "epoch": 17.525412960609913,
      "grad_norm": 1.253213882446289,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 55170
    },
    {
      "epoch": 17.52858958068615,
      "grad_norm": 1.8590034246444702,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 55180
    },
    {
      "epoch": 17.531766200762387,
      "grad_norm": 1.5364993810653687,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 55190
    },
    {
      "epoch": 17.534942820838626,
      "grad_norm": 15.687026023864746,
      "learning_rate": 2e-05,
      "loss": 0.0715,
      "step": 55200
    },
    {
      "epoch": 17.538119440914866,
      "grad_norm": 1.2279218435287476,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 55210
    },
    {
      "epoch": 17.541296060991105,
      "grad_norm": 2.164961338043213,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 55220
    },
    {
      "epoch": 17.544472681067344,
      "grad_norm": 1.5264273881912231,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 55230
    },
    {
      "epoch": 17.547649301143583,
      "grad_norm": 1.307169795036316,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 55240
    },
    {
      "epoch": 17.550825921219822,
      "grad_norm": 2.056619167327881,
      "learning_rate": 2e-05,
      "loss": 0.0676,
      "step": 55250
    },
    {
      "epoch": 17.55400254129606,
      "grad_norm": 1.532488226890564,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 55260
    },
    {
      "epoch": 17.5571791613723,
      "grad_norm": 2.2891767024993896,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 55270
    },
    {
      "epoch": 17.56035578144854,
      "grad_norm": 2.344452381134033,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 55280
    },
    {
      "epoch": 17.56353240152478,
      "grad_norm": 3.1465952396392822,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 55290
    },
    {
      "epoch": 17.566709021601017,
      "grad_norm": 4.082746505737305,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 55300
    },
    {
      "epoch": 17.569885641677256,
      "grad_norm": 1.0895999670028687,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 55310
    },
    {
      "epoch": 17.573062261753496,
      "grad_norm": 2.8215689659118652,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 55320
    },
    {
      "epoch": 17.576238881829735,
      "grad_norm": 2.3419852256774902,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 55330
    },
    {
      "epoch": 17.579415501905974,
      "grad_norm": 2.5879836082458496,
      "learning_rate": 2e-05,
      "loss": 0.0677,
      "step": 55340
    },
    {
      "epoch": 17.58259212198221,
      "grad_norm": 1.4235808849334717,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 55350
    },
    {
      "epoch": 17.58576874205845,
      "grad_norm": 1.578283429145813,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 55360
    },
    {
      "epoch": 17.588945362134687,
      "grad_norm": 2.3410141468048096,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 55370
    },
    {
      "epoch": 17.592121982210926,
      "grad_norm": 1.6418137550354004,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 55380
    },
    {
      "epoch": 17.595298602287166,
      "grad_norm": 1.315388798713684,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 55390
    },
    {
      "epoch": 17.598475222363405,
      "grad_norm": 1.8626970052719116,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 55400
    },
    {
      "epoch": 17.601651842439644,
      "grad_norm": 1.5979012250900269,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 55410
    },
    {
      "epoch": 17.604828462515883,
      "grad_norm": 1.4699783325195312,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 55420
    },
    {
      "epoch": 17.608005082592122,
      "grad_norm": 1.8426636457443237,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 55430
    },
    {
      "epoch": 17.61118170266836,
      "grad_norm": 1.971346378326416,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 55440
    },
    {
      "epoch": 17.61118170266836,
      "eval_loss": 1.7767951488494873,
      "eval_mse": 1.7754388708821696,
      "eval_pearson": 0.39011293921104334,
      "eval_runtime": 7.4929,
      "eval_samples_per_second": 2877.404,
      "eval_spearmanr": 0.39780722750523123,
      "eval_steps_per_second": 11.344,
      "step": 55440
    },
    {
      "epoch": 17.6143583227446,
      "grad_norm": 2.926943778991699,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 55450
    },
    {
      "epoch": 17.61753494282084,
      "grad_norm": 3.328191041946411,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 55460
    },
    {
      "epoch": 17.62071156289708,
      "grad_norm": 1.7089303731918335,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 55470
    },
    {
      "epoch": 17.623888182973317,
      "grad_norm": 1.885577917098999,
      "learning_rate": 2e-05,
      "loss": 0.0658,
      "step": 55480
    },
    {
      "epoch": 17.627064803049556,
      "grad_norm": 2.715104818344116,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 55490
    },
    {
      "epoch": 17.630241423125796,
      "grad_norm": 1.2716692686080933,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 55500
    },
    {
      "epoch": 17.633418043202035,
      "grad_norm": 3.0552854537963867,
      "learning_rate": 2e-05,
      "loss": 0.0688,
      "step": 55510
    },
    {
      "epoch": 17.636594663278274,
      "grad_norm": 2.2645270824432373,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 55520
    },
    {
      "epoch": 17.63977128335451,
      "grad_norm": 2.407086133956909,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 55530
    },
    {
      "epoch": 17.64294790343075,
      "grad_norm": 2.3340811729431152,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 55540
    },
    {
      "epoch": 17.646124523506987,
      "grad_norm": 1.7915143966674805,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 55550
    },
    {
      "epoch": 17.649301143583227,
      "grad_norm": 1.6806867122650146,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 55560
    },
    {
      "epoch": 17.652477763659466,
      "grad_norm": 1.4365832805633545,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 55570
    },
    {
      "epoch": 17.655654383735705,
      "grad_norm": 1.7424441576004028,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 55580
    },
    {
      "epoch": 17.658831003811944,
      "grad_norm": 3.3526577949523926,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 55590
    },
    {
      "epoch": 17.662007623888183,
      "grad_norm": 1.294777274131775,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 55600
    },
    {
      "epoch": 17.665184243964422,
      "grad_norm": 3.310030937194824,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 55610
    },
    {
      "epoch": 17.66836086404066,
      "grad_norm": 1.5905146598815918,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 55620
    },
    {
      "epoch": 17.6715374841169,
      "grad_norm": 2.2305679321289062,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 55630
    },
    {
      "epoch": 17.67471410419314,
      "grad_norm": 5.192763328552246,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 55640
    },
    {
      "epoch": 17.67789072426938,
      "grad_norm": 1.877165675163269,
      "learning_rate": 2e-05,
      "loss": 0.0693,
      "step": 55650
    },
    {
      "epoch": 17.681067344345617,
      "grad_norm": 2.343323230743408,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 55660
    },
    {
      "epoch": 17.684243964421857,
      "grad_norm": 2.0306925773620605,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 55670
    },
    {
      "epoch": 17.687420584498096,
      "grad_norm": 2.685722827911377,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 55680
    },
    {
      "epoch": 17.69059720457433,
      "grad_norm": 2.273348569869995,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 55690
    },
    {
      "epoch": 17.69377382465057,
      "grad_norm": 1.2161614894866943,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 55700
    },
    {
      "epoch": 17.69695044472681,
      "grad_norm": 1.9212008714675903,
      "learning_rate": 2e-05,
      "loss": 0.0743,
      "step": 55710
    },
    {
      "epoch": 17.70012706480305,
      "grad_norm": 1.545563817024231,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 55720
    },
    {
      "epoch": 17.703303684879288,
      "grad_norm": 2.0244076251983643,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 55730
    },
    {
      "epoch": 17.706480304955527,
      "grad_norm": 1.6954954862594604,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 55740
    },
    {
      "epoch": 17.709656925031766,
      "grad_norm": 2.1658835411071777,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 55750
    },
    {
      "epoch": 17.711245235069885,
      "eval_loss": 1.791031002998352,
      "eval_mse": 1.7895295332585062,
      "eval_pearson": 0.3983231844763023,
      "eval_runtime": 7.3673,
      "eval_samples_per_second": 2926.463,
      "eval_spearmanr": 0.4057392076582953,
      "eval_steps_per_second": 11.538,
      "step": 55755
    },
    {
      "epoch": 17.712833545108005,
      "grad_norm": 1.0259279012680054,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 55760
    },
    {
      "epoch": 17.716010165184244,
      "grad_norm": 1.876234769821167,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 55770
    },
    {
      "epoch": 17.719186785260483,
      "grad_norm": 1.488798975944519,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 55780
    },
    {
      "epoch": 17.722363405336722,
      "grad_norm": 1.981584072113037,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 55790
    },
    {
      "epoch": 17.72554002541296,
      "grad_norm": 1.3839876651763916,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 55800
    },
    {
      "epoch": 17.7287166454892,
      "grad_norm": 2.4958178997039795,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 55810
    },
    {
      "epoch": 17.73189326556544,
      "grad_norm": 1.0965417623519897,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 55820
    },
    {
      "epoch": 17.73506988564168,
      "grad_norm": 2.6535239219665527,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 55830
    },
    {
      "epoch": 17.738246505717918,
      "grad_norm": 1.8033454418182373,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 55840
    },
    {
      "epoch": 17.741423125794157,
      "grad_norm": 1.8723974227905273,
      "learning_rate": 2e-05,
      "loss": 0.0583,
      "step": 55850
    },
    {
      "epoch": 17.744599745870396,
      "grad_norm": 2.791365385055542,
      "learning_rate": 2e-05,
      "loss": 0.069,
      "step": 55860
    },
    {
      "epoch": 17.74777636594663,
      "grad_norm": 1.412739634513855,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 55870
    },
    {
      "epoch": 17.75095298602287,
      "grad_norm": 2.989232301712036,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 55880
    },
    {
      "epoch": 17.75412960609911,
      "grad_norm": 7.0800089836120605,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 55890
    },
    {
      "epoch": 17.75730622617535,
      "grad_norm": 1.5700832605361938,
      "learning_rate": 2e-05,
      "loss": 0.0663,
      "step": 55900
    },
    {
      "epoch": 17.760482846251588,
      "grad_norm": 2.6484391689300537,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 55910
    },
    {
      "epoch": 17.763659466327827,
      "grad_norm": 1.2846012115478516,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 55920
    },
    {
      "epoch": 17.766836086404066,
      "grad_norm": 3.071434259414673,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 55930
    },
    {
      "epoch": 17.770012706480305,
      "grad_norm": 3.5490942001342773,
      "learning_rate": 2e-05,
      "loss": 0.0717,
      "step": 55940
    },
    {
      "epoch": 17.773189326556544,
      "grad_norm": 3.8855464458465576,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 55950
    },
    {
      "epoch": 17.776365946632783,
      "grad_norm": 1.7900224924087524,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 55960
    },
    {
      "epoch": 17.779542566709022,
      "grad_norm": 1.1448441743850708,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 55970
    },
    {
      "epoch": 17.78271918678526,
      "grad_norm": 2.576878786087036,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 55980
    },
    {
      "epoch": 17.7858958068615,
      "grad_norm": 2.59635591506958,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 55990
    },
    {
      "epoch": 17.78907242693774,
      "grad_norm": 1.9372503757476807,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 56000
    },
    {
      "epoch": 17.79224904701398,
      "grad_norm": 1.5703258514404297,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 56010
    },
    {
      "epoch": 17.795425667090218,
      "grad_norm": 1.7429553270339966,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 56020
    },
    {
      "epoch": 17.798602287166453,
      "grad_norm": 1.4649505615234375,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 56030
    },
    {
      "epoch": 17.801778907242692,
      "grad_norm": 1.4939967393875122,
      "learning_rate": 2e-05,
      "loss": 0.055,
      "step": 56040
    },
    {
      "epoch": 17.80495552731893,
      "grad_norm": 2.4446239471435547,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 56050
    },
    {
      "epoch": 17.80813214739517,
      "grad_norm": 2.380671977996826,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 56060
    },
    {
      "epoch": 17.81130876747141,
      "grad_norm": 2.150040864944458,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 56070
    },
    {
      "epoch": 17.81130876747141,
      "eval_loss": 1.8889758586883545,
      "eval_mse": 1.888229363570629,
      "eval_pearson": 0.37695248738054943,
      "eval_runtime": 7.4815,
      "eval_samples_per_second": 2881.774,
      "eval_spearmanr": 0.3859695286341421,
      "eval_steps_per_second": 11.361,
      "step": 56070
    },
    {
      "epoch": 17.81448538754765,
      "grad_norm": 2.039839267730713,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 56080
    },
    {
      "epoch": 17.817662007623888,
      "grad_norm": 2.546102523803711,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 56090
    },
    {
      "epoch": 17.820838627700127,
      "grad_norm": 2.235891103744507,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 56100
    },
    {
      "epoch": 17.824015247776366,
      "grad_norm": 1.8629813194274902,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 56110
    },
    {
      "epoch": 17.827191867852605,
      "grad_norm": 1.311354398727417,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 56120
    },
    {
      "epoch": 17.830368487928844,
      "grad_norm": 1.9978224039077759,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 56130
    },
    {
      "epoch": 17.833545108005083,
      "grad_norm": 1.6358290910720825,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 56140
    },
    {
      "epoch": 17.836721728081322,
      "grad_norm": 1.8052163124084473,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 56150
    },
    {
      "epoch": 17.83989834815756,
      "grad_norm": 2.1027591228485107,
      "learning_rate": 2e-05,
      "loss": 0.0705,
      "step": 56160
    },
    {
      "epoch": 17.8430749682338,
      "grad_norm": 29.312049865722656,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 56170
    },
    {
      "epoch": 17.84625158831004,
      "grad_norm": 1.2130520343780518,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 56180
    },
    {
      "epoch": 17.84942820838628,
      "grad_norm": 1.629509449005127,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 56190
    },
    {
      "epoch": 17.852604828462518,
      "grad_norm": 1.7942067384719849,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 56200
    },
    {
      "epoch": 17.855781448538753,
      "grad_norm": 3.0628316402435303,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 56210
    },
    {
      "epoch": 17.858958068614992,
      "grad_norm": 2.378340721130371,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 56220
    },
    {
      "epoch": 17.86213468869123,
      "grad_norm": 13.07367992401123,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 56230
    },
    {
      "epoch": 17.86531130876747,
      "grad_norm": 1.7953327894210815,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 56240
    },
    {
      "epoch": 17.86848792884371,
      "grad_norm": 1.7414230108261108,
      "learning_rate": 2e-05,
      "loss": 0.0711,
      "step": 56250
    },
    {
      "epoch": 17.87166454891995,
      "grad_norm": 2.0717451572418213,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 56260
    },
    {
      "epoch": 17.874841168996188,
      "grad_norm": 2.60417103767395,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 56270
    },
    {
      "epoch": 17.878017789072427,
      "grad_norm": 3.4426918029785156,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 56280
    },
    {
      "epoch": 17.881194409148666,
      "grad_norm": 3.5574111938476562,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 56290
    },
    {
      "epoch": 17.884371029224905,
      "grad_norm": 1.932476282119751,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 56300
    },
    {
      "epoch": 17.887547649301144,
      "grad_norm": 2.6882998943328857,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 56310
    },
    {
      "epoch": 17.890724269377383,
      "grad_norm": 1.6726844310760498,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 56320
    },
    {
      "epoch": 17.893900889453622,
      "grad_norm": 1.7027416229248047,
      "learning_rate": 2e-05,
      "loss": 0.0678,
      "step": 56330
    },
    {
      "epoch": 17.89707750952986,
      "grad_norm": 3.862020492553711,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 56340
    },
    {
      "epoch": 17.9002541296061,
      "grad_norm": 3.195047616958618,
      "learning_rate": 2e-05,
      "loss": 0.0685,
      "step": 56350
    },
    {
      "epoch": 17.90343074968234,
      "grad_norm": 2.0981874465942383,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 56360
    },
    {
      "epoch": 17.90660736975858,
      "grad_norm": 2.7459163665771484,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 56370
    },
    {
      "epoch": 17.909783989834814,
      "grad_norm": 2.368818521499634,
      "learning_rate": 2e-05,
      "loss": 0.0732,
      "step": 56380
    },
    {
      "epoch": 17.911372299872934,
      "eval_loss": 1.8097655773162842,
      "eval_mse": 1.808388255810251,
      "eval_pearson": 0.3894769560190364,
      "eval_runtime": 7.2987,
      "eval_samples_per_second": 2953.963,
      "eval_spearmanr": 0.39946243190680886,
      "eval_steps_per_second": 11.646,
      "step": 56385
    },
    {
      "epoch": 17.912960609911053,
      "grad_norm": 2.3317418098449707,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 56390
    },
    {
      "epoch": 17.916137229987292,
      "grad_norm": 1.8861948251724243,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 56400
    },
    {
      "epoch": 17.91931385006353,
      "grad_norm": 1.9954893589019775,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 56410
    },
    {
      "epoch": 17.92249047013977,
      "grad_norm": 1.9737979173660278,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 56420
    },
    {
      "epoch": 17.92566709021601,
      "grad_norm": 1.6048938035964966,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 56430
    },
    {
      "epoch": 17.92884371029225,
      "grad_norm": 1.9474979639053345,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 56440
    },
    {
      "epoch": 17.932020330368488,
      "grad_norm": 3.256200075149536,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 56450
    },
    {
      "epoch": 17.935196950444727,
      "grad_norm": 3.5346736907958984,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 56460
    },
    {
      "epoch": 17.938373570520966,
      "grad_norm": 1.543899655342102,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 56470
    },
    {
      "epoch": 17.941550190597205,
      "grad_norm": 2.2153093814849854,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 56480
    },
    {
      "epoch": 17.944726810673444,
      "grad_norm": 2.315558671951294,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 56490
    },
    {
      "epoch": 17.947903430749683,
      "grad_norm": 2.427828073501587,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 56500
    },
    {
      "epoch": 17.951080050825922,
      "grad_norm": 2.055826425552368,
      "learning_rate": 2e-05,
      "loss": 0.0725,
      "step": 56510
    },
    {
      "epoch": 17.95425667090216,
      "grad_norm": 2.046140193939209,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 56520
    },
    {
      "epoch": 17.9574332909784,
      "grad_norm": 1.2151457071304321,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 56530
    },
    {
      "epoch": 17.96060991105464,
      "grad_norm": 1.676470398902893,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 56540
    },
    {
      "epoch": 17.963786531130875,
      "grad_norm": 1.5605038404464722,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 56550
    },
    {
      "epoch": 17.966963151207114,
      "grad_norm": 1.614151120185852,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 56560
    },
    {
      "epoch": 17.970139771283353,
      "grad_norm": 1.389812707901001,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 56570
    },
    {
      "epoch": 17.973316391359592,
      "grad_norm": 14.079116821289062,
      "learning_rate": 2e-05,
      "loss": 0.0662,
      "step": 56580
    },
    {
      "epoch": 17.97649301143583,
      "grad_norm": 2.0485100746154785,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 56590
    },
    {
      "epoch": 17.97966963151207,
      "grad_norm": 2.6640400886535645,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 56600
    },
    {
      "epoch": 17.98284625158831,
      "grad_norm": 3.128378391265869,
      "learning_rate": 2e-05,
      "loss": 0.0591,
      "step": 56610
    },
    {
      "epoch": 17.98602287166455,
      "grad_norm": 1.7217755317687988,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 56620
    },
    {
      "epoch": 17.989199491740788,
      "grad_norm": 2.60129714012146,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 56630
    },
    {
      "epoch": 17.992376111817027,
      "grad_norm": 1.137926459312439,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 56640
    },
    {
      "epoch": 17.995552731893266,
      "grad_norm": 1.8891355991363525,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 56650
    },
    {
      "epoch": 17.998729351969505,
      "grad_norm": 2.3987419605255127,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 56660
    },
    {
      "epoch": 18.001905972045744,
      "grad_norm": 2.8475255966186523,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 56670
    },
    {
      "epoch": 18.005082592121983,
      "grad_norm": 1.019661784172058,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 56680
    },
    {
      "epoch": 18.008259212198222,
      "grad_norm": 3.041919231414795,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 56690
    },
    {
      "epoch": 18.01143583227446,
      "grad_norm": 1.8037142753601074,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 56700
    },
    {
      "epoch": 18.01143583227446,
      "eval_loss": 1.8320657014846802,
      "eval_mse": 1.8316204860944696,
      "eval_pearson": 0.407291792401724,
      "eval_runtime": 7.4023,
      "eval_samples_per_second": 2912.59,
      "eval_spearmanr": 0.4172313493574215,
      "eval_steps_per_second": 11.483,
      "step": 56700
    },
    {
      "epoch": 18.0146124523507,
      "grad_norm": 2.9113683700561523,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 56710
    },
    {
      "epoch": 18.017789072426936,
      "grad_norm": 3.3398094177246094,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 56720
    },
    {
      "epoch": 18.020965692503175,
      "grad_norm": 2.276975631713867,
      "learning_rate": 2e-05,
      "loss": 0.0687,
      "step": 56730
    },
    {
      "epoch": 18.024142312579414,
      "grad_norm": 3.5310287475585938,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 56740
    },
    {
      "epoch": 18.027318932655653,
      "grad_norm": 1.553519606590271,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 56750
    },
    {
      "epoch": 18.030495552731892,
      "grad_norm": 1.4173983335494995,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 56760
    },
    {
      "epoch": 18.03367217280813,
      "grad_norm": 2.631094217300415,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 56770
    },
    {
      "epoch": 18.03684879288437,
      "grad_norm": 2.6964359283447266,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 56780
    },
    {
      "epoch": 18.04002541296061,
      "grad_norm": 1.2998707294464111,
      "learning_rate": 2e-05,
      "loss": 0.0524,
      "step": 56790
    },
    {
      "epoch": 18.04320203303685,
      "grad_norm": 2.59476375579834,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 56800
    },
    {
      "epoch": 18.046378653113088,
      "grad_norm": 3.890787363052368,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 56810
    },
    {
      "epoch": 18.049555273189327,
      "grad_norm": 5.735112190246582,
      "learning_rate": 2e-05,
      "loss": 0.0507,
      "step": 56820
    },
    {
      "epoch": 18.052731893265566,
      "grad_norm": 2.410883903503418,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 56830
    },
    {
      "epoch": 18.055908513341805,
      "grad_norm": 1.6553692817687988,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 56840
    },
    {
      "epoch": 18.059085133418044,
      "grad_norm": 2.112942934036255,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 56850
    },
    {
      "epoch": 18.062261753494283,
      "grad_norm": 3.887836217880249,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 56860
    },
    {
      "epoch": 18.065438373570522,
      "grad_norm": 1.6950377225875854,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 56870
    },
    {
      "epoch": 18.06861499364676,
      "grad_norm": 1.401845097541809,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 56880
    },
    {
      "epoch": 18.071791613722997,
      "grad_norm": 1.443448781967163,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 56890
    },
    {
      "epoch": 18.074968233799236,
      "grad_norm": 1.807030439376831,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 56900
    },
    {
      "epoch": 18.078144853875475,
      "grad_norm": 1.686302900314331,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 56910
    },
    {
      "epoch": 18.081321473951714,
      "grad_norm": 2.3189032077789307,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 56920
    },
    {
      "epoch": 18.084498094027953,
      "grad_norm": 2.1532793045043945,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 56930
    },
    {
      "epoch": 18.087674714104192,
      "grad_norm": 1.7421832084655762,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 56940
    },
    {
      "epoch": 18.09085133418043,
      "grad_norm": 1.7944294214248657,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 56950
    },
    {
      "epoch": 18.09402795425667,
      "grad_norm": 7.955099582672119,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 56960
    },
    {
      "epoch": 18.09720457433291,
      "grad_norm": 1.645302653312683,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 56970
    },
    {
      "epoch": 18.10038119440915,
      "grad_norm": 2.9132449626922607,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 56980
    },
    {
      "epoch": 18.103557814485388,
      "grad_norm": 1.6923377513885498,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 56990
    },
    {
      "epoch": 18.106734434561627,
      "grad_norm": 3.172888994216919,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 57000
    },
    {
      "epoch": 18.109911054637866,
      "grad_norm": 1.477475643157959,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 57010
    },
    {
      "epoch": 18.111499364675986,
      "eval_loss": 1.7826907634735107,
      "eval_mse": 1.7811861361840011,
      "eval_pearson": 0.40716041511338225,
      "eval_runtime": 7.5823,
      "eval_samples_per_second": 2843.473,
      "eval_spearmanr": 0.41283816993555594,
      "eval_steps_per_second": 11.21,
      "step": 57015
    },
    {
      "epoch": 18.113087674714105,
      "grad_norm": 24.274951934814453,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 57020
    },
    {
      "epoch": 18.116264294790344,
      "grad_norm": 2.2758078575134277,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 57030
    },
    {
      "epoch": 18.119440914866583,
      "grad_norm": 1.837814211845398,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 57040
    },
    {
      "epoch": 18.122617534942822,
      "grad_norm": 2.076425790786743,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 57050
    },
    {
      "epoch": 18.125794155019058,
      "grad_norm": 1.6191718578338623,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 57060
    },
    {
      "epoch": 18.128970775095297,
      "grad_norm": 23.191055297851562,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 57070
    },
    {
      "epoch": 18.132147395171536,
      "grad_norm": 1.3147003650665283,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 57080
    },
    {
      "epoch": 18.135324015247775,
      "grad_norm": 1.465198040008545,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 57090
    },
    {
      "epoch": 18.138500635324014,
      "grad_norm": 1.468334674835205,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 57100
    },
    {
      "epoch": 18.141677255400253,
      "grad_norm": 1.4165047407150269,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 57110
    },
    {
      "epoch": 18.144853875476493,
      "grad_norm": 12.677523612976074,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 57120
    },
    {
      "epoch": 18.14803049555273,
      "grad_norm": 1.6044437885284424,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 57130
    },
    {
      "epoch": 18.15120711562897,
      "grad_norm": 2.5614614486694336,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 57140
    },
    {
      "epoch": 18.15438373570521,
      "grad_norm": 1.8981200456619263,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 57150
    },
    {
      "epoch": 18.15756035578145,
      "grad_norm": 1.6091225147247314,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 57160
    },
    {
      "epoch": 18.160736975857688,
      "grad_norm": 1.9859936237335205,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 57170
    },
    {
      "epoch": 18.163913595933927,
      "grad_norm": 3.8571698665618896,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 57180
    },
    {
      "epoch": 18.167090216010166,
      "grad_norm": 2.2378013134002686,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 57190
    },
    {
      "epoch": 18.170266836086405,
      "grad_norm": 3.107513904571533,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 57200
    },
    {
      "epoch": 18.173443456162644,
      "grad_norm": 1.566863775253296,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 57210
    },
    {
      "epoch": 18.176620076238883,
      "grad_norm": 1.3329819440841675,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 57220
    },
    {
      "epoch": 18.17979669631512,
      "grad_norm": 1.7231894731521606,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 57230
    },
    {
      "epoch": 18.182973316391358,
      "grad_norm": 2.0448904037475586,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 57240
    },
    {
      "epoch": 18.186149936467597,
      "grad_norm": 1.4501582384109497,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 57250
    },
    {
      "epoch": 18.189326556543836,
      "grad_norm": 2.5181522369384766,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 57260
    },
    {
      "epoch": 18.192503176620075,
      "grad_norm": 1.0876049995422363,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 57270
    },
    {
      "epoch": 18.195679796696314,
      "grad_norm": 2.14652156829834,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 57280
    },
    {
      "epoch": 18.198856416772554,
      "grad_norm": 1.5255422592163086,
      "learning_rate": 2e-05,
      "loss": 0.054,
      "step": 57290
    },
    {
      "epoch": 18.202033036848793,
      "grad_norm": 1.1434460878372192,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 57300
    },
    {
      "epoch": 18.20520965692503,
      "grad_norm": 1.5535180568695068,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 57310
    },
    {
      "epoch": 18.20838627700127,
      "grad_norm": 1.6696300506591797,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 57320
    },
    {
      "epoch": 18.21156289707751,
      "grad_norm": 1.5316966772079468,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 57330
    },
    {
      "epoch": 18.21156289707751,
      "eval_loss": 1.9388039112091064,
      "eval_mse": 1.9377573795132823,
      "eval_pearson": 0.36835187423851423,
      "eval_runtime": 7.2902,
      "eval_samples_per_second": 2957.387,
      "eval_spearmanr": 0.3768806859635106,
      "eval_steps_per_second": 11.659,
      "step": 57330
    },
    {
      "epoch": 18.21473951715375,
      "grad_norm": 1.5225416421890259,
      "learning_rate": 2e-05,
      "loss": 0.0517,
      "step": 57340
    },
    {
      "epoch": 18.217916137229988,
      "grad_norm": 2.4572651386260986,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 57350
    },
    {
      "epoch": 18.221092757306227,
      "grad_norm": 10.019461631774902,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 57360
    },
    {
      "epoch": 18.224269377382466,
      "grad_norm": 1.986750841140747,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 57370
    },
    {
      "epoch": 18.227445997458705,
      "grad_norm": 1.986760139465332,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 57380
    },
    {
      "epoch": 18.230622617534944,
      "grad_norm": 1.5243638753890991,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 57390
    },
    {
      "epoch": 18.23379923761118,
      "grad_norm": 1.625236988067627,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 57400
    },
    {
      "epoch": 18.23697585768742,
      "grad_norm": 1.61202871799469,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 57410
    },
    {
      "epoch": 18.240152477763658,
      "grad_norm": 1.807173490524292,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 57420
    },
    {
      "epoch": 18.243329097839897,
      "grad_norm": 3.2762959003448486,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 57430
    },
    {
      "epoch": 18.246505717916136,
      "grad_norm": 2.229722499847412,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 57440
    },
    {
      "epoch": 18.249682337992375,
      "grad_norm": 3.019717216491699,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 57450
    },
    {
      "epoch": 18.252858958068614,
      "grad_norm": 1.506746530532837,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 57460
    },
    {
      "epoch": 18.256035578144854,
      "grad_norm": 1.153099536895752,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 57470
    },
    {
      "epoch": 18.259212198221093,
      "grad_norm": 1.5451469421386719,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 57480
    },
    {
      "epoch": 18.26238881829733,
      "grad_norm": 2.181509256362915,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 57490
    },
    {
      "epoch": 18.26556543837357,
      "grad_norm": 1.9414292573928833,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 57500
    },
    {
      "epoch": 18.26874205844981,
      "grad_norm": 2.66312837600708,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 57510
    },
    {
      "epoch": 18.27191867852605,
      "grad_norm": 1.9340804815292358,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 57520
    },
    {
      "epoch": 18.275095298602288,
      "grad_norm": 3.0919806957244873,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 57530
    },
    {
      "epoch": 18.278271918678527,
      "grad_norm": 2.912060499191284,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 57540
    },
    {
      "epoch": 18.281448538754766,
      "grad_norm": 3.6543595790863037,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 57550
    },
    {
      "epoch": 18.284625158831005,
      "grad_norm": 1.8294076919555664,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 57560
    },
    {
      "epoch": 18.287801778907244,
      "grad_norm": 4.057737827301025,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 57570
    },
    {
      "epoch": 18.29097839898348,
      "grad_norm": 1.4519059658050537,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 57580
    },
    {
      "epoch": 18.29415501905972,
      "grad_norm": 1.4771853685379028,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 57590
    },
    {
      "epoch": 18.297331639135958,
      "grad_norm": 1.6767207384109497,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 57600
    },
    {
      "epoch": 18.300508259212197,
      "grad_norm": 4.217902660369873,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 57610
    },
    {
      "epoch": 18.303684879288436,
      "grad_norm": 3.6513612270355225,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 57620
    },
    {
      "epoch": 18.306861499364675,
      "grad_norm": 1.4794830083847046,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 57630
    },
    {
      "epoch": 18.310038119440915,
      "grad_norm": 3.9850611686706543,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 57640
    },
    {
      "epoch": 18.311626429479034,
      "eval_loss": 1.868989109992981,
      "eval_mse": 1.8675245297806604,
      "eval_pearson": 0.38004467445855267,
      "eval_runtime": 7.4853,
      "eval_samples_per_second": 2880.298,
      "eval_spearmanr": 0.3864398541889694,
      "eval_steps_per_second": 11.356,
      "step": 57645
    },
    {
      "epoch": 18.313214739517154,
      "grad_norm": 1.698955774307251,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 57650
    },
    {
      "epoch": 18.316391359593393,
      "grad_norm": 1.7045694589614868,
      "learning_rate": 2e-05,
      "loss": 0.0632,
      "step": 57660
    },
    {
      "epoch": 18.319567979669632,
      "grad_norm": 2.8125267028808594,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 57670
    },
    {
      "epoch": 18.32274459974587,
      "grad_norm": 2.240943670272827,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 57680
    },
    {
      "epoch": 18.32592121982211,
      "grad_norm": 1.3673416376113892,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 57690
    },
    {
      "epoch": 18.32909783989835,
      "grad_norm": 2.3189587593078613,
      "learning_rate": 2e-05,
      "loss": 0.0637,
      "step": 57700
    },
    {
      "epoch": 18.332274459974588,
      "grad_norm": 1.5830934047698975,
      "learning_rate": 2e-05,
      "loss": 0.0683,
      "step": 57710
    },
    {
      "epoch": 18.335451080050827,
      "grad_norm": 3.1424994468688965,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 57720
    },
    {
      "epoch": 18.338627700127066,
      "grad_norm": 2.06242299079895,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 57730
    },
    {
      "epoch": 18.341804320203305,
      "grad_norm": 1.6136523485183716,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 57740
    },
    {
      "epoch": 18.34498094027954,
      "grad_norm": 3.740957021713257,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 57750
    },
    {
      "epoch": 18.34815756035578,
      "grad_norm": 2.3759427070617676,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 57760
    },
    {
      "epoch": 18.35133418043202,
      "grad_norm": 1.1278289556503296,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 57770
    },
    {
      "epoch": 18.35451080050826,
      "grad_norm": 1.0936741828918457,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 57780
    },
    {
      "epoch": 18.357687420584497,
      "grad_norm": 1.3244242668151855,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 57790
    },
    {
      "epoch": 18.360864040660736,
      "grad_norm": 2.146176338195801,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 57800
    },
    {
      "epoch": 18.364040660736975,
      "grad_norm": 1.565475583076477,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 57810
    },
    {
      "epoch": 18.367217280813215,
      "grad_norm": 1.0357420444488525,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 57820
    },
    {
      "epoch": 18.370393900889454,
      "grad_norm": 8.98519229888916,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 57830
    },
    {
      "epoch": 18.373570520965693,
      "grad_norm": 1.1255130767822266,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 57840
    },
    {
      "epoch": 18.376747141041932,
      "grad_norm": 2.015906572341919,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 57850
    },
    {
      "epoch": 18.37992376111817,
      "grad_norm": 1.9784595966339111,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 57860
    },
    {
      "epoch": 18.38310038119441,
      "grad_norm": 2.264455556869507,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 57870
    },
    {
      "epoch": 18.38627700127065,
      "grad_norm": 1.287937045097351,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 57880
    },
    {
      "epoch": 18.389453621346888,
      "grad_norm": 5.419484615325928,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 57890
    },
    {
      "epoch": 18.392630241423127,
      "grad_norm": 1.547865867614746,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 57900
    },
    {
      "epoch": 18.395806861499366,
      "grad_norm": 2.2127649784088135,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 57910
    },
    {
      "epoch": 18.398983481575602,
      "grad_norm": 1.5190008878707886,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 57920
    },
    {
      "epoch": 18.40216010165184,
      "grad_norm": 1.0899643898010254,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 57930
    },
    {
      "epoch": 18.40533672172808,
      "grad_norm": 1.8087279796600342,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 57940
    },
    {
      "epoch": 18.40851334180432,
      "grad_norm": 1.5549497604370117,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 57950
    },
    {
      "epoch": 18.41168996188056,
      "grad_norm": 5.013720989227295,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 57960
    },
    {
      "epoch": 18.41168996188056,
      "eval_loss": 1.872814655303955,
      "eval_mse": 1.8711170027770296,
      "eval_pearson": 0.38055750745014316,
      "eval_runtime": 7.4982,
      "eval_samples_per_second": 2875.342,
      "eval_spearmanr": 0.3862278352147461,
      "eval_steps_per_second": 11.336,
      "step": 57960
    },
    {
      "epoch": 18.414866581956797,
      "grad_norm": 8.432845115661621,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 57970
    },
    {
      "epoch": 18.418043202033036,
      "grad_norm": 1.1722757816314697,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 57980
    },
    {
      "epoch": 18.421219822109276,
      "grad_norm": 2.397738456726074,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 57990
    },
    {
      "epoch": 18.424396442185515,
      "grad_norm": 1.495664358139038,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 58000
    },
    {
      "epoch": 18.427573062261754,
      "grad_norm": 2.00012469291687,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 58010
    },
    {
      "epoch": 18.430749682337993,
      "grad_norm": 1.3238208293914795,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 58020
    },
    {
      "epoch": 18.433926302414232,
      "grad_norm": 1.9904985427856445,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 58030
    },
    {
      "epoch": 18.43710292249047,
      "grad_norm": 2.3526597023010254,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 58040
    },
    {
      "epoch": 18.44027954256671,
      "grad_norm": 2.0321805477142334,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 58050
    },
    {
      "epoch": 18.44345616264295,
      "grad_norm": 1.8996596336364746,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 58060
    },
    {
      "epoch": 18.44663278271919,
      "grad_norm": 14.342513084411621,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 58070
    },
    {
      "epoch": 18.449809402795427,
      "grad_norm": 2.6576623916625977,
      "learning_rate": 2e-05,
      "loss": 0.0632,
      "step": 58080
    },
    {
      "epoch": 18.452986022871663,
      "grad_norm": 2.8809313774108887,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 58090
    },
    {
      "epoch": 18.456162642947902,
      "grad_norm": 1.1702561378479004,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 58100
    },
    {
      "epoch": 18.45933926302414,
      "grad_norm": 1.3629705905914307,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 58110
    },
    {
      "epoch": 18.46251588310038,
      "grad_norm": 1.79311203956604,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 58120
    },
    {
      "epoch": 18.46569250317662,
      "grad_norm": 1.1053240299224854,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 58130
    },
    {
      "epoch": 18.46886912325286,
      "grad_norm": 1.3457050323486328,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 58140
    },
    {
      "epoch": 18.472045743329097,
      "grad_norm": 1.302815556526184,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 58150
    },
    {
      "epoch": 18.475222363405337,
      "grad_norm": 2.775419235229492,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 58160
    },
    {
      "epoch": 18.478398983481576,
      "grad_norm": 1.5134860277175903,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 58170
    },
    {
      "epoch": 18.481575603557815,
      "grad_norm": 2.069167137145996,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 58180
    },
    {
      "epoch": 18.484752223634054,
      "grad_norm": 1.5755780935287476,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 58190
    },
    {
      "epoch": 18.487928843710293,
      "grad_norm": 1.5116729736328125,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 58200
    },
    {
      "epoch": 18.491105463786532,
      "grad_norm": 1.6872373819351196,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 58210
    },
    {
      "epoch": 18.49428208386277,
      "grad_norm": 1.4860146045684814,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 58220
    },
    {
      "epoch": 18.49745870393901,
      "grad_norm": 4.679394245147705,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 58230
    },
    {
      "epoch": 18.50063532401525,
      "grad_norm": 1.7051182985305786,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 58240
    },
    {
      "epoch": 18.50381194409149,
      "grad_norm": 1.3206647634506226,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 58250
    },
    {
      "epoch": 18.506988564167724,
      "grad_norm": 2.352099657058716,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 58260
    },
    {
      "epoch": 18.510165184243963,
      "grad_norm": 2.903240442276001,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 58270
    },
    {
      "epoch": 18.511753494282082,
      "eval_loss": 1.8556270599365234,
      "eval_mse": 1.854608519718134,
      "eval_pearson": 0.3995380265020912,
      "eval_runtime": 7.3889,
      "eval_samples_per_second": 2917.872,
      "eval_spearmanr": 0.40732238120271064,
      "eval_steps_per_second": 11.504,
      "step": 58275
    },
    {
      "epoch": 18.513341804320202,
      "grad_norm": 1.6680665016174316,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 58280
    },
    {
      "epoch": 18.51651842439644,
      "grad_norm": 2.2132914066314697,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 58290
    },
    {
      "epoch": 18.51969504447268,
      "grad_norm": 1.1015721559524536,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 58300
    },
    {
      "epoch": 18.52287166454892,
      "grad_norm": 2.104750871658325,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 58310
    },
    {
      "epoch": 18.52604828462516,
      "grad_norm": 1.4303733110427856,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 58320
    },
    {
      "epoch": 18.529224904701397,
      "grad_norm": 1.8194148540496826,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 58330
    },
    {
      "epoch": 18.532401524777637,
      "grad_norm": 1.2002469301223755,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 58340
    },
    {
      "epoch": 18.535578144853876,
      "grad_norm": 1.5245037078857422,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 58350
    },
    {
      "epoch": 18.538754764930115,
      "grad_norm": 1.7701575756072998,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 58360
    },
    {
      "epoch": 18.541931385006354,
      "grad_norm": 3.195652961730957,
      "learning_rate": 2e-05,
      "loss": 0.0596,
      "step": 58370
    },
    {
      "epoch": 18.545108005082593,
      "grad_norm": 2.0406417846679688,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 58380
    },
    {
      "epoch": 18.548284625158832,
      "grad_norm": 1.4471038579940796,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 58390
    },
    {
      "epoch": 18.55146124523507,
      "grad_norm": 1.2946244478225708,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 58400
    },
    {
      "epoch": 18.55463786531131,
      "grad_norm": 1.9013276100158691,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 58410
    },
    {
      "epoch": 18.55781448538755,
      "grad_norm": 1.3180222511291504,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 58420
    },
    {
      "epoch": 18.560991105463785,
      "grad_norm": 2.055495500564575,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 58430
    },
    {
      "epoch": 18.564167725540024,
      "grad_norm": 4.085051536560059,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 58440
    },
    {
      "epoch": 18.567344345616263,
      "grad_norm": 1.5989253520965576,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 58450
    },
    {
      "epoch": 18.570520965692502,
      "grad_norm": 1.699864387512207,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 58460
    },
    {
      "epoch": 18.57369758576874,
      "grad_norm": 2.891857147216797,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 58470
    },
    {
      "epoch": 18.57687420584498,
      "grad_norm": 1.528519868850708,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 58480
    },
    {
      "epoch": 18.58005082592122,
      "grad_norm": 1.26329505443573,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 58490
    },
    {
      "epoch": 18.58322744599746,
      "grad_norm": 1.5574145317077637,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 58500
    },
    {
      "epoch": 18.586404066073698,
      "grad_norm": 1.4964077472686768,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 58510
    },
    {
      "epoch": 18.589580686149937,
      "grad_norm": 2.9993910789489746,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 58520
    },
    {
      "epoch": 18.592757306226176,
      "grad_norm": 1.3010001182556152,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 58530
    },
    {
      "epoch": 18.595933926302415,
      "grad_norm": 2.460646629333496,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 58540
    },
    {
      "epoch": 18.599110546378654,
      "grad_norm": 2.2956337928771973,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 58550
    },
    {
      "epoch": 18.602287166454893,
      "grad_norm": 1.364660620689392,
      "learning_rate": 2e-05,
      "loss": 0.0668,
      "step": 58560
    },
    {
      "epoch": 18.605463786531132,
      "grad_norm": 1.3417738676071167,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 58570
    },
    {
      "epoch": 18.60864040660737,
      "grad_norm": 4.960067272186279,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 58580
    },
    {
      "epoch": 18.61181702668361,
      "grad_norm": 2.808431625366211,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 58590
    },
    {
      "epoch": 18.61181702668361,
      "eval_loss": 1.8079400062561035,
      "eval_mse": 1.8067508036944608,
      "eval_pearson": 0.40760260151559896,
      "eval_runtime": 7.4748,
      "eval_samples_per_second": 2884.352,
      "eval_spearmanr": 0.41572768417805606,
      "eval_steps_per_second": 11.372,
      "step": 58590
    },
    {
      "epoch": 18.614993646759846,
      "grad_norm": 1.7948517799377441,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 58600
    },
    {
      "epoch": 18.618170266836085,
      "grad_norm": 1.083667516708374,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 58610
    },
    {
      "epoch": 18.621346886912324,
      "grad_norm": 2.305325508117676,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 58620
    },
    {
      "epoch": 18.624523506988563,
      "grad_norm": 3.274489641189575,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 58630
    },
    {
      "epoch": 18.627700127064802,
      "grad_norm": 2.0414514541625977,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 58640
    },
    {
      "epoch": 18.63087674714104,
      "grad_norm": 1.543359637260437,
      "learning_rate": 2e-05,
      "loss": 0.0583,
      "step": 58650
    },
    {
      "epoch": 18.63405336721728,
      "grad_norm": 1.606505036354065,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 58660
    },
    {
      "epoch": 18.63722998729352,
      "grad_norm": 2.953807830810547,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 58670
    },
    {
      "epoch": 18.64040660736976,
      "grad_norm": 1.8410332202911377,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 58680
    },
    {
      "epoch": 18.643583227445998,
      "grad_norm": 2.2397682666778564,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 58690
    },
    {
      "epoch": 18.646759847522237,
      "grad_norm": 2.839298725128174,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 58700
    },
    {
      "epoch": 18.649936467598476,
      "grad_norm": 2.6120903491973877,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 58710
    },
    {
      "epoch": 18.653113087674715,
      "grad_norm": 1.051503300666809,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 58720
    },
    {
      "epoch": 18.656289707750954,
      "grad_norm": 7.473965167999268,
      "learning_rate": 2e-05,
      "loss": 0.0511,
      "step": 58730
    },
    {
      "epoch": 18.659466327827193,
      "grad_norm": 1.554229974746704,
      "learning_rate": 2e-05,
      "loss": 0.0661,
      "step": 58740
    },
    {
      "epoch": 18.662642947903432,
      "grad_norm": 1.5172700881958008,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 58750
    },
    {
      "epoch": 18.66581956797967,
      "grad_norm": 2.125798225402832,
      "learning_rate": 2e-05,
      "loss": 0.0597,
      "step": 58760
    },
    {
      "epoch": 18.668996188055907,
      "grad_norm": 3.054305076599121,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 58770
    },
    {
      "epoch": 18.672172808132146,
      "grad_norm": 1.1127409934997559,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 58780
    },
    {
      "epoch": 18.675349428208385,
      "grad_norm": 2.8511483669281006,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 58790
    },
    {
      "epoch": 18.678526048284624,
      "grad_norm": 1.0804531574249268,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 58800
    },
    {
      "epoch": 18.681702668360863,
      "grad_norm": 2.6117167472839355,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 58810
    },
    {
      "epoch": 18.684879288437102,
      "grad_norm": 1.8473016023635864,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 58820
    },
    {
      "epoch": 18.68805590851334,
      "grad_norm": 4.179043292999268,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 58830
    },
    {
      "epoch": 18.69123252858958,
      "grad_norm": 1.3092776536941528,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 58840
    },
    {
      "epoch": 18.69440914866582,
      "grad_norm": 2.0461931228637695,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 58850
    },
    {
      "epoch": 18.69758576874206,
      "grad_norm": 2.6433615684509277,
      "learning_rate": 2e-05,
      "loss": 0.0645,
      "step": 58860
    },
    {
      "epoch": 18.700762388818298,
      "grad_norm": 3.3666019439697266,
      "learning_rate": 2e-05,
      "loss": 0.0654,
      "step": 58870
    },
    {
      "epoch": 18.703939008894537,
      "grad_norm": 4.967503070831299,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 58880
    },
    {
      "epoch": 18.707115628970776,
      "grad_norm": 2.0517754554748535,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 58890
    },
    {
      "epoch": 18.710292249047015,
      "grad_norm": 1.7353471517562866,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 58900
    },
    {
      "epoch": 18.711880559085134,
      "eval_loss": 1.8545706272125244,
      "eval_mse": 1.8534313932061195,
      "eval_pearson": 0.39954346629451365,
      "eval_runtime": 7.4898,
      "eval_samples_per_second": 2878.565,
      "eval_spearmanr": 0.40762762009340325,
      "eval_steps_per_second": 11.349,
      "step": 58905
    },
    {
      "epoch": 18.713468869123254,
      "grad_norm": 1.5076682567596436,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 58910
    },
    {
      "epoch": 18.716645489199493,
      "grad_norm": 1.1121731996536255,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 58920
    },
    {
      "epoch": 18.719822109275732,
      "grad_norm": 1.8630465269088745,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 58930
    },
    {
      "epoch": 18.72299872935197,
      "grad_norm": 2.517695903778076,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 58940
    },
    {
      "epoch": 18.726175349428207,
      "grad_norm": 1.317298412322998,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 58950
    },
    {
      "epoch": 18.729351969504446,
      "grad_norm": 2.3492202758789062,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 58960
    },
    {
      "epoch": 18.732528589580685,
      "grad_norm": 2.211507558822632,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 58970
    },
    {
      "epoch": 18.735705209656924,
      "grad_norm": 1.4545212984085083,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 58980
    },
    {
      "epoch": 18.738881829733163,
      "grad_norm": 1.5816453695297241,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 58990
    },
    {
      "epoch": 18.742058449809402,
      "grad_norm": 1.4413337707519531,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 59000
    },
    {
      "epoch": 18.74523506988564,
      "grad_norm": 1.2929917573928833,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 59010
    },
    {
      "epoch": 18.74841168996188,
      "grad_norm": 2.132932662963867,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 59020
    },
    {
      "epoch": 18.75158831003812,
      "grad_norm": 5.116435527801514,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 59030
    },
    {
      "epoch": 18.75476493011436,
      "grad_norm": 2.980074405670166,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 59040
    },
    {
      "epoch": 18.757941550190598,
      "grad_norm": 3.5802505016326904,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 59050
    },
    {
      "epoch": 18.761118170266837,
      "grad_norm": 1.848958134651184,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 59060
    },
    {
      "epoch": 18.764294790343076,
      "grad_norm": 1.5479828119277954,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 59070
    },
    {
      "epoch": 18.767471410419315,
      "grad_norm": 1.9246759414672852,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 59080
    },
    {
      "epoch": 18.770648030495554,
      "grad_norm": 2.3636748790740967,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 59090
    },
    {
      "epoch": 18.773824650571793,
      "grad_norm": 1.2528636455535889,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 59100
    },
    {
      "epoch": 18.77700127064803,
      "grad_norm": 3.542504072189331,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 59110
    },
    {
      "epoch": 18.780177890724268,
      "grad_norm": 1.5412472486495972,
      "learning_rate": 2e-05,
      "loss": 0.0675,
      "step": 59120
    },
    {
      "epoch": 18.783354510800507,
      "grad_norm": 2.276336908340454,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 59130
    },
    {
      "epoch": 18.786531130876746,
      "grad_norm": 1.4413222074508667,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 59140
    },
    {
      "epoch": 18.789707750952985,
      "grad_norm": 1.404236078262329,
      "learning_rate": 2e-05,
      "loss": 0.061,
      "step": 59150
    },
    {
      "epoch": 18.792884371029224,
      "grad_norm": 4.017709732055664,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 59160
    },
    {
      "epoch": 18.796060991105463,
      "grad_norm": 1.131374478340149,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 59170
    },
    {
      "epoch": 18.799237611181702,
      "grad_norm": 1.9706114530563354,
      "learning_rate": 2e-05,
      "loss": 0.0652,
      "step": 59180
    },
    {
      "epoch": 18.80241423125794,
      "grad_norm": 1.5125924348831177,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 59190
    },
    {
      "epoch": 18.80559085133418,
      "grad_norm": 1.3173218965530396,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 59200
    },
    {
      "epoch": 18.80876747141042,
      "grad_norm": 1.345359206199646,
      "learning_rate": 2e-05,
      "loss": 0.0521,
      "step": 59210
    },
    {
      "epoch": 18.81194409148666,
      "grad_norm": 1.592577576637268,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 59220
    },
    {
      "epoch": 18.81194409148666,
      "eval_loss": 1.751333475112915,
      "eval_mse": 1.74989499061028,
      "eval_pearson": 0.43322514456621575,
      "eval_runtime": 7.3731,
      "eval_samples_per_second": 2924.157,
      "eval_spearmanr": 0.4392570496196515,
      "eval_steps_per_second": 11.528,
      "step": 59220
    },
    {
      "epoch": 18.815120711562898,
      "grad_norm": 1.6159971952438354,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 59230
    },
    {
      "epoch": 18.818297331639137,
      "grad_norm": 1.3675546646118164,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 59240
    },
    {
      "epoch": 18.821473951715376,
      "grad_norm": 2.2132229804992676,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 59250
    },
    {
      "epoch": 18.824650571791615,
      "grad_norm": 1.473811388015747,
      "learning_rate": 2e-05,
      "loss": 0.0583,
      "step": 59260
    },
    {
      "epoch": 18.827827191867854,
      "grad_norm": 1.4756157398223877,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 59270
    },
    {
      "epoch": 18.831003811944093,
      "grad_norm": 2.8639981746673584,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 59280
    },
    {
      "epoch": 18.83418043202033,
      "grad_norm": 1.8355776071548462,
      "learning_rate": 2e-05,
      "loss": 0.055,
      "step": 59290
    },
    {
      "epoch": 18.837357052096568,
      "grad_norm": 2.018911600112915,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 59300
    },
    {
      "epoch": 18.840533672172807,
      "grad_norm": 1.668436050415039,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 59310
    },
    {
      "epoch": 18.843710292249046,
      "grad_norm": 2.9967901706695557,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 59320
    },
    {
      "epoch": 18.846886912325285,
      "grad_norm": 1.6138314008712769,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 59330
    },
    {
      "epoch": 18.850063532401524,
      "grad_norm": 2.2521963119506836,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 59340
    },
    {
      "epoch": 18.853240152477763,
      "grad_norm": 1.6872798204421997,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 59350
    },
    {
      "epoch": 18.856416772554002,
      "grad_norm": 1.8603768348693848,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 59360
    },
    {
      "epoch": 18.85959339263024,
      "grad_norm": 1.7051022052764893,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 59370
    },
    {
      "epoch": 18.86277001270648,
      "grad_norm": 2.7280259132385254,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 59380
    },
    {
      "epoch": 18.86594663278272,
      "grad_norm": 1.7013591527938843,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 59390
    },
    {
      "epoch": 18.86912325285896,
      "grad_norm": 2.5493874549865723,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 59400
    },
    {
      "epoch": 18.872299872935198,
      "grad_norm": 1.1533393859863281,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 59410
    },
    {
      "epoch": 18.875476493011437,
      "grad_norm": 1.7775840759277344,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 59420
    },
    {
      "epoch": 18.878653113087676,
      "grad_norm": 1.8741447925567627,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 59430
    },
    {
      "epoch": 18.881829733163915,
      "grad_norm": 1.5896602869033813,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 59440
    },
    {
      "epoch": 18.885006353240154,
      "grad_norm": 2.206658363342285,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 59450
    },
    {
      "epoch": 18.88818297331639,
      "grad_norm": 12.299551010131836,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 59460
    },
    {
      "epoch": 18.89135959339263,
      "grad_norm": 1.5005996227264404,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 59470
    },
    {
      "epoch": 18.894536213468868,
      "grad_norm": 1.5312544107437134,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 59480
    },
    {
      "epoch": 18.897712833545107,
      "grad_norm": 1.383865237236023,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 59490
    },
    {
      "epoch": 18.900889453621346,
      "grad_norm": 1.6600016355514526,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 59500
    },
    {
      "epoch": 18.904066073697585,
      "grad_norm": 2.981462001800537,
      "learning_rate": 2e-05,
      "loss": 0.0618,
      "step": 59510
    },
    {
      "epoch": 18.907242693773824,
      "grad_norm": 3.2364346981048584,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 59520
    },
    {
      "epoch": 18.910419313850063,
      "grad_norm": 2.263446569442749,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 59530
    },
    {
      "epoch": 18.912007623888183,
      "eval_loss": 1.7809052467346191,
      "eval_mse": 1.779400209157083,
      "eval_pearson": 0.42858020751698195,
      "eval_runtime": 7.6119,
      "eval_samples_per_second": 2832.421,
      "eval_spearmanr": 0.43640266336988587,
      "eval_steps_per_second": 11.167,
      "step": 59535
    },
    {
      "epoch": 18.913595933926302,
      "grad_norm": 3.0298080444335938,
      "learning_rate": 2e-05,
      "loss": 0.065,
      "step": 59540
    },
    {
      "epoch": 18.91677255400254,
      "grad_norm": 1.155694842338562,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 59550
    },
    {
      "epoch": 18.91994917407878,
      "grad_norm": 1.089402198791504,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 59560
    },
    {
      "epoch": 18.92312579415502,
      "grad_norm": 1.874654769897461,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 59570
    },
    {
      "epoch": 18.92630241423126,
      "grad_norm": 1.3496363162994385,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 59580
    },
    {
      "epoch": 18.929479034307498,
      "grad_norm": 3.8823156356811523,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 59590
    },
    {
      "epoch": 18.932655654383737,
      "grad_norm": 1.843915343284607,
      "learning_rate": 2e-05,
      "loss": 0.0724,
      "step": 59600
    },
    {
      "epoch": 18.935832274459976,
      "grad_norm": 1.5327706336975098,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 59610
    },
    {
      "epoch": 18.939008894536215,
      "grad_norm": 1.9950618743896484,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 59620
    },
    {
      "epoch": 18.94218551461245,
      "grad_norm": 2.4358458518981934,
      "learning_rate": 2e-05,
      "loss": 0.0639,
      "step": 59630
    },
    {
      "epoch": 18.94536213468869,
      "grad_norm": 3.0892493724823,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 59640
    },
    {
      "epoch": 18.94853875476493,
      "grad_norm": 1.2288463115692139,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 59650
    },
    {
      "epoch": 18.951715374841168,
      "grad_norm": 1.1586755514144897,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 59660
    },
    {
      "epoch": 18.954891994917407,
      "grad_norm": 1.8493249416351318,
      "learning_rate": 2e-05,
      "loss": 0.0631,
      "step": 59670
    },
    {
      "epoch": 18.958068614993646,
      "grad_norm": 2.206732749938965,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 59680
    },
    {
      "epoch": 18.961245235069885,
      "grad_norm": 3.547452688217163,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 59690
    },
    {
      "epoch": 18.964421855146124,
      "grad_norm": 2.9406003952026367,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 59700
    },
    {
      "epoch": 18.967598475222363,
      "grad_norm": 1.4351229667663574,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 59710
    },
    {
      "epoch": 18.970775095298603,
      "grad_norm": 1.487955927848816,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 59720
    },
    {
      "epoch": 18.97395171537484,
      "grad_norm": 1.8390612602233887,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 59730
    },
    {
      "epoch": 18.97712833545108,
      "grad_norm": 2.306353807449341,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 59740
    },
    {
      "epoch": 18.98030495552732,
      "grad_norm": 1.3863497972488403,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 59750
    },
    {
      "epoch": 18.98348157560356,
      "grad_norm": 1.6507506370544434,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 59760
    },
    {
      "epoch": 18.986658195679798,
      "grad_norm": 3.7462096214294434,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 59770
    },
    {
      "epoch": 18.989834815756037,
      "grad_norm": 1.9377403259277344,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 59780
    },
    {
      "epoch": 18.993011435832276,
      "grad_norm": 3.2401680946350098,
      "learning_rate": 2e-05,
      "loss": 0.0627,
      "step": 59790
    },
    {
      "epoch": 18.99618805590851,
      "grad_norm": 3.2212517261505127,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 59800
    },
    {
      "epoch": 18.99936467598475,
      "grad_norm": 1.2098886966705322,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 59810
    },
    {
      "epoch": 19.00254129606099,
      "grad_norm": 1.7912031412124634,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 59820
    },
    {
      "epoch": 19.00571791613723,
      "grad_norm": 1.1422957181930542,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 59830
    },
    {
      "epoch": 19.008894536213468,
      "grad_norm": 1.9757434129714966,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 59840
    },
    {
      "epoch": 19.012071156289707,
      "grad_norm": 2.310875415802002,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 59850
    },
    {
      "epoch": 19.012071156289707,
      "eval_loss": 1.9468774795532227,
      "eval_mse": 1.9455687292818677,
      "eval_pearson": 0.3823304875700217,
      "eval_runtime": 7.3014,
      "eval_samples_per_second": 2952.849,
      "eval_spearmanr": 0.3908272071832946,
      "eval_steps_per_second": 11.642,
      "step": 59850
    },
    {
      "epoch": 19.015247776365946,
      "grad_norm": 2.2586355209350586,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 59860
    },
    {
      "epoch": 19.018424396442185,
      "grad_norm": 1.322451114654541,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 59870
    },
    {
      "epoch": 19.021601016518424,
      "grad_norm": 1.3909779787063599,
      "learning_rate": 2e-05,
      "loss": 0.0496,
      "step": 59880
    },
    {
      "epoch": 19.024777636594663,
      "grad_norm": 2.1029021739959717,
      "learning_rate": 2e-05,
      "loss": 0.0538,
      "step": 59890
    },
    {
      "epoch": 19.027954256670903,
      "grad_norm": 1.3887784481048584,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 59900
    },
    {
      "epoch": 19.03113087674714,
      "grad_norm": 1.987151026725769,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 59910
    },
    {
      "epoch": 19.03430749682338,
      "grad_norm": 1.2782648801803589,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 59920
    },
    {
      "epoch": 19.03748411689962,
      "grad_norm": 1.2547507286071777,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 59930
    },
    {
      "epoch": 19.04066073697586,
      "grad_norm": 1.3634885549545288,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 59940
    },
    {
      "epoch": 19.043837357052098,
      "grad_norm": 1.461201786994934,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 59950
    },
    {
      "epoch": 19.047013977128337,
      "grad_norm": 1.3566228151321411,
      "learning_rate": 2e-05,
      "loss": 0.051,
      "step": 59960
    },
    {
      "epoch": 19.050190597204573,
      "grad_norm": 1.3102566003799438,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 59970
    },
    {
      "epoch": 19.05336721728081,
      "grad_norm": 1.6267170906066895,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 59980
    },
    {
      "epoch": 19.05654383735705,
      "grad_norm": 2.1446268558502197,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 59990
    },
    {
      "epoch": 19.05972045743329,
      "grad_norm": 1.6673007011413574,
      "learning_rate": 2e-05,
      "loss": 0.0549,
      "step": 60000
    },
    {
      "epoch": 19.06289707750953,
      "grad_norm": 2.981182813644409,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 60010
    },
    {
      "epoch": 19.066073697585768,
      "grad_norm": 2.0751094818115234,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 60020
    },
    {
      "epoch": 19.069250317662007,
      "grad_norm": 1.1647871732711792,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 60030
    },
    {
      "epoch": 19.072426937738246,
      "grad_norm": 1.4548238515853882,
      "learning_rate": 2e-05,
      "loss": 0.0532,
      "step": 60040
    },
    {
      "epoch": 19.075603557814485,
      "grad_norm": 1.6885972023010254,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 60050
    },
    {
      "epoch": 19.078780177890724,
      "grad_norm": 1.4447253942489624,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 60060
    },
    {
      "epoch": 19.081956797966964,
      "grad_norm": 3.0158634185791016,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 60070
    },
    {
      "epoch": 19.085133418043203,
      "grad_norm": 3.034533977508545,
      "learning_rate": 2e-05,
      "loss": 0.0549,
      "step": 60080
    },
    {
      "epoch": 19.08831003811944,
      "grad_norm": 1.0413626432418823,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 60090
    },
    {
      "epoch": 19.09148665819568,
      "grad_norm": 1.2205218076705933,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 60100
    },
    {
      "epoch": 19.09466327827192,
      "grad_norm": 3.119008779525757,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 60110
    },
    {
      "epoch": 19.09783989834816,
      "grad_norm": 1.4282480478286743,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 60120
    },
    {
      "epoch": 19.101016518424398,
      "grad_norm": 2.3073017597198486,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 60130
    },
    {
      "epoch": 19.104193138500634,
      "grad_norm": 2.3978257179260254,
      "learning_rate": 2e-05,
      "loss": 0.0505,
      "step": 60140
    },
    {
      "epoch": 19.107369758576873,
      "grad_norm": 2.4826366901397705,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 60150
    },
    {
      "epoch": 19.110546378653112,
      "grad_norm": 1.3867762088775635,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 60160
    },
    {
      "epoch": 19.11213468869123,
      "eval_loss": 1.8972935676574707,
      "eval_mse": 1.8957192309027977,
      "eval_pearson": 0.3749100562989154,
      "eval_runtime": 7.4911,
      "eval_samples_per_second": 2878.092,
      "eval_spearmanr": 0.38563572986891825,
      "eval_steps_per_second": 11.347,
      "step": 60165
    },
    {
      "epoch": 19.11372299872935,
      "grad_norm": 3.505364418029785,
      "learning_rate": 2e-05,
      "loss": 0.0532,
      "step": 60170
    },
    {
      "epoch": 19.11689961880559,
      "grad_norm": 1.5996760129928589,
      "learning_rate": 2e-05,
      "loss": 0.0526,
      "step": 60180
    },
    {
      "epoch": 19.12007623888183,
      "grad_norm": 1.1246165037155151,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 60190
    },
    {
      "epoch": 19.123252858958068,
      "grad_norm": 3.9779305458068848,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 60200
    },
    {
      "epoch": 19.126429479034307,
      "grad_norm": 1.3250651359558105,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 60210
    },
    {
      "epoch": 19.129606099110546,
      "grad_norm": 2.9545929431915283,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 60220
    },
    {
      "epoch": 19.132782719186785,
      "grad_norm": 1.6621043682098389,
      "learning_rate": 2e-05,
      "loss": 0.0525,
      "step": 60230
    },
    {
      "epoch": 19.135959339263025,
      "grad_norm": 4.665378093719482,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 60240
    },
    {
      "epoch": 19.139135959339264,
      "grad_norm": 1.7776366472244263,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 60250
    },
    {
      "epoch": 19.142312579415503,
      "grad_norm": 21.729270935058594,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 60260
    },
    {
      "epoch": 19.14548919949174,
      "grad_norm": 1.946319341659546,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 60270
    },
    {
      "epoch": 19.14866581956798,
      "grad_norm": 1.3759089708328247,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 60280
    },
    {
      "epoch": 19.15184243964422,
      "grad_norm": 6.908325672149658,
      "learning_rate": 2e-05,
      "loss": 0.0549,
      "step": 60290
    },
    {
      "epoch": 19.15501905972046,
      "grad_norm": 2.1065871715545654,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 60300
    },
    {
      "epoch": 19.158195679796695,
      "grad_norm": 1.393224835395813,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 60310
    },
    {
      "epoch": 19.161372299872934,
      "grad_norm": 2.4839894771575928,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 60320
    },
    {
      "epoch": 19.164548919949173,
      "grad_norm": 1.0462714433670044,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 60330
    },
    {
      "epoch": 19.167725540025412,
      "grad_norm": 2.2396888732910156,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 60340
    },
    {
      "epoch": 19.17090216010165,
      "grad_norm": 1.7326569557189941,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 60350
    },
    {
      "epoch": 19.17407878017789,
      "grad_norm": 1.4580146074295044,
      "learning_rate": 2e-05,
      "loss": 0.0509,
      "step": 60360
    },
    {
      "epoch": 19.17725540025413,
      "grad_norm": 1.6965739727020264,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 60370
    },
    {
      "epoch": 19.180432020330368,
      "grad_norm": 1.188127875328064,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 60380
    },
    {
      "epoch": 19.183608640406607,
      "grad_norm": 3.5547735691070557,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 60390
    },
    {
      "epoch": 19.186785260482846,
      "grad_norm": 1.171579360961914,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 60400
    },
    {
      "epoch": 19.189961880559085,
      "grad_norm": 1.2738606929779053,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 60410
    },
    {
      "epoch": 19.193138500635325,
      "grad_norm": 1.3840957880020142,
      "learning_rate": 2e-05,
      "loss": 0.0526,
      "step": 60420
    },
    {
      "epoch": 19.196315120711564,
      "grad_norm": 4.879695892333984,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 60430
    },
    {
      "epoch": 19.199491740787803,
      "grad_norm": 2.6836867332458496,
      "learning_rate": 2e-05,
      "loss": 0.0519,
      "step": 60440
    },
    {
      "epoch": 19.202668360864042,
      "grad_norm": 1.3064008951187134,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 60450
    },
    {
      "epoch": 19.20584498094028,
      "grad_norm": 1.8679096698760986,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 60460
    },
    {
      "epoch": 19.20902160101652,
      "grad_norm": 2.8019425868988037,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 60470
    },
    {
      "epoch": 19.212198221092756,
      "grad_norm": 3.3812649250030518,
      "learning_rate": 2e-05,
      "loss": 0.0516,
      "step": 60480
    },
    {
      "epoch": 19.212198221092756,
      "eval_loss": 1.8580889701843262,
      "eval_mse": 1.856714294286849,
      "eval_pearson": 0.395802872242597,
      "eval_runtime": 7.6443,
      "eval_samples_per_second": 2820.386,
      "eval_spearmanr": 0.4074318569682456,
      "eval_steps_per_second": 11.119,
      "step": 60480
    },
    {
      "epoch": 19.215374841168995,
      "grad_norm": 1.3581080436706543,
      "learning_rate": 2e-05,
      "loss": 0.0525,
      "step": 60490
    },
    {
      "epoch": 19.218551461245234,
      "grad_norm": 1.2118422985076904,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 60500
    },
    {
      "epoch": 19.221728081321473,
      "grad_norm": 1.959108829498291,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 60510
    },
    {
      "epoch": 19.224904701397712,
      "grad_norm": 1.5884499549865723,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 60520
    },
    {
      "epoch": 19.22808132147395,
      "grad_norm": 1.0986862182617188,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 60530
    },
    {
      "epoch": 19.23125794155019,
      "grad_norm": 5.944279670715332,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 60540
    },
    {
      "epoch": 19.23443456162643,
      "grad_norm": 1.638077974319458,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 60550
    },
    {
      "epoch": 19.23761118170267,
      "grad_norm": 1.793052077293396,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 60560
    },
    {
      "epoch": 19.240787801778907,
      "grad_norm": 1.2777596712112427,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 60570
    },
    {
      "epoch": 19.243964421855146,
      "grad_norm": 1.9496203660964966,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 60580
    },
    {
      "epoch": 19.247141041931386,
      "grad_norm": 16.297401428222656,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 60590
    },
    {
      "epoch": 19.250317662007625,
      "grad_norm": 1.4739329814910889,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 60600
    },
    {
      "epoch": 19.253494282083864,
      "grad_norm": 1.9307470321655273,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 60610
    },
    {
      "epoch": 19.256670902160103,
      "grad_norm": 1.5863016843795776,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 60620
    },
    {
      "epoch": 19.259847522236342,
      "grad_norm": 2.280953884124756,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 60630
    },
    {
      "epoch": 19.26302414231258,
      "grad_norm": 1.5722618103027344,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 60640
    },
    {
      "epoch": 19.26620076238882,
      "grad_norm": 1.137800931930542,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 60650
    },
    {
      "epoch": 19.269377382465056,
      "grad_norm": 1.5319522619247437,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 60660
    },
    {
      "epoch": 19.272554002541295,
      "grad_norm": 4.215997219085693,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 60670
    },
    {
      "epoch": 19.275730622617534,
      "grad_norm": 1.4878016710281372,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 60680
    },
    {
      "epoch": 19.278907242693773,
      "grad_norm": 2.6923606395721436,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 60690
    },
    {
      "epoch": 19.282083862770012,
      "grad_norm": 4.601679801940918,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 60700
    },
    {
      "epoch": 19.28526048284625,
      "grad_norm": 15.328091621398926,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 60710
    },
    {
      "epoch": 19.28843710292249,
      "grad_norm": 1.6613097190856934,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 60720
    },
    {
      "epoch": 19.29161372299873,
      "grad_norm": 1.7818883657455444,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 60730
    },
    {
      "epoch": 19.29479034307497,
      "grad_norm": 1.585069179534912,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 60740
    },
    {
      "epoch": 19.297966963151207,
      "grad_norm": 1.609899878501892,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 60750
    },
    {
      "epoch": 19.301143583227446,
      "grad_norm": 4.473396301269531,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 60760
    },
    {
      "epoch": 19.304320203303686,
      "grad_norm": 7.207911014556885,
      "learning_rate": 2e-05,
      "loss": 0.0588,
      "step": 60770
    },
    {
      "epoch": 19.307496823379925,
      "grad_norm": 1.7406866550445557,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 60780
    },
    {
      "epoch": 19.310673443456164,
      "grad_norm": 2.133535385131836,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 60790
    },
    {
      "epoch": 19.312261753494283,
      "eval_loss": 1.8716943264007568,
      "eval_mse": 1.8702066707153544,
      "eval_pearson": 0.4109466016348173,
      "eval_runtime": 7.2802,
      "eval_samples_per_second": 2961.438,
      "eval_spearmanr": 0.4220584835978333,
      "eval_steps_per_second": 11.675,
      "step": 60795
    },
    {
      "epoch": 19.313850063532403,
      "grad_norm": 1.7663122415542603,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 60800
    },
    {
      "epoch": 19.317026683608642,
      "grad_norm": 1.3673367500305176,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 60810
    },
    {
      "epoch": 19.32020330368488,
      "grad_norm": 2.114403247833252,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 60820
    },
    {
      "epoch": 19.323379923761117,
      "grad_norm": 2.8944413661956787,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 60830
    },
    {
      "epoch": 19.326556543837356,
      "grad_norm": 2.1035706996917725,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 60840
    },
    {
      "epoch": 19.329733163913595,
      "grad_norm": 1.6977719068527222,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 60850
    },
    {
      "epoch": 19.332909783989834,
      "grad_norm": 1.444541573524475,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 60860
    },
    {
      "epoch": 19.336086404066073,
      "grad_norm": 0.9973548054695129,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 60870
    },
    {
      "epoch": 19.339263024142312,
      "grad_norm": 1.1697924137115479,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 60880
    },
    {
      "epoch": 19.34243964421855,
      "grad_norm": 1.883907675743103,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 60890
    },
    {
      "epoch": 19.34561626429479,
      "grad_norm": 1.4736955165863037,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 60900
    },
    {
      "epoch": 19.34879288437103,
      "grad_norm": 1.870516061782837,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 60910
    },
    {
      "epoch": 19.35196950444727,
      "grad_norm": 1.9664769172668457,
      "learning_rate": 2e-05,
      "loss": 0.051,
      "step": 60920
    },
    {
      "epoch": 19.355146124523507,
      "grad_norm": 1.9819918870925903,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 60930
    },
    {
      "epoch": 19.358322744599747,
      "grad_norm": 1.5921096801757812,
      "learning_rate": 2e-05,
      "loss": 0.0518,
      "step": 60940
    },
    {
      "epoch": 19.361499364675986,
      "grad_norm": 6.144972324371338,
      "learning_rate": 2e-05,
      "loss": 0.0591,
      "step": 60950
    },
    {
      "epoch": 19.364675984752225,
      "grad_norm": 1.9842690229415894,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 60960
    },
    {
      "epoch": 19.367852604828464,
      "grad_norm": 1.7368019819259644,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 60970
    },
    {
      "epoch": 19.371029224904703,
      "grad_norm": 1.6523326635360718,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 60980
    },
    {
      "epoch": 19.374205844980942,
      "grad_norm": 1.5433318614959717,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 60990
    },
    {
      "epoch": 19.377382465057178,
      "grad_norm": 1.7686063051223755,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 61000
    },
    {
      "epoch": 19.380559085133417,
      "grad_norm": 1.947226643562317,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 61010
    },
    {
      "epoch": 19.383735705209656,
      "grad_norm": 1.4677350521087646,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 61020
    },
    {
      "epoch": 19.386912325285895,
      "grad_norm": 1.5421963930130005,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 61030
    },
    {
      "epoch": 19.390088945362134,
      "grad_norm": 1.804297685623169,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 61040
    },
    {
      "epoch": 19.393265565438373,
      "grad_norm": 2.45064640045166,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 61050
    },
    {
      "epoch": 19.396442185514612,
      "grad_norm": 1.9731227159500122,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 61060
    },
    {
      "epoch": 19.39961880559085,
      "grad_norm": 2.518174648284912,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 61070
    },
    {
      "epoch": 19.40279542566709,
      "grad_norm": 1.8055346012115479,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 61080
    },
    {
      "epoch": 19.40597204574333,
      "grad_norm": 1.4636894464492798,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 61090
    },
    {
      "epoch": 19.40914866581957,
      "grad_norm": 1.5064318180084229,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 61100
    },
    {
      "epoch": 19.412325285895808,
      "grad_norm": 1.1779201030731201,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 61110
    },
    {
      "epoch": 19.412325285895808,
      "eval_loss": 1.8388817310333252,
      "eval_mse": 1.8379530927854035,
      "eval_pearson": 0.38764344838819254,
      "eval_runtime": 7.5894,
      "eval_samples_per_second": 2840.796,
      "eval_spearmanr": 0.3964998519071259,
      "eval_steps_per_second": 11.2,
      "step": 61110
    },
    {
      "epoch": 19.415501905972047,
      "grad_norm": 1.8312278985977173,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 61120
    },
    {
      "epoch": 19.418678526048286,
      "grad_norm": 1.9805619716644287,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 61130
    },
    {
      "epoch": 19.421855146124525,
      "grad_norm": 1.5583522319793701,
      "learning_rate": 2e-05,
      "loss": 0.0593,
      "step": 61140
    },
    {
      "epoch": 19.425031766200764,
      "grad_norm": 1.9847692251205444,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 61150
    },
    {
      "epoch": 19.428208386277003,
      "grad_norm": 1.5695374011993408,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 61160
    },
    {
      "epoch": 19.43138500635324,
      "grad_norm": 2.2858641147613525,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 61170
    },
    {
      "epoch": 19.434561626429478,
      "grad_norm": 1.116349697113037,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 61180
    },
    {
      "epoch": 19.437738246505717,
      "grad_norm": 1.8570830821990967,
      "learning_rate": 2e-05,
      "loss": 0.0519,
      "step": 61190
    },
    {
      "epoch": 19.440914866581956,
      "grad_norm": 1.1003459692001343,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 61200
    },
    {
      "epoch": 19.444091486658195,
      "grad_norm": 1.2926872968673706,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 61210
    },
    {
      "epoch": 19.447268106734434,
      "grad_norm": 1.7451627254486084,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 61220
    },
    {
      "epoch": 19.450444726810673,
      "grad_norm": 2.2086844444274902,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 61230
    },
    {
      "epoch": 19.453621346886912,
      "grad_norm": 1.2781145572662354,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 61240
    },
    {
      "epoch": 19.45679796696315,
      "grad_norm": 2.0298385620117188,
      "learning_rate": 2e-05,
      "loss": 0.0583,
      "step": 61250
    },
    {
      "epoch": 19.45997458703939,
      "grad_norm": 1.6227586269378662,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 61260
    },
    {
      "epoch": 19.46315120711563,
      "grad_norm": 2.092164993286133,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 61270
    },
    {
      "epoch": 19.46632782719187,
      "grad_norm": 1.519450306892395,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 61280
    },
    {
      "epoch": 19.469504447268108,
      "grad_norm": 1.8039380311965942,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 61290
    },
    {
      "epoch": 19.472681067344347,
      "grad_norm": 1.6208258867263794,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 61300
    },
    {
      "epoch": 19.475857687420586,
      "grad_norm": 2.057631254196167,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 61310
    },
    {
      "epoch": 19.479034307496825,
      "grad_norm": 1.787764549255371,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 61320
    },
    {
      "epoch": 19.482210927573064,
      "grad_norm": 1.2217615842819214,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 61330
    },
    {
      "epoch": 19.4853875476493,
      "grad_norm": 1.0856256484985352,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 61340
    },
    {
      "epoch": 19.48856416772554,
      "grad_norm": 1.673687219619751,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 61350
    },
    {
      "epoch": 19.491740787801778,
      "grad_norm": 1.4561870098114014,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 61360
    },
    {
      "epoch": 19.494917407878017,
      "grad_norm": 1.2041878700256348,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 61370
    },
    {
      "epoch": 19.498094027954256,
      "grad_norm": 1.009895920753479,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 61380
    },
    {
      "epoch": 19.501270648030495,
      "grad_norm": 1.5847288370132446,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 61390
    },
    {
      "epoch": 19.504447268106734,
      "grad_norm": 2.09892201423645,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 61400
    },
    {
      "epoch": 19.507623888182973,
      "grad_norm": 2.1290271282196045,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 61410
    },
    {
      "epoch": 19.510800508259212,
      "grad_norm": 1.5212472677230835,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 61420
    },
    {
      "epoch": 19.51238881829733,
      "eval_loss": 1.7745441198349,
      "eval_mse": 1.7732755539755654,
      "eval_pearson": 0.4271018394902133,
      "eval_runtime": 7.5871,
      "eval_samples_per_second": 2841.676,
      "eval_spearmanr": 0.4380685116372931,
      "eval_steps_per_second": 11.203,
      "step": 61425
    },
    {
      "epoch": 19.51397712833545,
      "grad_norm": 6.233619689941406,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 61430
    },
    {
      "epoch": 19.51715374841169,
      "grad_norm": 2.4887163639068604,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 61440
    },
    {
      "epoch": 19.52033036848793,
      "grad_norm": 1.130544662475586,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 61450
    },
    {
      "epoch": 19.52350698856417,
      "grad_norm": 2.8954873085021973,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 61460
    },
    {
      "epoch": 19.526683608640408,
      "grad_norm": 1.0498180389404297,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 61470
    },
    {
      "epoch": 19.529860228716647,
      "grad_norm": 2.3223049640655518,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 61480
    },
    {
      "epoch": 19.533036848792886,
      "grad_norm": 2.0642738342285156,
      "learning_rate": 2e-05,
      "loss": 0.0549,
      "step": 61490
    },
    {
      "epoch": 19.536213468869125,
      "grad_norm": 2.1696484088897705,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 61500
    },
    {
      "epoch": 19.53939008894536,
      "grad_norm": 2.1825923919677734,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 61510
    },
    {
      "epoch": 19.5425667090216,
      "grad_norm": 1.7917460203170776,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 61520
    },
    {
      "epoch": 19.54574332909784,
      "grad_norm": 1.313357949256897,
      "learning_rate": 2e-05,
      "loss": 0.0493,
      "step": 61530
    },
    {
      "epoch": 19.548919949174078,
      "grad_norm": 1.1573737859725952,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 61540
    },
    {
      "epoch": 19.552096569250317,
      "grad_norm": 138.45318603515625,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 61550
    },
    {
      "epoch": 19.555273189326556,
      "grad_norm": 2.1262528896331787,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 61560
    },
    {
      "epoch": 19.558449809402795,
      "grad_norm": 1.9119045734405518,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 61570
    },
    {
      "epoch": 19.561626429479034,
      "grad_norm": 3.1839778423309326,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 61580
    },
    {
      "epoch": 19.564803049555273,
      "grad_norm": 2.927391767501831,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 61590
    },
    {
      "epoch": 19.567979669631512,
      "grad_norm": 1.877095103263855,
      "learning_rate": 2e-05,
      "loss": 0.0508,
      "step": 61600
    },
    {
      "epoch": 19.57115628970775,
      "grad_norm": 2.183373212814331,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 61610
    },
    {
      "epoch": 19.57433290978399,
      "grad_norm": 1.7671080827713013,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 61620
    },
    {
      "epoch": 19.57750952986023,
      "grad_norm": 3.8235628604888916,
      "learning_rate": 2e-05,
      "loss": 0.0636,
      "step": 61630
    },
    {
      "epoch": 19.58068614993647,
      "grad_norm": 1.3188982009887695,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 61640
    },
    {
      "epoch": 19.583862770012708,
      "grad_norm": 1.4622893333435059,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 61650
    },
    {
      "epoch": 19.587039390088947,
      "grad_norm": 1.7148538827896118,
      "learning_rate": 2e-05,
      "loss": 0.0566,
      "step": 61660
    },
    {
      "epoch": 19.590216010165186,
      "grad_norm": 2.3390607833862305,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 61670
    },
    {
      "epoch": 19.59339263024142,
      "grad_norm": 1.2905828952789307,
      "learning_rate": 2e-05,
      "loss": 0.054,
      "step": 61680
    },
    {
      "epoch": 19.59656925031766,
      "grad_norm": 3.414170980453491,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 61690
    },
    {
      "epoch": 19.5997458703939,
      "grad_norm": 1.337847352027893,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 61700
    },
    {
      "epoch": 19.60292249047014,
      "grad_norm": 2.0338051319122314,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 61710
    },
    {
      "epoch": 19.606099110546378,
      "grad_norm": 5.772870063781738,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 61720
    },
    {
      "epoch": 19.609275730622617,
      "grad_norm": 1.1990337371826172,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 61730
    },
    {
      "epoch": 19.612452350698856,
      "grad_norm": 2.0263149738311768,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 61740
    },
    {
      "epoch": 19.612452350698856,
      "eval_loss": 1.84786856174469,
      "eval_mse": 1.8459794100284135,
      "eval_pearson": 0.3940494410127594,
      "eval_runtime": 7.3806,
      "eval_samples_per_second": 2921.164,
      "eval_spearmanr": 0.4023065876641072,
      "eval_steps_per_second": 11.517,
      "step": 61740
    },
    {
      "epoch": 19.615628970775095,
      "grad_norm": 1.9340988397598267,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 61750
    },
    {
      "epoch": 19.618805590851334,
      "grad_norm": 1.2047231197357178,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 61760
    },
    {
      "epoch": 19.621982210927573,
      "grad_norm": 2.3481383323669434,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 61770
    },
    {
      "epoch": 19.625158831003812,
      "grad_norm": 1.5737558603286743,
      "learning_rate": 2e-05,
      "loss": 0.0647,
      "step": 61780
    },
    {
      "epoch": 19.62833545108005,
      "grad_norm": 2.218362331390381,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 61790
    },
    {
      "epoch": 19.63151207115629,
      "grad_norm": 1.4850834608078003,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 61800
    },
    {
      "epoch": 19.63468869123253,
      "grad_norm": 2.9262681007385254,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 61810
    },
    {
      "epoch": 19.63786531130877,
      "grad_norm": 1.6183842420578003,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 61820
    },
    {
      "epoch": 19.641041931385008,
      "grad_norm": 1.6206917762756348,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 61830
    },
    {
      "epoch": 19.644218551461247,
      "grad_norm": 2.505913496017456,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 61840
    },
    {
      "epoch": 19.647395171537482,
      "grad_norm": 1.7562624216079712,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 61850
    },
    {
      "epoch": 19.65057179161372,
      "grad_norm": 1.766347050666809,
      "learning_rate": 2e-05,
      "loss": 0.05,
      "step": 61860
    },
    {
      "epoch": 19.65374841168996,
      "grad_norm": 1.9388136863708496,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 61870
    },
    {
      "epoch": 19.6569250317662,
      "grad_norm": 8.12208366394043,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 61880
    },
    {
      "epoch": 19.66010165184244,
      "grad_norm": 1.0791239738464355,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 61890
    },
    {
      "epoch": 19.663278271918678,
      "grad_norm": 1.5407986640930176,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 61900
    },
    {
      "epoch": 19.666454891994917,
      "grad_norm": 1.5682264566421509,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 61910
    },
    {
      "epoch": 19.669631512071156,
      "grad_norm": 0.9824707508087158,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 61920
    },
    {
      "epoch": 19.672808132147395,
      "grad_norm": 1.9298187494277954,
      "learning_rate": 2e-05,
      "loss": 0.055,
      "step": 61930
    },
    {
      "epoch": 19.675984752223634,
      "grad_norm": 1.6678825616836548,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 61940
    },
    {
      "epoch": 19.679161372299873,
      "grad_norm": 3.1795852184295654,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 61950
    },
    {
      "epoch": 19.682337992376112,
      "grad_norm": 1.9421051740646362,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 61960
    },
    {
      "epoch": 19.68551461245235,
      "grad_norm": 1.2889907360076904,
      "learning_rate": 2e-05,
      "loss": 0.0516,
      "step": 61970
    },
    {
      "epoch": 19.68869123252859,
      "grad_norm": 3.0553765296936035,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 61980
    },
    {
      "epoch": 19.69186785260483,
      "grad_norm": 1.202048659324646,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 61990
    },
    {
      "epoch": 19.69504447268107,
      "grad_norm": 1.2742892503738403,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 62000
    },
    {
      "epoch": 19.698221092757308,
      "grad_norm": 1.4367507696151733,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 62010
    },
    {
      "epoch": 19.701397712833547,
      "grad_norm": 2.0140812397003174,
      "learning_rate": 2e-05,
      "loss": 0.0526,
      "step": 62020
    },
    {
      "epoch": 19.704574332909782,
      "grad_norm": 3.1099720001220703,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 62030
    },
    {
      "epoch": 19.70775095298602,
      "grad_norm": 2.292332172393799,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 62040
    },
    {
      "epoch": 19.71092757306226,
      "grad_norm": 1.5366485118865967,
      "learning_rate": 2e-05,
      "loss": 0.0597,
      "step": 62050
    },
    {
      "epoch": 19.71251588310038,
      "eval_loss": 1.924675703048706,
      "eval_mse": 1.9233185593073354,
      "eval_pearson": 0.37528700460505116,
      "eval_runtime": 7.5018,
      "eval_samples_per_second": 2873.962,
      "eval_spearmanr": 0.3829201711294656,
      "eval_steps_per_second": 11.331,
      "step": 62055
    },
    {
      "epoch": 19.7141041931385,
      "grad_norm": 1.5294722318649292,
      "learning_rate": 2e-05,
      "loss": 0.0569,
      "step": 62060
    },
    {
      "epoch": 19.71728081321474,
      "grad_norm": 2.0329370498657227,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 62070
    },
    {
      "epoch": 19.720457433290978,
      "grad_norm": 2.708136796951294,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 62080
    },
    {
      "epoch": 19.723634053367217,
      "grad_norm": 1.5188454389572144,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 62090
    },
    {
      "epoch": 19.726810673443456,
      "grad_norm": 2.0142641067504883,
      "learning_rate": 2e-05,
      "loss": 0.0748,
      "step": 62100
    },
    {
      "epoch": 19.729987293519695,
      "grad_norm": 2.568718194961548,
      "learning_rate": 2e-05,
      "loss": 0.0591,
      "step": 62110
    },
    {
      "epoch": 19.733163913595934,
      "grad_norm": 1.7047425508499146,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 62120
    },
    {
      "epoch": 19.736340533672173,
      "grad_norm": 2.3360939025878906,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 62130
    },
    {
      "epoch": 19.739517153748412,
      "grad_norm": 1.396563172340393,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 62140
    },
    {
      "epoch": 19.74269377382465,
      "grad_norm": 1.0987741947174072,
      "learning_rate": 2e-05,
      "loss": 0.0581,
      "step": 62150
    },
    {
      "epoch": 19.74587039390089,
      "grad_norm": 3.301772117614746,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 62160
    },
    {
      "epoch": 19.74904701397713,
      "grad_norm": 2.283027172088623,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 62170
    },
    {
      "epoch": 19.75222363405337,
      "grad_norm": 3.4610612392425537,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 62180
    },
    {
      "epoch": 19.755400254129604,
      "grad_norm": 2.4865946769714355,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 62190
    },
    {
      "epoch": 19.758576874205843,
      "grad_norm": 1.9966018199920654,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 62200
    },
    {
      "epoch": 19.761753494282082,
      "grad_norm": 2.5354747772216797,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 62210
    },
    {
      "epoch": 19.76493011435832,
      "grad_norm": 1.1740493774414062,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 62220
    },
    {
      "epoch": 19.76810673443456,
      "grad_norm": 3.4990181922912598,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 62230
    },
    {
      "epoch": 19.7712833545108,
      "grad_norm": 2.4648869037628174,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 62240
    },
    {
      "epoch": 19.77445997458704,
      "grad_norm": 1.0344597101211548,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 62250
    },
    {
      "epoch": 19.777636594663278,
      "grad_norm": 1.3771394491195679,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 62260
    },
    {
      "epoch": 19.780813214739517,
      "grad_norm": 2.981743335723877,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 62270
    },
    {
      "epoch": 19.783989834815756,
      "grad_norm": 0.9470714330673218,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 62280
    },
    {
      "epoch": 19.787166454891995,
      "grad_norm": 1.275375485420227,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 62290
    },
    {
      "epoch": 19.790343074968234,
      "grad_norm": 2.463351011276245,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 62300
    },
    {
      "epoch": 19.793519695044473,
      "grad_norm": 1.887612223625183,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 62310
    },
    {
      "epoch": 19.796696315120712,
      "grad_norm": 1.268659234046936,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 62320
    },
    {
      "epoch": 19.79987293519695,
      "grad_norm": 3.3044888973236084,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 62330
    },
    {
      "epoch": 19.80304955527319,
      "grad_norm": 1.3269051313400269,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 62340
    },
    {
      "epoch": 19.80622617534943,
      "grad_norm": 2.2511322498321533,
      "learning_rate": 2e-05,
      "loss": 0.0591,
      "step": 62350
    },
    {
      "epoch": 19.80940279542567,
      "grad_norm": 4.356104850769043,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 62360
    },
    {
      "epoch": 19.812579415501904,
      "grad_norm": 1.1588863134384155,
      "learning_rate": 2e-05,
      "loss": 0.0508,
      "step": 62370
    },
    {
      "epoch": 19.812579415501904,
      "eval_loss": 1.9238886833190918,
      "eval_mse": 1.9225486058112864,
      "eval_pearson": 0.37290253410953256,
      "eval_runtime": 7.5818,
      "eval_samples_per_second": 2843.651,
      "eval_spearmanr": 0.38503442721758124,
      "eval_steps_per_second": 11.211,
      "step": 62370
    },
    {
      "epoch": 19.815756035578143,
      "grad_norm": 1.6082470417022705,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 62380
    },
    {
      "epoch": 19.818932655654383,
      "grad_norm": 1.2945371866226196,
      "learning_rate": 2e-05,
      "loss": 0.0525,
      "step": 62390
    },
    {
      "epoch": 19.82210927573062,
      "grad_norm": 1.8213282823562622,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 62400
    },
    {
      "epoch": 19.82528589580686,
      "grad_norm": 1.292353630065918,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 62410
    },
    {
      "epoch": 19.8284625158831,
      "grad_norm": 2.4069008827209473,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 62420
    },
    {
      "epoch": 19.83163913595934,
      "grad_norm": 1.3612319231033325,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 62430
    },
    {
      "epoch": 19.834815756035578,
      "grad_norm": 1.600506067276001,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 62440
    },
    {
      "epoch": 19.837992376111817,
      "grad_norm": 1.8129997253417969,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 62450
    },
    {
      "epoch": 19.841168996188056,
      "grad_norm": 8.548813819885254,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 62460
    },
    {
      "epoch": 19.844345616264295,
      "grad_norm": 1.3020185232162476,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 62470
    },
    {
      "epoch": 19.847522236340534,
      "grad_norm": 1.2443482875823975,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 62480
    },
    {
      "epoch": 19.850698856416773,
      "grad_norm": 2.0710620880126953,
      "learning_rate": 2e-05,
      "loss": 0.0552,
      "step": 62490
    },
    {
      "epoch": 19.853875476493013,
      "grad_norm": 2.9217543601989746,
      "learning_rate": 2e-05,
      "loss": 0.0626,
      "step": 62500
    },
    {
      "epoch": 19.85705209656925,
      "grad_norm": 2.2547214031219482,
      "learning_rate": 2e-05,
      "loss": 0.0509,
      "step": 62510
    },
    {
      "epoch": 19.86022871664549,
      "grad_norm": 1.116964340209961,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 62520
    },
    {
      "epoch": 19.863405336721726,
      "grad_norm": 1.8696794509887695,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 62530
    },
    {
      "epoch": 19.866581956797965,
      "grad_norm": 2.3845443725585938,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 62540
    },
    {
      "epoch": 19.869758576874204,
      "grad_norm": 2.0831868648529053,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 62550
    },
    {
      "epoch": 19.872935196950444,
      "grad_norm": 1.2788928747177124,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 62560
    },
    {
      "epoch": 19.876111817026683,
      "grad_norm": 2.8911328315734863,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 62570
    },
    {
      "epoch": 19.87928843710292,
      "grad_norm": 1.2302064895629883,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 62580
    },
    {
      "epoch": 19.88246505717916,
      "grad_norm": 1.579559087753296,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 62590
    },
    {
      "epoch": 19.8856416772554,
      "grad_norm": 2.2974720001220703,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 62600
    },
    {
      "epoch": 19.88881829733164,
      "grad_norm": 1.8654289245605469,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 62610
    },
    {
      "epoch": 19.891994917407878,
      "grad_norm": 3.63472580909729,
      "learning_rate": 2e-05,
      "loss": 0.0576,
      "step": 62620
    },
    {
      "epoch": 19.895171537484117,
      "grad_norm": 1.4643609523773193,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 62630
    },
    {
      "epoch": 19.898348157560356,
      "grad_norm": 1.6676207780838013,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 62640
    },
    {
      "epoch": 19.901524777636595,
      "grad_norm": 1.5141366720199585,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 62650
    },
    {
      "epoch": 19.904701397712834,
      "grad_norm": 2.1148972511291504,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 62660
    },
    {
      "epoch": 19.907878017789074,
      "grad_norm": 1.0481038093566895,
      "learning_rate": 2e-05,
      "loss": 0.0605,
      "step": 62670
    },
    {
      "epoch": 19.911054637865313,
      "grad_norm": 2.129335403442383,
      "learning_rate": 2e-05,
      "loss": 0.0597,
      "step": 62680
    },
    {
      "epoch": 19.912642947903432,
      "eval_loss": 1.7975770235061646,
      "eval_mse": 1.7964057833066898,
      "eval_pearson": 0.3862284511133949,
      "eval_runtime": 7.5793,
      "eval_samples_per_second": 2844.579,
      "eval_spearmanr": 0.39378495810631864,
      "eval_steps_per_second": 11.215,
      "step": 62685
    }
  ],
  "logging_steps": 10,
  "max_steps": 62960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 315,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.222199866274611e+18,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
