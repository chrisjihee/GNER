{
  "best_metric": 0.5164531731204351,
  "best_model_checkpoint": "output-lfs/GNER-QE-HR-max_sampled=3/FacebookAI/roberta-large-ep1-lr2e-5/checkpoint-7500",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 12590,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003971405877680699,
      "grad_norm": 25.042797088623047,
      "learning_rate": 2e-05,
      "loss": 1.7172,
      "step": 50
    },
    {
      "epoch": 0.007942811755361398,
      "grad_norm": 8.186473846435547,
      "learning_rate": 2e-05,
      "loss": 1.2471,
      "step": 100
    },
    {
      "epoch": 0.011914217633042097,
      "grad_norm": 17.888599395751953,
      "learning_rate": 2e-05,
      "loss": 1.1694,
      "step": 150
    },
    {
      "epoch": 0.015885623510722795,
      "grad_norm": 8.129303932189941,
      "learning_rate": 2e-05,
      "loss": 1.1271,
      "step": 200
    },
    {
      "epoch": 0.019857029388403495,
      "grad_norm": 21.036684036254883,
      "learning_rate": 2e-05,
      "loss": 1.1759,
      "step": 250
    },
    {
      "epoch": 0.023828435266084195,
      "grad_norm": 16.384174346923828,
      "learning_rate": 2e-05,
      "loss": 1.1245,
      "step": 300
    },
    {
      "epoch": 0.02779984114376489,
      "grad_norm": 28.112346649169922,
      "learning_rate": 2e-05,
      "loss": 1.1233,
      "step": 350
    },
    {
      "epoch": 0.03177124702144559,
      "grad_norm": 6.773285865783691,
      "learning_rate": 2e-05,
      "loss": 1.0967,
      "step": 400
    },
    {
      "epoch": 0.035742652899126294,
      "grad_norm": 5.5462470054626465,
      "learning_rate": 2e-05,
      "loss": 1.0357,
      "step": 450
    },
    {
      "epoch": 0.03971405877680699,
      "grad_norm": 5.672882556915283,
      "learning_rate": 2e-05,
      "loss": 1.1641,
      "step": 500
    },
    {
      "epoch": 0.03971405877680699,
      "eval_loss": 2.8400180339813232,
      "eval_mse": 2.8394853864336778,
      "eval_pearson": 0.466820084614894,
      "eval_runtime": 38.5386,
      "eval_samples_per_second": 1430.564,
      "eval_spearmanr": 0.48400219212876566,
      "eval_steps_per_second": 11.936,
      "step": 500
    },
    {
      "epoch": 0.043685464654487687,
      "grad_norm": 6.3454766273498535,
      "learning_rate": 2e-05,
      "loss": 1.0765,
      "step": 550
    },
    {
      "epoch": 0.04765687053216839,
      "grad_norm": 10.276050567626953,
      "learning_rate": 2e-05,
      "loss": 1.1339,
      "step": 600
    },
    {
      "epoch": 0.051628276409849086,
      "grad_norm": 16.811447143554688,
      "learning_rate": 2e-05,
      "loss": 1.0714,
      "step": 650
    },
    {
      "epoch": 0.05559968228752978,
      "grad_norm": 7.293944358825684,
      "learning_rate": 2e-05,
      "loss": 1.1437,
      "step": 700
    },
    {
      "epoch": 0.059571088165210485,
      "grad_norm": 10.12267017364502,
      "learning_rate": 2e-05,
      "loss": 1.0125,
      "step": 750
    },
    {
      "epoch": 0.06354249404289118,
      "grad_norm": 10.969571113586426,
      "learning_rate": 2e-05,
      "loss": 1.0882,
      "step": 800
    },
    {
      "epoch": 0.06751389992057188,
      "grad_norm": 25.5118408203125,
      "learning_rate": 2e-05,
      "loss": 1.0743,
      "step": 850
    },
    {
      "epoch": 0.07148530579825259,
      "grad_norm": 8.750042915344238,
      "learning_rate": 2e-05,
      "loss": 1.0869,
      "step": 900
    },
    {
      "epoch": 0.07545671167593328,
      "grad_norm": 9.523608207702637,
      "learning_rate": 2e-05,
      "loss": 1.0773,
      "step": 950
    },
    {
      "epoch": 0.07942811755361398,
      "grad_norm": 14.144851684570312,
      "learning_rate": 2e-05,
      "loss": 1.0581,
      "step": 1000
    },
    {
      "epoch": 0.07942811755361398,
      "eval_loss": 2.329904794692993,
      "eval_mse": 2.3293675393651845,
      "eval_pearson": 0.5053907069631404,
      "eval_runtime": 38.9244,
      "eval_samples_per_second": 1416.386,
      "eval_spearmanr": 0.5193747204124388,
      "eval_steps_per_second": 11.818,
      "step": 1000
    },
    {
      "epoch": 0.08339952343129468,
      "grad_norm": 15.035171508789062,
      "learning_rate": 2e-05,
      "loss": 1.0444,
      "step": 1050
    },
    {
      "epoch": 0.08737092930897537,
      "grad_norm": 14.624832153320312,
      "learning_rate": 2e-05,
      "loss": 1.0229,
      "step": 1100
    },
    {
      "epoch": 0.09134233518665608,
      "grad_norm": 11.062423706054688,
      "learning_rate": 2e-05,
      "loss": 1.0834,
      "step": 1150
    },
    {
      "epoch": 0.09531374106433678,
      "grad_norm": 7.344407558441162,
      "learning_rate": 2e-05,
      "loss": 1.0015,
      "step": 1200
    },
    {
      "epoch": 0.09928514694201747,
      "grad_norm": 18.198923110961914,
      "learning_rate": 2e-05,
      "loss": 1.0323,
      "step": 1250
    },
    {
      "epoch": 0.10325655281969817,
      "grad_norm": 7.084932327270508,
      "learning_rate": 2e-05,
      "loss": 0.9754,
      "step": 1300
    },
    {
      "epoch": 0.10722795869737888,
      "grad_norm": 15.261704444885254,
      "learning_rate": 2e-05,
      "loss": 1.0086,
      "step": 1350
    },
    {
      "epoch": 0.11119936457505956,
      "grad_norm": 11.670191764831543,
      "learning_rate": 2e-05,
      "loss": 1.0398,
      "step": 1400
    },
    {
      "epoch": 0.11517077045274027,
      "grad_norm": 11.241065979003906,
      "learning_rate": 2e-05,
      "loss": 0.9528,
      "step": 1450
    },
    {
      "epoch": 0.11914217633042097,
      "grad_norm": 13.51512336730957,
      "learning_rate": 2e-05,
      "loss": 0.9868,
      "step": 1500
    },
    {
      "epoch": 0.11914217633042097,
      "eval_loss": 2.2395107746124268,
      "eval_mse": 2.2393908277918215,
      "eval_pearson": 0.4965724429614159,
      "eval_runtime": 38.3173,
      "eval_samples_per_second": 1438.829,
      "eval_spearmanr": 0.5030081773836549,
      "eval_steps_per_second": 12.005,
      "step": 1500
    },
    {
      "epoch": 0.12311358220810167,
      "grad_norm": 16.33647918701172,
      "learning_rate": 2e-05,
      "loss": 1.0223,
      "step": 1550
    },
    {
      "epoch": 0.12708498808578236,
      "grad_norm": 10.812167167663574,
      "learning_rate": 2e-05,
      "loss": 0.954,
      "step": 1600
    },
    {
      "epoch": 0.13105639396346305,
      "grad_norm": 11.122114181518555,
      "learning_rate": 2e-05,
      "loss": 0.9839,
      "step": 1650
    },
    {
      "epoch": 0.13502779984114377,
      "grad_norm": 14.818586349487305,
      "learning_rate": 2e-05,
      "loss": 0.9902,
      "step": 1700
    },
    {
      "epoch": 0.13899920571882446,
      "grad_norm": 17.729520797729492,
      "learning_rate": 2e-05,
      "loss": 0.951,
      "step": 1750
    },
    {
      "epoch": 0.14297061159650518,
      "grad_norm": 10.209368705749512,
      "learning_rate": 2e-05,
      "loss": 0.9829,
      "step": 1800
    },
    {
      "epoch": 0.14694201747418587,
      "grad_norm": 18.119361877441406,
      "learning_rate": 2e-05,
      "loss": 0.9359,
      "step": 1850
    },
    {
      "epoch": 0.15091342335186655,
      "grad_norm": 17.515296936035156,
      "learning_rate": 2e-05,
      "loss": 0.917,
      "step": 1900
    },
    {
      "epoch": 0.15488482922954727,
      "grad_norm": 10.292546272277832,
      "learning_rate": 2e-05,
      "loss": 0.907,
      "step": 1950
    },
    {
      "epoch": 0.15885623510722796,
      "grad_norm": 7.390719413757324,
      "learning_rate": 2e-05,
      "loss": 0.9425,
      "step": 2000
    },
    {
      "epoch": 0.15885623510722796,
      "eval_loss": 2.172447443008423,
      "eval_mse": 2.1721432423077864,
      "eval_pearson": 0.4883661708440676,
      "eval_runtime": 38.886,
      "eval_samples_per_second": 1417.784,
      "eval_spearmanr": 0.49127892282886276,
      "eval_steps_per_second": 11.829,
      "step": 2000
    },
    {
      "epoch": 0.16282764098490865,
      "grad_norm": 13.420585632324219,
      "learning_rate": 2e-05,
      "loss": 0.9461,
      "step": 2050
    },
    {
      "epoch": 0.16679904686258937,
      "grad_norm": 13.588155746459961,
      "learning_rate": 2e-05,
      "loss": 0.9229,
      "step": 2100
    },
    {
      "epoch": 0.17077045274027006,
      "grad_norm": 11.012008666992188,
      "learning_rate": 2e-05,
      "loss": 0.9046,
      "step": 2150
    },
    {
      "epoch": 0.17474185861795075,
      "grad_norm": 10.402708053588867,
      "learning_rate": 2e-05,
      "loss": 0.9205,
      "step": 2200
    },
    {
      "epoch": 0.17871326449563146,
      "grad_norm": 11.97704029083252,
      "learning_rate": 2e-05,
      "loss": 0.9125,
      "step": 2250
    },
    {
      "epoch": 0.18268467037331215,
      "grad_norm": 13.355082511901855,
      "learning_rate": 2e-05,
      "loss": 0.9097,
      "step": 2300
    },
    {
      "epoch": 0.18665607625099284,
      "grad_norm": 16.424680709838867,
      "learning_rate": 2e-05,
      "loss": 0.9205,
      "step": 2350
    },
    {
      "epoch": 0.19062748212867356,
      "grad_norm": 14.607839584350586,
      "learning_rate": 2e-05,
      "loss": 0.8707,
      "step": 2400
    },
    {
      "epoch": 0.19459888800635425,
      "grad_norm": 10.493001937866211,
      "learning_rate": 2e-05,
      "loss": 0.8878,
      "step": 2450
    },
    {
      "epoch": 0.19857029388403494,
      "grad_norm": 12.193024635314941,
      "learning_rate": 2e-05,
      "loss": 0.9005,
      "step": 2500
    },
    {
      "epoch": 0.19857029388403494,
      "eval_loss": 2.362046241760254,
      "eval_mse": 2.3620177805056155,
      "eval_pearson": 0.46537896509075843,
      "eval_runtime": 38.7325,
      "eval_samples_per_second": 1423.405,
      "eval_spearmanr": 0.4737656649346325,
      "eval_steps_per_second": 11.876,
      "step": 2500
    },
    {
      "epoch": 0.20254169976171565,
      "grad_norm": 14.327752113342285,
      "learning_rate": 2e-05,
      "loss": 0.9129,
      "step": 2550
    },
    {
      "epoch": 0.20651310563939634,
      "grad_norm": 17.770212173461914,
      "learning_rate": 2e-05,
      "loss": 0.9049,
      "step": 2600
    },
    {
      "epoch": 0.21048451151707703,
      "grad_norm": 16.52044677734375,
      "learning_rate": 2e-05,
      "loss": 0.862,
      "step": 2650
    },
    {
      "epoch": 0.21445591739475775,
      "grad_norm": 20.182369232177734,
      "learning_rate": 2e-05,
      "loss": 0.8612,
      "step": 2700
    },
    {
      "epoch": 0.21842732327243844,
      "grad_norm": 14.214755058288574,
      "learning_rate": 2e-05,
      "loss": 0.875,
      "step": 2750
    },
    {
      "epoch": 0.22239872915011913,
      "grad_norm": 15.972296714782715,
      "learning_rate": 2e-05,
      "loss": 0.8579,
      "step": 2800
    },
    {
      "epoch": 0.22637013502779985,
      "grad_norm": 11.957767486572266,
      "learning_rate": 2e-05,
      "loss": 0.89,
      "step": 2850
    },
    {
      "epoch": 0.23034154090548054,
      "grad_norm": 15.193829536437988,
      "learning_rate": 2e-05,
      "loss": 0.8269,
      "step": 2900
    },
    {
      "epoch": 0.23431294678316125,
      "grad_norm": 14.835867881774902,
      "learning_rate": 2e-05,
      "loss": 0.8147,
      "step": 2950
    },
    {
      "epoch": 0.23828435266084194,
      "grad_norm": 10.907630920410156,
      "learning_rate": 2e-05,
      "loss": 0.8323,
      "step": 3000
    },
    {
      "epoch": 0.23828435266084194,
      "eval_loss": 1.6475552320480347,
      "eval_mse": 1.6474964265500582,
      "eval_pearson": 0.45690649286654805,
      "eval_runtime": 38.8937,
      "eval_samples_per_second": 1417.504,
      "eval_spearmanr": 0.4662573860868559,
      "eval_steps_per_second": 11.827,
      "step": 3000
    },
    {
      "epoch": 0.24225575853852263,
      "grad_norm": 12.321954727172852,
      "learning_rate": 2e-05,
      "loss": 0.8273,
      "step": 3050
    },
    {
      "epoch": 0.24622716441620335,
      "grad_norm": 14.682475090026855,
      "learning_rate": 2e-05,
      "loss": 0.8127,
      "step": 3100
    },
    {
      "epoch": 0.25019857029388404,
      "grad_norm": 13.57192325592041,
      "learning_rate": 2e-05,
      "loss": 0.7803,
      "step": 3150
    },
    {
      "epoch": 0.2541699761715647,
      "grad_norm": 17.729957580566406,
      "learning_rate": 2e-05,
      "loss": 0.8414,
      "step": 3200
    },
    {
      "epoch": 0.2581413820492454,
      "grad_norm": 10.08948040008545,
      "learning_rate": 2e-05,
      "loss": 0.8354,
      "step": 3250
    },
    {
      "epoch": 0.2621127879269261,
      "grad_norm": 13.29136848449707,
      "learning_rate": 2e-05,
      "loss": 0.8421,
      "step": 3300
    },
    {
      "epoch": 0.26608419380460685,
      "grad_norm": 16.13437271118164,
      "learning_rate": 2e-05,
      "loss": 0.8193,
      "step": 3350
    },
    {
      "epoch": 0.27005559968228754,
      "grad_norm": 13.46451187133789,
      "learning_rate": 2e-05,
      "loss": 0.8125,
      "step": 3400
    },
    {
      "epoch": 0.27402700555996823,
      "grad_norm": 9.021159172058105,
      "learning_rate": 2e-05,
      "loss": 0.8167,
      "step": 3450
    },
    {
      "epoch": 0.2779984114376489,
      "grad_norm": 13.014342308044434,
      "learning_rate": 2e-05,
      "loss": 0.8123,
      "step": 3500
    },
    {
      "epoch": 0.2779984114376489,
      "eval_loss": 2.480153799057007,
      "eval_mse": 2.4799965178379253,
      "eval_pearson": 0.48723180228134444,
      "eval_runtime": 38.3162,
      "eval_samples_per_second": 1438.87,
      "eval_spearmanr": 0.4867654961266955,
      "eval_steps_per_second": 12.005,
      "step": 3500
    },
    {
      "epoch": 0.2819698173153296,
      "grad_norm": 15.6411714553833,
      "learning_rate": 2e-05,
      "loss": 0.7822,
      "step": 3550
    },
    {
      "epoch": 0.28594122319301035,
      "grad_norm": 17.28482437133789,
      "learning_rate": 2e-05,
      "loss": 0.8269,
      "step": 3600
    },
    {
      "epoch": 0.28991262907069104,
      "grad_norm": 25.090274810791016,
      "learning_rate": 2e-05,
      "loss": 0.82,
      "step": 3650
    },
    {
      "epoch": 0.29388403494837173,
      "grad_norm": 13.726791381835938,
      "learning_rate": 2e-05,
      "loss": 0.7759,
      "step": 3700
    },
    {
      "epoch": 0.2978554408260524,
      "grad_norm": 11.574103355407715,
      "learning_rate": 2e-05,
      "loss": 0.7704,
      "step": 3750
    },
    {
      "epoch": 0.3018268467037331,
      "grad_norm": 10.553818702697754,
      "learning_rate": 2e-05,
      "loss": 0.7739,
      "step": 3800
    },
    {
      "epoch": 0.3057982525814138,
      "grad_norm": 16.62685203552246,
      "learning_rate": 2e-05,
      "loss": 0.8219,
      "step": 3850
    },
    {
      "epoch": 0.30976965845909454,
      "grad_norm": 20.25404167175293,
      "learning_rate": 2e-05,
      "loss": 0.7835,
      "step": 3900
    },
    {
      "epoch": 0.31374106433677523,
      "grad_norm": 12.361505508422852,
      "learning_rate": 2e-05,
      "loss": 0.7462,
      "step": 3950
    },
    {
      "epoch": 0.3177124702144559,
      "grad_norm": 16.586374282836914,
      "learning_rate": 2e-05,
      "loss": 0.7891,
      "step": 4000
    },
    {
      "epoch": 0.3177124702144559,
      "eval_loss": 2.2240989208221436,
      "eval_mse": 2.22337154710119,
      "eval_pearson": 0.47577119088213193,
      "eval_runtime": 38.9338,
      "eval_samples_per_second": 1416.045,
      "eval_spearmanr": 0.4794578331793148,
      "eval_steps_per_second": 11.815,
      "step": 4000
    },
    {
      "epoch": 0.3216838760921366,
      "grad_norm": 15.748615264892578,
      "learning_rate": 2e-05,
      "loss": 0.7754,
      "step": 4050
    },
    {
      "epoch": 0.3256552819698173,
      "grad_norm": 9.624612808227539,
      "learning_rate": 2e-05,
      "loss": 0.798,
      "step": 4100
    },
    {
      "epoch": 0.329626687847498,
      "grad_norm": 19.089344024658203,
      "learning_rate": 2e-05,
      "loss": 0.7217,
      "step": 4150
    },
    {
      "epoch": 0.33359809372517873,
      "grad_norm": 17.63349151611328,
      "learning_rate": 2e-05,
      "loss": 0.7563,
      "step": 4200
    },
    {
      "epoch": 0.3375694996028594,
      "grad_norm": 14.47409725189209,
      "learning_rate": 2e-05,
      "loss": 0.7691,
      "step": 4250
    },
    {
      "epoch": 0.3415409054805401,
      "grad_norm": 10.019146919250488,
      "learning_rate": 2e-05,
      "loss": 0.7493,
      "step": 4300
    },
    {
      "epoch": 0.3455123113582208,
      "grad_norm": 15.148628234863281,
      "learning_rate": 2e-05,
      "loss": 0.7642,
      "step": 4350
    },
    {
      "epoch": 0.3494837172359015,
      "grad_norm": 22.10051155090332,
      "learning_rate": 2e-05,
      "loss": 0.7577,
      "step": 4400
    },
    {
      "epoch": 0.3534551231135822,
      "grad_norm": 11.172929763793945,
      "learning_rate": 2e-05,
      "loss": 0.7229,
      "step": 4450
    },
    {
      "epoch": 0.3574265289912629,
      "grad_norm": 15.316251754760742,
      "learning_rate": 2e-05,
      "loss": 0.7778,
      "step": 4500
    },
    {
      "epoch": 0.3574265289912629,
      "eval_loss": 2.266510248184204,
      "eval_mse": 2.2661967486357173,
      "eval_pearson": 0.4738805411647141,
      "eval_runtime": 38.9338,
      "eval_samples_per_second": 1416.046,
      "eval_spearmanr": 0.4829597808192096,
      "eval_steps_per_second": 11.815,
      "step": 4500
    },
    {
      "epoch": 0.3613979348689436,
      "grad_norm": 11.694838523864746,
      "learning_rate": 2e-05,
      "loss": 0.7469,
      "step": 4550
    },
    {
      "epoch": 0.3653693407466243,
      "grad_norm": 18.030086517333984,
      "learning_rate": 2e-05,
      "loss": 0.7063,
      "step": 4600
    },
    {
      "epoch": 0.369340746624305,
      "grad_norm": 14.193029403686523,
      "learning_rate": 2e-05,
      "loss": 0.7314,
      "step": 4650
    },
    {
      "epoch": 0.3733121525019857,
      "grad_norm": 16.923311233520508,
      "learning_rate": 2e-05,
      "loss": 0.7566,
      "step": 4700
    },
    {
      "epoch": 0.37728355837966643,
      "grad_norm": 12.392088890075684,
      "learning_rate": 2e-05,
      "loss": 0.7084,
      "step": 4750
    },
    {
      "epoch": 0.3812549642573471,
      "grad_norm": 14.12427043914795,
      "learning_rate": 2e-05,
      "loss": 0.7732,
      "step": 4800
    },
    {
      "epoch": 0.3852263701350278,
      "grad_norm": 19.23697280883789,
      "learning_rate": 2e-05,
      "loss": 0.7312,
      "step": 4850
    },
    {
      "epoch": 0.3891977760127085,
      "grad_norm": 12.954909324645996,
      "learning_rate": 2e-05,
      "loss": 0.692,
      "step": 4900
    },
    {
      "epoch": 0.3931691818903892,
      "grad_norm": 12.615019798278809,
      "learning_rate": 2e-05,
      "loss": 0.7512,
      "step": 4950
    },
    {
      "epoch": 0.3971405877680699,
      "grad_norm": 12.126391410827637,
      "learning_rate": 2e-05,
      "loss": 0.678,
      "step": 5000
    },
    {
      "epoch": 0.3971405877680699,
      "eval_loss": 1.9307359457015991,
      "eval_mse": 1.9303013225697747,
      "eval_pearson": 0.4162615612950827,
      "eval_runtime": 38.9591,
      "eval_samples_per_second": 1415.124,
      "eval_spearmanr": 0.4434735276518765,
      "eval_steps_per_second": 11.807,
      "step": 5000
    },
    {
      "epoch": 0.4011119936457506,
      "grad_norm": 14.616156578063965,
      "learning_rate": 2e-05,
      "loss": 0.7274,
      "step": 5050
    },
    {
      "epoch": 0.4050833995234313,
      "grad_norm": 14.607202529907227,
      "learning_rate": 2e-05,
      "loss": 0.6896,
      "step": 5100
    },
    {
      "epoch": 0.409054805401112,
      "grad_norm": 12.047818183898926,
      "learning_rate": 2e-05,
      "loss": 0.6987,
      "step": 5150
    },
    {
      "epoch": 0.4130262112787927,
      "grad_norm": 14.809544563293457,
      "learning_rate": 2e-05,
      "loss": 0.6817,
      "step": 5200
    },
    {
      "epoch": 0.4169976171564734,
      "grad_norm": 10.5010404586792,
      "learning_rate": 2e-05,
      "loss": 0.691,
      "step": 5250
    },
    {
      "epoch": 0.42096902303415407,
      "grad_norm": 13.706686019897461,
      "learning_rate": 2e-05,
      "loss": 0.7132,
      "step": 5300
    },
    {
      "epoch": 0.4249404289118348,
      "grad_norm": 12.66915225982666,
      "learning_rate": 2e-05,
      "loss": 0.7084,
      "step": 5350
    },
    {
      "epoch": 0.4289118347895155,
      "grad_norm": 11.697881698608398,
      "learning_rate": 2e-05,
      "loss": 0.6781,
      "step": 5400
    },
    {
      "epoch": 0.4328832406671962,
      "grad_norm": 15.464094161987305,
      "learning_rate": 2e-05,
      "loss": 0.725,
      "step": 5450
    },
    {
      "epoch": 0.4368546465448769,
      "grad_norm": 11.374295234680176,
      "learning_rate": 2e-05,
      "loss": 0.7057,
      "step": 5500
    },
    {
      "epoch": 0.4368546465448769,
      "eval_loss": 2.0300700664520264,
      "eval_mse": 2.029919144157262,
      "eval_pearson": 0.4909676791444183,
      "eval_runtime": 38.9499,
      "eval_samples_per_second": 1415.46,
      "eval_spearmanr": 0.49214279191173893,
      "eval_steps_per_second": 11.81,
      "step": 5500
    },
    {
      "epoch": 0.44082605242255757,
      "grad_norm": 29.232263565063477,
      "learning_rate": 2e-05,
      "loss": 0.6876,
      "step": 5550
    },
    {
      "epoch": 0.44479745830023826,
      "grad_norm": 11.945473670959473,
      "learning_rate": 2e-05,
      "loss": 0.7086,
      "step": 5600
    },
    {
      "epoch": 0.448768864177919,
      "grad_norm": 12.766159057617188,
      "learning_rate": 2e-05,
      "loss": 0.6445,
      "step": 5650
    },
    {
      "epoch": 0.4527402700555997,
      "grad_norm": 9.328984260559082,
      "learning_rate": 2e-05,
      "loss": 0.6338,
      "step": 5700
    },
    {
      "epoch": 0.4567116759332804,
      "grad_norm": 14.705805778503418,
      "learning_rate": 2e-05,
      "loss": 0.7053,
      "step": 5750
    },
    {
      "epoch": 0.46068308181096107,
      "grad_norm": 21.27800178527832,
      "learning_rate": 2e-05,
      "loss": 0.6471,
      "step": 5800
    },
    {
      "epoch": 0.46465448768864176,
      "grad_norm": 25.497684478759766,
      "learning_rate": 2e-05,
      "loss": 0.6818,
      "step": 5850
    },
    {
      "epoch": 0.4686258935663225,
      "grad_norm": 33.115779876708984,
      "learning_rate": 2e-05,
      "loss": 0.6168,
      "step": 5900
    },
    {
      "epoch": 0.4725972994440032,
      "grad_norm": 12.291999816894531,
      "learning_rate": 2e-05,
      "loss": 0.6652,
      "step": 5950
    },
    {
      "epoch": 0.4765687053216839,
      "grad_norm": 13.330140113830566,
      "learning_rate": 2e-05,
      "loss": 0.6644,
      "step": 6000
    },
    {
      "epoch": 0.4765687053216839,
      "eval_loss": 2.3517491817474365,
      "eval_mse": 2.3512836732470332,
      "eval_pearson": 0.4110913865039223,
      "eval_runtime": 38.9265,
      "eval_samples_per_second": 1416.309,
      "eval_spearmanr": 0.4327414008347725,
      "eval_steps_per_second": 11.817,
      "step": 6000
    },
    {
      "epoch": 0.4805401111993646,
      "grad_norm": 12.99537181854248,
      "learning_rate": 2e-05,
      "loss": 0.685,
      "step": 6050
    },
    {
      "epoch": 0.48451151707704526,
      "grad_norm": 16.662145614624023,
      "learning_rate": 2e-05,
      "loss": 0.6653,
      "step": 6100
    },
    {
      "epoch": 0.48848292295472595,
      "grad_norm": 16.24030876159668,
      "learning_rate": 2e-05,
      "loss": 0.6518,
      "step": 6150
    },
    {
      "epoch": 0.4924543288324067,
      "grad_norm": 18.841812133789062,
      "learning_rate": 2e-05,
      "loss": 0.6715,
      "step": 6200
    },
    {
      "epoch": 0.4964257347100874,
      "grad_norm": 11.543652534484863,
      "learning_rate": 2e-05,
      "loss": 0.6568,
      "step": 6250
    },
    {
      "epoch": 0.5003971405877681,
      "grad_norm": 12.567648887634277,
      "learning_rate": 2e-05,
      "loss": 0.6341,
      "step": 6300
    },
    {
      "epoch": 0.5043685464654488,
      "grad_norm": 11.900442123413086,
      "learning_rate": 2e-05,
      "loss": 0.6551,
      "step": 6350
    },
    {
      "epoch": 0.5083399523431295,
      "grad_norm": 11.425633430480957,
      "learning_rate": 2e-05,
      "loss": 0.6404,
      "step": 6400
    },
    {
      "epoch": 0.5123113582208102,
      "grad_norm": 15.521214485168457,
      "learning_rate": 2e-05,
      "loss": 0.631,
      "step": 6450
    },
    {
      "epoch": 0.5162827640984908,
      "grad_norm": 11.084749221801758,
      "learning_rate": 2e-05,
      "loss": 0.6212,
      "step": 6500
    },
    {
      "epoch": 0.5162827640984908,
      "eval_loss": 1.8453552722930908,
      "eval_mse": 1.8450371033641724,
      "eval_pearson": 0.49119925803095366,
      "eval_runtime": 38.9785,
      "eval_samples_per_second": 1414.422,
      "eval_spearmanr": 0.4980406099480029,
      "eval_steps_per_second": 11.801,
      "step": 6500
    },
    {
      "epoch": 0.5202541699761716,
      "grad_norm": 10.867938041687012,
      "learning_rate": 2e-05,
      "loss": 0.6554,
      "step": 6550
    },
    {
      "epoch": 0.5242255758538522,
      "grad_norm": 14.323691368103027,
      "learning_rate": 2e-05,
      "loss": 0.6422,
      "step": 6600
    },
    {
      "epoch": 0.528196981731533,
      "grad_norm": 13.398982048034668,
      "learning_rate": 2e-05,
      "loss": 0.6091,
      "step": 6650
    },
    {
      "epoch": 0.5321683876092137,
      "grad_norm": 10.942957878112793,
      "learning_rate": 2e-05,
      "loss": 0.6359,
      "step": 6700
    },
    {
      "epoch": 0.5361397934868943,
      "grad_norm": 12.852925300598145,
      "learning_rate": 2e-05,
      "loss": 0.586,
      "step": 6750
    },
    {
      "epoch": 0.5401111993645751,
      "grad_norm": 34.123931884765625,
      "learning_rate": 2e-05,
      "loss": 0.6297,
      "step": 6800
    },
    {
      "epoch": 0.5440826052422557,
      "grad_norm": 12.0291166305542,
      "learning_rate": 2e-05,
      "loss": 0.6073,
      "step": 6850
    },
    {
      "epoch": 0.5480540111199365,
      "grad_norm": 22.545692443847656,
      "learning_rate": 2e-05,
      "loss": 0.6093,
      "step": 6900
    },
    {
      "epoch": 0.5520254169976172,
      "grad_norm": 13.766809463500977,
      "learning_rate": 2e-05,
      "loss": 0.6077,
      "step": 6950
    },
    {
      "epoch": 0.5559968228752978,
      "grad_norm": 12.970190048217773,
      "learning_rate": 2e-05,
      "loss": 0.5882,
      "step": 7000
    },
    {
      "epoch": 0.5559968228752978,
      "eval_loss": 2.1256890296936035,
      "eval_mse": 2.1254431238448093,
      "eval_pearson": 0.48643414426534787,
      "eval_runtime": 38.8989,
      "eval_samples_per_second": 1417.315,
      "eval_spearmanr": 0.4949877611664486,
      "eval_steps_per_second": 11.826,
      "step": 7000
    },
    {
      "epoch": 0.5599682287529786,
      "grad_norm": 14.371223449707031,
      "learning_rate": 2e-05,
      "loss": 0.6102,
      "step": 7050
    },
    {
      "epoch": 0.5639396346306592,
      "grad_norm": 14.286293029785156,
      "learning_rate": 2e-05,
      "loss": 0.6082,
      "step": 7100
    },
    {
      "epoch": 0.56791104050834,
      "grad_norm": 15.020322799682617,
      "learning_rate": 2e-05,
      "loss": 0.6289,
      "step": 7150
    },
    {
      "epoch": 0.5718824463860207,
      "grad_norm": 20.40814971923828,
      "learning_rate": 2e-05,
      "loss": 0.6253,
      "step": 7200
    },
    {
      "epoch": 0.5758538522637013,
      "grad_norm": 14.132614135742188,
      "learning_rate": 2e-05,
      "loss": 0.6265,
      "step": 7250
    },
    {
      "epoch": 0.5798252581413821,
      "grad_norm": 8.4468355178833,
      "learning_rate": 2e-05,
      "loss": 0.6123,
      "step": 7300
    },
    {
      "epoch": 0.5837966640190627,
      "grad_norm": 18.39106559753418,
      "learning_rate": 2e-05,
      "loss": 0.6109,
      "step": 7350
    },
    {
      "epoch": 0.5877680698967435,
      "grad_norm": 13.984782218933105,
      "learning_rate": 2e-05,
      "loss": 0.561,
      "step": 7400
    },
    {
      "epoch": 0.5917394757744241,
      "grad_norm": 18.17032814025879,
      "learning_rate": 2e-05,
      "loss": 0.5749,
      "step": 7450
    },
    {
      "epoch": 0.5957108816521048,
      "grad_norm": 21.984539031982422,
      "learning_rate": 2e-05,
      "loss": 0.5987,
      "step": 7500
    },
    {
      "epoch": 0.5957108816521048,
      "eval_loss": 2.179295778274536,
      "eval_mse": 2.1787414831225362,
      "eval_pearson": 0.5164531731204351,
      "eval_runtime": 38.7452,
      "eval_samples_per_second": 1422.938,
      "eval_spearmanr": 0.5248429103907944,
      "eval_steps_per_second": 11.872,
      "step": 7500
    },
    {
      "epoch": 0.5996822875297856,
      "grad_norm": 15.690851211547852,
      "learning_rate": 2e-05,
      "loss": 0.5903,
      "step": 7550
    },
    {
      "epoch": 0.6036536934074662,
      "grad_norm": 16.36664390563965,
      "learning_rate": 2e-05,
      "loss": 0.561,
      "step": 7600
    },
    {
      "epoch": 0.607625099285147,
      "grad_norm": 12.501832962036133,
      "learning_rate": 2e-05,
      "loss": 0.5972,
      "step": 7650
    },
    {
      "epoch": 0.6115965051628276,
      "grad_norm": 13.890914916992188,
      "learning_rate": 2e-05,
      "loss": 0.5727,
      "step": 7700
    },
    {
      "epoch": 0.6155679110405083,
      "grad_norm": 11.852864265441895,
      "learning_rate": 2e-05,
      "loss": 0.623,
      "step": 7750
    },
    {
      "epoch": 0.6195393169181891,
      "grad_norm": 13.029851913452148,
      "learning_rate": 2e-05,
      "loss": 0.5729,
      "step": 7800
    },
    {
      "epoch": 0.6235107227958697,
      "grad_norm": 14.812576293945312,
      "learning_rate": 2e-05,
      "loss": 0.5945,
      "step": 7850
    },
    {
      "epoch": 0.6274821286735505,
      "grad_norm": 11.261337280273438,
      "learning_rate": 2e-05,
      "loss": 0.5666,
      "step": 7900
    },
    {
      "epoch": 0.6314535345512311,
      "grad_norm": 20.974769592285156,
      "learning_rate": 2e-05,
      "loss": 0.6014,
      "step": 7950
    },
    {
      "epoch": 0.6354249404289118,
      "grad_norm": 13.547707557678223,
      "learning_rate": 2e-05,
      "loss": 0.5679,
      "step": 8000
    },
    {
      "epoch": 0.6354249404289118,
      "eval_loss": 2.019111394882202,
      "eval_mse": 2.018557877760446,
      "eval_pearson": 0.5094229119799519,
      "eval_runtime": 38.9439,
      "eval_samples_per_second": 1415.679,
      "eval_spearmanr": 0.5244783537244713,
      "eval_steps_per_second": 11.812,
      "step": 8000
    },
    {
      "epoch": 0.6393963463065926,
      "grad_norm": 11.156755447387695,
      "learning_rate": 2e-05,
      "loss": 0.5609,
      "step": 8050
    },
    {
      "epoch": 0.6433677521842732,
      "grad_norm": 10.532243728637695,
      "learning_rate": 2e-05,
      "loss": 0.5837,
      "step": 8100
    },
    {
      "epoch": 0.647339158061954,
      "grad_norm": 12.61867618560791,
      "learning_rate": 2e-05,
      "loss": 0.5715,
      "step": 8150
    },
    {
      "epoch": 0.6513105639396346,
      "grad_norm": 13.837369918823242,
      "learning_rate": 2e-05,
      "loss": 0.5856,
      "step": 8200
    },
    {
      "epoch": 0.6552819698173153,
      "grad_norm": 14.521502494812012,
      "learning_rate": 2e-05,
      "loss": 0.5754,
      "step": 8250
    },
    {
      "epoch": 0.659253375694996,
      "grad_norm": 10.925289154052734,
      "learning_rate": 2e-05,
      "loss": 0.5478,
      "step": 8300
    },
    {
      "epoch": 0.6632247815726767,
      "grad_norm": 14.895631790161133,
      "learning_rate": 2e-05,
      "loss": 0.5568,
      "step": 8350
    },
    {
      "epoch": 0.6671961874503575,
      "grad_norm": 10.465271949768066,
      "learning_rate": 2e-05,
      "loss": 0.5314,
      "step": 8400
    },
    {
      "epoch": 0.6711675933280381,
      "grad_norm": 17.865554809570312,
      "learning_rate": 2e-05,
      "loss": 0.5541,
      "step": 8450
    },
    {
      "epoch": 0.6751389992057188,
      "grad_norm": 12.639333724975586,
      "learning_rate": 2e-05,
      "loss": 0.5656,
      "step": 8500
    },
    {
      "epoch": 0.6751389992057188,
      "eval_loss": 2.0833497047424316,
      "eval_mse": 2.082475076023245,
      "eval_pearson": 0.47878360037947565,
      "eval_runtime": 38.929,
      "eval_samples_per_second": 1416.221,
      "eval_spearmanr": 0.5000626549197835,
      "eval_steps_per_second": 11.816,
      "step": 8500
    },
    {
      "epoch": 0.6791104050833995,
      "grad_norm": 9.043925285339355,
      "learning_rate": 2e-05,
      "loss": 0.5396,
      "step": 8550
    },
    {
      "epoch": 0.6830818109610802,
      "grad_norm": 11.205466270446777,
      "learning_rate": 2e-05,
      "loss": 0.5628,
      "step": 8600
    },
    {
      "epoch": 0.687053216838761,
      "grad_norm": 16.020231246948242,
      "learning_rate": 2e-05,
      "loss": 0.5335,
      "step": 8650
    },
    {
      "epoch": 0.6910246227164416,
      "grad_norm": 11.742071151733398,
      "learning_rate": 2e-05,
      "loss": 0.5325,
      "step": 8700
    },
    {
      "epoch": 0.6949960285941224,
      "grad_norm": 12.223122596740723,
      "learning_rate": 2e-05,
      "loss": 0.5578,
      "step": 8750
    },
    {
      "epoch": 0.698967434471803,
      "grad_norm": 10.43203353881836,
      "learning_rate": 2e-05,
      "loss": 0.5498,
      "step": 8800
    },
    {
      "epoch": 0.7029388403494837,
      "grad_norm": 10.479650497436523,
      "learning_rate": 2e-05,
      "loss": 0.5744,
      "step": 8850
    },
    {
      "epoch": 0.7069102462271644,
      "grad_norm": 17.779705047607422,
      "learning_rate": 2e-05,
      "loss": 0.5141,
      "step": 8900
    },
    {
      "epoch": 0.7108816521048451,
      "grad_norm": 15.059798240661621,
      "learning_rate": 2e-05,
      "loss": 0.5523,
      "step": 8950
    },
    {
      "epoch": 0.7148530579825259,
      "grad_norm": 12.864444732666016,
      "learning_rate": 2e-05,
      "loss": 0.5356,
      "step": 9000
    },
    {
      "epoch": 0.7148530579825259,
      "eval_loss": 2.0445802211761475,
      "eval_mse": 2.0440314091152283,
      "eval_pearson": 0.4598153165695012,
      "eval_runtime": 38.4133,
      "eval_samples_per_second": 1435.231,
      "eval_spearmanr": 0.48106481121875033,
      "eval_steps_per_second": 11.975,
      "step": 9000
    },
    {
      "epoch": 0.7188244638602065,
      "grad_norm": 16.588266372680664,
      "learning_rate": 2e-05,
      "loss": 0.5516,
      "step": 9050
    },
    {
      "epoch": 0.7227958697378872,
      "grad_norm": 14.657552719116211,
      "learning_rate": 2e-05,
      "loss": 0.5643,
      "step": 9100
    },
    {
      "epoch": 0.7267672756155679,
      "grad_norm": 15.337239265441895,
      "learning_rate": 2e-05,
      "loss": 0.5291,
      "step": 9150
    },
    {
      "epoch": 0.7307386814932486,
      "grad_norm": 12.246013641357422,
      "learning_rate": 2e-05,
      "loss": 0.5282,
      "step": 9200
    },
    {
      "epoch": 0.7347100873709294,
      "grad_norm": 12.546231269836426,
      "learning_rate": 2e-05,
      "loss": 0.5233,
      "step": 9250
    },
    {
      "epoch": 0.73868149324861,
      "grad_norm": 18.621957778930664,
      "learning_rate": 2e-05,
      "loss": 0.5469,
      "step": 9300
    },
    {
      "epoch": 0.7426528991262907,
      "grad_norm": 12.873438835144043,
      "learning_rate": 2e-05,
      "loss": 0.5357,
      "step": 9350
    },
    {
      "epoch": 0.7466243050039714,
      "grad_norm": 14.649953842163086,
      "learning_rate": 2e-05,
      "loss": 0.5185,
      "step": 9400
    },
    {
      "epoch": 0.7505957108816521,
      "grad_norm": 14.805234909057617,
      "learning_rate": 2e-05,
      "loss": 0.5464,
      "step": 9450
    },
    {
      "epoch": 0.7545671167593329,
      "grad_norm": 14.644816398620605,
      "learning_rate": 2e-05,
      "loss": 0.5142,
      "step": 9500
    },
    {
      "epoch": 0.7545671167593329,
      "eval_loss": 2.2210192680358887,
      "eval_mse": 2.2207843956607074,
      "eval_pearson": 0.5093354810453579,
      "eval_runtime": 38.7901,
      "eval_samples_per_second": 1421.29,
      "eval_spearmanr": 0.5192913525092696,
      "eval_steps_per_second": 11.859,
      "step": 9500
    },
    {
      "epoch": 0.7585385226370135,
      "grad_norm": 19.858440399169922,
      "learning_rate": 2e-05,
      "loss": 0.5361,
      "step": 9550
    },
    {
      "epoch": 0.7625099285146942,
      "grad_norm": 14.825722694396973,
      "learning_rate": 2e-05,
      "loss": 0.5063,
      "step": 9600
    },
    {
      "epoch": 0.7664813343923749,
      "grad_norm": 11.014215469360352,
      "learning_rate": 2e-05,
      "loss": 0.5377,
      "step": 9650
    },
    {
      "epoch": 0.7704527402700556,
      "grad_norm": 7.9722580909729,
      "learning_rate": 2e-05,
      "loss": 0.5158,
      "step": 9700
    },
    {
      "epoch": 0.7744241461477362,
      "grad_norm": 13.467750549316406,
      "learning_rate": 2e-05,
      "loss": 0.5504,
      "step": 9750
    },
    {
      "epoch": 0.778395552025417,
      "grad_norm": 15.272906303405762,
      "learning_rate": 2e-05,
      "loss": 0.5141,
      "step": 9800
    },
    {
      "epoch": 0.7823669579030977,
      "grad_norm": 13.676804542541504,
      "learning_rate": 2e-05,
      "loss": 0.5078,
      "step": 9850
    },
    {
      "epoch": 0.7863383637807784,
      "grad_norm": 7.6838483810424805,
      "learning_rate": 2e-05,
      "loss": 0.5036,
      "step": 9900
    },
    {
      "epoch": 0.7903097696584591,
      "grad_norm": 20.238239288330078,
      "learning_rate": 2e-05,
      "loss": 0.5,
      "step": 9950
    },
    {
      "epoch": 0.7942811755361397,
      "grad_norm": 14.703752517700195,
      "learning_rate": 2e-05,
      "loss": 0.4925,
      "step": 10000
    },
    {
      "epoch": 0.7942811755361397,
      "eval_loss": 1.7496553659439087,
      "eval_mse": 1.7490260493839236,
      "eval_pearson": 0.42090842130167405,
      "eval_runtime": 38.5332,
      "eval_samples_per_second": 1430.766,
      "eval_spearmanr": 0.44414629995231625,
      "eval_steps_per_second": 11.938,
      "step": 10000
    },
    {
      "epoch": 0.7982525814138205,
      "grad_norm": 16.00139045715332,
      "learning_rate": 2e-05,
      "loss": 0.5169,
      "step": 10050
    },
    {
      "epoch": 0.8022239872915012,
      "grad_norm": 9.269623756408691,
      "learning_rate": 2e-05,
      "loss": 0.4975,
      "step": 10100
    },
    {
      "epoch": 0.8061953931691819,
      "grad_norm": 11.394576072692871,
      "learning_rate": 2e-05,
      "loss": 0.5179,
      "step": 10150
    },
    {
      "epoch": 0.8101667990468626,
      "grad_norm": 18.8260440826416,
      "learning_rate": 2e-05,
      "loss": 0.5393,
      "step": 10200
    },
    {
      "epoch": 0.8141382049245433,
      "grad_norm": 11.214956283569336,
      "learning_rate": 2e-05,
      "loss": 0.5128,
      "step": 10250
    },
    {
      "epoch": 0.818109610802224,
      "grad_norm": 13.564400672912598,
      "learning_rate": 2e-05,
      "loss": 0.5035,
      "step": 10300
    },
    {
      "epoch": 0.8220810166799047,
      "grad_norm": 10.349615097045898,
      "learning_rate": 2e-05,
      "loss": 0.5064,
      "step": 10350
    },
    {
      "epoch": 0.8260524225575854,
      "grad_norm": 16.99013328552246,
      "learning_rate": 2e-05,
      "loss": 0.5344,
      "step": 10400
    },
    {
      "epoch": 0.8300238284352661,
      "grad_norm": 13.414935111999512,
      "learning_rate": 2e-05,
      "loss": 0.4921,
      "step": 10450
    },
    {
      "epoch": 0.8339952343129468,
      "grad_norm": 10.139684677124023,
      "learning_rate": 2e-05,
      "loss": 0.4822,
      "step": 10500
    },
    {
      "epoch": 0.8339952343129468,
      "eval_loss": 2.160649061203003,
      "eval_mse": 2.160412513902965,
      "eval_pearson": 0.47293693440524,
      "eval_runtime": 38.5735,
      "eval_samples_per_second": 1429.27,
      "eval_spearmanr": 0.4806618726872146,
      "eval_steps_per_second": 11.925,
      "step": 10500
    },
    {
      "epoch": 0.8379666401906275,
      "grad_norm": 11.760217666625977,
      "learning_rate": 2e-05,
      "loss": 0.4741,
      "step": 10550
    },
    {
      "epoch": 0.8419380460683081,
      "grad_norm": 11.004039764404297,
      "learning_rate": 2e-05,
      "loss": 0.4885,
      "step": 10600
    },
    {
      "epoch": 0.8459094519459889,
      "grad_norm": 10.808449745178223,
      "learning_rate": 2e-05,
      "loss": 0.4801,
      "step": 10650
    },
    {
      "epoch": 0.8498808578236696,
      "grad_norm": 16.36687469482422,
      "learning_rate": 2e-05,
      "loss": 0.5017,
      "step": 10700
    },
    {
      "epoch": 0.8538522637013503,
      "grad_norm": 9.728631019592285,
      "learning_rate": 2e-05,
      "loss": 0.5183,
      "step": 10750
    },
    {
      "epoch": 0.857823669579031,
      "grad_norm": 14.838491439819336,
      "learning_rate": 2e-05,
      "loss": 0.5025,
      "step": 10800
    },
    {
      "epoch": 0.8617950754567116,
      "grad_norm": 23.84255599975586,
      "learning_rate": 2e-05,
      "loss": 0.4864,
      "step": 10850
    },
    {
      "epoch": 0.8657664813343924,
      "grad_norm": 9.827221870422363,
      "learning_rate": 2e-05,
      "loss": 0.4691,
      "step": 10900
    },
    {
      "epoch": 0.8697378872120731,
      "grad_norm": 11.896946907043457,
      "learning_rate": 2e-05,
      "loss": 0.4747,
      "step": 10950
    },
    {
      "epoch": 0.8737092930897538,
      "grad_norm": 10.0236234664917,
      "learning_rate": 2e-05,
      "loss": 0.4853,
      "step": 11000
    },
    {
      "epoch": 0.8737092930897538,
      "eval_loss": 1.9533045291900635,
      "eval_mse": 1.9529052698036569,
      "eval_pearson": 0.4633151920407216,
      "eval_runtime": 38.7331,
      "eval_samples_per_second": 1423.382,
      "eval_spearmanr": 0.47388473605186443,
      "eval_steps_per_second": 11.876,
      "step": 11000
    },
    {
      "epoch": 0.8776806989674345,
      "grad_norm": 15.71738338470459,
      "learning_rate": 2e-05,
      "loss": 0.4742,
      "step": 11050
    },
    {
      "epoch": 0.8816521048451151,
      "grad_norm": 15.497909545898438,
      "learning_rate": 2e-05,
      "loss": 0.4852,
      "step": 11100
    },
    {
      "epoch": 0.8856235107227959,
      "grad_norm": 14.947338104248047,
      "learning_rate": 2e-05,
      "loss": 0.492,
      "step": 11150
    },
    {
      "epoch": 0.8895949166004765,
      "grad_norm": 12.669053077697754,
      "learning_rate": 2e-05,
      "loss": 0.4972,
      "step": 11200
    },
    {
      "epoch": 0.8935663224781573,
      "grad_norm": 11.816987037658691,
      "learning_rate": 2e-05,
      "loss": 0.462,
      "step": 11250
    },
    {
      "epoch": 0.897537728355838,
      "grad_norm": 11.368663787841797,
      "learning_rate": 2e-05,
      "loss": 0.4946,
      "step": 11300
    },
    {
      "epoch": 0.9015091342335186,
      "grad_norm": 14.148427963256836,
      "learning_rate": 2e-05,
      "loss": 0.4942,
      "step": 11350
    },
    {
      "epoch": 0.9054805401111994,
      "grad_norm": 15.612747192382812,
      "learning_rate": 2e-05,
      "loss": 0.4623,
      "step": 11400
    },
    {
      "epoch": 0.90945194598888,
      "grad_norm": 12.973295211791992,
      "learning_rate": 2e-05,
      "loss": 0.4692,
      "step": 11450
    },
    {
      "epoch": 0.9134233518665608,
      "grad_norm": 12.937139511108398,
      "learning_rate": 2e-05,
      "loss": 0.455,
      "step": 11500
    },
    {
      "epoch": 0.9134233518665608,
      "eval_loss": 2.208775281906128,
      "eval_mse": 2.2083236649508122,
      "eval_pearson": 0.4038930495316882,
      "eval_runtime": 38.3708,
      "eval_samples_per_second": 1436.822,
      "eval_spearmanr": 0.42942761200473567,
      "eval_steps_per_second": 11.988,
      "step": 11500
    },
    {
      "epoch": 0.9173947577442415,
      "grad_norm": 13.015955924987793,
      "learning_rate": 2e-05,
      "loss": 0.4767,
      "step": 11550
    },
    {
      "epoch": 0.9213661636219221,
      "grad_norm": 12.639739036560059,
      "learning_rate": 2e-05,
      "loss": 0.4526,
      "step": 11600
    },
    {
      "epoch": 0.9253375694996029,
      "grad_norm": 14.247420310974121,
      "learning_rate": 2e-05,
      "loss": 0.459,
      "step": 11650
    },
    {
      "epoch": 0.9293089753772835,
      "grad_norm": 11.882623672485352,
      "learning_rate": 2e-05,
      "loss": 0.4767,
      "step": 11700
    },
    {
      "epoch": 0.9332803812549643,
      "grad_norm": 18.120601654052734,
      "learning_rate": 2e-05,
      "loss": 0.4642,
      "step": 11750
    },
    {
      "epoch": 0.937251787132645,
      "grad_norm": 21.26953887939453,
      "learning_rate": 2e-05,
      "loss": 0.4359,
      "step": 11800
    },
    {
      "epoch": 0.9412231930103256,
      "grad_norm": 11.466399192810059,
      "learning_rate": 2e-05,
      "loss": 0.4907,
      "step": 11850
    },
    {
      "epoch": 0.9451945988880064,
      "grad_norm": 13.728879928588867,
      "learning_rate": 2e-05,
      "loss": 0.4522,
      "step": 11900
    },
    {
      "epoch": 0.949166004765687,
      "grad_norm": 16.68488121032715,
      "learning_rate": 2e-05,
      "loss": 0.4612,
      "step": 11950
    },
    {
      "epoch": 0.9531374106433678,
      "grad_norm": 13.332747459411621,
      "learning_rate": 2e-05,
      "loss": 0.4699,
      "step": 12000
    },
    {
      "epoch": 0.9531374106433678,
      "eval_loss": 2.1091225147247314,
      "eval_mse": 2.108770906311122,
      "eval_pearson": 0.4546584214229331,
      "eval_runtime": 38.7471,
      "eval_samples_per_second": 1422.867,
      "eval_spearmanr": 0.46631331053195857,
      "eval_steps_per_second": 11.872,
      "step": 12000
    },
    {
      "epoch": 0.9571088165210484,
      "grad_norm": 20.49355697631836,
      "learning_rate": 2e-05,
      "loss": 0.4802,
      "step": 12050
    },
    {
      "epoch": 0.9610802223987291,
      "grad_norm": 16.075748443603516,
      "learning_rate": 2e-05,
      "loss": 0.4669,
      "step": 12100
    },
    {
      "epoch": 0.9650516282764099,
      "grad_norm": 12.575979232788086,
      "learning_rate": 2e-05,
      "loss": 0.4493,
      "step": 12150
    },
    {
      "epoch": 0.9690230341540905,
      "grad_norm": 12.356815338134766,
      "learning_rate": 2e-05,
      "loss": 0.4661,
      "step": 12200
    },
    {
      "epoch": 0.9729944400317713,
      "grad_norm": 10.203320503234863,
      "learning_rate": 2e-05,
      "loss": 0.4288,
      "step": 12250
    },
    {
      "epoch": 0.9769658459094519,
      "grad_norm": 11.171967506408691,
      "learning_rate": 2e-05,
      "loss": 0.4509,
      "step": 12300
    },
    {
      "epoch": 0.9809372517871326,
      "grad_norm": 11.074141502380371,
      "learning_rate": 2e-05,
      "loss": 0.4628,
      "step": 12350
    },
    {
      "epoch": 0.9849086576648134,
      "grad_norm": 9.253753662109375,
      "learning_rate": 2e-05,
      "loss": 0.4435,
      "step": 12400
    },
    {
      "epoch": 0.988880063542494,
      "grad_norm": 14.376739501953125,
      "learning_rate": 2e-05,
      "loss": 0.4308,
      "step": 12450
    },
    {
      "epoch": 0.9928514694201748,
      "grad_norm": 13.637407302856445,
      "learning_rate": 2e-05,
      "loss": 0.4453,
      "step": 12500
    },
    {
      "epoch": 0.9928514694201748,
      "eval_loss": 1.8952277898788452,
      "eval_mse": 1.8945878187326448,
      "eval_pearson": 0.46073442412479715,
      "eval_runtime": 38.5731,
      "eval_samples_per_second": 1429.287,
      "eval_spearmanr": 0.4833629948398833,
      "eval_steps_per_second": 11.925,
      "step": 12500
    },
    {
      "epoch": 0.9968228752978554,
      "grad_norm": 11.57766342163086,
      "learning_rate": 2e-05,
      "loss": 0.4734,
      "step": 12550
    }
  ],
  "logging_steps": 50,
  "max_steps": 12590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.509104756627866e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
