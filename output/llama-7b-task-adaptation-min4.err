(GNER) ~/proj/GNER git:[main]
bash setup-env.sh

Looking for: ['python=3.11']

warning  libmamba Cache file "/home/chrisjihee/miniforge3/pkgs/cache/497deca9.json" was modified by another program
warning  libmamba Cache file "/home/chrisjihee/miniforge3/pkgs/cache/09cdf8bf.json" was modified by another program
conda-forge/noarch                                  17.2MB @  34.7MB/s  0.6s
conda-forge/linux-64                                39.6MB @  38.5MB/s  3.1s
Transaction

  Prefix: /home/chrisjihee/miniforge3/envs/GNER

  Updating specs:

   - python=3.11


  Package               Version  Build               Channel           Size
─────────────────────────────────────────────────────────────────────────────
  Install:
─────────────────────────────────────────────────────────────────────────────

  + _libgcc_mutex           0.1  conda_forge         conda-forge     Cached
  + ld_impl_linux-64       2.43  h712a8e2_2          conda-forge     Cached
  + ca-certificates   2024.8.30  hbcca054_0          conda-forge     Cached
  + libgomp              14.2.0  h77fa898_1          conda-forge     Cached
  + _openmp_mutex           4.5  2_gnu               conda-forge     Cached
  + libgcc               14.2.0  h77fa898_1          conda-forge     Cached
  + libzlib               1.3.1  hb9d3cd8_2          conda-forge     Cached
  + libexpat              2.6.4  h5888daf_0          conda-forge     Cached
  + libgcc-ng            14.2.0  h69a702a_1          conda-forge     Cached
  + openssl               3.3.2  hb9d3cd8_0          conda-forge     Cached
  + libsqlite            3.47.0  hadc24fc_1          conda-forge     Cached
  + tk                   8.6.13  noxft_h4845f30_101  conda-forge     Cached
  + libxcrypt            4.4.36  hd590300_1          conda-forge     Cached
  + libffi                3.4.2  h7f98852_5          conda-forge     Cached
  + bzip2                 1.0.8  h4bc722e_7          conda-forge     Cached
  + ncurses                 6.5  he02047a_1          conda-forge     Cached
  + libuuid              2.38.1  h0b41bf4_0          conda-forge     Cached
  + libnsl                2.0.1  hd590300_0          conda-forge     Cached
  + xz                    5.2.6  h166bdaf_0          conda-forge     Cached
  + readline                8.2  h8228510_1          conda-forge     Cached
  + tzdata                2024b  hc8b5060_0          conda-forge     Cached
  + python              3.11.10  hc5c86c4_3_cpython  conda-forge     Cached
  + wheel                0.44.0  pyhd8ed1ab_0        conda-forge     Cached
  + setuptools           75.3.0  pyhd8ed1ab_0        conda-forge     Cached
  + pip                  24.3.1  pyh8b19718_0        conda-forge     Cached

  Summary:

  Install: 25 packages

  Total download: 0 B

─────────────────────────────────────────────────────────────────────────────



Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done

To activate this environment, use

     $ mamba activate GNER

To deactivate an active environment, use

     $ mamba deactivate

Run 'mamba init' to be able to run mamba activate/deactivate
and start a new shell session. Or use conda to activate/deactivate.


Looking for: ['cuda-libraries-dev=11.8', 'cuda-libraries=11.8', 'cuda-nvcc=11.8', 'cuda-nvrtc=11.8', 'cuda-nvrtc-dev=11.8', 'cuda-runtime=11.8', 'cuda-cccl=11.8']

warning  libmamba Cache file "/home/chrisjihee/miniforge3/pkgs/cache/c9ddbd6b.json" was modified by another program
nvidia/linux-64 (check zst)                        Checked  0.1s
warning  libmamba Cache file "/home/chrisjihee/miniforge3/pkgs/cache/b121c3e7.json" was modified by another program
nvidia/noarch (check zst)                           Checked  0.0s
warning  libmamba Could not parse mod/etag header
pytorch/linux-64 (check zst)                        Checked  0.0s
warning  libmamba Could not parse mod/etag header
pytorch/noarch (check zst)                          Checked  0.0s
conda-forge/linux-64                                        Using cache
conda-forge/noarch                                          Using cache
nvidia/linux-64                                    193.3kB @   5.3MB/s  0.1s
pytorch/noarch                                       9.9kB @ 180.0kB/s  0.1s
pytorch/linux-64                                   194.9kB @   3.0MB/s  0.1s
nvidia/noarch                                       17.1kB @ 240.2kB/s  0.1s

Pinned packages:
  - python 3.11.*


Transaction

  Prefix: /home/chrisjihee/miniforge3/envs/GNER

  Updating specs:

   - cuda-libraries-dev=11.8
   - cuda-libraries=11.8
   - cuda-nvcc=11.8
   - cuda-nvrtc=11.8
   - cuda-nvrtc-dev=11.8
   - cuda-runtime=11.8
   - cuda-cccl=11.8
   - ca-certificates
   - openssl


  Package                          Version  Build       Channel           Size
────────────────────────────────────────────────────────────────────────────────
  Install:
────────────────────────────────────────────────────────────────────────────────

  + cuda-nvcc                      11.8.89  0           nvidia          Cached
  + cuda-nvrtc                     11.8.89  0           nvidia          Cached
  + cuda-cccl                      11.8.89  0           nvidia          Cached
  + cuda-nvrtc-dev                 11.8.89  0           nvidia          Cached
  + cuda-version                      12.6  3           nvidia          Cached
  + cuda-driver-dev_linux-64       12.6.77  0           nvidia          Cached
  + cuda-cccl_linux-64             12.6.77  0           nvidia          Cached
  + cuda-cudart-static_linux-64    12.6.77  0           nvidia          Cached
  + cuda-cudart_linux-64           12.6.77  0           nvidia          Cached
  + cuda-cudart-dev_linux-64       12.6.77  0           nvidia          Cached
  + libstdcxx                       14.2.0  hc0a3c3a_1  conda-forge     Cached
  + libstdcxx-ng                    14.2.0  h4852527_1  conda-forge     Cached
  + libnvjitlink                   12.6.77  0           nvidia          Cached
  + libnvjpeg                    12.3.3.54  0           nvidia          Cached
  + libnpp                       12.3.1.54  0           nvidia          Cached
  + libcurand                    10.3.7.77  0           nvidia          Cached
  + libcufile                     1.11.1.6  0           nvidia          Cached
  + libcufft                      11.3.0.4  0           nvidia          Cached
  + libcublas                     12.6.3.3  0           nvidia          Cached
  + cuda-driver-dev                12.6.77  0           nvidia          Cached
  + cuda-cudart-static             12.6.77  0           nvidia          Cached
  + cuda-cudart                    12.6.77  0           nvidia          Cached
  + libcusparse                   12.5.4.2  0           nvidia          Cached
  + libnpp-dev                   12.3.1.54  0           nvidia          Cached
  + libcurand-dev                10.3.7.77  0           nvidia          Cached
  + libcufile-dev                 1.11.1.6  0           nvidia          Cached
  + libcufft-dev                  11.3.0.4  0           nvidia          Cached
  + libcublas-dev                 12.6.3.3  0           nvidia          Cached
  + cuda-cudart-dev                12.6.77  0           nvidia          Cached
  + libcusparse-dev               12.5.4.2  0           nvidia          Cached
  + libcusolver                   11.7.1.2  0           nvidia          Cached
  + libnvjpeg-dev                12.3.3.54  0           nvidia          Cached
  + cuda-profiler-api              12.6.77  0           nvidia          Cached
  + libcusolver-dev               11.7.1.2  0           nvidia          Cached
  + cuda-libraries                  11.8.0  0           nvidia          Cached
  + cuda-libraries-dev              11.8.0  0           nvidia          Cached
  + cuda-runtime                    11.8.0  0           nvidia          Cached

  Summary:

  Install: 37 packages

  Total download: 0 B

────────────────────────────────────────────────────────────────────────────────



Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118
Collecting nltk (from -r requirements.txt (line 2))
  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting numpy<2.0.0 (from -r requirements.txt (line 3))
  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting datasets (from -r requirements.txt (line 4))
  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)
Collecting protobuf (from -r requirements.txt (line 5))
  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)
Collecting sentencepiece (from -r requirements.txt (line 6))
  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting torch<2.3.0 (from -r requirements.txt (line 10))
  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (819.2 MB)
Collecting deepspeed==0.13.1 (from -r requirements.txt (line 11))
  Using cached deepspeed-0.13.1-py3-none-any.whl
Collecting accelerate<1.0.0 (from -r requirements.txt (line 12))
  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)
Collecting transformers<4.39.0 (from -r requirements.txt (line 13))
  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)
Collecting hjson (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)
Collecting ninja (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)
Collecting packaging>=20.0 (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached https://download.pytorch.org/whl/packaging-24.1-py3-none-any.whl (53 kB)
Collecting psutil (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting py-cpuinfo (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)
Collecting pydantic (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)
Collecting pynvml (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)
Collecting tqdm (from deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)
Collecting click (from nltk->-r requirements.txt (line 2))
  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
Collecting joblib (from nltk->-r requirements.txt (line 2))
  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)
Collecting regex>=2021.8.3 (from nltk->-r requirements.txt (line 2))
  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Collecting filelock (from datasets->-r requirements.txt (line 4))
  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)
Collecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 4))
  Using cached pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 4))
  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)
Collecting pandas (from datasets->-r requirements.txt (line 4))
  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
Collecting requests>=2.32.2 (from datasets->-r requirements.txt (line 4))
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting xxhash (from datasets->-r requirements.txt (line 4))
  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 4))
  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)
Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 4))
  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)
Collecting aiohttp (from datasets->-r requirements.txt (line 4))
  Using cached aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)
Collecting huggingface-hub>=0.23.0 (from datasets->-r requirements.txt (line 4))
  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)
Collecting pyyaml>=5.1 (from datasets->-r requirements.txt (line 4))
  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.8.0 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting sympy (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)
Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu11==8.7.0.84 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)
Collecting nvidia-cublas-cu11==11.11.3.6 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu11==10.9.0.58 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu11==10.3.0.86 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-nccl-cu11==2.19.3 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)
Collecting nvidia-nvtx-cu11==11.8.86 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)
Collecting triton==2.2.0 (from torch<2.3.0->-r requirements.txt (line 10))
  Using cached https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)
Collecting safetensors>=0.4.3 (from accelerate<1.0.0->-r requirements.txt (line 12))
  Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting tokenizers<0.19,>=0.14 (from transformers<4.39.0->-r requirements.txt (line 13))
  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)
Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)
Collecting attrs>=17.3.0 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)
Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)
Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets->-r requirements.txt (line 4))
  Using cached yarl-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)
Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets->-r requirements.txt (line 4))
  Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)
Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets->-r requirements.txt (line 4))
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets->-r requirements.txt (line 4))
  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets->-r requirements.txt (line 4))
  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch<2.3.0->-r requirements.txt (line 10))
  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting python-dateutil>=2.8.2 (from pandas->datasets->-r requirements.txt (line 4))
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas->datasets->-r requirements.txt (line 4))
  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->datasets->-r requirements.txt (line 4))
  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting annotated-types>=0.6.0 (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.23.4 (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 11))
  Using cached pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch<2.3.0->-r requirements.txt (line 10))
  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 4))
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r requirements.txt (line 4))
  Using cached propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)
Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)
Using cached datasets-3.1.0-py3-none-any.whl (480 kB)
Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)
Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)
Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl (13.1 MB)
Using cached nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (23.2 MB)
Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)
Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)
Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl (58.1 MB)
Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl (128.2 MB)
Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl (204.1 MB)
Using cached nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl (99 kB)
Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)
Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)
Using cached dill-0.3.8-py3-none-any.whl (116 kB)
Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)
Using cached aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)
Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)
Using cached pyarrow-18.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.0 MB)
Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)
Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)
Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)
Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Using cached click-8.1.7-py3-none-any.whl (97 kB)
Using cached filelock-3.16.1-py3-none-any.whl (16 kB)
Using cached hjson-3.1.0-py3-none-any.whl (54 kB)
Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)
Using cached joblib-1.4.2-py3-none-any.whl (301 kB)
Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)
Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)
Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
Using cached psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)
Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)
Using cached pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
Using cached pynvml-11.5.3-py3-none-any.whl (53 kB)
Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)
Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)
Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached attrs-24.2.0-py3-none-any.whl (63 kB)
Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)
Using cached charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)
Using cached frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)
Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)
Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)
Using cached yarl-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)
Using cached propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)
Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Installing collected packages: sentencepiece, pytz, py-cpuinfo, ninja, mpmath, hjson, xxhash, urllib3, tzdata, typing-extensions, tqdm, sympy, six, safetensors, regex, pyyaml, pynvml, pyarrow, psutil, protobuf, propcache, packaging, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, multidict, MarkupSafe, joblib, idna, fsspec, frozenlist, filelock, dill, click, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, triton, requests, python-dateutil, pydantic-core, nvidia-cusolver-cu11, nvidia-cudnn-cu11, nltk, multiprocess, jinja2, aiosignal, torch, pydantic, pandas, huggingface-hub, aiohttp, tokenizers, deepspeed, accelerate, transformers, datasets
Successfully installed MarkupSafe-3.0.2 accelerate-0.34.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 datasets-3.1.0 deepspeed-0.13.1 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 hjson-3.1.0 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 ninja-1.11.1.1 nltk-3.9.1 numpy-1.26.4 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 packaging-24.1 pandas-2.2.3 propcache-0.2.0 protobuf-5.28.3 psutil-6.1.0 py-cpuinfo-9.0.0 pyarrow-18.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pynvml-11.5.3 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 six-1.16.0 sympy-1.13.3 tokenizers-0.15.2 torch-2.2.2+cu118 tqdm-4.67.0 transformers-4.38.2 triton-2.2.0 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.17.1
++ shuf -i25000-30000 -n1
+ port=25404
+ MODEL_NAME_OR_PATH=yahma/llama-7b-hf
+ DATA_DIR=data
+ TRAIN_JSON_DIR=data/pile-ner.json
+ DATA_CONFIG_DIR=configs/dataset_configs/task_adaptation_configs
+ INSTRUCTION_FILE=configs/instruction_configs/instruction.json
+ OUTPUT_DIR=output/llama-7b-task-adaptation
+ DEEPSPEED_CONFIG=configs/deepspeed_configs/deepspeed_zero2_llama.json
+ RUN_NAME=llama-7B-experiment
+ deepspeed --include=localhost:0,1,2,3 --master_port 25404 src/run.py --bf16 True --tf32 True --do_train --do_predict --predict_with_generate --model_name_or_path yahma/llama-7b-hf --data_dir data --preprocessing_num_workers 24 --metric_for_best_model eval_average_f1 --greater_is_better True --train_json_dir data/pile-ner.json --data_config_dir configs/dataset_configs/task_adaptation_configs --instruction_file configs/instruction_configs/instruction.json --output_dir output/llama-7b-task-adaptation --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 64 --gradient_checkpointing True --learning_rate 2e-05 --weight_decay 0. --warmup_ratio 0.04 --num_train_epochs 0.1 --lr_scheduler_type cosine --deepspeed configs/deepspeed_configs/deepspeed_zero2_llama.json --run_name llama-7B-experiment --max_source_length 640 --max_target_length 640 --generation_max_length 1280 --overwrite_output_dir --overwrite_cache --logging_strategy steps --logging_steps 5 --save_strategy steps --save_steps 10 --seed 1234
[2024-11-08 08:53:41,914] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:44,197] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-08 08:53:44,225] [INFO] [runner.py:568:main] cmd = /home/chrisjihee/miniforge3/envs/GNER/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=25404 --enable_each_rank_log=None src/run.py --bf16 True --tf32 True --do_train --do_predict --predict_with_generate --model_name_or_path yahma/llama-7b-hf --data_dir data --preprocessing_num_workers 24 --metric_for_best_model eval_average_f1 --greater_is_better True --train_json_dir data/pile-ner.json --data_config_dir configs/dataset_configs/task_adaptation_configs --instruction_file configs/instruction_configs/instruction.json --output_dir output/llama-7b-task-adaptation --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 64 --gradient_checkpointing True --learning_rate 2e-05 --weight_decay 0. --warmup_ratio 0.04 --num_train_epochs 0.1 --lr_scheduler_type cosine --deepspeed configs/deepspeed_configs/deepspeed_zero2_llama.json --run_name llama-7B-experiment --max_source_length 640 --max_target_length 640 --generation_max_length 1280 --overwrite_output_dir --overwrite_cache --logging_strategy steps --logging_steps 5 --save_strategy steps --save_steps 10 --seed 1234
[2024-11-08 08:53:46,167] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:46,812] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-11-08 08:53:46,812] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-11-08 08:53:46,812] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-11-08 08:53:46,812] [INFO] [launch.py:163:main] dist_world_size=4
[2024-11-08 08:53:46,812] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-11-08 08:53:51,543] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:51,576] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:51,588] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:51,590] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-08 08:53:51,726] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-08 08:53:51,757] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-08 08:53:51,772] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-08 08:53:51,773] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-08 08:53:51,773] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/08/2024 08:53:51 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:329] 2024-11-08 08:53:52,298 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/08/2024 08:53:53 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
11/08/2024 08:53:53 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
11/08/2024 08:53:53 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
11/08/2024 08:53:53 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed_configs/deepspeed_zero2_llama.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=1280,
generation_num_beams=None,
gradient_accumulation_steps=64,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/llama-7b-task-adaptation/runs/Nov08_08-53-49_ptlm3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_average_f1,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=0.1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=output/llama-7b-task-adaptation,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=llama-7B-experiment,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=10,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=1234,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.04,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-e60fd0a2766fdda3
11/08/2024 08:53:53 - INFO - datasets.builder - Using custom data configuration default-e60fd0a2766fdda3
Loading Dataset Infos from /home/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/08/2024 08:53:53 - INFO - datasets.info - Loading Dataset Infos from /home/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Overwrite dataset info from restored data version if exists.
11/08/2024 08:53:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/08/2024 08:53:53 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Found cached dataset gner_dataset (/home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89)
11/08/2024 08:53:53 - INFO - datasets.builder - Found cached dataset gner_dataset (/home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89)
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/08/2024 08:53:53 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/08/2024 08:53:53 - INFO - datasets.arrow_dataset - Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/08/2024 08:53:53 - INFO - datasets.arrow_dataset - Listing files in /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|configuration_utils.py:728] 2024-11-08 08:53:53,674 >> loading configuration file config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/config.json
[INFO|configuration_utils.py:791] 2024-11-08 08:53:53,676 >> Model config LlamaConfig {
  "_name_or_path": "yahma/llama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 32000
}

[WARNING|logging.py:329] 2024-11-08 08:53:53,920 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[INFO|tokenization_utils_base.py:2046] 2024-11-08 08:53:54,223 >> loading file tokenizer.model from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/tokenizer.model
[INFO|tokenization_utils_base.py:2046] 2024-11-08 08:53:54,223 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-08 08:53:54,223 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2046] 2024-11-08 08:53:54,223 >> loading file special_tokens_map.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/special_tokens_map.json
[INFO|tokenization_utils_base.py:2046] 2024-11-08 08:53:54,223 >> loading file tokenizer_config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/tokenizer_config.json
[WARNING|logging.py:329] 2024-11-08 08:53:54,223 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:329] 2024-11-08 08:53:54,230 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s][INFO|modeling_utils.py:3257] 2024-11-08 08:53:54,582 >> loading weights file pytorch_model.bin from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/pytorch_model.bin.index.json
[INFO|configuration_utils.py:845] 2024-11-08 08:53:54,584 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.09s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.09s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.31s/it]
[INFO|modeling_utils.py:3992] 2024-11-08 08:54:01,362 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4000] 2024-11-08 08:54:01,362 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at yahma/llama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:800] 2024-11-08 08:54:01,555 >> loading configuration file generation_config.json from cache at /home/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/generation_config.json
[INFO|configuration_utils.py:845] 2024-11-08 08:54:01,555 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.62s/it]
Using custom data configuration default-5a9388237d1fda30
11/08/2024 08:54:02 - INFO - datasets.builder - Using custom data configuration default-5a9388237d1fda30
Loading Dataset Infos from /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/datasets/packaged_modules/json
11/08/2024 08:54:02 - INFO - datasets.info - Loading Dataset Infos from /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
11/08/2024 08:54:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/08/2024 08:54:02 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/08/2024 08:54:02 - INFO - datasets.builder - Found cached dataset json (/home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/08/2024 08:54:02 - INFO - datasets.info - Loading Dataset info from /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/08/2024 08:54:02 - INFO - __main__ - Use data/pile-ner.json as train dataset, len(dataset) = 105659
11/08/2024 08:54:02 - INFO - __main__ - len(dataset) = 105659
Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00000_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00000_of_00024.arrow
Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00001_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00001_of_00024.arrow
Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00002_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00002_of_00024.arrow
Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00003_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00003_of_00024.arrow
Process #4 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00004_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #4 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00004_of_00024.arrow
Process #5 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00005_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #5 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00005_of_00024.arrow
Process #6 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00006_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #6 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00006_of_00024.arrow
Process #7 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00007_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #7 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00007_of_00024.arrow
Process #8 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00008_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #8 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00008_of_00024.arrow
Process #9 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00009_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #9 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00009_of_00024.arrow
Process #10 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00010_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #10 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00010_of_00024.arrow
Process #11 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00011_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #11 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00011_of_00024.arrow
Process #12 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00012_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #12 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00012_of_00024.arrow
Process #13 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00013_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #13 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00013_of_00024.arrow
Process #14 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00014_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #14 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00014_of_00024.arrow
Process #15 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00015_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #15 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00015_of_00024.arrow
Process #16 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00016_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #16 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00016_of_00024.arrow
Process #17 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00017_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #17 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00017_of_00024.arrow
Process #18 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00018_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #18 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00018_of_00024.arrow
Process #19 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00019_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #19 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00019_of_00024.arrow
Process #20 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00020_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #20 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00020_of_00024.arrow
Process #21 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00021_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #21 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00021_of_00024.arrow
Process #22 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00022_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #22 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00022_of_00024.arrow
Process #23 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00023_of_00024.arrow
11/08/2024 08:54:02 - INFO - datasets.arrow_dataset - Process #23 will write at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00023_of_00024.arrow
Spawning 24 processes
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Spawning 24 processes
Running tokenizer on train dataset (num_proc=24):   0%|                                                                                                                                                                                  | 0/105659 [00:00<?, ? examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00000_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00000_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00001_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00001_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   0%|                                                                                                                                                                         | 35/105659 [00:00<26:54, 65.42 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00002_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00002_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00003_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00003_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00004_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00004_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   0%|▎                                                                                                                                                                      | 206/105659 [00:00<04:18, 408.27 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00005_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00005_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00006_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00006_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00007_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00007_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00008_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00008_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00009_of_00024.arrow
11/08/2024 08:54:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00009_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   1%|█                                                                                                                                                                     | 676/105659 [00:00<01:17, 1350.17 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00010_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00010_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00011_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00011_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00012_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00012_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00013_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00013_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00014_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00014_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   1%|██                                                                                                                                                                   | 1325/105659 [00:00<00:41, 2522.33 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00015_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00015_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00016_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00016_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00017_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00017_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   2%|███▌                                                                                                                                                                 | 2275/105659 [00:00<00:24, 4135.75 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00018_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00018_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00019_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00019_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   6%|█████████▎                                                                                                                                                           | 5931/105659 [00:01<00:11, 8725.18 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00020_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00020_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00022_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00022_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00021_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00021_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00023_of_00024.arrow
11/08/2024 08:54:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/json/default-5a9388237d1fda30/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f5a309d71b0f0c37_00023_of_00024.arrow
Running tokenizer on train dataset (num_proc=24): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:14<00:00, 7045.94 examples/s]
Concatenating 24 shards
11/08/2024 08:54:26 - INFO - datasets.arrow_dataset - Concatenating 24 shards
Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00000_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #0 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00000_of_00024.arrow
Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00001_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #1 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00001_of_00024.arrow
Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00002_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #2 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00002_of_00024.arrow
Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00003_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #3 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00003_of_00024.arrow
Process #4 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00004_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #4 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00004_of_00024.arrow
Process #5 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00005_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #5 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00005_of_00024.arrow
Process #6 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00006_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #6 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00006_of_00024.arrow
Process #7 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00007_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #7 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00007_of_00024.arrow
Process #8 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00008_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #8 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00008_of_00024.arrow
Process #9 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00009_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #9 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00009_of_00024.arrow
Process #10 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00010_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #10 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00010_of_00024.arrow
Process #11 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00011_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #11 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00011_of_00024.arrow
Process #12 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00012_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #12 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00012_of_00024.arrow
Process #13 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00013_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #13 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00013_of_00024.arrow
Process #14 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00014_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #14 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00014_of_00024.arrow
Process #15 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00015_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #15 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00015_of_00024.arrow
Process #16 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00016_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #16 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00016_of_00024.arrow
Process #17 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00017_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #17 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00017_of_00024.arrow
Process #18 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00018_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #18 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00018_of_00024.arrow
Process #19 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00019_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #19 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00019_of_00024.arrow
Process #20 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00020_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #20 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00020_of_00024.arrow
Process #21 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00021_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #21 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00021_of_00024.arrow
Process #22 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00022_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #22 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00022_of_00024.arrow
Process #23 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00023_of_00024.arrow
11/08/2024 08:54:28 - INFO - datasets.arrow_dataset - Process #23 will write at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00023_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   0%|                                                                                                                                                                                  | 0/105659 [00:00<?, ? examples/s]Spawning 24 processes
11/08/2024 08:54:36 - INFO - datasets.arrow_dataset - Spawning 24 processes
Running tokenizer on train dataset (num_proc=24):   0%|                                                                                                                                                                         | 34/105659 [00:00<40:04, 43.93 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00000_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00000_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00001_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00001_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24):   2%|██▊                                                                                                                                                                 | 111/6470 [00:00<00:28, 226.04 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00002_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00002_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00003_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00003_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   0%|▎                                                                                                                                                                      | 233/105659 [00:00<05:10, 339.43 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00004_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00004_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24):   7%|██████████▉                                                                                                                                                         | 430/6470 [00:00<00:06, 887.67 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00005_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00005_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00006_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00006_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00007_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00007_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   1%|█▏                                                                                                                                                                    | 748/105659 [00:01<01:30, 1157.99 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00008_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00008_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00009_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00009_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24):  23%|█████████████████████████████████████▋                                                                                                                            | 1504/6470 [00:00<00:01, 3053.74 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00010_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00010_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00011_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00011_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   1%|██                                                                                                                                                                   | 1314/105659 [00:01<00:53, 1959.21 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00012_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00012_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24):  39%|███████████████████████████████████████████████████████████████                                                                                                   | 2517/6470 [00:00<00:00, 4576.60 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00013_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00013_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   2%|███▍                                                                                                                                                                 | 2222/105659 [00:01<00:31, 3316.11 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00014_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00014_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00015_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00015_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   3%|█████                                                                                                                                                                | 3253/105659 [00:01<00:22, 4589.60 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00016_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00016_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00018_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00018_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00017_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00017_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   4%|██████▉                                                                                                                                                              | 4451/105659 [00:01<00:16, 6257.36 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00019_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00019_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00020_of_00024.arrow
11/08/2024 08:54:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00020_of_00024.arrow
Running tokenizer on train dataset (num_proc=24):   5%|████████▌                                                                                                                                                            | 5459/105659 [00:01<00:13, 7157.62 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00022_of_00024.arrow
11/08/2024 08:54:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00022_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24):  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 5398/6470 [00:01<00:00, 5131.08 examples/s]Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00021_of_00024.arrow
11/08/2024 08:54:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00021_of_00024.arrow
Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00023_of_00024.arrow
11/08/2024 08:54:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-3aac3408209c7373_00023_of_00024.arrow
Running tokenizer on prediction dataset (num_proc=24): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:03<00:00, 1732.68 examples/s]
Running tokenizer on train dataset (num_proc=24):  21%|██████████████████████████████████▊                                                                                                                                 | 22397/105659 [00:05<00:30, 2752.83 examples/s]Concatenating 24 shards
11/08/2024 08:54:43 - INFO - datasets.arrow_dataset - Concatenating 24 shards
Running tokenizer on train dataset (num_proc=24): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:37<00:00, 2794.61 examples/s]
Running tokenizer on train dataset (num_proc=24): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:40<00:00, 2602.72 examples/s]
Running tokenizer on train dataset (num_proc=24): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:42<00:00, 2477.37 examples/s]
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:601] 2024-11-08 08:55:22,077 >> Using auto half precision backend
[2024-11-08 08:55:22,183] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown
Running tokenizer on prediction dataset (num_proc=24): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:01<00:00, 3249.43 examples/s]
Running tokenizer on prediction dataset (num_proc=24): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2902.13 examples/s]
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=24):  99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 6377/6470 [00:01<00:00, 4385.12 examples/s]/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=24): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2215.86 examples/s]
/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[2024-11-08 08:55:47,613] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /home/chrisjihee/miniforge3/envs/GNER/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /home/chrisjihee/miniforge3/envs/GNER/include -isystem /home/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o
FAILED: multi_tensor_adam.cuda.o
/home/chrisjihee/miniforge3/envs/GNER/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /home/chrisjihee/miniforge3/envs/GNER/include -isystem /home/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o
<command-line>: fatal error: cuda_runtime.h: No such file or directory
compilation terminated.
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /home/chrisjihee/miniforge3/envs/GNER/include -isystem /home/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2096, in _run_ninja_build
    subprocess.run(
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
    main()
  File "/home/chrisjihee/proj/GNER/src/run.py", line 462, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1779, in _inner_training_loop
    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1318, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1815, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1221, in _configure_optimizer
    basic_optimizer = self._configure_basic_optimizer(model_parameters)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1298, in _configure_basic_optimizer
    optimizer = FusedAdam(
                ^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
    fused_adam_cuda = FusedAdamBuilder().load()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 478, in load
    return self.jit_load(verbose)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 522, in jit_load
    op_module = load(name=self.name,
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1306, in load
    return _jit_compile(
           ^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1710, in _jit_compile
    _write_ninja_file_and_build_library(
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1823, in _write_ninja_file_and_build_library
    _run_ninja_build(
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2112, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error building extension 'fused_adam'
Loading extension module fused_adam...
Traceback (most recent call last):
  File "/home/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
    main()
  File "/home/chrisjihee/proj/GNER/src/run.py", line 462, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1779, in _inner_training_loop
    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1318, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1815, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1221, in _configure_optimizer
    basic_optimizer = self._configure_basic_optimizer(model_parameters)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1298, in _configure_basic_optimizer
    optimizer = FusedAdam(
                ^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
    fused_adam_cuda = FusedAdamBuilder().load()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 478, in load
    return self.jit_load(verbose)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 522, in jit_load
    op_module = load(name=self.name,
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1306, in load
    return _jit_compile(
           ^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1736, in _jit_compile
    return _import_module_from_library(name, build_directory, is_python_module)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2132, in _import_module_from_library
    module = importlib.util.module_from_spec(spec)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 573, in module_from_spec
  File "<frozen importlib._bootstrap_external>", line 1233, in create_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ImportError: /home/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: undefined symbol: _ZN3c104cuda14ExchangeDeviceEa
Loading extension module fused_adam...
Traceback (most recent call last):
  File "/home/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
    main()
  File "/home/chrisjihee/proj/GNER/src/run.py", line 462, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1779, in _inner_training_loop
    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1318, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1815, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1221, in _configure_optimizer
    basic_optimizer = self._configure_basic_optimizer(model_parameters)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1298, in _configure_basic_optimizer
    optimizer = FusedAdam(
                ^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
    fused_adam_cuda = FusedAdamBuilder().load()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 478, in load
    return self.jit_load(verbose)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 522, in jit_load
    op_module = load(name=self.name,
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1306, in load
    return _jit_compile(
           ^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1736, in _jit_compile
Loading extension module fused_adam...
    return _import_module_from_library(name, build_directory, is_python_module)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2132, in _import_module_from_library
    module = importlib.util.module_from_spec(spec)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 573, in module_from_spec
  File "<frozen importlib._bootstrap_external>", line 1233, in create_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ImportError: /home/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: undefined symbol: _ZN3c104cuda14ExchangeDeviceEa
Traceback (most recent call last):
  File "/home/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
    main()
  File "/home/chrisjihee/proj/GNER/src/run.py", line 462, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1779, in _inner_training_loop
    model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1318, in prepare
    result = self._prepare_deepspeed(*args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1815, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 171, in initialize
    engine = DeepSpeedEngine(args=args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 308, in __init__
    self._configure_optimizer(optimizer, model_parameters)
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1221, in _configure_optimizer
    basic_optimizer = self._configure_basic_optimizer(model_parameters)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1298, in _configure_basic_optimizer
    optimizer = FusedAdam(
                ^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
    fused_adam_cuda = FusedAdamBuilder().load()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 478, in load
    return self.jit_load(verbose)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 522, in jit_load
    op_module = load(name=self.name,
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1306, in load
    return _jit_compile(
           ^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1736, in _jit_compile
    return _import_module_from_library(name, build_directory, is_python_module)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2132, in _import_module_from_library
    module = importlib.util.module_from_spec(spec)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 573, in module_from_spec
  File "<frozen importlib._bootstrap_external>", line 1233, in create_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ImportError: /home/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: undefined symbol: _ZN3c104cuda14ExchangeDeviceEa
[2024-11-08 08:56:11,889] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3748858
[2024-11-08 08:56:11,913] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3748859
[2024-11-08 08:56:11,936] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3748860
[2024-11-08 08:56:11,951] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3748861
[2024-11-08 08:56:11,951] [ERROR] [launch.py:321:sigkill_handler] ['/home/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=3', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '24', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '64', '--gradient_checkpointing', 'True', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '0.1', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero2_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--save_steps', '10', '--seed', '1234'] exits with return code = 1
