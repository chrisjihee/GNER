(GNER) ~/proj/GNER git:[main]
bash scripts/train_llama_task_adaptation-no_eval.sh
++ shuf -i25000-30000 -n1
+ port=29887
+ MODEL_NAME_OR_PATH=yahma/llama-7b-hf
+ DATA_DIR=data
+ TRAIN_JSON_DIR=data/pile-ner.json
+ DATA_CONFIG_DIR=configs/dataset_configs/task_adaptation_configs
+ INSTRUCTION_FILE=configs/instruction_configs/instruction.json
+ OUTPUT_DIR=output/llama-7b-task-adaptation
+ DEEPSPEED_CONFIG=configs/deepspeed_configs/deepspeed_zero1_llama.json
+ RUN_NAME=llama-7B-experiment
+ deepspeed --include=localhost:0,1,2,3,4,5,6,7 --master_port 29887 src/run.py --bf16 True --tf32 True --do_train --do_predict --predict_with_generate --model_name_or_path yahma/llama-7b-hf --data_dir data --preprocessing_num_workers 4 --metric_for_best_model eval_average_f1 --greater_is_better True --train_json_dir data/pile-ner.json --data_config_dir configs/dataset_configs/task_adaptation_configs --instruction_file configs/instruction_configs/instruction.json --output_dir output/llama-7b-task-adaptation --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --learning_rate 2e-05 --weight_decay 0. --warmup_ratio 0.04 --num_train_epochs 3 --lr_scheduler_type cosine --deepspeed configs/deepspeed_configs/deepspeed_zero1_llama.json --run_name llama-7B-experiment --max_source_length 640 --max_target_length 640 --generation_max_length 1280 --overwrite_output_dir --overwrite_cache --logging_strategy steps --logging_steps 10 --save_strategy steps --save_steps 100 --seed 1234
[2024-11-07 16:13:02,612] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:06,083] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-07 16:13:06,084] [INFO] [runner.py:568:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29887 --enable_each_rank_log=None src/run.py --bf16 True --tf32 True --do_train --do_predict --predict_with_generate --model_name_or_path yahma/llama-7b-hf --data_dir data --preprocessing_num_workers 4 --metric_for_best_model eval_average_f1 --greater_is_better True --train_json_dir data/pile-ner.json --data_config_dir configs/dataset_configs/task_adaptation_configs --instruction_file configs/instruction_configs/instruction.json --output_dir output/llama-7b-task-adaptation --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 32 --learning_rate 2e-05 --weight_decay 0. --warmup_ratio 0.04 --num_train_epochs 3 --lr_scheduler_type cosine --deepspeed configs/deepspeed_configs/deepspeed_zero1_llama.json --run_name llama-7B-experiment --max_source_length 640 --max_target_length 640 --generation_max_length 1280 --overwrite_output_dir --overwrite_cache --logging_strategy steps --logging_steps 10 --save_strategy steps --save_steps 100 --seed 1234
[2024-11-07 16:13:10,622] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:14,194] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-07 16:13:14,194] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-07 16:13:14,194] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-07 16:13:14,194] [INFO] [launch.py:164:main] dist_world_size=8
[2024-11-07 16:13:14,194] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-07 16:13:14,195] [INFO] [launch.py:256:main] process 2153172 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=0', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,195] [INFO] [launch.py:256:main] process 2153173 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=1', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,196] [INFO] [launch.py:256:main] process 2153175 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=2', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,196] [INFO] [launch.py:256:main] process 2153176 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=3', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,197] [INFO] [launch.py:256:main] process 2153177 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=4', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,197] [INFO] [launch.py:256:main] process 2153178 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=5', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,198] [INFO] [launch.py:256:main] process 2153179 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=6', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:14,198] [INFO] [launch.py:256:main] process 2153180 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=7', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234']
[2024-11-07 16:13:19,898] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:21,286] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-07 16:13:21,286] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-07 16:13:23,070] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
[2024-11-07 16:13:23,154] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-11-07 16:13:23,582] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-07 16:13:23,615] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-07 16:13:23,620] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-07 16:13:23,626] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-07 16:13:23,644] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  async_io: please install the libaio-dev package with apt [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:23,841] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-07 16:13:23,897] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:23 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:24 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:24,297] [INFO] [comm.py:637:init_distributed] cdb=None
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:24,357] [INFO] [comm.py:637:init_distributed] cdb=None
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:24,434] [INFO] [comm.py:637:init_distributed] cdb=None
[WARNING|logging.py:329] 2024-11-07 16:13:24,454 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:329] 2024-11-07 16:13:24,468 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-11-07 16:13:24,491] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-07 16:13:24,589] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.17s/it]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
11/07/2024 16:13:27 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed_configs/deepspeed_zero1_llama.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=True,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=1280,
generation_num_beams=None,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/llama-7b-task-adaptation/runs/Nov07_16-13-18_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=eval_average_f1,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=output/llama-7b-task-adaptation,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=llama-7B-experiment,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=1234,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=False,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.04,
warmup_steps=0,
weight_decay=0.0,
)
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True
Using custom data configuration default-e60fd0a2766fdda3
11/07/2024 16:13:27 - INFO - datasets.builder - Using custom data configuration default-e60fd0a2766fdda3
Loading Dataset Infos from /raid/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/07/2024 16:13:27 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/.cache/huggingface/modules/datasets_modules/datasets/gner_dataset/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Overwrite dataset info from restored data version if exists.
11/07/2024 16:13:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/07/2024 16:13:27 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
11/07/2024 16:13:27 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Found cached dataset gner_dataset (/raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89)
11/07/2024 16:13:27 - INFO - datasets.builder - Found cached dataset gner_dataset (/raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89)
Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/07/2024 16:13:27 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/07/2024 16:13:27 - INFO - datasets.arrow_dataset - Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
11/07/2024 16:13:27 - INFO - datasets.arrow_dataset - Listing files in /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-11-07 16:13:27,731 >> loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/config.json
[INFO|configuration_utils.py:792] 2024-11-07 16:13:27,732 >> Model config LlamaConfig {
  "_name_or_path": "yahma/llama-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 32000
}

[WARNING|logging.py:329] 2024-11-07 16:13:27,877 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:329] 2024-11-07 16:13:27,885 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Generating train split: 0 examples [00:00, ? examples/s][WARNING|logging.py:329] 2024-11-07 16:13:27,921 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[INFO|tokenization_utils_base.py:2027] 2024-11-07 16:13:27,949 >> loading file tokenizer.model from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/tokenizer.model
[INFO|tokenization_utils_base.py:2027] 2024-11-07 16:13:27,949 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:2027] 2024-11-07 16:13:27,949 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2027] 2024-11-07 16:13:27,949 >> loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-11-07 16:13:27,949 >> loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/tokenizer_config.json
[WARNING|logging.py:329] 2024-11-07 16:13:27,949 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:329] 2024-11-07 16:13:28,002 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[INFO|modeling_utils.py:3476] 2024-11-07 16:13:28,201 >> loading weights file pytorch_model.bin from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/pytorch_model.bin.index.json
[INFO|configuration_utils.py:826] 2024-11-07 16:13:28,202 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

[WARNING|logging.py:329] 2024-11-07 16:13:29,378 >> You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Generating train split: 105659 examples [00:02, 45496.28 examples/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.05s/it]
[INFO|modeling_utils.py:4350] 2024-11-07 16:13:43,454 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4358] 2024-11-07 16:13:43,454 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at yahma/llama-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-11-07 16:13:43,648 >> loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--yahma--llama-7b-hf/snapshots/cf33055e5df9cc533abd7ea4707bf727ca2ada75/generation_config.json
[INFO|configuration_utils.py:826] 2024-11-07 16:13:43,649 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.24s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.47s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.11s/it]
Using custom data configuration default-7c1e414cce45c188
11/07/2024 16:13:44 - INFO - datasets.builder - Using custom data configuration default-7c1e414cce45c188
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/datasets/packaged_modules/json
11/07/2024 16:13:44 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
11/07/2024 16:13:44 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
11/07/2024 16:13:44 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
Found cached dataset json (/raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
11/07/2024 16:13:44 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
11/07/2024 16:13:44 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7
11/07/2024 16:13:44 - INFO - __main__ - Use data/pile-ner.json as train dataset, len(dataset) = 105659
11/07/2024 16:13:44 - INFO - __main__ - len(dataset) = 105659
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.39s/it]
Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00000_of_00004.arrow
11/07/2024 16:13:44 - INFO - datasets.arrow_dataset - Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00000_of_00004.arrow
Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00001_of_00004.arrow
11/07/2024 16:13:44 - INFO - datasets.arrow_dataset - Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00001_of_00004.arrow
Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00002_of_00004.arrow
11/07/2024 16:13:44 - INFO - datasets.arrow_dataset - Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00002_of_00004.arrow
Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00003_of_00004.arrow
11/07/2024 16:13:44 - INFO - datasets.arrow_dataset - Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00003_of_00004.arrow
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.34s/it]
Spawning 4 processes
11/07/2024 16:13:45 - INFO - datasets.arrow_dataset - Spawning 4 processes
Running tokenizer on train dataset (num_proc=4):   0%|                                                                                                                                                                                   | 0/105659 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00000_of_00004.arrow
11/07/2024 16:13:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00000_of_00004.arrow
Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00001_of_00004.arrow
11/07/2024 16:13:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00001_of_00004.arrow
Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00002_of_00004.arrow
11/07/2024 16:13:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00002_of_00004.arrow
Running tokenizer on train dataset (num_proc=4):   0%|                                                                                                                                                                          | 35/105659 [00:00<20:19, 86.64 examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00003_of_00004.arrow
11/07/2024 16:13:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/json/default-7c1e414cce45c188/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-3b7b2da630ce940b_00003_of_00004.arrow
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:45<00:00, 2316.31 examples/s]
Concatenating 4 shards
11/07/2024 16:14:31 - INFO - datasets.arrow_dataset - Concatenating 4 shards
Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00000_of_00004.arrow
11/07/2024 16:14:38 - INFO - datasets.arrow_dataset - Process #0 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00000_of_00004.arrow
Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00001_of_00004.arrow
11/07/2024 16:14:38 - INFO - datasets.arrow_dataset - Process #1 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00001_of_00004.arrow
Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00002_of_00004.arrow
11/07/2024 16:14:38 - INFO - datasets.arrow_dataset - Process #2 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00002_of_00004.arrow
Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00003_of_00004.arrow
11/07/2024 16:14:38 - INFO - datasets.arrow_dataset - Process #3 will write at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00003_of_00004.arrow
Running tokenizer on train dataset (num_proc=4):   0%|                                                                                                                                                                                   | 0/105659 [00:00<?, ? examples/s]Spawning 4 processes
11/07/2024 16:14:40 - INFO - datasets.arrow_dataset - Spawning 4 processes
Running tokenizer on train dataset (num_proc=4):   0%|                                                                                                                                                                          | 35/105659 [00:00<22:14, 79.14 examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00000_of_00004.arrow
11/07/2024 16:14:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00000_of_00004.arrow
Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00001_of_00004.arrow
11/07/2024 16:14:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00001_of_00004.arrow
Running tokenizer on prediction dataset (num_proc=4):   2%|██▉                                                                                                                                                                  | 115/6470 [00:00<00:24, 255.52 examples/s]Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00002_of_00004.arrow
11/07/2024 16:14:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00002_of_00004.arrow
Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00003_of_00004.arrow
11/07/2024 16:14:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/.cache/huggingface/datasets/gner_dataset/default-e60fd0a2766fdda3/0.0.0/0b99e6e3a59ef6dcb9af89de1cd95359e538dad5813fd3c6b8ef999024427d89/cache-8538fc4547d3f27a_00003_of_00004.arrow
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2795.09 examples/s]
Running tokenizer on train dataset (num_proc=4):   5%|████████▊                                                                                                                                                             | 5575/105659 [00:02<00:40, 2446.53 examples/s]Concatenating 4 shards
11/07/2024 16:14:42 - INFO - datasets.arrow_dataset - Concatenating 4 shards
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:45<00:00, 2301.34 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:46<00:00, 2290.00 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:46<00:00, 2290.03 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:46<00:00, 2288.37 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:46<00:00, 2275.31 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:47<00:00, 2227.84 examples/s]
Running tokenizer on train dataset (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105659/105659 [00:48<00:00, 2167.31 examples/s]
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:571] 2024-11-07 16:15:30,925 >> Using auto half precision backend
[2024-11-07 16:15:31,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2991.15 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2990.30 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 3017.81 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2816.30 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:02<00:00, 2755.77 examples/s]
Running tokenizer on prediction dataset (num_proc=4):  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 4548/6470 [00:01<00:00, 5083.40 examples/s]/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=4):  59%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 3829/6470 [00:01<00:00, 4515.69 examples/s]/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=4):  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 5890/6470 [00:01<00:00, 5050.61 examples/s]/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=4):  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 5799/6470 [00:01<00:00, 5419.56 examples/s]/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:03<00:00, 1853.11 examples/s]
Running tokenizer on prediction dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6470/6470 [00:03<00:00, 1930.15 examples/s]
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[2024-11-07 16:16:35,092] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /raid/chrisjihee/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...
/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/3] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /raid/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o
FAILED: multi_tensor_adam.cuda.o
/usr/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /raid/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o
/usr/include/c++/11/bits/std_function.h:435:145: error: parameter packs not expanded with ‘...’:
  435 |         function(_Functor&& __f)
      |                                                                                                                                                 ^
/usr/include/c++/11/bits/std_function.h:435:145: note:         ‘_ArgTypes’
/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:
  530 |         operator=(_Functor&& __f)
      |                                                                                                                                                  ^
/usr/include/c++/11/bits/std_function.h:530:146: note:         ‘_ArgTypes’
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/TH -isystem /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/include/THC -isystem /raid/chrisjihee/miniforge3/envs/GNER/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o
ninja: build stopped: subcommand failed.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2107, in _run_ninja_build
[rank6]:     subprocess.run(
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/subprocess.py", line 571, in run
[rank6]:     raise CalledProcessError(retcode, process.args,
[rank6]: subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

[rank6]: The above exception was the direct cause of the following exception:

[rank6]: Traceback (most recent call last):
[rank6]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank6]:     main()
[rank6]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank6]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank6]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank6]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank6]:     result = self._prepare_deepspeed(*args)
[rank6]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank6]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank6]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank6]:     engine = DeepSpeedEngine(args=args,
[rank6]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank6]:     self._configure_optimizer(optimizer, model_parameters)
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank6]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank6]:     optimizer = FusedAdam(
[rank6]:                 ^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank6]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank6]:     return self.jit_load(verbose)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank6]:     op_module = load(name=self.name,
[rank6]:                 ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank6]:     return _jit_compile(
[rank6]:            ^^^^^^^^^^^^^
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1719, in _jit_compile
[rank6]:     _write_ninja_file_and_build_library(
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1832, in _write_ninja_file_and_build_library
[rank6]:     _run_ninja_build(
[rank6]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2123, in _run_ninja_build
[rank6]:     raise RuntimeError(message) from e
[rank6]: RuntimeError: Error building extension 'fused_adam'
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...

Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank0]:     main()
[rank0]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank0]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank0]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank0]:     result = self._prepare_deepspeed(*args)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank0]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank0]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank0]:     self._configure_optimizer(optimizer, model_parameters)
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank0]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank0]:     optimizer = FusedAdam(
[rank0]:                 ^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank0]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank0]:     return self.jit_load(verbose)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank0]:     op_module = load(name=self.name,
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank0]:     return _jit_compile(
[rank0]:            ^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank0]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank0]:     module = importlib.util.module_from_spec(spec)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank0]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank0]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank0]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank2]: Traceback (most recent call last):
[rank2]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank2]:     main()
[rank2]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank2]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank2]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank2]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank2]:     result = self._prepare_deepspeed(*args)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank2]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank2]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank2]:     engine = DeepSpeedEngine(args=args,
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank2]:     self._configure_optimizer(optimizer, model_parameters)
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank2]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank2]:     optimizer = FusedAdam(
[rank2]:                 ^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank2]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank2]:     return self.jit_load(verbose)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank2]:     op_module = load(name=self.name,
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank2]:     return _jit_compile(
[rank2]:            ^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank2]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank2]:     module = importlib.util.module_from_spec(spec)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank2]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank2]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank2]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank1]: Traceback (most recent call last):
[rank1]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank1]:     main()
[rank1]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank1]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank1]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank1]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank1]:     result = self._prepare_deepspeed(*args)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank1]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank1]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank1]:     engine = DeepSpeedEngine(args=args,
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank1]:     self._configure_optimizer(optimizer, model_parameters)
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank1]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank1]:     optimizer = FusedAdam(
[rank1]:                 ^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank1]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank1]:     return self.jit_load(verbose)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank1]:     op_module = load(name=self.name,
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank1]:     return _jit_compile(
[rank1]:            ^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank1]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank1]:     module = importlib.util.module_from_spec(spec)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank1]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank1]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank1]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank4]: Traceback (most recent call last):
[rank4]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank4]:     main()
[rank4]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank4]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank4]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank4]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank4]:     result = self._prepare_deepspeed(*args)
[rank4]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank4]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank4]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank4]:     engine = DeepSpeedEngine(args=args,
[rank4]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank4]:     self._configure_optimizer(optimizer, model_parameters)
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank4]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank4]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank4]:     optimizer = FusedAdam(
[rank4]:                 ^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank4]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank4]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank4]:     return self.jit_load(verbose)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank4]:     op_module = load(name=self.name,
[rank4]:                 ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank4]:     return _jit_compile(
[rank4]:            ^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank4]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank4]:     module = importlib.util.module_from_spec(spec)
[rank4]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank4]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank4]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank4]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank7]: Traceback (most recent call last):
[rank7]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank7]:     main()
[rank7]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank7]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank7]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank7]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank7]:     result = self._prepare_deepspeed(*args)
[rank7]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank7]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank7]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank7]:     engine = DeepSpeedEngine(args=args,
[rank7]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank7]:     self._configure_optimizer(optimizer, model_parameters)
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank7]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank7]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank7]:     optimizer = FusedAdam(
[rank7]:                 ^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank7]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank7]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank7]:     return self.jit_load(verbose)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank7]:     op_module = load(name=self.name,
[rank7]:                 ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank7]:     return _jit_compile(
[rank7]:            ^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank7]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank7]:     module = importlib.util.module_from_spec(spec)
[rank7]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank7]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank7]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank7]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank3]: Traceback (most recent call last):
[rank3]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank3]:     main()
[rank3]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank3]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank3]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank3]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank3]:     result = self._prepare_deepspeed(*args)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank3]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank3]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank3]:     engine = DeepSpeedEngine(args=args,
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank3]:     self._configure_optimizer(optimizer, model_parameters)
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank3]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank3]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank3]:     optimizer = FusedAdam(
[rank3]:                 ^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank3]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank3]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank3]:     return self.jit_load(verbose)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank3]:     op_module = load(name=self.name,
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank3]:     return _jit_compile(
[rank3]:            ^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank3]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank3]:     module = importlib.util.module_from_spec(spec)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank3]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank3]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank3]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[rank5]: Traceback (most recent call last):
[rank5]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 510, in <module>
[rank5]:     main()
[rank5]:   File "/raid/chrisjihee/proj/GNER/src/run.py", line 462, in main
[rank5]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1539, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/transformers/trainer.py", line 1690, in _inner_training_loop
[rank5]:     model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(
[rank5]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank5]:     result = self._prepare_deepspeed(*args)
[rank5]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank5]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank5]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/__init__.py", line 181, in initialize
[rank5]:     engine = DeepSpeedEngine(args=args,
[rank5]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 307, in __init__
[rank5]:     self._configure_optimizer(optimizer, model_parameters)
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1232, in _configure_optimizer
[rank5]:     basic_optimizer = self._configure_basic_optimizer(model_parameters)
[rank5]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1309, in _configure_basic_optimizer
[rank5]:     optimizer = FusedAdam(
[rank5]:                 ^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py", line 94, in __init__
[rank5]:     fused_adam_cuda = FusedAdamBuilder().load()
[rank5]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 480, in load
[rank5]:     return self.jit_load(verbose)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 524, in jit_load
[rank5]:     op_module = load(name=self.name,
[rank5]:                 ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1309, in load
[rank5]:     return _jit_compile(
[rank5]:            ^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 1745, in _jit_compile
[rank5]:     return _import_module_from_library(name, build_directory, is_python_module)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/raid/chrisjihee/miniforge3/envs/GNER/lib/python3.11/site-packages/torch/utils/cpp_extension.py", line 2143, in _import_module_from_library
[rank5]:     module = importlib.util.module_from_spec(spec)
[rank5]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "<frozen importlib._bootstrap>", line 573, in module_from_spec
[rank5]:   File "<frozen importlib._bootstrap_external>", line 1233, in create_module
[rank5]:   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
[rank5]: ImportError: /raid/chrisjihee/.cache/torch_extensions/py311_cu118/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory
[2024-11-07 16:16:59,238] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153172
[2024-11-07 16:16:59,280] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153173
[2024-11-07 16:16:59,319] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153175
[2024-11-07 16:16:59,319] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153176
[2024-11-07 16:16:59,849] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153177
[2024-11-07 16:16:59,887] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153178
[2024-11-07 16:16:59,924] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153179
[2024-11-07 16:16:59,959] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2153180
[2024-11-07 16:16:59,994] [ERROR] [launch.py:325:sigkill_handler] ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python3.11', '-u', 'src/run.py', '--local_rank=7', '--bf16', 'True', '--tf32', 'True', '--do_train', '--do_predict', '--predict_with_generate', '--model_name_or_path', 'yahma/llama-7b-hf', '--data_dir', 'data', '--preprocessing_num_workers', '4', '--metric_for_best_model', 'eval_average_f1', '--greater_is_better', 'True', '--train_json_dir', 'data/pile-ner.json', '--data_config_dir', 'configs/dataset_configs/task_adaptation_configs', '--instruction_file', 'configs/instruction_configs/instruction.json', '--output_dir', 'output/llama-7b-task-adaptation', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '32', '--learning_rate', '2e-05', '--weight_decay', '0.', '--warmup_ratio', '0.04', '--num_train_epochs', '3', '--lr_scheduler_type', 'cosine', '--deepspeed', 'configs/deepspeed_configs/deepspeed_zero1_llama.json', '--run_name', 'llama-7B-experiment', '--max_source_length', '640', '--max_target_length', '640', '--generation_max_length', '1280', '--overwrite_output_dir', '--overwrite_cache', '--logging_strategy', 'steps', '--logging_steps', '10', '--save_strategy', 'steps', '--save_steps', '100', '--seed', '1234'] exits with return code = 1
